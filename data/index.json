[{"content":"Throughout the years, I\u0026rsquo;ve been part of a few medium- to large-scale system migrations. As in, rewriting old logic in a new language or stack. The goal is usually better scalability, resilience, and maintainability, or more flexibility to adapt to changing requirements. Now, whether rewriting your system is the right move is its own debate.\nA common question that shows up during a migration is, \u0026ldquo;How do we make sure the new system behaves exactly like the old one, minus the icky parts?\u0026rdquo; Another one is, \u0026ldquo;How do we build the new system while the old one keeps changing without disrupting the business?\u0026rdquo;\nThere\u0026rsquo;s no universal playbook. It depends on how gnarly the old system is, how ambitious the new system is, and how much risk the business can stomach. After going through a few of these migrations, I realized one approach keeps showing up. So I\u0026rsquo;ll expand on it here.\nThe idea is that you shadow a slice of production traffic to the new system. The old system keeps serving real users. A copy of that same traffic is forwarded to the new system along with the old system\u0026rsquo;s response. The new system runs the same business logic and compares its outputs with the old one. The entire point is to make the new system return the exact same answer the old one would have, for the same inputs and the same state.\nAt the start, you don\u0026rsquo;t rip out bad behavior or ship new features. Everything is about output parity. Once the systems line up and the new one has processed enough real traffic to earn some trust, you start sending actual user traffic to it. If something blows up, you roll back. If it behaves as expected, you push more traffic. Eventually the old system gets to ride off into the sunset.\nThis workflow is typically known as shadow testing or tap and compare testing.\nThe scenario Say we have a Python service with a handful of read and write endpoints the business depends on. It\u0026rsquo;s been around for a while, and different teams have patched it over the years. Some of the logic does what it does for reasons nobody remembers anymore. It still works, but it\u0026rsquo;s getting harder to maintain. Also, the business wants a tighter SLO. So the team decides to rewrite it in Go.\nTo keep the scope tight, I\u0026rsquo;m only talking about HTTP read and write endpoints on the main request path. The same applies to gRPC, minus the transport details. I\u0026rsquo;m ignoring everything else: message queues, background workers, async job processing, analytics pipelines, and other side channels that also need migrating.\nDuring shadow testing, the Python service stays on the main request path. All real user traffic still goes to the Python service. A proxy or load balancer sitting in front of it forwards requests as usual, gets an answer back, and returns that answer to the user.\nThat same proxy also emits tap events. Each tap event contains a copy of the request and the canonical response the Python service sent to the user. Those tap events go to the Go service on a shadow path. From the outside world, nothing has changed. Clients talk to Python, and Python talks to the live production database.\nThe Go service never serves real users during this phase. It only sees tap events. For each event, it reconstructs the request, runs its version of the logic against a separate datastore, and compares its outputs with the Python response recorded in the event. The Python response is always the source of truth.\nThe Go service has its own datastore, usually a snapshot or replica of production that\u0026rsquo;s been detached so it can be written freely. This is the sister datastore. The Go service only talks to it for reads and writes. It never touches the real production DB. The sister datastore is close enough to show real-world behavior but isolated enough that nothing breaks.\nWith this setup in place, you spend time fixing differences. If the Python service returns a specific payload shape or some quirky value, the Go service has to match it. If Python gets a bug fix or a new feature, you update Go. You keep doing this until shadow traffic stops producing mismatches. Then you start thinking about cutover.\nStart with read endpoints Reads don\u0026rsquo;t change anything in the database, so they are easier to start with.\nOn the main path, a user sends a request. The proxy forwards it to the Python service as usual. The Python service reads from the real database, builds a response, and returns it to the caller.\nWhile that is happening, the proxy also constructs a tap event. At minimum, this event contains:\nThe original request: method, URL, headers, body. The canonical Python response: status code, headers, body. The proxy sends this tap event to the Go service via an internal HTTP or RPC endpoint. Alternatively, it can publish the event to a Kafka stream, where a consumer eventually forwards it to the internal tap endpoint.\nThe important thing is that the tap event captures the exact input and output of the Python service as seen by the real user.\nA typical read path diagram during tap compare looks like this:\ngraph TD subgraph MAIN_PATH [MAIN PATH] User([User]) --\u003e Proxy Proxy --\u003e Python Python \u003c-- reads production state --\u003e ProdDB[(Prod DB)] end subgraph SHADOW_PATH [SHADOW PATH] Proxy -- \"tap event: {request, python_resp}\" --\u003e Go Go \u003c--\u003e SisterDB[(Sister DB)] Go --\u003e Log[Log mismatch?] end classDef userStyle fill:#6b7280,stroke:#4b5563,color:#fff classDef proxyStyle fill:#7c3aed,stroke:#5b21b6,color:#fff classDef pythonStyle fill:#2563eb,stroke:#1d4ed8,color:#fff classDef goStyle fill:#0d9488,stroke:#0f766e,color:#fff classDef dbStyle fill:#ca8a04,stroke:#a16207,color:#fff classDef logStyle fill:#dc2626,stroke:#b91c1c,color:#fff class User userStyle class Proxy proxyStyle class Python pythonStyle class Go goStyle class ProdDB,SisterDB dbStyle class Log logStyle From the Go service\u0026rsquo;s point of view, a tap event is just structured data. A simple shape might look like this on the wire:\n{ \u0026#34;request\u0026#34;: { \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;/users/123?verbose=true\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;...\u0026#34;: [\u0026#34;...\u0026#34;] }, \u0026#34;body\u0026#34;: \u0026#34;...\u0026#34; }, \u0026#34;python_response\u0026#34;: { \u0026#34;status\u0026#34;: 200, \u0026#34;headers\u0026#34;: { \u0026#34;...\u0026#34;: [\u0026#34;...\u0026#34;] }, \u0026#34;body\u0026#34;: \u0026#34;{ \\\u0026#34;id\\\u0026#34;: \\\u0026#34;123\\\u0026#34;, \\\u0026#34;name\\\u0026#34;: \\\u0026#34;Alice\\\u0026#34; }\u0026#34; } } The Go side reconstructs the request, runs its own logic against the sister datastore, and compares its answer with python_response. No extra call back into Python. No race between a second read and the response that already went to the user.\nOn the Go side, a handler for a read tap event might look like this:\ntype TapRequest struct { Method string `json:\u0026#34;method\u0026#34;` URL string `json:\u0026#34;url\u0026#34;` Headers map[string][]string `json:\u0026#34;headers\u0026#34;` Body []byte `json:\u0026#34;body\u0026#34;` } type TapResponse struct { Status int `json:\u0026#34;status\u0026#34;` Headers map[string][]string `json:\u0026#34;headers\u0026#34;` Body []byte `json:\u0026#34;body\u0026#34;` } type TapEvent struct { Request TapRequest `json:\u0026#34;request\u0026#34;` PythonResponse TapResponse `json:\u0026#34;python_response\u0026#34;` } func TapHandleGetUser(w http.ResponseWriter, r *http.Request) { // This endpoint is internal only. // It receives tap events from the proxy, not real user traffic. var tap TapEvent if err := json.NewDecoder(r.Body).Decode(\u0026amp;tap); err != nil { http.Error(w, \u0026#34;bad tap payload\u0026#34;, http.StatusBadRequest) return } // Rebuild something close to the original HTTP request. reqURL, err := url.Parse(tap.Request.URL) if err != nil { http.Error(w, \u0026#34;bad url\u0026#34;, http.StatusBadRequest) return } // Body is a one-shot stream, so buffer it for reuse. bodyBytes := append([]byte(nil), tap.Request.Body...) goReq := \u0026amp;http.Request{ Method: tap.Request.Method, URL: reqURL, Header: http.Header(tap.Request.Headers), Body: io.NopCloser(bytes.NewReader(bodyBytes)), } // Go service: run candidate logic against sister datastore. goResp, goErr := goUserService.GetUser(r.Context(), goReq) if goErr != nil { log.Printf(\u0026#34;go candidate error: %v\u0026#34;, goErr) } // Normalize and compare off the main response path. // The real user already got python_response. go func() { normalizedPython := normalizeHTTP(tap.PythonResponse) normalizedGo := normalizeHTTP(goResp) if !deepEqual(normalizedPython, normalizedGo) { log.Printf( \u0026#34;read mismatch: url=%s python=%v go=%v\u0026#34;, tap.Request.URL, normalizedPython, normalizedGo, ) } }() // Optional debugging response for whoever is calling the tap // endpoint. w.WriteHeader(http.StatusNoContent) } A few things to notice:\nTruth lives with the Python response that already went to the user. The Go service sees exactly the same request the Python service saw. Comparison happens off the user path. Users never wait on the Go service. The Go service only touches the sister datastore, never the real one. The tap handler doesn\u0026rsquo;t return any payload. It just compares service outputs and emits logs. When the read diffs drop to zero (or near zero) against live traffic, you can trust the Go implementation matches the Python one.\nWrite endpoints are trickier Write endpoints change state, so they are harder to migrate.\nOn the main path, only the Python service is allowed to mutate production state.\nA typical write looks like this on the main path:\nUser sends a write request. Proxy forwards it to the Python service. Python runs the real write logic, talks to the live database, sends emails, charges cards, and returns a response. Proxy returns that response to the user. That path is the only one touching production. The Go service must not:\nwrite anything to the real production database trigger real external side effects call any real Python write endpoint in a way that causes a second write For writes, the tap event pushed by the proxy looks quite similar to reads:\n{ \u0026#34;request\u0026#34;: { \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;/users\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;...\u0026#34;: [\u0026#34;...\u0026#34;] }, \u0026#34;body\u0026#34;: \u0026#34;{ \\\u0026#34;email\\\u0026#34;: \\\u0026#34;alice@example.com\\\u0026#34;, \\\u0026#34;name\\\u0026#34;: \\\u0026#34;Alice\\\u0026#34; }\u0026#34; }, \u0026#34;python_response\u0026#34;: { \u0026#34;status\u0026#34;: 201, \u0026#34;headers\u0026#34;: { \u0026#34;...\u0026#34;: [\u0026#34;...\u0026#34;] }, \u0026#34;body\u0026#34;: \u0026#34;{ \\\u0026#34;id\\\u0026#34;: \\\u0026#34;123\\\u0026#34;, \\\u0026#34;email\\\u0026#34;: \\\u0026#34;alice@example.com\\\u0026#34; }\u0026#34; } } The write path diagram during tap compare becomes:\ngraph TD subgraph MAIN_PATH [MAIN PATH] User([User]) --\u003e Proxy Proxy --\u003e Python Python \u003c-- writes prod state, triggers side effects --\u003e ProdDB[(Prod DB)] end subgraph SHADOW_PATH [SHADOW PATH] Proxy -- \"tap event: {request, python_resp}\" --\u003e Go Go \u003c--\u003e SisterDB[(Sister DB)] Go --\u003e Log[Log mismatch?] end classDef userStyle fill:#6b7280,stroke:#4b5563,color:#fff classDef proxyStyle fill:#7c3aed,stroke:#5b21b6,color:#fff classDef pythonStyle fill:#2563eb,stroke:#1d4ed8,color:#fff classDef goStyle fill:#0d9488,stroke:#0f766e,color:#fff classDef dbStyle fill:#ca8a04,stroke:#a16207,color:#fff classDef logStyle fill:#dc2626,stroke:#b91c1c,color:#fff class User userStyle class Proxy proxyStyle class Python pythonStyle class Go goStyle class ProdDB,SisterDB dbStyle class Log logStyle On the Go side, the write tap handler follows the same pattern as reads but has more corner cases to think through.\nA shadow write handler might look like this:\ntype UserInput struct { Email string `json:\u0026#34;email\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` // ... other fields } type User struct { ID string `json:\u0026#34;id\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` // ... other fields } func TapHandleCreateUser(w http.ResponseWriter, r *http.Request) { // Internal only. Receives tap events for CreateUser. var tap TapEvent if err := json.NewDecoder(r.Body).Decode(\u0026amp;tap); err != nil { http.Error(w, \u0026#34;bad tap payload\u0026#34;, http.StatusBadRequest) return } // Decode the original request body once. var input UserInput if err := json.Unmarshal(tap.Request.Body, \u0026amp;input); err != nil { log.Printf(\u0026#34;bad original json: %v\u0026#34;, err) return } // The Python response is canonical: this is what the real user saw. pyUser, err := decodePythonUser(tap.PythonResponse) if err != nil { log.Printf(\u0026#34;bad python response: %v\u0026#34;, err) return } // Run the Go write path against the sister datastore. // This must never talk to the live production DB. goUser, goErr := goUserService.CreateUserInSisterStore( r.Context(), input, ) if goErr != nil { log.Printf(\u0026#34;go candidate write error: %v\u0026#34;, goErr) } // Compare results asynchronously. go func() { normalizedPython := normalizeUser(pyUser) normalizedGo := normalizeUser(goUser) if !compareUsers(normalizedPython, normalizedGo) { log.Printf( \u0026#34;write mismatch: email=%s python=%v go=%v\u0026#34;, normalizedPython.Email, normalizedPython, normalizedGo, ) } }() w.WriteHeader(http.StatusNoContent) } You are comparing how each system transforms the same request into a domain object and response. You are not trying to drive the Python service a second time. You are not trying to rebuild the Python result from scratch against changed state.\nBut with this setup, the write path has several corner cases to think through.\nUniqueness, validation, and state-dependent logic Uniqueness checks, conditional updates, and other validations that depend on database state are sensitive to timing. The Python write runs against the actual production state at the moment the main request hits. The Go write runs against whatever state exists in the sister datastore when the tap event arrives.\nIf the sister datastore is a snapshot that is not continuously replicated, it will drift almost immediately. Even with streaming replication, there may be short lags. That means:\nA create request that was valid in prod might look invalid against a slightly stale snapshot if another request changed state in between. A conditional update like \u0026ldquo;only update if version is X\u0026rdquo; can take different branches if the sister store has not applied the latest change yet. A multi-entity invariant that Python enforced with a transaction might appear broken in the sister store if replication replayed statements in a different order relative to the tap event. You should expect some write comparisons to be noisy because of state drift and treat those separately. In practice you often:\nKeep replication as close to real time as you can, or regularly reseed the sister datastore. Attach a few state fingerprints to the tap event, like the version of the row before and after the write, so you can tell when the sister store is simply behind. Filter out mismatches that can be traced to obvious replication lag when you look at diff reports. The important thing is: when you see a mismatch, you can decide whether it is a real logic difference or just the sister store living in a slightly different universe for that request.\nIdempotency, retries, and ordering Real systems don\u0026rsquo;t get one clean write per user action. You get retries, duplicates, and concurrent updates.\nOn the main path, you might have:\nA user hitting \u0026ldquo;submit\u0026rdquo; twice. A client retrying on a network timeout. Two services racing to update the same record. Your Python service probably already has a story for this, such as idempotency keys, version checks, or last-write-wins semantics. The tap path needs to reflect what actually happened, not an idealized story.\nBecause the tap event is constructed from the real request and real response at the proxy, it naturally honors whatever the Python service did. If a retry was coalesced into a single write under an idempotency key, you will see a single successful response in the tap stream. If the second retry was rejected as a conflict, you will see that error. The Go service just needs to implement the same semantics against the sister datastore.\nWhat still bites you is ordering. Tap events may arrive at the Go service a little out of order relative to how mutations hit production. If two writes race, Python might process them in order A, B while the tap messages arrive as B, A. The sister datastore will then experience a different sequence of state changes than production did, which can yield legitimate differences in final state.\nYou can\u0026rsquo;t fully eliminate this. What you can do is:\nKeep tap delivery low latency and best-effort ordered. Focus your comparisons more on single-request behavior (did CreateUser behave the same) than on multi-request history until you are comfortable with the noise. Use version numbers or timestamps in the domain model to detect when the sister store is applying changes in a different order, and treat those as \u0026ldquo;not comparable\u0026rdquo; rather than bugs. External side effects Writes often have external side effects: emails, payment gateways, cache invalidations, search indexing, analytics.\nThe tap path isolates database writes by using the sister datastore, but that is not enough on its own. You have to run the Go service in a mode where those side-effectful calls are either disabled or mocked.\nThe usual pattern is:\nCentralize side-effectful behavior behind interfaces or specific modules. In normal production mode, those modules call real providers. In tap compare mode, they are wired to no-op or record-only implementations. You want the code paths that decide \u0026ldquo;should we send a welcome email\u0026rdquo; or \u0026ldquo;should we charge this card\u0026rdquo; to run, because they influence the domain model and response shape. You don\u0026rsquo;t want the actual email to go out or the real payment provider to be hit twice.\nOn the Python side, you don\u0026rsquo;t need dry runs or special write endpoints. The real main path already did the work, and the tap event gives you the results. The only thing the Python service might need for tap compare is a dedicated read endpoint that returns a normalized view of state if you want to sample post-write state directly. That read endpoint must not cause extra writes or side effects.\nWhat tap compare can and can\u0026rsquo;t tell you on writes It tells you:\nFor a given real user request and the production state that existed at that moment, what the Python service chose to return. Whether your Go service, running against a similar but separate view of state, tends to produce the same shape and content of domain objects and responses. Whether your Go write path can execute at all against realistic traffic without panicking or tripping over obvious logic errors. It doesn\u0026rsquo;t guarantee:\nThat the Go service produces exactly the same side effects in exactly the same order as the Python service. External systems and replication noise get in the way. That the Go service behaves identically under arbitrary concurrent write histories. You saw the histories that actually happened during the tap window, which might miss some edge case interleavings. That all mismatches are bugs. Some will be explained by replication lag, idempotency behavior, or intended fixes. The right way to think about it is: tap compare lets you align the new system with the old one for the traffic you actually have, under the state and timing conditions you actually experienced. It shrinks the unknowns before you put the new system in front of real users.\nFrom tap handlers to production handlers The Tap* handlers are test-only. They will never be promoted to production. They exist to validate the domain logic, not to serve users. The 204 No Content response makes this clear.\nHere\u0026rsquo;s how the pieces fit together:\nCore domain logic: methods on goUserService that take a context and input, return a response. This is the code you\u0026rsquo;re actually testing. Tap handlers: call the domain logic, compare against the Python response, discard the result. Pure validation. Production handlers: call the same domain logic and write real HTTP responses. This is what users hit after cutover. Both tap and production handlers call the same domain logic. The difference is what happens to the result. Tap handlers compare and throw away. Production handlers serialize and return.\nA production handler might look like this:\nfunc HandleGetUser(w http.ResponseWriter, r *http.Request) { resp, err := goUserService.GetUser(r.Context(), r) if err != nil { writeError(w, err) return } writeHTTP(w, resp) } During tap compare, TapHandleGetUser feeds the same inputs into goUserService.GetUser and compares resp against the Python response. Meanwhile, HandleGetUser exists but isn\u0026rsquo;t on the main path yet. It might serve staging traffic or a canary behind a flag.\nOnce the diffs drop to zero, you have evidence goUserService.GetUser works correctly. At that point, you route real traffic to HandleGetUser. The domain logic has already been validated. The production handler just wires it to HTTP.\nOnce the production handlers have started to serve real traffic, you can remvove the tap tests:\nDelete the tap handlers. The Tap* prefix makes them easy to find. Remove tap-only wiring. Strip out comparison code and sister-datastore plumbing. Point domain logic at the real datastore. Flip a config or swap the write path. Flip the proxy. Route traffic to HandleGetUser and HandleCreateUser. Optionally keep a thin tap path. Mirror a small slice of traffic for extra safety. Tap compare is scaffolding. Once you trust the domain logic, you throw it away and let the production handlers take over.\nOther risks and pitfalls A few things worth calling out beyond what the write section already covers:\nLogging and privacy: Dumping full requests and responses on every mismatch is a good way to leak user data. If this is relevant in your case, log IDs and fingerprints, not full payloads. Non-deterministic data: Auto-incremented IDs diverge, timestamps can differ by milliseconds, 10.0 vs 10 doesn\u0026rsquo;t matter. Normalize or ignore these fields. Bug compatibility: The Python code has bugs. The Go code may fix them, which shows up as a mismatch. Sometimes you replicate the bug to keep the migration low-risk, then fix it later once the new system is live. Cost and blast radius: Shadowing production traffic is expensive. Plan for the extra load so the tap path doesn\u0026rsquo;t degrade the main path. Parting words Typically, you don\u0026rsquo;t have to build all the plumbing by hand. Proxies like Envoy, NGINX, and HAProxy, or a service mesh like Istio, can help you mirror traffic, capture tap events, and feed them into a shadow service. I left out tool-specific workflows so that the core concept doesn\u0026rsquo;t get obscured.\nTap compare doesn\u0026rsquo;t remove all the risk from a migration, but it moves a lot of it into a place you can see: mismatched payloads, noisy writes, and gaps in business logic. Once those are understood, switching over to the new service is less of a big bang and more of a boring configuration change, followed by trimming a pile of Tap* code you no longer need.\n","permalink":"https://rednafi.com/system/tap-compare-testing/","summary":"\u003cp\u003eThroughout the years, I\u0026rsquo;ve been part of a few medium- to large-scale system migrations. As\nin, rewriting old logic in a new language or stack. The goal is usually better scalability,\nresilience, and maintainability, or more flexibility to adapt to changing requirements. Now,\nwhether rewriting your system is the right move is its own debate.\u003c/p\u003e\n\u003cp\u003eA common question that shows up during a migration is, \u0026ldquo;How do we make sure the new system\nbehaves exactly like the old one, minus the icky parts?\u0026rdquo; Another one is, \u0026ldquo;How do we build\nthe new system while the old one keeps changing without disrupting the business?\u0026rdquo;\u003c/p\u003e","title":"Tap compare testing for service migration"},{"content":" A man with a watch knows what time it is. A man with two watches is never sure.\n— Segal\u0026rsquo;s Law\nTake this example:\nfunc validate(input string) (bool, error) { // Validation check 1 if input == \u0026#34;\u0026#34; { return false, nil } // Validation check 2 if isCorrupted(input) { return false, nil } // System check if err := checkUpstream(); err != nil { return false, err } return true, nil } This function returns two signals: a boolean to indicate if the string is valid, and an error to explain any problem the function might run into.\nThe issue is that these two signals are independent. Put together, they produce four possible combinations:\ntrue, nil: The input is valid and the function encountered no issues. This is the only obvious mode. false, nil: Implies the function didn\u0026rsquo;t hit a system error but the input was invalid. However, in many codebases, this combination is accidentally used to hide real errors that were swallowed. true, err: A contradiction. The function claims success and failure at the same time. false, err: Looks like a clean failure, but it creates a priority trap. The Go convention dictates you must check the error first. If a caller checks the boolean first, they might see false and treat a major system crash as a simple validation failure. In this specific case, we never return true, err, but the caller doesn\u0026rsquo;t know that. They have to read the code to understand which subset of the possible combinations the function actually uses.\nSplintered failure modes For lack of a better term, I call this splintered failure modes. It is one of the cases that the adage make illegal state unrepresentable aims to prevent.\nIn our case, validate encodes the success/failure state in two places. These two signals can disagree. The boolean tries to express validity, and the error tries to express system failure, yet both attempt to answer the same question: did this succeed?\nWhen combinations like false, nil or true, err appear, the caller needs to know how to reconcile the conflicting states.\nRepresent failure modes exclusively via the error We fix the ambiguity by removing the boolean status flag entirely.\nIn this refactored version, the error assumes total responsibility for the function\u0026rsquo;s state (success vs. failure). The first return value becomes purely the payload.\nThe caller checks one place and one place only: the error.\n// We return the data (string), not a flag (bool) func validate(input string) (string, error) { if input == \u0026#34;\u0026#34; { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;input cannot be empty\u0026#34;) } if isCorrupted(input) { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;input is corrupted\u0026#34;) } if err := checkUpstream(); err != nil { return \u0026#34;\u0026#34;, err } // If we are here, the data is valid return input, nil } This makes the call site trivial because the state is no longer split. If the error is non-nil, the operation failed. If it is nil, the operation succeeded.\nDistinguishing failure types within the error Sometimes the caller of a function needs to take different actions depending on the type of an error. In that case, just knowing whether a function succeeded or failed isn\u0026rsquo;t enough.\nRemoving the boolean removes the ambiguity, but it introduces a new question: How do we distinguish between \u0026ldquo;validation error\u0026rdquo; and \u0026ldquo;system failure\u0026rdquo;?\nPreviously, the boolean represented validation outcome (valid/invalid), and the error represented the system failures (crash/upstream). Now that we have consolidated everything into error, we need a way to differentiate the kind of failure without re-introducing a second return value.\nSentinel errors We can use sentinel errors to encode multiple failure modes into one error variable. The error return value remains the single source of truth for \u0026ldquo;did it fail?\u0026rdquo;, but the content of that error tells us \u0026ldquo;how it failed.\u0026rdquo;\nvar ( // Domain/Logic failures ErrEmpty = errors.New(\u0026#34;input cannot be empty\u0026#34;) ErrCorrupted = errors.New(\u0026#34;input is corrupted\u0026#34;) // System/Mechanical failures ErrSystem = errors.New(\u0026#34;system failure\u0026#34;) ) func validate(input string) (string, error) { if input == \u0026#34;\u0026#34; { return \u0026#34;\u0026#34;, ErrEmpty } if isCorrupted(input) { return \u0026#34;\u0026#34;, ErrCorrupted } if err := checkUpstream(); err != nil { // We could return err directly, or wrap it return \u0026#34;\u0026#34;, ErrSystem } return input, nil } We have unified the failure state (it is always just an error), but we haven\u0026rsquo;t lost the granularity. The caller can now use errors.Is to switch between the failure modes:\nval, err := validate(userData) if err != nil { switch { case errors.Is(err, ErrEmpty): // Handle logic failure 1 (e.g. prompt user) return case errors.Is(err, ErrCorrupted): // Handle logic failure 2 (e.g. reject payload) return case errors.Is(err, ErrSystem): // Handle system failure (e.g. alert ops team) log.Fatal(err) default: log.Fatal(err) } } Error types If sentinels aren\u0026rsquo;t enough (for example, if you need to know which field failed validation), you can use error types. This allows the single error value to carry structured metadata while still adhering to the standard error interface.\nHere, we map both \u0026ldquo;Empty\u0026rdquo; and \u0026ldquo;Corrupted\u0026rdquo; to a ValidationError type, while leaving system errors as standard errors.\ntype ValidationError struct { Field string Reason string } func (e *ValidationError) Error() string { return fmt.Sprintf(\u0026#34;invalid %s: %s\u0026#34;, e.Field, e.Reason) } func validate(input string) (string, error) { if input == \u0026#34;\u0026#34; { return \u0026#34;\u0026#34;, \u0026amp;ValidationError{Field: \u0026#34;input\u0026#34;, Reason: \u0026#34;empty\u0026#34;} } if isCorrupted(input) { return \u0026#34;\u0026#34;, \u0026amp;ValidationError{Field: \u0026#34;input\u0026#34;, Reason: \u0026#34;corrupted\u0026#34;} } if err := checkUpstream(); err != nil { return \u0026#34;\u0026#34;, err } return input, nil } The caller can then use errors.As to inspect the failure mode in detail:\nval, err := validate(userData) if err != nil { var vErr *ValidationError // Check if the error is a logical ValidationError if errors.As(err, \u0026amp;vErr) { fmt.Printf(\u0026#34;Validation failed on %s: %s\u0026#34;, vErr.Field, vErr.Reason) return } // If not, it is a system failure log.Fatal(err) } By sticking to the error value as the single indicator of failure, we eliminate the \u0026ldquo;two watches\u0026rdquo; paradox. Whether the failure is a simple validation error or a catastrophic system crash, all the failure modes are encapsulated inside the single error value itself.\n","permalink":"https://rednafi.com/go/splintered-failure-modes/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eA man with a watch knows what time it is. A man with two watches is never sure.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e— \u003ca href=\"https://en.wikipedia.org/wiki/Segal%2527s_law\"\u003eSegal\u0026rsquo;s Law\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eTake this example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003evalidate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003einput\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003estring\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003ebool\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eerror\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e// Validation check 1\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003einput\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003efalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003enil\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e// Validation check 2\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eisCorrupted\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003einput\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003efalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003enil\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e// System check\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eerr\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e:=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003echeckUpstream\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eerr\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e!=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003enil\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003efalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eerr\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003etrue\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003enil\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis function returns two signals: a \u003cstrong\u003eboolean\u003c/strong\u003e to indicate if the string is valid, and an\n\u003cstrong\u003eerror\u003c/strong\u003e to explain any problem the function might run into.\u003c/p\u003e","title":"Splintered failure modes in Go"},{"content":"When testing Go code that spawns subprocesses, you usually have three options.\nRun the real command. It invokes the actual binary that creates the subprocess and asserts against the output. However, that makes tests slow and tied to the environment. You have to make sure the same binary exists and behaves the same everywhere, which is harder than it sounds.\nFake it. Mock the subprocess to keep tests fast and isolated. The problem is that the fake version doesn\u0026rsquo;t behave like a real process. It won\u0026rsquo;t fail, write to stderr, or exit with a non-zero code. That makes it hard to trust the result, and over time the mock can drift away from what the real command actually does.\nRe-exec. I discovered this neat trick while watching Mitchel Hashimoto\u0026rsquo;s Advanced Testing with Go talk. In fact, it originated in the stdlib os/exec test suite. With re-exec, your test binary spawns a new subprocess that runs itself again. Inside that subprocess, the code emulates the behavior of the real command. The parent process then interacts with this subprocess exactly as it would with a real command. In short:\nThe parent test process spawns the subprocess. The subprocess emulates the behavior of the target command. The parent process interacts with the emulated subprocess as if it were the real command. This setup makes re-exec a middle ground between mocking and invoking the actual subprocess.\nThe first two paths are well-trodden, so let\u0026rsquo;s look closer at the third one. Here\u0026rsquo;s how it works:\nThe test re-launches itself with a special flag or environment variable to signal it\u0026rsquo;s running in \u0026ldquo;child\u0026rdquo; mode. In this mode, it acts as the subprocess and can print output, write to stderr, or exit with any code you want. This subprocess basically emulates the real command\u0026rsquo;s subprocess. The main test process then runs as usual and interacts with it just like it would with a real subprocess. You still get a real subprocess, but the behavior of your original binary invocation is emulated inside it. So you don\u0026rsquo;t invoke the original command. Observe:\n// /cmd/echo/main.go package main import ( \u0026#34;os/exec\u0026#34; ) // RunEcho executes the system \u0026#34;echo\u0026#34; command with the provided message // and returns the command\u0026#39;s output. func RunEcho(msg string) (string, error) { cmd := exec.Command(\u0026#34;echo\u0026#34;, msg) out, err := cmd.Output() return string(out), err } RunEcho invokes the system\u0026rsquo;s echo binary with some argument and returns the output. Now let\u0026rsquo;s test it using the re-exec trick:\n// /cmd/echo/main_test.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/exec\u0026#34; \u0026#34;testing\u0026#34; ) // TestEchoHelper runs when the binary is re-executed with // GO_WANT_HELPER_PROCESS=1. It prints its argument and exits, // emulating \u0026#34;echo\u0026#34;. func TestEchoHelper(t *testing.T) { if os.Getenv(\u0026#34;GO_WANT_HELPER_PROCESS\u0026#34;) != \u0026#34;1\u0026#34; { return } fmt.Print(os.Args[len(os.Args)-1]) os.Exit(0) } func TestRunEcho(t *testing.T) { // Spawn the same test binary as a subprocess instead of calling the // real \u0026#34;echo\u0026#34;. This runs only the TestEchoHelper test in a subprocess // which emulates the behavior of \u0026#34;echo\u0026#34; cmd := exec.Command( os.Args[0], \u0026#34;-test.run=TestEchoHelper\u0026#34;, \u0026#34;--\u0026#34;, \u0026#34;hello\u0026#34;, ) cmd.Env = append(os.Environ(), \u0026#34;GO_WANT_HELPER_PROCESS=1\u0026#34;) out, err := cmd.Output() if err != nil { t.Fatal(err) } if string(out) != \u0026#34;hello\u0026#34; { t.Fatalf(\u0026#34;got %q, want %q\u0026#34;, out, \u0026#34;hello\u0026#34;) } } TestRunEcho creates a command that re-runs the same test binary (os.Args[0]) as a subprocess via the exec.Command. The -test.run=TestEchoHelper flag tells Go\u0026rsquo;s test runner to execute only the TestEchoHelper function inside that new process. The \u0026quot;--\u0026quot; marks the end of the test runner\u0026rsquo;s own flags, and everything after it (\u0026quot;hello\u0026quot;) becomes an argument available to the helper process in os.Args.\nWhen this subprocess starts, it sees that the environment variable GO_WANT_HELPER_PROCESS=1 is set. That tells it to behave like a helper instead of running the full test suite. The TestEchoHelper function then prints its last argument (\u0026quot;hello\u0026quot;) to standard output and exits. In other words, we\u0026rsquo;re emulating echo inside TestEchoHelper. This part is intentionally kept simple, but you can do all kinds of things here to emulate the actual echo command. In real tests, this will also include different failure modes.\nFrom the parent process\u0026rsquo;s perspective, it looks just like running /bin/echo hello, except everything is happening within the Go test binary. The subprocess is real, but its behavior is entirely controlled by the test.\nYou might find it strange that the actual RunEcho function isn\u0026rsquo;t called anywhere. That\u0026rsquo;s on purpose. The goal of this example is not to test production logic, but to show how to emulate and control subprocesses inside a test environment. The production function here doesn\u0026rsquo;t contain any logic beyond calling exec.Command, so there\u0026rsquo;s nothing meaningful to verify yet.\nIn real code, typically, you\u0026rsquo;d split subprocess management into two parts: one that spawns the process and another that handles its output and errors. The handler is where the bulk of your logic should live. This way, the subprocess handling code can be tested in isolation without having to tie it with a real subprocess.\nConsider this example where the production code invokes the git switch mybranch command. The RunGitSwitch command calls the git binary with the appropriate arguments and passes the *exec.Cmd pointer to the handleGitSwitch function. This handler function has the bulk of the logic that interacts with the git subprocess.\n// path: /cmd/git/main.go package main import ( \u0026#34;os/exec\u0026#34; ) // handleGitSwitch runs a command and returns its output and error. func handleGitSwitch(cmd *exec.Cmd) (string, error) { out, err := cmd.CombinedOutput() return string(out), err } // RunGitSwitch constructs the subprocess to run \u0026#34;git switch\u0026#34;. func RunGitSwitch(branch string) (string, error) { cmd := exec.Command(\u0026#34;git\u0026#34;, \u0026#34;switch\u0026#34;, branch) return handleGitSwitch(cmd) } And the corresponding test:\n// path: /cmd/git/main_test.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/exec\u0026#34; \u0026#34;testing\u0026#34; ) // TestGitSwitchHelper acts as the fake \u0026#34;git switch\u0026#34; subprocess. func TestGitSwitchHelper(t *testing.T) { if os.Getenv(\u0026#34;GO_WANT_HELPER_PROCESS\u0026#34;) != \u0026#34;1\u0026#34; { return } // Emulate \u0026#34;git switch\u0026#34; output. fmt.Printf(\u0026#34;Switched to branch \u0026#39;%s\u0026#39;\\n\u0026#34;, os.Args[len(os.Args)-1]) os.Exit(0) } func TestGitSwitch(t *testing.T) { cmd := exec.Command( os.Args[0], \u0026#34;-test.run=TestGitSwitchHelper\u0026#34;, \u0026#34;--\u0026#34;, \u0026#34;feature-branch\u0026#34;, ) cmd.Env = append(os.Environ(), \u0026#34;GO_WANT_HELPER_PROCESS=1\u0026#34;) // This time we\u0026#39;re invoking the production handler. out, err := handleGitSwitch(cmd) if err != nil { t.Fatal(err) } want := \u0026#34;Switched to branch \u0026#39;feature-branch\u0026#39;\\n\u0026#34; if out != want { t.Fatalf(\u0026#34;got %q, want %q\u0026#34;, out, want) } } In this test, the subprocess behavior (git switch) is emulated by TestGitSwitchHelper. The helper prints predictable output that mimics the real command, but the subprocess itself is still a separate process spawned by the parent test.\nWhat\u0026rsquo;s under test here is handleGitSwitch, which manages subprocess execution, reads its output, and handles errors. The subprocess is fake in behavior but real in execution, which means the I/O boundaries are still exercised.\nThis separation between subprocess creation and handling keeps tests focused and repeatable. You can emulate different subprocess outcomes, such as errors or unexpected output, while keeping the process interaction logic untouched.\n","permalink":"https://rednafi.com/go/test-subprocesses/","summary":"\u003cp\u003eWhen testing Go code that spawns subprocesses, you usually have three options.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRun the real command.\u003c/strong\u003e It invokes the actual binary that creates the subprocess and\nasserts against the output. However, that makes tests slow and tied to the environment. You\nhave to make sure the same binary exists and behaves the same everywhere, which is harder\nthan it sounds.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFake it.\u003c/strong\u003e Mock the subprocess to keep tests fast and isolated. The problem is that the\nfake version doesn\u0026rsquo;t behave like a real process. It won\u0026rsquo;t fail, write to stderr, or exit\nwith a non-zero code. That makes it hard to trust the result, and over time the mock can\ndrift away from what the real command actually does.\u003c/p\u003e","title":"Re-exec testing Go subprocesses"},{"content":"Object-oriented (OO) patterns get a lot of flak in the Go community, and often for good reason.\nStill, I\u0026rsquo;ve found that principles like SOLID, despite their OO origin, can be useful guides when thinking about design in Go.\nRecently, while chatting with a few colleagues new to Go, I noticed that some of them had spontaneously rediscovered the Interface Segregation Principle (the \u0026ldquo;I\u0026rdquo; in SOLID) without even realizing it. The benefits were obvious, but without a shared vocabulary, it was harder to talk about and generalize the idea.\nSo I wanted to revisit ISP in the context of Go and show how small interfaces, implicit implementation, and consumer-defined contracts make interface segregation feel natural and lead to code that\u0026rsquo;s easier to test and maintain.\nClients should not be forced to depend on methods they do not use.\n— Robert C. Martin (SOLID, interface segregation principle)\nOr, put simply: your code shouldn\u0026rsquo;t accept anything it doesn\u0026rsquo;t use.\nConsider this example:\ntype FileStorage struct{} func (FileStorage) Save(data []byte) error { fmt.Println(\u0026#34;Saving data to disk...\u0026#34;) return nil } func (FileStorage) Load(id string) ([]byte, error) { fmt.Println(\u0026#34;Loading data from disk...\u0026#34;) return []byte(\u0026#34;data\u0026#34;), nil } FileStorage has two methods: Save and Load. Now suppose you write a function that only needs to save data:\nfunc Backup(fs FileStorage, data []byte) error { return fs.Save(data) } This works, but there are a few problems hiding here.\nBackup takes a FileStorage directly, so it only works with that type. If you later want to back up to memory, a network location, or an encrypted store, you\u0026rsquo;ll need to rewrite the function. Because it depends on a concrete type, your tests have to use FileStorage too, which might involve disk I/O or other side effects you don\u0026rsquo;t want in unit tests. And from the function signature, it\u0026rsquo;s not obvious what part of FileStorage the function actually uses.\nInstead of depending on a specific type, we can depend on an abstraction. In Go, you can achieve that through an interface. So let\u0026rsquo;s define one:\ntype Storage interface { Save(data []byte) error Load(id string) ([]byte, error) } Now Backup can take a Storage instead:\nfunc Backup(store Storage, data []byte) error { return store.Save(data) } Backup now depends on behavior, not implementation. You can plug in anything that satisfies Storage, something that writes to disk, memory, or even a remote service. And FileStorage still works without any change.\nYou can also test it with a fake:\ntype FakeStorage struct{} func (FakeStorage) Save(data []byte) error { return nil } func (FakeStorage) Load(id string) ([]byte, error) { return nil, nil } func TestBackup(t *testing.T) { fake := FakeStorage{} err := Backup(fake, []byte(\u0026#34;test-data\u0026#34;)) if err != nil { t.Fatal(err) } } That\u0026rsquo;s a step forward. It fixes the coupling issue and makes the tests free of side effects. However, there\u0026rsquo;s still one issue: Backup only calls Save, yet the Storage interface includes both Save and Load. If Storage later gains more methods, every fake must grow too, even if those methods aren\u0026rsquo;t used. That\u0026rsquo;s exactly what the ISP warns against.\nThe above interface is too broad. So let\u0026rsquo;s narrow it to match what the function actually needs:\ntype Saver interface { Save(data []byte) error } Then update the function:\nfunc Backup(s Saver, data []byte) error { return s.Save(data) } Now the intent is clear. Backup only depends on Save. A test double can just implement that one method:\ntype FakeSaver struct{} func (FakeSaver) Save(data []byte) error { return nil } func TestBackup(t *testing.T) { fake := FakeSaver{} err := Backup(fake, []byte(\u0026#34;test-data\u0026#34;)) if err != nil { t.Fatal(err) } } The original FileStorage still works fine:\nfs := FileStorage{} _ = Backup(fs, []byte(\u0026#34;backup-data\u0026#34;)) Go\u0026rsquo;s implicit interface satisfaction makes this less ceremonious. Any type with a Save method automatically satisfies Saver.\nThis pattern reflects a broader Go convention: define small interfaces on the consumer side, close to the code that uses them. The consumer knows what subset of behavior it needs and can define a minimal contract for it. If you define the interface on the producer side instead, every consumer is forced to depend on that definition. A single change to the producer\u0026rsquo;s interface can ripple across your codebase unnecessarily.\nFrom Go code review comments:\nGo interfaces generally belong in the package that uses values of the interface type, not the package that implements those values. The implementing package should return concrete (usually pointer or struct) types: that way, new methods can be added to implementations without requiring extensive refactoring.\nThis isn\u0026rsquo;t a strict rule. The standard library defines producer-side interfaces like io.Reader and io.Writer, which is fine because they\u0026rsquo;re stable and general-purpose. But for application code, interfaces usually exist in only two places: production code and tests. Keeping them near the consumer reduces coupling between multiple packages and keeps the code easier to evolve.\nYou\u0026rsquo;ll see this same idea pop up all the time. Take the AWS SDK, for example. It\u0026rsquo;s tempting to define a big S3 client interface and use it everywhere:\ntype S3Client interface { PutObject( ctx context.Context, input *s3.PutObjectInput, opts ...func(*s3.Options)) (*s3.PutObjectOutput, error) GetObject( ctx context.Context, input *s3.GetObjectInput, opts ...func(*s3.Options)) (*s3.GetObjectOutput, error) ListObjectsV2( ctx context.Context, input *s3.ListObjectsV2Input, opts ...func(*s3.Options)) (*s3.ListObjectsV2Output, error) // ...and many more } Depending on such a large interface couples your code to far more than it uses. Any change or addition to this interface can ripple through your code and tests for no good reason.\nFor example, if your code uploads files, it only needs the PutObject method:\nfunc UploadReport( ctx context.Context, client S3Client, data []byte, ) error { _, err := client.PutObject( ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(\u0026#34;reports\u0026#34;), Key: aws.String(\u0026#34;daily.csv\u0026#34;), Body: bytes.NewReader(data), }, ) return err } But accepting the full S3Client here ties UploadReport to an interface that\u0026rsquo;s too broad. A fake must implement all the methods just to satisfy it.\nIt\u0026rsquo;s better to define a small, consumer-side interface that captures only the operations you need. This is exactly what the AWS SDK doc recommends for testing.\nTo support mocking, use Go interfaces instead of concrete service client, paginators, and waiter types, such as s3.Client. This allows your application to use patterns like dependency injection to test your application logic.\nSimilar to what we\u0026rsquo;ve seen before, you can define a single method interface:\ntype Uploader interface { PutObject( ctx context.Context, input *s3.PutObjectInput, opts ...func(*s3.Options)) (*s3.PutObjectOutput, error) } And then use it in the function:\nfunc UploadReport(ctx context.Context, u Uploader, data []byte) error { _, err := u.PutObject( ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(\u0026#34;reports\u0026#34;), Key: aws.String(\u0026#34;daily.csv\u0026#34;), Body: bytes.NewReader(data), }, ) return err } The intent is obvious: this function uploads data and depends only on PutObject. The fake for tests is now tiny:\ntype FakeUploader struct{} func (FakeUploader) PutObject( _ context.Context, _ *s3.PutObjectInput, _ ...func(*s3.Options)) (*s3.PutObjectOutput, error) { return \u0026amp;s3.PutObjectOutput{}, nil } If we distill the workflow as a general rule of thumb, it\u0026rsquo;d look like this:\nInsert a seam between two tightly coupled components by placing a consumer-side interface that exposes only the methods the caller invokes.\nFin!\n","permalink":"https://rednafi.com/go/interface-segregation/","summary":"\u003cp\u003eObject-oriented (OO) patterns get a lot of flak in the Go community, and often for good\nreason.\u003c/p\u003e\n\u003cp\u003eStill, I\u0026rsquo;ve found that principles like \u003ca href=\"https://en.wikipedia.org/wiki/SOLID\"\u003eSOLID\u003c/a\u003e, despite their OO origin, can be useful\nguides when thinking about design in Go.\u003c/p\u003e\n\u003cp\u003eRecently, while chatting with a few colleagues new to Go, I noticed that some of them had\nspontaneously rediscovered the Interface Segregation Principle (the \u0026ldquo;I\u0026rdquo; in SOLID) without\neven realizing it. The benefits were obvious, but without a shared vocabulary, it was harder\nto talk about and generalize the idea.\u003c/p\u003e","title":"Revisiting interface segregation in Go"},{"content":"Along with propagating deadlines and cancellation signals, Go\u0026rsquo;s context package can also carry request-scoped values across API boundaries and processes.\nThere are only two public API constructs associated with context values:\nfunc WithValue(parent Context, key, val any) Context func (c Context) Value(key any) any WithValue can take any comparable value as both the key and the value. The key defines how the stored value is identified, and the value can be any data you want to pass through the call chain.\nValue, on the other hand, also returns any, which means the compiler cannot infer the concrete type at compile time. To use the returned data safely, you must perform a type assertion.\nA naive workflow to store and retrieve values in a context looks like this:\nctx := context.Background() // Store some value against a key ctx = context.WithValue(ctx, \u0026#34;userID\u0026#34;, 42) // Retrieve the value v := ctx.Value(\u0026#34;userID\u0026#34;) // Value returns any, so you need a type assertion id, ok := v.(int) if !ok { fmt.Println(\u0026#34;unexpected type\u0026#34;) } fmt.Println(id) // 42 WithValue returns a new context that wraps the parent. Value walks up the chain of contexts and returns the first matching key it finds. Since the return type is any, a type assertion is required to recover the original type. Without the ok check, a mismatch would cause a panic.\nThe issue with this setup is that it risks collision. If another package sets a value against the same key, one overwrites the other:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; ) func main() { ctx := context.WithValue(context.Background(), \u0026#34;key\u0026#34;, \u0026#34;from-main\u0026#34;) ctx = foo(ctx) fmt.Println(ctx.Value(\u0026#34;key\u0026#34;)) // from-foo } func foo(ctx context.Context) context.Context { // Accidentally reuse the same key in another package return context.WithValue(ctx, \u0026#34;key\u0026#34;, \u0026#34;from-foo\u0026#34;) } The first value becomes inaccessible because WithValue returns a new derived context that shadows parent values with the same key. The original value still exists in the parent context but is unreachable through the reassigned variable.\nTo understand why this collision occurs, you need to know how Go compares interface values. When you assign a value to an interface{} (or any), Go boxes that value into an internal representation made up of two machine words: one points to the type information, and the other points to the underlying data.\nFor example:\nvar a any = \u0026#34;key\u0026#34; var b any = \u0026#34;key\u0026#34; fmt.Println(a == b) // true Each boxed interface here stores two things: a pointer to the type string and a pointer to the data \u0026quot;key\u0026quot;. Since both type and data pointers match, the comparison returns true.\nWithValue stores both the key and the value as any. When you later call Value, Go compares the boxed key you pass in with those stored in the context chain. If two different packages use the same built-in key type and data, like both passing \u0026quot;key\u0026quot; as a string, their boxed representations look identical. Go sees them as equal, and the most recent value shadows the earlier one.\nIf you want to learn more about how interfaces are represented and compared, Russ Cox\u0026rsquo;s post on Go interface internals explains it in detail with pretty pictures.\nThe fix is to make sure the keys have unique types so their boxed representations differ. If you define a custom type, the type pointer changes even if the data looks the same. For example:\ntype userKey string var a any = userKey(\u0026#34;key\u0026#34;) var b any = \u0026#34;key\u0026#34; fmt.Println(a == b) // false Even though the underlying value is \u0026quot;key\u0026quot;, the two interfaces now hold different type information, so Go considers them unequal. That difference in type identity is what prevents collisions.\nThe context documentation gives this advice:\nThe provided key must be comparable and should not be of type string or any other built-in type to avoid collisions between packages using context. Users of WithValue should define their own types for keys. To avoid allocating when assigning to an interface{}, context keys often have concrete type struct{}. Alternatively, exported context key variables\u0026rsquo; static type should be a pointer or interface.\nIn short:\nKeys must be comparable (string, int, struct, pointer, etc.) Define unique key types per package to avoid collisions Use struct{} keys to avoid allocation when stored as any Exported key variables should have pointer or interface types Here\u0026rsquo;s how defining a unique key type prevents collisions:\ntype userIDKey string // Store value ctx := context.WithValue(context.Background(), userIDKey(\u0026#34;id\u0026#34;), 42) // Retrieve value id := ctx.Value(userIDKey(\u0026#34;id\u0026#34;)) fmt.Println(id) // 42 Even if another package uses the string \u0026quot;id\u0026quot;, the key types differ, so they cannot collide.\nTo avoid allocation when WithValue assigns the inbound value to interface any, you can define an empty struct key. Unlike strings or integers, which allocate when boxed into an interface, a zero-sized struct occupies no memory and needs no allocation:\ntype key struct{} // Store value ctx := context.WithValue(context.Background(), key{}, \u0026#34;value\u0026#34;) // Retrieve value v := ctx.Value(key{}) fmt.Println(v) // value Empty structs are ideal for local, unexported keys. They are unique by type and add no overhead.\nAlternatively, exported keys can use pointers, which also avoid allocation and guarantee uniqueness. When a pointer is boxed into an interface, no data copy occurs because the interface just holds the pointer reference. Pointers are also ideal for keys that need to be shared across packages.\ntype userIDKey struct { name string } // Struct pointer as key var UserIDKey = \u0026amp;userIDKey{\u0026#34;user-id\u0026#34;} // Store value. No allocation here since userIDKey is a pointer // to a struct ctx := context.WithValue(context.Background(), UserIDKey, 42) // Retrieve value id := ctx.Value(UserIDKey) fmt.Println(id) // 42 Here, UserIDKey points to a unique struct instance, so equality checks work by pointer identity. The name field exists only for debugging. This avoids allocation and ensures exported keys remain unique even when shared between packages.\nWhen exposing context values across APIs, you can approach it in two ways depending on how much control and safety you want to give your users.\n1. Expose keys directly You can export the key itself and let users interact with it freely:\ntype APIKey string // Allow the other packages to directly use this key var APIKeyContextKey = APIKey(\u0026#34;api-key\u0026#34;) // Store value. An allocation will occur since the key is of type string ctx := context.WithValue(context.Background(), APIKeyContextKey, \u0026#34;secret\u0026#34;) // Retrieve value v := ctx.Value(APIKeyContextKey).(string) // caller must do this assertion fmt.Println(v) // secret When you export the key directly the caller gains direct access, but they also must:\ndo the type assertion themselves and handle the ok result to avoid panics ensure they don\u0026rsquo;t accidentally overwrite values using the wrong key The net/http package uses this approach for some of its exported context keys:\ntype contextKey struct { name string } // Notice the exported keys var ( ServerContextKey = \u0026amp;contextKey{\u0026#34;http-server\u0026#34;} LocalAddrContextKey = \u0026amp;contextKey{\u0026#34;local-addr\u0026#34;} ) Each variable points to a distinct struct, making them unique by pointer identity.\nThe serve_test.go file uses these keys like this:\nctx := context.WithValue( context.Background(), http.ServerContextKey, srv, ) // Type assertion to recover the concrete type srv2, ok := ctx.Value(http.ServerContextKey).(*http.Server) if ok { fmt.Println(srv == srv2) // true } The server value is stored in the context and later retrieved using the same pointer key. The user must perform a type assertion and handle it safely.\n2. Expose accessor functions The other approach is to hide the key and provide accessor functions to set and retrieve values. This removes the need for users to remember the right key type or perform type assertions manually.\n// Define a private key type to avoid collisions type contextKey struct { name string } // Define the key var userIDKey = \u0026amp;contextKey{\u0026#34;user-id\u0026#34;} // Public accessor to store a value to ctx func WithUserID(ctx context.Context, id int) context.Context { // No allocation here since userIDKey is a pointer to a struct return context.WithValue(ctx, userIDKey, id) } // Public accessor to fetch a value from ctx func UserIDFromContext(ctx context.Context) (int, bool) { v, ok := ctx.Value(userIDKey).(int) return v, ok } // Store value ctx := WithUserID(context.Background(), 42) // Retrieve value id, ok := UserIDFromContext(ctx) if ok { fmt.Println(id) // 42 } else { fmt.Println(\u0026#34;no user ID found in context\u0026#34;) } This approach centralizes how values are stored and retrieved from the context. It ensures the correct key and type are always used, preventing collisions and runtime panics. It also keeps the calling code shorter since your API users won\u0026rsquo;t need to repeat type assertions everywhere.\nWithX / XFromContext accessors appear throughout the Go standard library:\nnet/http/httptrace\nfunc WithClientTrace( ctx context.Context, trace *ClientTrace, ) context.Context func ContextClientTrace(ctx context.Context) *ClientTrace runtime/pprof\nfunc WithLabels(ctx context.Context, labels LabelSet) context.Context func Labels(ctx context.Context) LabelSet You can find similar examples outside of the stdlib. For instance, the OpenTelemetry Go SDK follows the same model:\nfunc ContextWithSpan(parent context.Context, span Span) context.Context func SpanFromContext(ctx context.Context) Span This technique standardizes how values are passed across APIs, eliminates redundant type assertions, and prevents key misuse across packages.\nClosing words I usually use a pointer to a struct as a key and expose accessor functions when building user-facing APIs. Otherwise, in services, I often define empty struct keys and expose them publicly to avoid the ceremony around accessor functions.\n","permalink":"https://rednafi.com/go/avoid-context-key-collisions/","summary":"\u003cp\u003eAlong with propagating deadlines and cancellation signals, Go\u0026rsquo;s \u003ccode\u003econtext\u003c/code\u003e package can also\ncarry request-scoped values across API boundaries and processes.\u003c/p\u003e\n\u003cp\u003eThere are only two public API constructs associated with context values:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eWithValue\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eparent\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eContext\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003ekey\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eval\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eany\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eContext\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ec\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eContext\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eValue\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ekey\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eany\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eany\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003eWithValue\u003c/code\u003e can take any comparable value as both the key and the value. The key defines how\nthe stored value is identified, and the value can be any data you want to pass through the\ncall chain.\u003c/p\u003e","title":"Avoiding collisions in Go context keys"},{"content":"When it comes to test organization, Go\u0026rsquo;s standard testing library only gives you a few options. I think that\u0026rsquo;s a great thing because there are fewer details to remember and fewer things to onboard people to. However, during code reviews, I often see people contravene a few common conventions around test organization, especially those who are new to the language.\nIf we distill the most common questions that come up when organizing tests, they are:\nWhere to put the unit tests for a package How to enable white-box and black-box testing Where the executable examples, benchmarks, and fuzz tests should live Where the integration and end-to-end tests for a service should live To answer these, let\u0026rsquo;s consider a simple test subject.\nSystem under test (SUT) Let\u0026rsquo;s define a small app called myapp that contains a single package mypkg. It has a Greet function that returns a greeting message as a string. We\u0026rsquo;ll use this throughout the discussion and evolve the directory structure as needed.\nmyapp/ └── mypkg/ ├── greet.go └── greet_test.go Here\u0026rsquo;s how greet.go looks:\n// greet.go package mypkg func Greet(name string) string { if name == \u0026#34;\u0026#34; { return \u0026#34;Hello, stranger\u0026#34; } return \u0026#34;Hello, \u0026#34; + name } In-package tests Most Go tests live next to the code they verify. These are called in-package tests, and they share the same package name as the code under test. This setup gives them access to unexported functions and variables, making them ideal for unit tests that target specific internal logic.\n// greet_test.go package mypkg // The test file lives under `mypkg` import \u0026#34;testing\u0026#34; func TestGreet(t *testing.T) { got := Greet(\u0026#34;Go\u0026#34;) // The test can access mypkg deps without an import want := \u0026#34;Hello, Go\u0026#34; if got != want { t.Fatalf(\u0026#34;Greet() = %q, want %q\u0026#34;, got, want) } } The structure stays the same:\nmyapp/ └── mypkg/ ├── greet.go # under package mypkg └── greet_test.go # under package mypkg These are your bread-and-butter unit tests. You can run them with go test ./..., and they\u0026rsquo;ll have full access to unexported details in the package.\nThe Go documentation explains it as:\nThe test file can be in the same package as the one being tested. If the test file is in the same package, it may refer to unexported identifiers within the package.\nThis approach is called white-box testing. Your test code has full access to the package internals, allowing you to test them directly when needed. For example, if there\u0026rsquo;s an unexported function in greet.go, the test in greet_test.go can call it directly. Following the test pyramid, most tests in your system should be written this way.\nCo-located external tests Sometimes you want to verify that your package behaves correctly from the outside. At this point, you\u0026rsquo;re not concerned with its internals and just want to confirm that the public API works as intended.\nGo makes this possible by letting you write tests under a package name that ends with _test. This creates a separate test package that lives alongside the package under test. For example:\n// greet_external_test.go package mypkg_test // Note the package definition import ( \u0026#34;testing\u0026#34; \u0026#34;myapp/mypkg\u0026#34; // Explicitly import the SUT package ) func TestGreetExternal(t *testing.T) { got := mypkg.Greet(\u0026#34;External\u0026#34;) want := \u0026#34;Hello, External\u0026#34; if got != want { t.Fatalf(\u0026#34;unexpected output: got %q, want %q\u0026#34;, got, want) } } Your directory now includes both internal and external tests:\nmyapp/ └── mypkg/ ├── greet.go # under package mypkg ├── greet_test.go # under package mypkg └── greet_external_test.go # under package mypkg_test In this setup, the mypkg directory can only contain the mypkg and mypkg_test packages. The compiler recognizes the _test suffix and disallows any other package names in the same directory.\nA key detail is that the Go test harness doesn\u0026rsquo;t build the tests of mypkg_test together with those of mypkg. It compiles two separate test binaries: one containing the package code and its in-package tests, and another containing the external tests. Each binary runs independently, and the external one links against the compiled mypkg archive just like any other importing package. You can find more about this process in the Go documentation on how tests are run.\nThis structure is particularly useful for validating public contracts and ensuring that refactors don\u0026rsquo;t break exported APIs.\nAs noted in the official testing package docs:\nIf the file is in a separate _test package, the package being tested must be imported explicitly, and only its exported identifiers may be used. This is known as “black-box\u0026quot; testing.\nIt\u0026rsquo;s a neat way to test your package from the outside without moving your tests into a separate directory tree. You can find examples of this style in net/http, context, and errors.\nExamples, benchmarks, and fuzz tests Go\u0026rsquo;s testing tool treats examples, benchmarks, and fuzz tests as first-class test functions. They use the same go test command as your regular unit tests and usually live in the same package. This makes them part of the same discovery and execution process but with different entry points.\nHere\u0026rsquo;s how all three can coexist in the same package:\n// greet_test.go package mypkg // same package as the unit tests import ( \u0026#34;fmt\u0026#34; \u0026#34;testing\u0026#34; ) // ... other unit tests func ExampleGreet() { fmt.Println(Greet(\u0026#34;Alice\u0026#34;)) // Output: Hello, Alice } func BenchmarkGreet(b *testing.B) { for b.Loop() { Greet(\u0026#34;Go\u0026#34;) } } func FuzzGreet(f *testing.F) { f.Add(\u0026#34;Bob\u0026#34;) f.Fuzz(func(t *testing.T, name string) { Greet(name) }) } This setup doesn\u0026rsquo;t change your layout:\nmyapp/ └── mypkg/ ├── greet.go # under package mypkg └── greet_test.go # under package mypkg If you prefer to separate these test types, you can move them into their own file while keeping them in the same package:\nmyapp/ └── mypkg/ ├── greet.go # under package mypkg ├── greet_test.go # under package mypkg └── greet_bench_fuzz_example.go # under package mypkg In this layout, greet_bench_fuzz_example.go houses the benchmarks, fuzz tests, and examples, but all files still declare the same package mypkg. These are regular unit tests with specialized entry points. See how packages like encoding/json or html organize their fuzz tests.\nIt\u0026rsquo;s not a strict rule to keep them in the same package. You can also put them in a _test package. The sort package, for example, keeps its examples in sort_test.\nAs mentioned in the testing docs, benchmarks are discovered and executed with the -bench flag, and fuzz tests with the -fuzz flag.\nIntegration and end-to-end tests When your project grows into multiple packages, you\u0026rsquo;ll want to verify that everything works together, not just in isolation. That\u0026rsquo;s where integration and end-to-end tests come in. They typically live outside the package tree because they often span multiple packages or processes.\nmyapp/ ├── mypkg/ │ ├── greet.go # under package mypkg │ └── greet_test.go # under package mypkg └── integration/ └── greet_integration_test.go # under package integration Here\u0026rsquo;s what one might look like:\npackage integration import ( \u0026#34;testing\u0026#34; \u0026#34;myapp/mypkg\u0026#34; // Explicitly import the SUT pkg to use its deps ) func TestGreetFlow(t *testing.T) { got := mypkg.Greet(\u0026#34;Integration\u0026#34;) want := \u0026#34;Hello, Integration\u0026#34; if got != want { t.Fatalf(\u0026#34;unexpected output: got %q, want %q\u0026#34;, got, want) } } Integration tests import real packages and test their interactions. They can spin up servers, connect to databases, or coordinate subsystems. The integration test packages are just like any other package: to communicate with any other package, it needs to be imported explicitly.\nYou\u0026rsquo;ll see this pattern in kubernetes, which has a test directory with subpackages like integration and e2e.\nHaving a top-level package for testing only makes sense if you\u0026rsquo;re testing multiple packages. Otherwise, if you\u0026rsquo;re writing integration or functional tests for a single package, you can still nest the tests under the SUT package. In this case, integration tests for mypkg can be tucked away under mypkg/test.\nClosing The general rule of thumb is:\nUnit tests stay in the same package as the code. Black-box tests use a _test package in the same directory. Examples, benchmarks, and fuzz tests live with the unit tests, though you may put them in _test if needed. Integration and end-to-end tests live outside the SUT package tree. The following tree attempts to capture the full picture:\nmyapp/ ├── mypkg/ │ ├── greet.go # mypkg - production code │ ├── greet_test.go # mypkg - unit \u0026amp; white-box tests │ ├── greet_external_test.go # mypkg_test - black-box tests │ └── greet_bench_fuzz_example.go # examples, benchmarks, fuzz └── integration/ └── greet_integration_test.go # integration or e2e tests ","permalink":"https://rednafi.com/go/organizing-tests/","summary":"\u003cp\u003eWhen it comes to test organization, Go\u0026rsquo;s standard \u003ccode\u003etesting\u003c/code\u003e library only gives you a few\noptions. I think that\u0026rsquo;s a great thing because there are fewer details to remember and fewer\nthings to onboard people to. However, during code reviews, I often see people contravene a\nfew common conventions around test organization, especially those who are new to the\nlanguage.\u003c/p\u003e\n\u003cp\u003eIf we distill the most common questions that come up when organizing tests, they are:\u003c/p\u003e","title":"Organizing Go tests"},{"content":"Go has support for subtests starting from version 1.7. With t.Run, you can nest tests, assign names to cases, and let the runner execute work in parallel by calling t.Parallel from subtests if needed.\nFor small suites, a flat set of t.Run calls is usually enough. That\u0026rsquo;s where I tend to begin. As the suite grows, your setup and teardown requirements may demand subtest grouping. There are multiple ways to handle that.\nOne option is to group subtests using nested t.Run. However, since t.Run supports arbitrary nesting, it\u0026rsquo;s easy to create tests that are hard to read and reason about, especially when each group has its own setup and teardown. When you add calls to t.Parallel, it can also become unclear which groups of tests run sequentially and which run in parallel.\nThis is all a bit hand wavy without examples. We\u0026rsquo;ll start with the simplest possible subtest grouping and work our way up. Coming up with examples that make the point while still fitting in a blog is tricky, so you\u0026rsquo;ll have to bear with my toy examples and use a bit of imagination.\nSystem under test (SUT) Let\u0026rsquo;s say we\u0026rsquo;re writing tests for a calculator that, for the sake of argument, can only do addition and multiplication. Instead of going for table-driven tests, we\u0026rsquo;ll split the tests for addition and multiplication into two groups using subtests. The reason being, let\u0026rsquo;s say addition and multiplication need different kinds of setup and teardown for some reason.\nI know I\u0026rsquo;m reaching, but bear with me. I\u0026rsquo;d rather make the point without dragging in mocks, databases, or testcontainers and getting lost in details. But you can find similar setup in a real codebase everywhere where you might be talking to a database and your read and write path have separate test lifecycles.\nKeep it flat until you can\u0026rsquo;t If we didn\u0026rsquo;t need different setup and teardown for the two groups, the simplest way to test a system would be through a set of table-driven tests:\nfunc TestCalc(t *testing.T) { // Common setup and teardown tests := []struct { name string got int want int }{ {\u0026#34;1+1=2\u0026#34;, 1 + 1, 2}, {\u0026#34;2+3=5\u0026#34;, 2 + 3, 5}, {\u0026#34;2*2=4\u0026#34;, 2 * 2, 4}, {\u0026#34;3*3=9\u0026#34;, 3 * 3, 9}, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if tt.got != tt.want { t.Fatalf(\u0026#34;got %d, want %d\u0026#34;, tt.got, tt.want) } }) } } Running the tests returns:\n--- PASS: TestCalc (0.00s) --- PASS: TestCalc/1+1=2 (0.00s) --- PASS: TestCalc/2+3=5 (0.00s) --- PASS: TestCalc/2*2=4 (0.00s) --- PASS: TestCalc/3*3=9 (0.00s) PASS Unrolling the tests would give you this. The following is equivalent to the above test suite:\nfunc TestCalc(t *testing.T) { // Common setup and teardown // Addition t.Run(\u0026#34;1+1=2\u0026#34;, func(t *testing.T) { if 1+1 != 2 { t.Fatal(\u0026#34;want 2\u0026#34;) } }) t.Run(\u0026#34;2+3=5\u0026#34;, func(t *testing.T) { if 2+3 != 5 { t.Fatal(\u0026#34;want 5\u0026#34;) } }) // Multiplication t.Run(\u0026#34;2*2=4\u0026#34;, func(t *testing.T) { if 2*2 != 4 { t.Fatal(\u0026#34;want 4\u0026#34;) } }) t.Run(\u0026#34;3*3=9\u0026#34;, func(t *testing.T) { if 3*3 != 9 { t.Fatal(\u0026#34;want 9\u0026#34;) } }) } Observe that all the subtests live at the same level. The names of the tests are the indicator of which function of the calculator they\u0026rsquo;re testing. But this obviously doesn\u0026rsquo;t allow us to have separate lifecycles for the addition and multiplication groups. There\u0026rsquo;s no grouping as of now.\nGroup subtests with nested t.Run when lifecycle diverges To allow different setup and teardown for addition and multiplication, we can introduce grouping by nesting the subtests via t.Run. Notice:\nfunc TestCalc(t *testing.T) { // Common setup and teardown t.Run(\u0026#34;addition\u0026#34;, func(t *testing.T) { // addition-specific setup defer func() { // addition-specific teardown }() t.Run(\u0026#34;1+1=2\u0026#34;, func(t *testing.T) { if 1+1 != 2 { t.Fatal(\u0026#34;want 2\u0026#34;) } }) t.Run(\u0026#34;2+3=5\u0026#34;, func(t *testing.T) { if 2+3 != 5 { t.Fatal(\u0026#34;want 5\u0026#34;) } }) }) t.Run(\u0026#34;multiplication\u0026#34;, func(t *testing.T) { // multiplication-specific setup defer func() { // multiplication-specific teardown }() t.Run(\u0026#34;2*2=4\u0026#34;, func(t *testing.T) { if 2*2 != 4 { t.Fatal(\u0026#34;want 4\u0026#34;) } }) t.Run(\u0026#34;3*3=9\u0026#34;, func(t *testing.T) { if 3*3 != 9 { t.Fatal(\u0026#34;want 9\u0026#34;) } }) }) } In this case, you can run the common setup and teardown in the top-level test function and the groups can have their own lifecycle operations alongside. Introducing the group also allows us to name them properly and they show up when we run the tests:\n--- PASS: TestCalc (0.00s) --- PASS: TestCalc/addition (0.00s) --- PASS: TestCalc/addition/1+1=2 (0.00s) --- PASS: TestCalc/addition/2+3=5 (0.00s) --- PASS: TestCalc/multiplication (0.00s) --- PASS: TestCalc/multiplication/2*2=4 (0.00s) --- PASS: TestCalc/multiplication/3*3=9 (0.00s) PASS From the output it\u0026rsquo;s clear which subtests belong to which group. This setup also allows you to run the groups in parallel by calling t.Parallel in each group.\nfunc TestCalc(t *testing.T) { // Common setup and teardown t.Run(\u0026#34;addition\u0026#34;, func(t *testing.T) { t.Parallel() }) t.Run(\u0026#34;multiplication\u0026#34;, func(t *testing.T) { t.Parallel() }) } Starting with flat subtests and nesting them one extra level with t.Run should suffice in the majority of cases. Readability of your tests usually starts hurting when you need to introduce any additional nesting.\nI almost always frown when I encounter more than two degrees of nesting in a test suite. On top of that, if your overly nested subtests start calling t.Parallel then it\u0026rsquo;s quite difficult to reason about the test execution flow. Plus, maintaining the lifecycles of the nested subgroups can get out of hand pretty quickly.\nBut even when you\u0026rsquo;re grouping subtests with two degrees of nesting, if the individual test logic starts getting longer, that might start hurting readability. Named functions for the subtests can help here in most cases.\nExtract subtest groups into functions We can rewrite the subtest grouping example of the previous section by extracting subtests into two group-specific functions like this:\nfunc TestCalc(t *testing.T) { // Common setup and teardown t.Run(\u0026#34;addition\u0026#34;, addgroup) t.Run(\u0026#34;multiplication\u0026#34;, multgroup) } func addgroup(t *testing.T) { // addition-specific setup defer func() { // addition-specific teardown }() t.Run(\u0026#34;1+1=2\u0026#34;, func(t *testing.T) { if 1+1 != 2 { t.Fatal(\u0026#34;want 2\u0026#34;) } }) t.Run(\u0026#34;2+3=5\u0026#34;, func(t *testing.T) { if 2+3 != 5 { t.Fatal(\u0026#34;want 5\u0026#34;) } }) } func multgroup(t *testing.T) { // multiplication-specific setup defer func() { // multiplication-specific teardown }() t.Run(\u0026#34;2*2=4\u0026#34;, func(t *testing.T) { if 2*2 != 4 { t.Fatal(\u0026#34;want 4\u0026#34;) } }) t.Run(\u0026#34;3*3=9\u0026#34;, func(t *testing.T) { if 3*3 != 9 { t.Fatal(\u0026#34;want 9\u0026#34;) } }) } All we did here is extract the groups into their own functions. Other than that this test is identical to the previous two-degree subtest grouping. You can call t.Parallel from the subgroup functions:\nfunc TestCalc(t *testing.T) { // Common setup and teardown // ... } func addgroup(t *testing.T) { // Run the group in parallel t.Parallel() } func multgroup(t *testing.T) { // Run the group in parallel t.Parallel() } Or you can bring the t.Parallel at the top-level test function:\nfunc TestCalc(t *testing.T) { // Common setup and teardown t.Run(\u0026#34;addition\u0026#34;, func(t *testing.T) { t.Parallel() addgroup(t) // addgroup doesn\u0026#39;t have t.Parallel }) t.Run(\u0026#34;multiplication\u0026#34;, func(t *testing.T) { t.Parallel() multgroup(t) // multgroup doesn\u0026#39;t have t.Parallel }) } That\u0026rsquo;s all there is to it. But some people don\u0026rsquo;t like the manual wiring that we needed to do in the top-level TestCalc function. Also, in a larger codebase, you\u0026rsquo;ll need some discipline to make sure the pattern is followed by others extending the code.\nSo often people want the subtest groups to be automatically discovered without them having to manually wire them in the main test function. While I\u0026rsquo;m not a big fan of automagical group discovery, I got curious about it nonetheless. The gRPC-go has a group discovery function that does this.\ngRPC-go uses reflection to discover groups If we were writing tests inside the grpc-go repository, we could lean on its small helper package, internal/grpctest, which reflects over a value you pass in, discovers methods whose names start with Test, and runs each of those as a subtest. Crucially, the helper also runs setup before and teardown after each discovered test method, which gives you a clear spot for per-group lifecycle work. The public surface is tiny: RunSubTests(t, x) plus a default hook carrier Tester that you embed to get Setup and Teardown.\nHere is our same calculator suite in that style, as if we were adding tests inside grpc-go:\n// NOTE: This import path only works inside the grpc-go repo family. // External modules cannot import google.golang.org/grpc/internal/*. package calc import ( \u0026#34;testing\u0026#34; \u0026#34;google.golang.org/grpc/internal/grpctest\u0026#34; ) // CalcSuite: embed grpctest.Tester so we get Setup and Teardown hooks. // The runner will discover TestAddition and TestMultiplication below. type CalcSuite struct{ grpctest.Tester } // TestAddition is discovered because the name starts with \u0026#34;Test\u0026#34;. func (CalcSuite) TestAddition(t *testing.T) { // addition-specific setup and teardown for this group defer func() { // tear down addition fixtures }() t.Run(\u0026#34;1+1=2\u0026#34;, func(t *testing.T) { if 1+1 != 2 { t.Fatal(\u0026#34;want 2\u0026#34;) } }) t.Run(\u0026#34;2+3=5\u0026#34;, func(t *testing.T) { if 2+3 != 5 { t.Fatal(\u0026#34;want 5\u0026#34;) } }) } // A second discovered group. func (CalcSuite) TestMultiplication(t *testing.T) { // multiplication-specific setup and teardown for this group defer func() { // tear down multiplication fixtures }() t.Run(\u0026#34;2*2=4\u0026#34;, func(t *testing.T) { if 2*2 != 4 { t.Fatal(\u0026#34;want 4\u0026#34;) } }) t.Run(\u0026#34;3*3=9\u0026#34;, func(t *testing.T) { // call t.Parallel() here if overlapping is safe if 3*3 != 9 { t.Fatal(\u0026#34;want 9\u0026#34;) } }) } // Top-level entry that \u0026#34;go test\u0026#34; sees. // RunSubTests reflects over CalcSuite, // then runs Setup, the test method, then Teardown. func TestCalc(t *testing.T) { grpctest.RunSubTests(t, CalcSuite{}) } Outside grpc-go you can\u0026rsquo;t import google.golang.org/grpc/internal/grpctest because it lives under an internal/ path. Go\u0026rsquo;s visibility rule only allows packages within that module tree to use it. If you want the subtest discoverer, there\u0026rsquo;s nothing stopping you from blatantly copying the code. It\u0026rsquo;s only a few dozen lines and devoid of any dependencies other than the leak checker. You can drop the file in your tests, remove the leak checker code if you don\u0026rsquo;t need that, adjust the import paths, and start using RunSubTests. To avoid repetition, I\u0026rsquo;ll leave that as an exercise to the reader.\nAnother thing to point out is that grpctest.RunSubTests doesn\u0026rsquo;t change the standard scheduler; you still opt into concurrency with t.Parallel() where it is safe.\nSubgroup with third party libraries If you like automatic subgroup discovery but want something you can use outside grpc-go, two common options are testify\u0026rsquo;s suite and Bloomberg\u0026rsquo;s go-testgroup. Both let you organize tests into named groups and keep per-group setup/teardown close to the cases.\nTestify\u0026rsquo;s suite Testify models a suite as a struct with Test* methods and gives you s.Run for subtests and assertion helpers.\npackage calc import ( \u0026#34;testing\u0026#34; \u0026#34;github.com/stretchr/testify/suite\u0026#34; ) type CalcSuite struct{ suite.Suite } func (s *CalcSuite) TestAddition() { s.Run(\u0026#34;1+1=2\u0026#34;, func() { s.Equal(2, 1+1) }) s.Run(\u0026#34;2+3=5\u0026#34;, func() { s.Equal(5, 2+3) }) } func (s *CalcSuite) TestMultiplication() { s.Run(\u0026#34;2*2=4\u0026#34;, func() { s.Equal(4, 2*2) }) s.Run(\u0026#34;3*3=9\u0026#34;, func() { s.Equal(9, 3*3) }) } func TestCalc(t *testing.T) { suite.Run(t, new(CalcSuite)) } One limitation is that the suite runner doesn\u0026rsquo;t support using t.Parallel to run the suite methods (TestAddition, TestMultiplication) in parallel. Bloomberg\u0026rsquo;s test group allows you to do that.\nBloomberg\u0026rsquo;s go-testgroup Bloomberg\u0026rsquo;s library also groups by methods, but passes a *testgroup.T and provides two runners so you can choose serial or parallel execution at the group level.\npackage calc import ( \u0026#34;testing\u0026#34; \u0026#34;github.com/bloomberg/go-testgroup\u0026#34; ) type CalcGroup struct{} func (g *CalcGroup) Addition(t *testgroup.T) { t.Run(\u0026#34;1+1=2\u0026#34;, func(t *testgroup.T) { t.Equal(2, 1+1) }) t.Run(\u0026#34;2+3=5\u0026#34;, func(t *testgroup.T) { t.Equal(5, 2+3) }) } func (g *CalcGroup) Multiplication(t *testgroup.T) { t.Run(\u0026#34;2*2=4\u0026#34;, func(t *testgroup.T) { t.Equal(4, 2*2) }) t.Run(\u0026#34;3*3=9\u0026#34;, func(t *testgroup.T) { t.Equal(9, 3*3) }) } func TestCalcSerial(t *testing.T) { testgroup.RunSerially(t, \u0026amp;CalcGroup{}) } // Or run in parallel. // Don\u0026#39;t call t.Parallel inside methods func TestCalcParallel(t *testing.T) { testgroup.RunInParallel(t, \u0026amp;CalcGroup{}) } RunInParallel handles group-level parallelism for you and documents not to mix in your own t.Parallel inside those methods.\nClosing While there are multiple ways to organize subtest groups, I try to keep them flat for as long as possible. When grouping becomes necessary, I gradually add a single extra level of nesting with t.Run.\nIn larger tests, extracting groups into their own named functions improves readability and maintainability quite a bit. I almost never use reflection-based wiring because that\u0026rsquo;s one extra bit of code to carry around.\nI also tend to eschew pulling in third-party test suites unless I am already working in a codebase that uses them. Tools like testify or go-testgroup require you to define a struct and attach tests to it. I prefer to keep tests as standalone functions. In addition, testing frameworks often develop into mini-languages of their own, which makes onboarding harder. Notice how different the APIs of testify suite and go-testgroup are despite doing pretty much the same thing.\nIn my experience, even in large codebases, a bit of discipline is usually enough to get by with manual subtest grouping.\n","permalink":"https://rednafi.com/go/subtest-grouping/","summary":"\u003cp\u003eGo has \u003ca href=\"https://go.dev/blog/subtests\"\u003esupport for subtests\u003c/a\u003e starting from version 1.7. With \u003ccode\u003et.Run\u003c/code\u003e, you can nest tests,\nassign names to cases, and let the runner execute work in parallel by calling \u003ccode\u003et.Parallel\u003c/code\u003e\nfrom subtests if needed.\u003c/p\u003e\n\u003cp\u003eFor small suites, a flat set of \u003ccode\u003et.Run\u003c/code\u003e calls is usually enough. That\u0026rsquo;s where I tend to\nbegin. As the suite grows, your setup and teardown requirements may demand subtest grouping.\nThere are multiple ways to handle that.\u003c/p\u003e","title":"Subtest grouping in Go"},{"content":"I like to make the distinction between application structure and architecture. Structure is how you organize the directories and packages in your app while architecture is how different components talk to each other. The way your app talks to other services in a fleet can also be called architecture.\nWhile structure often influences architecture and vice versa, this distinction is important. This post is strictly about application structure and not library structure. Library structure is often driven by different design pressures than their app counterparts. There are a ton of canonical examples of good library structure in the stdlib, but it\u0026rsquo;s app structure where things get a bit more muddy.\nAt work, I not only write Go in a distributed system environment but also review potential candidates\u0026rsquo; assignments in the hiring pipeline. While there is no objectively right or wrong way to structure an app, I do see a common pitfall in candidates\u0026rsquo; submissions that is usually frowned upon in a Go application.\nApp structure should be driven by what it does and not what it\u0026rsquo;s built with. Let the domain guide the structure, not technology or the current language specific zeitgeist.\nBen Johnson\u0026rsquo;s Standard Package Layout is a good reference for this. He points out why approaches like monolithic packages, Rails style layouts, or grouping by module don\u0026rsquo;t fit well in Go. Then he lays out a map where the root package holds domain types, dependencies are grouped in separate packages, and the main package wires everything together.\nWhile Ben\u0026rsquo;s post is focused on what you should be doing, I want to keep this discussion a bit more open-ended and just talk about one bad pattern that you probably should avoid. The rest of the app structure is subjective and should be driven by requirements. Use your judgement.\nThe mistake I often see is people making a bunch of generically named packages like models, controllers, handlers and stuffing everything there. App structure like the following is quite common:\nmystore/ ├── controllers/ │ ├── order_controller.go │ └── user_controller.go ├── models/ │ ├── order.go │ └── user.go ├── handlers/ │ ├── http_handler.go │ └── webhook_handler.go └── main.go In Go there\u0026rsquo;s no file level separation, only package level separation. That means everything under models like order and product lives in the same namespace. The same is true for controllers and handlers.\nOnce you put multiple business domains under a generic umbrella, you tie them together. This might make sense in a language like Python where file names are prefixed in the fully qualified import path. In Python you\u0026rsquo;d import them as follows:\n# Identifiers live in the order namespace from mystore.models import order # Identifiers live in the http_handler namespace from mystore.handlers import http_handler But in Go the import path becomes this:\n// Identifiers from order.go, user.go, product.go // all live in the same namespace import \u0026#34;mystore/models\u0026#34; // Identifiers from http_handler.go \u0026amp; webhook_handler.go // all live in the same namespace import \u0026#34;mystore/handlers\u0026#34; There is no file level delineation in Go. If you put different domains under the same models directory, there is no indication at import time what domain a model belongs to. The only clue is the identifier name. This isn\u0026rsquo;t ideal when you want clear separation between domains.\nIn Go, packages define your bounded context, not files within a package. Domains should be delineated by top level packages, not by file names.\nFor your top level business logic, you want package level separation between domains. Order logic should live in order, user logic should live in user. These packages will be imported in many places throughout the app, and keeping them separate keeps dependencies clear.\nIt could look like this:\nmystore/ ├── order/ \u0026lt;-- business logic related to the order domain │ ├── order.go │ └── service.go ├── user/ \u0026lt;-- business logic related to the user domain │ ├── user.go │ └── service.go └── cmd/ \u0026lt;-- wire everything here └── mystore/ └── main.go Each domain owns its own logic and optional adapters. If you need to find order related code, you go to order. If you need user code, you go to user. Nothing is smooshed together under a generic bucket.\nThe details around how you layer your app can differ based on requirements, but the important point is that your top level directories shouldn\u0026rsquo;t just be generic buckets containing all domains. That makes navigation harder. A better approach is letting the domain guide the structure and only layering in technology when it matters.\nYou can place your transport concerns alongside the top level packages. A top level http package can hold handlers that import service functions from the domain packages. You can put all handlers under http or split them into http/order and http/user. Both are valid choices. If you put all handlers under http, that\u0026rsquo;s fine because they are usually imported in one place where you wire routes. The same is true for database adapters. You can put them all under postgres or split them into postgres/order and postgres/user. Both patterns are acceptable. The key difference is that domains need package level separation, while technology packages can be grouped because they are only wired at the edge.\nmystore/ ├── order/ │ ├── order.go │ └── service.go ├── user/ │ ├── user.go │ └── service.go ├── http/ \u0026lt;-- lumping all the handlers here is fine │ ├── order_handler.go │ └── user_handler.go ├── postgres/ \u0026lt;-- this is fine, but you can create sub pkgs too │ ├── order_repo.go │ └── user_repo.go └── cmd/ └── server/ └── main.go But depending on the complexity of your app, this is also absolutely fine:\nmystore/ ├── order/ │ ├── order.go │ └── service.go ├── user/ │ ├── user.go │ └── service.go ├── http/ \u0026lt;-- handlers are split by domain here │ ├── order/ │ │ └── handler.go │ └── user/ │ └── handler.go ├── postgres/ \u0026lt;-- repos are split by domain here │ ├── order/ │ │ └── repo.go │ └── user/ │ └── repo.go └── cmd/ └── server/ └── main.go The rule of thumb is that top level domains should never import anything from technology folders like http or postgres. Instead, http and postgres should always import from domain packages. You can add a linter to enforce this rule but since Go doesn\u0026rsquo;t allow import cycles, this is automatically enforced by the compiler.\n+-----------+ +-----------+ | order | | user | +-----------+ +-----------+ ^ ^ | | +------------------------------+ | http postgres | +------------------------------+ ^ | +---------+ | cmd | +---------+ Domains sit at the top. Technology packages depend on them, never the other way around. The cmd package wires everything together. This keeps the graph simple and keeps domains independent.\nAstute readers might notice that I have left out any discussion around the internal directory. This is intentional. Depending on your requirements, you might opt in for an internal directory or not. This isn\u0026rsquo;t important for our discussion. The main point I wanted to emphasize is that technology or architecture patterns shouldn\u0026rsquo;t guide your app structure. It should be based on something more persistent and nothing is more persistent than your application\u0026rsquo;s domain.\n","permalink":"https://rednafi.com/go/app-structure/","summary":"\u003cp\u003eI like to make the distinction between application structure and architecture. Structure is\nhow you organize the directories and packages in your app while architecture is how\ndifferent components talk to each other. The way your app talks to other services in a fleet\ncan also be called architecture.\u003c/p\u003e\n\u003cp\u003eWhile structure often influences architecture and vice versa, this distinction is important.\nThis post is strictly about application structure and not library structure. Library\nstructure is often driven by different design pressures than their app counterparts. There\nare a ton of canonical examples of good library structure in the stdlib, but it\u0026rsquo;s app\nstructure where things get a bit more muddy.\u003c/p\u003e","title":"Let the domain guide your application structure"},{"content":"With the advent of LLMs, the temptation to churn out a flood of unit tests for a false veneer of productivity and protection is stronger than ever.\nMy colleague Matthias Doepmann recently fired a shot at AI-generated tests that don\u0026rsquo;t validate the behavior of the System Under Test (SUT) but instead create needless ceremony around internal implementations. At best, these tests give a shallow illusion of confidence in the system\u0026rsquo;s correctness while breaking at the smallest change. At worst, they remain green even when the SUT\u0026rsquo;s behavior changes.\nIn practice, they add maintenance overhead and drag down code reviews. The frustration in that post wasn\u0026rsquo;t about violating some abstract testing philosophy. It came from having to wade through countless implementation-checking tests churned out by LLMs across components of a real, large-scale distributed system.\nI think the problem persists for three reasons:\nFirst, many developers have begun defaulting to LLMs for generating tests. Regrettably, even in critical systems. In greenfield projects with no test baseline, AI agents often go rogue and churn out these cheap implementation-checking tests. Google calls them interaction tests. Second, the prevalence of mocking libraries encourages this pattern. They make it too easy to write tests that assert \u0026ldquo;which function called which\u0026rdquo; instead of \u0026ldquo;what actually happened.\u0026rdquo; Third, once these tests exist, they create inertia and people keep piling on the same style of tests to be consistent. Test state, not interactions The general theme when writing unit tests should be checking the behavior of the system, not the scaffolding of its implementation. It doesn\u0026rsquo;t matter which method called which, how many times, or with what arguments.\nWhat matters is: if you give the SUT some input, does it return the expected output? In a stateful system, does the input cause the system to mutate some persistence layer in the expected way? That persistence layer doesn\u0026rsquo;t always need to be a real database; it could be an in-memory buffer.\nIn scenarios where your code invokes external systems, it is more useful to test your system with canned responses from upstream calls rather than testing which method is being called.\nThe salient point is: test outcomes, not implementation details. As the book Software Engineering at Google puts it: test state, not interactions:\nWith state testing, you observe the system itself to see what it looks like after invoking with it. With interaction testing, you instead check that the system took an expected sequence of actions on its collaborators in response to invoking it. Many tests will perform a combination of state and interaction validation.\nAnd the guidance that follows:\nBy far the most important way to ensure this is to write tests that invoke the system being tested in the same way its users would; that is, make calls against its public API rather than its implementation details. If tests work the same way as the system\u0026rsquo;s users, by definition, change that breaks a test might also break a user.\nI think the first step in the right direction is to accept that LLMs can\u0026rsquo;t substitute for thought. The first few critical tests in your systems shouldn\u0026rsquo;t be written by LLMs and you must vet the tests churned out by the genie that wants to leap. Next up, you can often get away without a mocking library and more often than not, they improve the quality and maintainability of your tests.\nMocking libraries often don\u0026rsquo;t help Mocking libraries come with their own idiosyncratic syntax and workflows. On most occasions, handwritten fakes are better than mocks. I\u0026rsquo;ll use Go to make my point here because that\u0026rsquo;s what I write the most these days, but the lesson applies to other languages too.\nConsider a simple UserService that depends on a DB interface. Its job is to delegate user creation to the database and return any error to the caller:\n// usersvc/usersvc.go package usersvc import \u0026#34;errors\u0026#34; var ErrDuplicate = errors.New(\u0026#34;duplicate user\u0026#34;) type DB interface { InsertUser(name string) error ListUsers() []string } type UserService struct { db DB } func NewUserService(db DB) *UserService { return \u0026amp;UserService{db: db} } // Baseline behavior: delegate to DB and surface errors to callers. func (s *UserService) CreateUser(name string) error { return s.db.InsertUser(name) } A mocking tool such as mockery can generate a mock implementation of the DB interface. The generated code records calls and arguments so that tests can later assert whether the expected interactions happened:\n// usersvc/mocks/mock_db.go // generated by: // mockery --name=DB --dir=usersvc --output=usersvc/mocks \\ // --outpkg=mocks --with-expecter // simplified to remove unnecessary details package mocks import \u0026#34;github.com/stretchr/testify/mock\u0026#34; type MockDB struct{ mock.Mock } func (m *MockDB) InsertUser(name string) error { args := m.Called(name) return args.Error(0) } func (m *MockDB) ListUsers() []string { args := m.Called() return args.Get(0).([]string) } Using this mock, a test can be written to check that CreateUser interacts with the dependency in the expected way:\n// usersvc/usersvc_mock_test.go package usersvc_test import ( \u0026#34;testing\u0026#34; \u0026#34;github.com/stretchr/testify/require\u0026#34; \u0026#34;example.com/app/usersvc\u0026#34; \u0026#34;example.com/app/usersvc/mocks\u0026#34; ) func TestUserService_CreateUser(t *testing.T) { db := mocks.NewMockDB(t) svc := usersvc.NewUserService(db) // Exact interaction expected. db.EXPECT().InsertUser(\u0026#34;alice\u0026#34;).Return(nil).Once() // Exercise public API. err := svc.CreateUser(\u0026#34;alice\u0026#34;) require.NoError(t, err) // Verify the interaction occurred. db.AssertExpectations(t) } This works mechanically, but it breaks down in practice:\nIt checks the collaborator call, not the result\nA useful test would assert that \u0026ldquo;alice\u0026rdquo; was actually added or that a duplicate error was returned. This one only verifies that InsertUser(\u0026quot;alice\u0026quot;) was invoked once.\nIt breaks on harmless refactors\nIf the database method is renamed while keeping the same semantics, callers see no difference but the test fails:\n// usersvc/usersvc.go (harmless refactor, behavior unchanged) package usersvc type DB interface { UpsertUser(name string) error // was InsertUser ListUsers() []string } func (s *UserService) CreateUser(name string) error { return s.db.UpsertUser(name) // same public behavior } The mock-based test no longer compiles or needs rewiring, even though the public behavior didn\u0026rsquo;t change.\nAnd worse, it survives real bugs\nIf an error is accidentally swallowed, callers get the wrong signal but the test still passes:\n// usersvc/usersvc.go (buggy refactor: behavior changed) package usersvc func (s *UserService) CreateUser(name string) error { _ = s.db.InsertUser(name) // ignore error by mistake return nil // callers think it succeeded } A real DB or an in-memory fake would raise a constraint error that should propagate. The mock test goes green anyway because it only checked the call path.\nThe common thread is that mocks lock tests to implementation details. They don\u0026rsquo;t protect the behavior that real users rely on.\nInterface-guided design and fakes A better approach is to keep the same interface but back it with a handwritten fake. The fake encodes the domain rules you care about, and tests can focus on outcomes instead of verifying which collaborator methods were called.\nHere, we\u0026rsquo;re hand writing the fake implementation of the DB interface instead of generating it via a mockgen library.\n// usersvc/usersvc_fake_test.go package usersvc_test import \u0026#34;example.com/app/usersvc\u0026#34; type FakeDB struct { seen map[string]struct{} order []string } func NewFakeDB() *FakeDB { return \u0026amp;FakeDB{seen: make(map[string]struct{})} } func (f *FakeDB) InsertUser(name string) error { if _, ok := f.seen[name]; ok { return usersvc.ErrDuplicate } f.seen[name] = struct{}{} f.order = append(f.order, name) return nil } func (f *FakeDB) ListUsers() []string { out := make([]string, len(f.order)) copy(out, f.order) return out } Tests with the fake read like a statement of expected behavior:\n// usersvc/usersvc_fake_test.go package usersvc_test import ( \u0026#34;testing\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; \u0026#34;github.com/stretchr/testify/require\u0026#34; \u0026#34;example.com/app/usersvc\u0026#34; ) func TestUserService_CreateUser(t *testing.T) { db := NewFakeDB() svc := usersvc.NewUserService(db) require.NoError(t, svc.CreateUser(\u0026#34;alice\u0026#34;)) assert.Equal(t, []string{\u0026#34;alice\u0026#34;}, db.ListUsers()) // outcome observed } func TestUserService_CreateUser_DuplicateSurfaces(t *testing.T) { db := NewFakeDB() svc := usersvc.NewUserService(db) require.NoError(t, svc.CreateUser(\u0026#34;alice\u0026#34;)) err := svc.CreateUser(\u0026#34;alice\u0026#34;) require.ErrorIs(t, err, usersvc.ErrDuplicate) // behavior enforced assert.Equal(t, []string{\u0026#34;alice\u0026#34;}, db.ListUsers()) // state unchanged } This avoids the fragility of mocks. The tests survive harmless refactors, fail when behavior changes, and stay readable without a mocking DSL.\nBut the cost is maintaining the fake as the interface evolves. However, in practice, that\u0026rsquo;s still easier than constantly updating brittle mock expectations and occasionally dealing with the mock library\u0026rsquo;s lengthy migration workflow.\nFakes vs real systems Sometimes the right move is to test against a real database running in a container. That is still state testing, just at a higher fidelity. The tradeoff is speed: you get stronger confidence in behavior, but the tests run slower.\nMost of the time, handwritten in-memory fakes are what you need, and most tests should stick to those. When you do need the same behavior you would see in production, tools like testcontainers let you spin up databases, queues, or caches inside containers. Your tests can then call the SUT normally, with its configuration pointing at the containerized service, just as production code would connect to a production resource.\nParting words This is not a rally against using LLMs for tests. But the seed tests, the first handful that set the standard, need to come from you. They define what correctness means in your system and give the ensuing tests a model to follow. If you hand that job to an LLM, you give up the chance to shape how the rest of the suite grows.\nThis isn\u0026rsquo;t to disparage mocking libraries either. But I have seen people armed with overzealous LLMs and mocks wreak havoc on a test suite and then unironically ask reviewers to review the mess. Instead of validating behavior, the suite fills up with fragile interaction checks that break on refactors and stay green through real bugs.\nMore often than not, you can skip mocking libraries and rely on handwritten fakes that check the behavior of the SUT instead of its interactions. The next person that needs to read and extend your tests might thank you for that.\n","permalink":"https://rednafi.com/go/test-state-not-interactions/","summary":"\u003cp\u003eWith the advent of LLMs, the temptation to churn out a flood of unit tests for a false\nveneer of productivity and protection is stronger than ever.\u003c/p\u003e\n\u003cp\u003eMy colleague Matthias Doepmann recently fired \u003ca href=\"https://revontulet.dev/p/2025-dont-let-your-mocks-mock-you/\"\u003ea shot at AI-generated tests\u003c/a\u003e that don\u0026rsquo;t\nvalidate the behavior of the System Under Test (SUT) but instead create needless ceremony\naround internal implementations. At best, these tests give a shallow illusion of confidence\nin the system\u0026rsquo;s correctness while breaking at the smallest change. At worst, they remain\ngreen even when the SUT\u0026rsquo;s behavior changes.\u003c/p\u003e","title":"Test state, not interactions"},{"content":"At work, a common mistake I notice when reviewing candidates\u0026rsquo; home assignments is how they wire goroutines to channels and then return early.\nThe pattern usually looks like this:\nstart a few goroutines each goroutine sends a result to its own unbuffered channel in the main goroutine, read from those channels one by one if any read contains an error, return early The trap is the early return. With an unbuffered channel, a send blocks until a receiver is ready. If you return before reading from the remaining channels, the goroutines writing to them block forever. That\u0026rsquo;s a goroutine leak.\nHere\u0026rsquo;s how the bug appears in a tiny example: one worker intentionally fails, causing the main goroutine to bail early. That early return skips the receive from ch2, leaving the sender on ch2 stuck.\ntype result struct{ err error } func Example() error { ch1 := make(chan result) // unbuffered ch2 := make(chan result) // unbuffered // Simulate a failing worker by sending an error into ch1. // This is intentional to trigger the early return below. go func() { ch1 \u0026lt;- result{err: fmt.Errorf(\u0026#34;oops\u0026#34;)} }() // Simulate a successful worker that will try to send into ch2. go func() { ch2 \u0026lt;- result{err: nil} }() // Receive the first result. res1 := \u0026lt;-ch1 if res1.err != nil { // We return right away because of the error. // Because we never read from ch2, the goroutine sending to ch2 // is now blocked forever on its send. That goroutine leaks. return res1.err } // This receive is skipped on the error path above. res2 := \u0026lt;-ch2 if res2.err != nil { return res2.err } return nil } One simple fix is to make sure you always read from both channels before you decide what to do. This guarantees that every send has a matching receive and no goroutine gets stuck:\nfunc ExampleDrain() error { ch1 := make(chan result) ch2 := make(chan result) go func() { ch1 \u0026lt;- result{err: fmt.Errorf(\u0026#34;oops\u0026#34;)} }() // same failure go func() { ch2 \u0026lt;- result{err: nil} }() // same success // Always receive both. Both sends now complete. res1 := \u0026lt;-ch1 res2 := \u0026lt;-ch2 if res1.err != nil { return res1.err } if res2.err != nil { return res2.err } return nil } This is safe but it means you always wait for both workers even when the first one already failed and the second result is irrelevant. If you want to return early without leaking, another option is to use buffered channels so the producers don\u0026rsquo;t block on send. A buffer of size one is enough for this pattern.\nfunc ExampleBuffered() error { ch1 := make(chan result, 1) // buffered so sends do not block ch2 := make(chan result, 1) go func() { ch1 \u0026lt;- result{err: fmt.Errorf(\u0026#34;oops\u0026#34;)} }() // failure go func() { ch2 \u0026lt;- result{err: nil} }() // success // Receive the first result and decide. res1 := \u0026lt;-ch1 if res1.err != nil { // Safe to return early. The send to ch2 already completed // into its buffer even though we have not read it yet. return res1.err } // Still read ch2 to consume its buffered value res2 := \u0026lt;-ch2 if res2.err != nil { return res2.err } return nil } Buffered channels remove the blocked send, but they also make it easier to forget that a second result exists at all. If that second value carries data you must process, you should still receive it. If it is truly fire and forget, buffering is fine.\nOften the cleanest approach is to drop the channel plumbing when you only need to run tasks and aggregate errors. The errgroup package lets each goroutine return an error while the group does the waiting. There is nothing to forget to receive, so there is nothing to leak.\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;golang.org/x/sync/errgroup\u0026#34; ) func ExampleErrgroup() error { var g errgroup.Group // Task 1 fails and returns an error. g.Go(func() error { return fmt.Errorf(\u0026#34;oops\u0026#34;) }) // Task 2 succeeds. g.Go(func() error { return nil }) // Wait waits for both tasks and returns the first error, if any. return g.Wait() } Sometimes you also want peers to stop once one task fails. errgroup.WithContext gives you a context that gets canceled as soon as any task returns an error. You pass that context into your workers and have them check ctx.Done() so they can exit quickly.\nimport ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;golang.org/x/sync/errgroup\u0026#34; ) func ExampleErrgroupWithContext() error { // When any task returns an error, ctx is canceled. g, ctx := errgroup.WithContext(context.Background()) // Task 1 fails quickly to simulate an early error. g.Go(func() error { return fmt.Errorf(\u0026#34;oops\u0026#34;) }) // Task 2 is long running but cooperates with cancellation. g.Go(func() error { for { select { case \u0026lt;-ctx.Done(): // Exits because Task 1 failed and canceled the context. return ctx.Err() default: time.Sleep(10 * time.Millisecond) } } }) return g.Wait() } At this point it is natural to ask if tools can catch the original bug for you. go vet cannot. Vet is static analysis that runs at build time. Whether a send blocks depends on runtime control flow and timing. Vet cannot prove that the function returns before a particular receive in a general way, so it doesn\u0026rsquo;t flag this pattern.\ngo test -race cannot either. The race detector detects unsynchronized concurrent memory access. A goroutine stuck on a channel send isn\u0026rsquo;t a data race. You may see a test hang until timeout, but the tool won\u0026rsquo;t point to a leaking goroutine.\nYou can turn this into a failing test with goleak from Uber. goleak fails if goroutines are still alive when a test ends. It snapshots all goroutines via the runtime, filters out the standard background ones, and reports the rest. Wire it into a test that triggers the early return and you will see the blocked sender\u0026rsquo;s stack in the output.\nHere is a test that leaks and fails:\npackage example_test import ( \u0026#34;fmt\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;go.uber.org/goleak\u0026#34; ) type result struct{ err error } func buggyEarlyReturn() error { ch1 := make(chan result) ch2 := make(chan result) // Force the early-return path by sending an error on ch1. go func() { ch1 \u0026lt;- result{err: fmt.Errorf(\u0026#34;oops\u0026#34;)} }() // This send will block forever on the failing path // because nobody receives ch2. go func() { ch2 \u0026lt;- result{err: nil} }() r1 := \u0026lt;-ch1 if r1.err != nil { return r1.err // leak: ch2 sender is stuck } _ = \u0026lt;-ch2 return nil } func TestBuggyLeaks(t *testing.T) { // fails if any goroutines are stuck at test end defer goleak.VerifyNone(t) _ = buggyEarlyReturn() } This test fails and prints the goroutine stack stuck in the send to ch2.\n=== RUN TestBuggyLeaks main_test.go:34: found unexpected goroutines: [Goroutine 24 in state chan send, with thing.buggyEarlyReturn.func2 on top of the stack: thing.buggyEarlyReturn.func2() .../main_test.go:20 +0x28 created by thing.buggyEarlyReturn in goroutine 22 .../main_test.go:20 +0xc0 ] --- FAIL: TestBuggyLeaks (0.44s) FAIL exit status 1 If you switch the implementation to a fixed version, the test passes. For example, the draining fix:\nfunc fixedDrain() error { ch1 := make(chan result) ch2 := make(chan result) go func() { ch1 \u0026lt;- result{err: fmt.Errorf(\u0026#34;oops\u0026#34;)} }() go func() { ch2 \u0026lt;- result{err: nil} }() r1 := \u0026lt;-ch1 r2 := \u0026lt;-ch2 if r1.err != nil { return r1.err } if r2.err != nil { return r2.err } return nil } func TestFixedNoLeaks(t *testing.T) { defer goleak.VerifyNone(t) _ = fixedDrain() } If you prefer suite wide enforcement, add goleak to your TestMain. This way your entire test run fails if any test leaks goroutines.\npackage main import ( \u0026#34;os\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;go.uber.org/goleak\u0026#34; ) func TestMain(m *testing.M) { // VerifyTestMain wraps the whole test run // and fails if any goroutines are left behind. goleak.VerifyTestMain(m) } If you start goroutines that send on channels, think carefully about early returns. An unbuffered send waits for a receive, and if you return before that receive happens, you\u0026rsquo;ve leaked a goroutine.\nYou can avoid this by:\nalways draining all channels buffering intentionally so sends don\u0026rsquo;t block or using errgroup, with or without context, so tasks return errors and cooperate on cancellation Add goleak to your tests so leaks surface early during development.\n","permalink":"https://rednafi.com/go/early-return-and-goroutine-leak/","summary":"\u003cp\u003eAt work, a common mistake I notice when reviewing candidates\u0026rsquo; home assignments is how they\nwire goroutines to channels and then return early.\u003c/p\u003e\n\u003cp\u003eThe pattern usually looks like this:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003estart a few goroutines\u003c/li\u003e\n\u003cli\u003eeach goroutine sends a \u003ccode\u003eresult\u003c/code\u003e to its own unbuffered channel\u003c/li\u003e\n\u003cli\u003ein the main goroutine, read from those channels one by one\u003c/li\u003e\n\u003cli\u003eif any read contains an error, return early\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe trap is the early return. With an unbuffered channel, a send blocks until a receiver is\nready. If you return before reading from the remaining channels, the goroutines writing to\nthem block forever. That\u0026rsquo;s a goroutine leak.\u003c/p\u003e","title":"Early return and goroutine leak"},{"content":"Unlike pytest or JUnit, Go\u0026rsquo;s standard testing framework doesn\u0026rsquo;t give you as many knobs for tuning the lifecycle of your tests.\nBy lifecycle I mean the usual setup and teardown hooks or fixtures that are common in other languages. I think this is a good thing because you don\u0026rsquo;t need to pick up many different framework-specific workflows for something so fundamental.\nGo gives you enough hooks to handle this with less ceremony. But it can still be tricky to figure out the right conventions for setup and teardown that don\u0026rsquo;t look odd to other Gophers, especially if you haven\u0026rsquo;t written Go for a while. This text explores some common ways to do lifecycle management in your Go tests.\nBefore we cover multiple testing scenarios, it\u0026rsquo;s useful to understand how Go\u0026rsquo;s test harness actually runs your tests.\nHow Go discovers and runs your tests When you type go test, Go doesn\u0026rsquo;t interpret test files directly. It collects all the _test.go files in a package, compiles them together with the rest of the package, and produces a temporary binary. That binary contains both your code and your tests, along with a small harness that drives them. The harness then runs the binary and reports results.\nFrom the \u0026ldquo;go test\u0026rdquo; command doc:\n\u0026ldquo;go test\u0026rdquo; automates testing the packages named by the import paths. [\u0026hellip;] recompiles each package along with any files with names matching the file pattern \u0026ldquo;*_test.go\u0026rdquo;.\nDiscovery Inside each package, the harness looks for test functions. A function qualifies if it has the form:\nfunc TestXxx(t *testing.T) where Xxx starts with an uppercase letter. There are no annotations or decorators, just naming convention. Functions that don\u0026rsquo;t match this signature are ignored.\nExecution By default, the harness runs tests sequentially. If you want concurrency, you can opt in at the test level. Calling t.Parallel() inside a test signals that this test may run alongside others in the same package that also call t.Parallel(). Tests that don\u0026rsquo;t opt in remain strictly ordered.\nScope of binaries Every package with tests produces its own binary, and those binaries are run independently. There is no global suite that links packages together, so setup and teardown only exist inside one package\u0026rsquo;s process. If you have ten packages containing tests, you get ten binaries, each with its own lifecycle.\nFor example:\nproject/ ├── go.mod ├── db/ │ ├── db.go │ └── db_test.go └── api/ ├── api.go └── api_test.go Running go test ./... produces two binaries: one for db and one for api. Each binary bundles the package code and its tests, and each binary runs on its own. The harness aggregates the results and prints a combined report, but execution itself is confined to the package.\nIt is important to note that there is no file-level scope. All _test.go files in a package are merged into a single binary, so there is no way to run setup once per file. Similarly, there is no cross-package scope. Go does not let you set up once for all tests in a module or tear down after the last package finishes. If you need orchestration across packages, it has to happen outside of go test, for example in a shell script or a CI pipeline step.\nWith this background, we can now look at the lifecycle hooks Go does provide. They apply at three levels: per test function, per group of subtests, and per package.\nThree different scopes Typically you need to perform setup and teardown before and after:\neach test function is executed (single test function scope) a group of tests is executed (multiple test function scope) the full test suite is executed (test package scope) Per-test setup and teardown The smallest scope is the test function itself. You create resources at the start of the test and clean them up when it ends. This pattern is common when you want each test to run against a fresh state with no leakage from other tests. The idiomatic way in Go is to wrap the setup in a helper and register the cleanup with t.Cleanup.\ntype TestDB struct{} // newTestDB sets up a fresh database for a single test func newTestDB(t *testing.T) *TestDB { t.Helper() db := \u0026amp;TestDB{} // cleanup tied to the function scope t.Cleanup(func() { db.Close() }) return db } func (db *TestDB) Close() {} func (db *TestDB) Insert(k, v string) error { return nil } func (db *TestDB) Query(k string) (string, error) { return \u0026#34;value\u0026#34;, nil } func TestInsert(t *testing.T) { db := newTestDB(t) // new DB created for this test only if err := db.Insert(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;); err != nil { t.Fatalf(\u0026#34;insert failed: %v\u0026#34;, err) } } In this example, TestInsert gets its own new database. The cleanup registered with t.Cleanup makes sure the database is closed when the test finishes. The resource is never shared with other tests, which gives you strong isolation. The downside is that if your setup is expensive, it will run before and after every test function, which can slow things down.\nGrouped setup and teardown with subtests The next scope is a group of subtests. Instead of repeating setup for every test, you create the resource once in the parent test and share it with the children. Teardown runs when the parent finishes. This works well when you want to test a flow of operations against the same shared state.\nfunc TestUserFlow(t *testing.T) { // new DB created once for this group // t.Cleanup() gets called after all the subtests finish and // the parent returns db := newTestDB(t) t.Run(\u0026#34;insert user\u0026#34;, func(t *testing.T) { if err := db.Insert(\u0026#34;user:1\u0026#34;, \u0026#34;alice\u0026#34;); err != nil { t.Fatal(err) } }) t.Run(\u0026#34;query user\u0026#34;, func(t *testing.T) { val, err := db.Query(\u0026#34;user:1\u0026#34;) if err != nil { t.Fatal(err) } if val != \u0026#34;alice\u0026#34; { t.Fatalf(\u0026#34;expected alice, got %s\u0026#34;, val) } }) } Here both subtests share the same database, and the cleanup runs once when TestUserFlow ends. This is useful when your tests need to act on shared state, like inserting a record and then querying it. The trade-off is that the tests are no longer fully independent, and if one subtest leaves the database in a bad state, others may fail in unexpected ways.\nPackage-wide setup and teardown with TestMain The broadest scope is the package. If you define TestMain, the test harness calls it instead of running the tests directly. You can perform setup, run all the tests, and then perform teardown. This allows you to reuse an expensive resource across all tests in the package.\nvar globalDB *TestDB func TestMain(m *testing.M) { globalDB = \u0026amp;TestDB{} // setup once for the entire package code := m.Run() globalDB.Close() // teardown after all tests os.Exit(code) } func TestGlobalInsert(t *testing.T) { if err := globalDB.Insert(\u0026#34;k\u0026#34;, \u0026#34;v\u0026#34;); err != nil { t.Fatal(err) } } Here the database is created once and reused by all tests in the package. The teardown runs when everything is finished. This can make your tests run much faster if setup is expensive, but you pay for it in global (package wide) state. If one test mutates the shared resource in an unexpected way, other tests may start failing, and debugging those failures can be difficult.\nAlso, remember your setup and teardown are still package bound, meaning each package can have its own TestMain. Reasoning about their order can get out of hand quickly. Make sure your tests never depends on the order of TestMain execution. Treat these like init functions and use them sparingly.\nCombining the levels These three scopes are not mutually exclusive. You can combine them when you need different levels of control. A typical pattern is to have TestMain start a package-wide service, create a shared schema or fixture in a parent test for a group of related subtests, and then still use per-test setup inside individual subtests for fine-grained isolation. Each call to newTestDB creates a fresh database, so using it at different levels produces different resources with different lifetimes.\nfunc TestOrders(t *testing.T) { schema := newTestDB(t) // group-level DB shared across subtests t.Run(\u0026#34;create order\u0026#34;, func(t *testing.T) { db := newTestDB(t) // per-test DB, fresh for this subtest only db.Insert(\u0026#34;order:1\u0026#34;, \u0026#34;widget\u0026#34;) }) t.Run(\u0026#34;query order\u0026#34;, func(t *testing.T) { // uses the group-level DB, so the state persists across subtests schema.Insert(\u0026#34;order:1\u0026#34;, \u0026#34;widget\u0026#34;) val, _ := schema.Query(\u0026#34;order:1\u0026#34;) if val != \u0026#34;widget\u0026#34; { t.Fatalf(\u0026#34;expected widget, got %s\u0026#34;, val) } }) } In this example, TestMain could be running a package-wide database server. The parent test TestOrders sets up a schema that is shared across its subtests. Inside, one subtest spins up its own per-test database to work in isolation, while another uses the shared schema to test how state persists across operations.\nThe combination of package, group, and function scopes gives you flexibility: reuse expensive resources when you need to, and isolate state when correctness depends on it. However, combining scopes can be hard to reason about when you have many different subtests under a single parent that are also interacting with some global state. I tend to avoid this whenever possible.\nParting words Most of your setup and teardown should happen at the function level. That gives you the strongest isolation and keeps each test self-contained.\nThe next most useful pattern is at the subtest group level, where you create a resource once in a parent test and let its children share it. Cleanup runs when the parent finishes, which makes sense when you really do want that shared state.\nPackage-level setup through TestMain should be rare. It is tempting when setup is expensive, but global state is the fastest way to end up with brittle tests. Mixing different scopes is possible, but usually creates more confusion than clarity, so reach for it only when you have no better option.\n","permalink":"https://rednafi.com/go/lifecycle-management-in-tests/","summary":"\u003cp\u003eUnlike pytest or JUnit, Go\u0026rsquo;s standard testing framework doesn\u0026rsquo;t give you as many knobs for\ntuning the lifecycle of your tests.\u003c/p\u003e\n\u003cp\u003eBy lifecycle I mean the usual setup and teardown hooks or fixtures that are common in other\nlanguages. I think this is a good thing because you don\u0026rsquo;t need to pick up many different\nframework-specific workflows for something so fundamental.\u003c/p\u003e\n\u003cp\u003eGo gives you enough hooks to handle this with less ceremony. But it can still be tricky to\nfigure out the right conventions for setup and teardown that don\u0026rsquo;t look odd to other\nGophers, especially if you haven\u0026rsquo;t written Go for a while. This text explores some common\nways to do lifecycle management in your Go tests.\u003c/p\u003e","title":"Lifecycle management in Go tests"},{"content":"No matter which language you\u0026rsquo;re writing your service in, it\u0026rsquo;s generally a good idea to separate your external dependencies from your business-domain logic. Let\u0026rsquo;s say your order service needs to make an RPC call to an external payment service like Stripe when a customer places an order.\nUsually in Go, people make a package called external or http and stash the logic of communicating with external services there. Then the business logic depends on the external package to invoke the RPC call. This is already better than directly making RPC calls inside your service functions, as that would make these two separate concerns (business logic and external-service wrangling) tightly coupled. Testing these concerns in isolation, therefore, would be a lot harder.\nWhile this is a fairly common practice, I was looking for a canonical name for this pattern to talk about it in a less hand-wavy way. Turns out Martin Fowler wrote a blog post on it a few moons ago, and he calls it the Gateway pattern. He explores the philosophy in more detail and gives some examples in JS. However, I thought that Gophers could benefit from a few examples to showcase how it translates to Go. Plus, I wanted to reify the following axiom:\nHigh-level modules should not depend on low-level modules. Both should depend on abstractions. Abstractions should not depend on details. Details should depend on abstractions.\n— Dependency inversion principle (D in SOLID), Uncle Bob\nIn this scenario, our business logic in the order package is the high-level module and external is the low-level module, as the latter concerns itself with transport details. Inside external, we could communicate with the external dependencies via either HTTP or gRPC. But that\u0026rsquo;s an implementation detail and shouldn\u0026rsquo;t make any difference to the high-level order package.\norder will communicate with external via a common interface. This is how we satisfy the \u0026ldquo;both should depend on abstractions\u0026rdquo; part of the ethos.\nOur app layout looks like this:\nyourapp/ ├── cmd/ # wire up the deps │ └── main.go ├── order/ # business logic in the service functions │ ├── service.go │ └── service_test.go ├── external/ # code to communicate with external deps │ └── stripe/ │ ├── gateway.go │ ├── mock_gateway.go │ └── gateway_test.go └── go.mod / go.sum Let\u0026rsquo;s walk through the flow from the bottom up. Think about walking back from the edge to the core, as in Alistair Cockburn\u0026rsquo;s Hexagonal Architecture lingo where edge represents the transport logic and core implies the business concerns.\nThe Stripe implementation lives in external/stripe/gateway.go. For simplicity\u0026rsquo;s sake, we\u0026rsquo;re pretending to call the Stripe API over HTTP, but this could be a gRPC call to another service.\n// external/stripe/gateway.go package stripe import \u0026#34;fmt\u0026#34; type StripeGateway struct { APIKey string } func NewStripeGateway(apiKey string) *StripeGateway { return \u0026amp;StripeGateway{APIKey: apiKey} } // Handle all the details of making HTTP calls to the Stripe service here. func (s *StripeGateway) Charge( amount int64, currency string, source string) (string, error) { fmt.Printf( \u0026#34;[Stripe] Charging %d %s to card %s\\n\u0026#34;, amount, currency, source, ) return \u0026#34;txn_live_123\u0026#34;, nil } // Make another HTTP call to the Stripe service to perform a refund. func (s *StripeGateway) Refund(transactionID string) error { fmt.Printf(\u0026#34;[Stripe] Refunding transaction %s\\n\u0026#34;, transactionID) return nil } Notice that the stripe package handles the details of communicating with the Stripe endpoint, but it doesn\u0026rsquo;t export any interface for the higher-level module to use. This is intentional.\nIn Go, the general advice is that the consumer should define the interface they want, not the provider.\nGo interfaces generally belong in the package that uses values of the interface type, not the package that implements those values.\n— Go code review comments\nThat gives the consumer full control over what it wants to depend on, and nothing more. You don\u0026rsquo;t accidentally couple your code to a bloated interface just because the implementation provided one. You define exactly the shape you need and mock that in your tests.\nClients should not be forced to depend on methods they do not use.\n— Interface segregation principle (I in SOLID), Uncle Bob\nSo, in the order package, we define a tiny private interface that reflects the use case.\n// order/service.go package order // The order service only requires the Charge method of a payment gateway. // So we define a tiny interface here on the consumer side rather // than on the producer side type paymentGateway interface { Charge(amount int64, currency string, source string) (string, error) } type Service struct { gateway paymentGateway } // Pass the Stripe implementation of paymentGateway at runtime here. func NewService(gateway paymentGateway) *Service { return \u0026amp;Service{gateway: gateway} } // In production, this calls .Charge on the Stripe implementation. // During tests, it calls .Charge on a mock gateway. func (s *Service) Checkout(amount int64, source string) error { _, err := s.gateway.Charge(amount, \u0026#34;USD\u0026#34;, source) return err } The order service doesn\u0026rsquo;t know or care which implementation of the gateway it\u0026rsquo;s using to perform some action. It just knows it can call Charge on the provided gateway type. It doesn\u0026rsquo;t need to care about the Refund method on the Stripe gateway implementation. Also, the paymentGateway interface is bound to the order package, so we\u0026rsquo;re not polluting the API surface with a bunch of tiny interfaces.\nNow, when testing the service logic, you just need to write a tiny mock implementation of paymentGateway and pass it to order.Service. You don\u0026rsquo;t need to reach into the external/stripe package or wire up anything complicated. You can place the fake right next to your service test. Since interface implementations in Go are implicitly satisfied, everything just works without much fuss.\n// order/service_test.go package order_test import ( \u0026#34;testing\u0026#34; \u0026#34;yourapp/order\u0026#34; ) type mockGateway struct { calledAmount int64 calledSource string } func (m *mockGateway) Charge( amount int64, currency, source string) (string, error) { m.calledAmount = amount m.calledSource = source return \u0026#34;txn_mock\u0026#34;, nil } func TestCheckoutCallsCharge(t *testing.T) { mock := \u0026amp;mockGateway{} svc := order.NewService(mock) err := svc.Checkout(1000, \u0026#34;test_source_abc\u0026#34;) if err != nil { t.Fatalf(\u0026#34;unexpected error: %v\u0026#34;, err) } if mock.calledAmount != 1000 { t.Errorf(\u0026#34;expected amount 1000, got %d\u0026#34;, mock.calledAmount) } if mock.calledSource != \u0026#34;test_source_abc\u0026#34; { t.Errorf(\u0026#34;want source test_source_abc, got %s\u0026#34;, mock.calledSource) } } The test is focused only on what matters: Does the service call Charge with the correct arguments? We\u0026rsquo;re not testing Stripe here. That\u0026rsquo;s its own concern.\nYou can still write tests for the Stripe client if you want. You\u0026rsquo;d do that in external/stripe/gateway_test.go.\n// external/stripe/gateway_test.go package stripe_test import ( \u0026#34;testing\u0026#34; \u0026#34;yourapp/external/stripe\u0026#34; ) func TestStripeGateway_Charge(t *testing.T) { gw := stripe.NewStripeGateway(\u0026#34;dummy-key\u0026#34;) txn, err := gw.Charge(1000, \u0026#34;USD\u0026#34;, \u0026#34;tok_abc\u0026#34;) if err != nil { t.Fatalf(\u0026#34;unexpected error: %v\u0026#34;, err) } if txn == \u0026#34;\u0026#34; { t.Fatal(\u0026#34;expected transaction ID, got empty string\u0026#34;) } } Finally, everything is wired together in cmd/main.go.\n// cmd/main.go package main import ( \u0026#34;yourapp/external/stripe\u0026#34; \u0026#34;yourapp/order\u0026#34; ) func main() { stripeGw := stripe.NewStripeGateway(\u0026#34;live-api-key\u0026#34;) // Passing the real Stripe gateway to the order service. orderSvc := order.NewService(stripeGw) _ = orderSvc.Checkout(5000, \u0026#34;tok_live_card_xyz\u0026#34;) } It\u0026rsquo;s also common to call gateways \u0026ldquo;client.\u0026rdquo; Some people prefer that name. However, I think client is way overloaded, which makes it hard to discuss the pattern clearly. There\u0026rsquo;s the HTTP client, the gRPC client, and then your own client that wraps these. It gets confusing fast. I prefer \u0026ldquo;gateway,\u0026rdquo; as Martin Fowler used in his original text.\nIn Go context, the core idea is that a service function uses a locally defined gateway interface to communicate with external gateway providers. This way, the service and the external providers are unaware of each other\u0026rsquo;s existence and can be tested independently.\n","permalink":"https://rednafi.com/go/gateway-pattern/","summary":"\u003cp\u003eNo matter which language you\u0026rsquo;re writing your service in, it\u0026rsquo;s generally a good idea to\nseparate your external dependencies from your business-domain logic. Let\u0026rsquo;s say your \u003cem\u003eorder\nservice\u003c/em\u003e needs to make an RPC call to an external \u003cem\u003epayment service\u003c/em\u003e like Stripe when a\ncustomer places an order.\u003c/p\u003e\n\u003cp\u003eUsually in Go, people make a package called \u003ccode\u003eexternal\u003c/code\u003e or \u003ccode\u003ehttp\u003c/code\u003e and stash the logic of\ncommunicating with external services there. Then the business logic depends on the\n\u003ccode\u003eexternal\u003c/code\u003e package to invoke the RPC call. This is already better than directly making RPC\ncalls inside your service functions, as that would make these two separate concerns\n(business logic and external-service wrangling) tightly coupled. Testing these concerns in\nisolation, therefore, would be a lot harder.\u003c/p\u003e","title":"Gateway pattern for external service calls"},{"content":"As your test suite grows, you need ways to toggle certain kinds of tests on or off. Maybe you want to enable snapshot tests, skip long-running integration tests, or switch between real services and mocks. In every case, you\u0026rsquo;re really saying, \u0026ldquo;Run this test only if X is true.\u0026rdquo;\nSo where does X come from?\nI like to rely on Go\u0026rsquo;s standard tooling so that integration and snapshot tests can live right beside ordinary unit tests. Because I usually run these heavier tests in testcontainers, I don\u0026rsquo;t always want them running while I\u0026rsquo;m iterating on a feature or chasing a bug. So I need to enable them in an optional manner.\nTo fetch the X and conditionally run some tests, you\u0026rsquo;ll typically see three approaches:\nBuild tags – place integration or snapshot tests in files guarded by build tags, so they\u0026rsquo;re compiled only when you include the tag. Environment variables – have each test look for an environment variable (e.g., RUN_INTEGRATION=1) and skip itself if it\u0026rsquo;s absent. Custom go test flags (my preferred approach) – define your own flags so you can run, for example, go test -run Integration -integration. Build tags are hard to discover Build tags are special comments you place at the top of a .go file to tell Go to include that file only when certain tags are set during the build. This is how they typically look:\n//go:build snapshot package main import \u0026#34;testing\u0026#34; func TestSnapshot(t *testing.T) { t.Log(\u0026#34;running snapshot\u0026#34;) } This file will only be compiled and included when you run:\ngo test -tags=snapshot If you don\u0026rsquo;t pass the tag, the file is skipped entirely during the build. Go won\u0026rsquo;t even see the test.\nThe upside is that it gives you a clean separation. You can group slow tests or environment-dependent tests into their own files. But the downsides add up quickly.\nFirst, there\u0026rsquo;s no way to discover which tags are used without grepping through the codebase. Go itself won\u0026rsquo;t tell you. go help test doesn\u0026rsquo;t mention them. There\u0026rsquo;s no built-in list or summary. You need to solely depend on documentation.\nSecond, build tags are applied per file, not per package. That means if even one test in a file is guarded by a tag, the entire file is excluded unless the tag is passed. This makes it difficult to mix optional and always-on tests in the same file.\nAnd third, once you have more than a couple of tags, managing them becomes guesswork. You end up running things like:\ngo test -tags=slow,mock,external But you no longer remember what each one does or what combinations are safe. There\u0026rsquo;s no validation. It gets messy fast.\nEnvvars are a bit better Environment variables let you control test behavior at runtime. You don\u0026rsquo;t need to recompile anything, and you can pass them inline when running tests.\nHere\u0026rsquo;s a typical example:\nimport \u0026#34;os\u0026#34; func TestSnapshot(t *testing.T) { if os.Getenv(\u0026#34;SNAPSHOT\u0026#34;) != \u0026#34;1\u0026#34; { t.Skip(\u0026#34;set SNAPSHOT=1 to run this test\u0026#34;) } t.Log(\u0026#34;running snapshot\u0026#34;) } You run it like:\nSNAPSHOT=1 go test -v This is more dynamic than build tags. You don\u0026rsquo;t have to split tests into separate files, and you don\u0026rsquo;t have to rebuild with special flags. More importantly, the test itself can detect when the environment variable is missing and tell you what to do. It can skip itself and print a message like \u0026ldquo;set SNAPSHOT=1 to run this test.\u0026rdquo; That feedback loop is helpful.\nBut the discovery problem remains. There\u0026rsquo;s no built-in way to ask, \u0026ldquo;what environment variables does this test suite support?\u0026rdquo; You still have to read the code to find out.\nIt can get worse if the check is buried deep in a helper. Maybe some setup logic does:\nif os.Getenv(\u0026#34;SNAPSHOT\u0026#34;) == \u0026#34;1\u0026#34; { useRealService() } Now the test runs, but the behavior changes silently based on the environment. Nothing in the test output tells you that the envvar was involved. You may not even realize that you\u0026rsquo;re running in a different mode.\nAnd just like with build tags, there\u0026rsquo;s no central registry. No docs or summary. You can only hope someone left a good comment or wrote it down somewhere.\nCustom flags are almost always better The cleanest and most discoverable way to control optional test behavior in Go is by defining your own test flags. They\u0026rsquo;re typed, explicit, and work well with Go\u0026rsquo;s built-in tooling. Instead of toggling tests with magic file-level build tags or invisible environment variables, you can wire up test configuration using the flag package, just like any other Go binary.\nThere are two common approaches for defining test flags:\nPackage-level flags via TestMain Per-file flags via init(). Both approaches register the flag in the global flag set, so every test in the package can see the value once parsing has happened. The trade-off is indirection versus locality: TestMain centralizes all flags in one place, while file-level init() keeps each flag next to the code that cares about it.\nHere\u0026rsquo;s how it looks with TestMain:\npackage snapshot_test import ( \u0026#34;flag\u0026#34; \u0026#34;os\u0026#34; \u0026#34;testing\u0026#34; ) var snapshot = flag.Bool(\u0026#34;snapshot\u0026#34;, false, \u0026#34;run snapshot tests\u0026#34;) func TestMain(m *testing.M) { flag.Parse() os.Exit(m.Run()) } func TestSnapshot(t *testing.T) { if !*snapshot { t.Skip(\u0026#34;pass -snapshot to run this test\u0026#34;) } t.Log(\u0026#34;running snapshot\u0026#34;) } And here\u0026rsquo;s the equivalent using init() to keep everything in the same file:\npackage snapshot_test import ( \u0026#34;flag\u0026#34; \u0026#34;testing\u0026#34; ) var snapshot bool func init() { flag.BoolVar(\u0026amp;snapshot, \u0026#34;snapshot\u0026#34;, false, \u0026#34;run snapshot tests\u0026#34;) } func TestSnapshot(t *testing.T) { if !snapshot { t.Skip(\u0026#34;pass -snapshot to run this test\u0026#34;) } t.Log(\u0026#34;running snapshot\u0026#34;) } Once you\u0026rsquo;ve defined a flag, you run the snapshot tests like this:\ngo test -v -snapshot You can also list all the flags using:\ngo test -v -args -h This prints all registered flags, including your own:\n-snapshot run snapshot tests -test.v verbose: print all tests as they are run. -test.run run only those tests and examples matching the regular expression. # ... A detail about names: built-in flags show up in the help output with a test. prefix (-test.v, -test.run, -test.timeout), yet you pass them without that prefix (-v, -run, -timeout) while running tests. The Go tool strips test. for you. Custom flags don\u0026rsquo;t get this treatment. Whatever string you register is the exact string you must pass. If you register snapshot you run:\ngo test -snapshot If you register test.snapshot you must run:\ngo test -test.snapshot There is no automatic collapsing just because the name starts with test..\nThe flag -args lets you pass additional arguments to the test binary. When the binary sees -h after -args, it prints every flag and exits. No tests run, though the binary is built. That one command exposes the full configuration surface of your tests.\nIf you namespace your flags like this:\nflag.BoolVar(\u0026amp;snapshot, \u0026#34;custom.snapshot\u0026#34;, false, \u0026#34;run snapshot tests\u0026#34;) Then you can grep for them:\ngo test -v -args -h | grep custom Define the global flags in TestMain when several files need the same switches or when you have package-wide setup (containers, databases, global mocks). Define flags in init() when a switch is relevant to one test file and you want the declaration right next to the logic it controls. I usually prefer per-test- file-level flags that don\u0026rsquo;t need to depend on any global magic.\nEither way, the flag lives in code, is easy to grep, appears in -h, and tells everyone exactly what it controls. The only downside I can think of with this approach is that, similar to the environment variable technique, you\u0026rsquo;ll have to check for the flag in every test and make a decision. But in practice, I prefer the flexibility over the all-or-nothing approach with build tags.\nI think flags are the best way to configure your apps and tools. Even when environment variables are involved, I often map them to flags for documentation purposes. The goal is to give users a single -h command they can run to see all available options for tuning behavior. Tests are no exception. I was quite happy to find out that Peter Bourgon conveyed the same sentiment in this seminal 2018 blog post.\n","permalink":"https://rednafi.com/go/test-config-with-flags/","summary":"\u003cp\u003eAs your test suite grows, you need ways to toggle certain kinds of tests on or off. Maybe\nyou want to enable \u003ca href=\"https://www.reddit.com/r/golang/comments/yytw1f/snapshot_testing_in_golang/\"\u003esnapshot tests\u003c/a\u003e, skip long-running \u003ca href=\"https://www.reddit.com/r/golang/comments/18xmkuz/how_do_you_write_integration_tests_in_go/\"\u003eintegration tests\u003c/a\u003e, or switch\nbetween real services and mocks. In every case, you\u0026rsquo;re really saying, \u0026ldquo;Run this test only if\n\u003cem\u003eX\u003c/em\u003e is true.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSo where does \u003cem\u003eX\u003c/em\u003e come from?\u003c/p\u003e\n\u003cp\u003eI like to rely on Go\u0026rsquo;s standard tooling so that integration and snapshot tests can live\nright beside ordinary unit tests. Because I usually run these heavier tests in\n\u003ca href=\"https://testcontainers.com/\"\u003etestcontainers\u003c/a\u003e, I don\u0026rsquo;t always want them running while I\u0026rsquo;m iterating on a feature or\nchasing a bug. So I need to enable them in an optional manner.\u003c/p\u003e","title":"Flags for discoverable test config in Go"},{"content":"When working with Go in an industrial programming context, I feel like dependency injection (DI) often gets a bad rep because of DI frameworks. But DI as a technique is quite useful. It just tends to get explained with too many OO jargons and triggers PTSD among those who came to Go to escape GoF theology.\nDependency Injection is a 25-dollar term for a 5-cent concept.\n— James Shore\nDI basically means passing values into a constructor instead of creating them inside it. That\u0026rsquo;s really it. Observe:\ntype server struct { db DB } // NewServer constructs a server instance func NewServer() *server { db := DB{} // The dependency is created here return \u0026amp;server{db: db} } Here, NewServer creates its own DB. Instead, to inject the dependency, build DB elsewhere and pass it in as a constructor parameter:\nfunc NewServer(db DB) *server { return \u0026amp;server{db: db} } Now the constructor no longer decides how a database is built; it simply receives one.\nIn Go, DI is often done using interfaces. You collate the behavior you care about in an interface, and then provide different concrete implementations for different contexts. In production, you pass a real implementation of DB. In unit tests, you pass a fake implementation that behaves the same way from the caller\u0026rsquo;s perspective but avoids real database calls.\nHere\u0026rsquo;s how that looks:\n// behaviour we care about type DB interface { Get(id string) (string, error) Save(id, value string) error } type server struct{ db DB } // NewServer accepts a DB implementation and passes it to server func NewServer(db DB) *server { return \u0026amp;server{db: db} } A real implementation of DB might look like this:\ntype RealDB struct{ url string } func NewDB(url string) *RealDB { return \u0026amp;RealDB{url: url} } func (r *RealDB) Get(id string) (string, error) { // pretend we hit Postgres return \u0026#34;real value\u0026#34;, nil } func (r *RealDB) Save(id, value string) error { return nil } And a fake implementation for unit tests might be:\ntype FakeDB struct{ data map[string]string } func NewFake() *FakeDB { return \u0026amp;FakeDB{data: map[string]string{}} } func (f *FakeDB) Get(id string) (string, error) { return f.data[id], nil } func (f *FakeDB) Save(id, value string) error { f.data[id] = value return nil } Use the fake in unit tests like so:\nfunc TestServerGet(t *testing.T) { fake := NewFake() _ = fake.Save(\u0026#34;42\u0026#34;, \u0026#34;fake\u0026#34;) srv := NewServer(fake) val, _ := srv.db.Get(\u0026#34;42\u0026#34;) if val != \u0026#34;fake\u0026#34; { t.Fatalf(\u0026#34;want fake, got %s\u0026#34;, val) } } The compiler guarantees both RealDB and FakeDB satisfy DB, and during tests, we can swap out the implementations without much ceremony.\nWhy frameworks turn mild annoyance into actual pain Once NewServer grows half a dozen dependencies, wiring them by hand can feel noisy. That\u0026rsquo;s when a DI framework starts looking tempting.\nWith Uber\u0026rsquo;s dig, you register each constructor as a provider. Provide takes a function, uses reflection to inspect its parameters and return type, and adds it as a node in an internal dependency graph. Nothing is executed yet. Things only run when you call .Invoke() on the container.\nBut that reflection-driven magic is also where the pain starts. As your graph grows, it gets harder to tell which constructor feeds which one. Some constructor takes one parameter, some takes three. There\u0026rsquo;s no single place you can glance at to understand the wiring. It\u0026rsquo;s all figured out inside the container at runtime.\nLet the container figure it out!\n— every DI framework ever\nfunc BuildContainer() *dig.Container { c := dig.New() // Each Provide call teaches dig about one node in the graph. c.Provide(NewConfig) // produces *Config c.Provide(NewDB) // wants *Config, produces *DB c.Provide(NewRepo) // wants *DB, produces *Repo c.Provide(NewFlagClient) // produces *FlagClient c.Provide(NewService) // wants *Repo, *FlagClient, produces *Service c.Provide(NewServer) // wants *Service, produces *server return c } func main() { // Invoke starts the graph; dig sorts and calls constructors if err := BuildContainer().Invoke( func(s *server) { s.Run() }); err != nil { panic(err) } } Now try commenting out NewFlagClient. The code still compiles. There\u0026rsquo;s no error until runtime, when dig fails to construct NewService due to a missing dependency. And the error message you get?\ndig invoke failed: could not build arguments for function main.main.func1 (prog.go:87) : failed to build *main.Server : could not build arguments for function main.NewServer (prog.go:65) : failed to build *main.Service: missing dependencies for function main.NewService (prog.go:55) : missing type: *main.FlagClient That\u0026rsquo;s five stack frames deep, far from where the problem started. Now you\u0026rsquo;re digging through dig\u0026rsquo;s internals to reconstruct the graph in your head.\nGoogle\u0026rsquo;s wire takes a different approach: it shifts the graph-building to code generation. You collect your constructors in a wire.NewSet, call wire.Build, and the generator writes a wire_gen.go that wires everything up explicitly.\nvar serverSet = wire.NewSet( NewConfig, NewDB, NewRepo, NewFlagClient, // comment out to see Wire complain NewService, NewServer, ) func InitializeServer() (*server, error) { wire.Build(serverSet) return nil, nil // replaced by generated code } Comment out NewFlagClient and Wire fails earlier — during generation:\nwire: ../../service/wire.go:13:2: cannot find dependency for *flags.Client It\u0026rsquo;s better than dig\u0026rsquo;s runtime panic, but still comes with its own headaches:\nYou need to remember to run go generate ./... whenever constructor signatures change. When something breaks, you\u0026rsquo;re stuck reading through hundreds of lines of autogenerated glue to trace the issue. You have to teach every teammate Wire\u0026rsquo;s DSL — wire.NewSet, wire.Build, build tags, and sentinel rules. And if you ever switch to something different like dig, you\u0026rsquo;ll need to learn a completely different set of concepts: Provide, Invoke, scopes, named values, etc. While DI frameworks tend to use vocabularies like provider or container to give you an essense of familiarity, they still reinvent the API surface every time. Switching between them means relearning a new mental model.\nSo the promise of \u0026ldquo;just register your providers and forget about wiring\u0026rdquo; ends up trading clear, compile-time control for either reflection or hidden generator logic — and yet another abstraction layer you have to debug.\nThe boring alternative: keep wiring explicit In Go, you can just wire your own dependencies manually. Like this:\nfunc main() { cfg := NewConfig() db := NewDB(cfg.DSN) repo := NewRepo(db) flags := NewFlagClient(cfg.FlagURL) svc := NewService(repo, flags, cfg.APIKey) srv := NewServer(svc, cfg.ListenAddr) srv.Run() } Longer? Yes. But:\nThe call order is the dependency graph.\nErrors are handled right where they happen.\nIf a constructor changes, the compiler points straight at every broken call:\n./main.go:33:39: not enough arguments in call to NewService have (*Repo, *FlagClient) want (*Repo, *FlagClient, string) No reflection, no generated code, no global state. Go type-checks the dependency graph early and loudly, exactly how it should be. And also, it doesn\u0026rsquo;t confuse your LSP, so your IDE keeps on being useful.\nIf main() really grows unwieldy, split your code:\nfunc buildInfra(cfg *Config) (*DB, *FlagClient, error) { // ... } func buildService(cfg *Config) (*Service, error) { db, flags, err := buildInfra(cfg) if err != nil { return nil, err } return NewService(NewRepo(db), flags, cfg.APIKey), nil } func main() { cfg := NewConfig() svc, err := buildService(cfg) if err != nil { log.Fatal(err) } NewServer(svc, cfg.ListenAddr).Run() } Each helper is a regular function that anyone can skim without reading a framework manual. Also, you usually build all of your dependency in one place and it\u0026rsquo;s really not that big of a deal if your builder function takes in 20 parameters and builds all the dependencies. Just put each function parameter on their own line and use gofumpt to format the code to make it readable.\nReflection works elsewhere, so why not here? Other languages lean on containers because often times constructors cannot be overloaded and compile times hurt. Go already gives you:\nFirst-class functions so constructors are plain values. Interfaces so implementations swap cleanly in tests. Fast compilation so feedback loops stay tight. A DI framework often fixes problems Go already solved and trades away readability to do it.\nThe most magical thing about Go is how little magic it allows.\n— Some Gopher on Reddit\nYou might still want a framework It\u0026rsquo;s tempting to make a blanket statement saying that you should never pick up a DI framework, but context matters here.\nI was watching Uber\u0026rsquo;s GopherCon talk on Go at scale and how their DI framework Fx (which uses dig underneath) allows them to achieve consistency at scale. If you\u0026rsquo;re Uber and have all the observability tools in place to get around the downsides, then you\u0026rsquo;ll know.\nAlso, if you\u0026rsquo;re working in a codebase that\u0026rsquo;s already leveraging a framework and it works well, then it doesn\u0026rsquo;t make sense to refactor it without any incentives.\nOr, you\u0026rsquo;re writing one of those languages where using a DI framework is the norm, and you\u0026rsquo;ll be called a weirdo if you try to reinvent the wheel there.\nHowever, in my experience, even in organizations that maintain a substantial number of Go repos, DI frameworks add more confusion than they\u0026rsquo;re worth. If your experience is otherwise, I\u0026rsquo;d love to be proven wrong.\nThe post got a fair bit of discussion going around the web. You might find it interesting.\nhackernews r/golang r/experienceddevs r/programming ","permalink":"https://rednafi.com/go/di-frameworks-bleh/","summary":"\u003cp\u003eWhen working with Go in an \u003ca href=\"https://peter.bourgon.org/go-for-industrial-programming/\"\u003eindustrial programming\u003c/a\u003e context, I feel like dependency\ninjection (DI) often gets a bad rep because of \u003cem\u003eDI frameworks\u003c/em\u003e. But DI as a technique is\nquite useful. It just tends to get explained with too many OO jargons and triggers PTSD\namong those who came to Go to escape GoF theology.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eDependency Injection is a 25-dollar term for a 5-cent concept.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e— James Shore\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eDI basically means \u003cem\u003epassing values into a constructor instead of creating them inside it\u003c/em\u003e.\nThat\u0026rsquo;s really it. Observe:\u003c/p\u003e","title":"You probably don't need a DI framework"},{"content":"By default, Go copies values when you pass them around. But sometimes, that can be undesirable. For example, if you accidentally copy a mutex and multiple goroutines work on separate instances of the lock, they won\u0026rsquo;t be properly synchronized. In those cases, passing a pointer to the lock avoids the copy and works as expected.\nTake this example: passing a sync.WaitGroup by value will break things in subtle ways:\nfunc f(wg sync.WaitGroup) { // ... do something with the waitgroup } func main() { var wg sync.WaitGroup f(wg) // oops! wg is getting copied here! } sync.WaitGroup lets you wait for multiple goroutines to finish some work. Under the hood, it\u0026rsquo;s a struct with methods like Add, Done, and Wait to sync concurrently running goroutines.\nThat snippet compiles fine but leads to buggy behavior because we\u0026rsquo;re copying the lock instead of referencing it in the f function.\nLuckily, go vet catches it. If you run vet on that code, you\u0026rsquo;ll get a warning like this:\nf passes lock by value: sync.WaitGroup contains sync.noCopy call of f copies lock value: sync.WaitGroup contains sync.noCopy This means we\u0026rsquo;re passing wg by value when we should be passing a reference. Here\u0026rsquo;s the fix:\nfunc f(wg *sync.WaitGroup) { // pass by reference // ... do something with the waitgroup } func main() { var wg sync.WaitGroup f(\u0026amp;wg) // pass a pointer to wg } Since this kind of incorrect copy doesn\u0026rsquo;t throw a compile-time error, if you skip go vet, you might never catch it. Another reason to always vet your code.\nI was curious how the Go toolchain enforces this. The clue is in the vet warning:\ncall of f copies lock value: sync.WaitGroup contains sync.noCopy So the sync.noCopy struct inside sync.WaitGroup is doing something to alert go vet when you pass it by value.\nLooking at the implementation of sync.WaitGroup, you\u0026rsquo;ll see:\ntype WaitGroup struct { noCopy noCopy state atomic.Uint64 sema uint32 } Then I traced the definition of noCopy in sync/cond.go:\n// noCopy may be added to structs which must not be copied // after the first use. // Note that it must not be embedded, due to the Lock and Unlock methods. type noCopy struct{} // Lock is a no-op used by -copylocks checker from `go vet`. func (*noCopy) Lock() {} func (*noCopy) Unlock() {} Just having those no-op Lock and Unlock methods on noCopy is enough. This implements the Locker interface. Then if you put that struct inside another one, go vet will flag cases where you try to copy the outer struct.\nAlso, note the comment: don\u0026rsquo;t embed noCopy. Include it explicitly. Embedding would expose Lock and Unlock on the outer struct, which you probably don\u0026rsquo;t want.\nThe Go toolchain enforces this with the copylock checker. It\u0026rsquo;s part of go vet. You can exclusively invoke it with go vet -copylocks ./.... It looks for value copies of any struct that nests a struct with Lock and Unlock methods. It doesn\u0026rsquo;t matter what those methods do, just having them is enough.\nWhen vet runs, it walks the AST and applies the checker on assignments, function calls, return values, struct literals, range loops, channel sends, basically anywhere values can get copied. If it sees you copying a struct with noCopy, it yells.\nInterestingly, if you define noCopy as anything other than a struct and implement the Locker interface, vet ignores that. I tested this on Go 1.24:\ntype noCopy int // this is valid but vet doesn\u0026#39;t get triggered func (*noCopy) Lock() {} func (*noCopy) Unlock() {} This doesn\u0026rsquo;t trigger vet. It only works when noCopy is a struct. The reason is that vet takes a shortcut in the copylock checker when deciding whether to trigger the warning. Currently, it explicitly looks for a struct that satisfies the Locker interface and ignores any other type even if it implements the interface.\nYou\u0026rsquo;ll see this in other parts of the sync package too. sync.Mutex uses the same trick:\ntype Mutex struct { _ noCopy mu isync.Mutex } Same with sync.Once:\ntype Once struct { done uint32 m Mutex noCopy noCopy } Here\u0026rsquo;s a complete example of abusing -copylocks to prevent copying our own struct:\ntype Svc struct{ _ noCopy } type noCopy struct{} func (*noCopy) Lock() {} func (*noCopy) Unlock() {} // Use this func main() { var svc Svc _ = svc // go vet will complain about this copy op } Running go vet on this gives:\nassignment copies lock value to s: play.Svc contains play.noCopy call of fmt.Println copies lock value: play.Svc contains play.noCopy Someone on Reddit asked me what actually triggers the copylock checker in go vet — is it the struct\u0026rsquo;s literal name noCopy or the fact that it implements the Locker interface?\nThe name noCopy isn\u0026rsquo;t special. You can call it whatever you want. As long as it implements the Locker interface, go vet will complain if the surrounding struct gets copied. See this Go Playground snippet.\n","permalink":"https://rednafi.com/go/prevent-struct-copies/","summary":"\u003cp\u003eBy default, Go copies values when you pass them around. But sometimes, that can be\nundesirable. For example, if you accidentally copy a mutex and multiple goroutines work on\nseparate instances of the lock, they won\u0026rsquo;t be properly synchronized. In those cases, passing\na pointer to the lock avoids the copy and works as expected.\u003c/p\u003e\n\u003cp\u003eTake this example: passing a \u003ccode\u003esync.WaitGroup\u003c/code\u003e by value will break things in subtle ways:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003ef\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ewg\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003esync\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eWaitGroup\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e// ... do something with the waitgroup\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003evar\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003ewg\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003esync\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eWaitGroup\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nf\"\u003ef\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ewg\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// oops! wg is getting copied here!\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003esync.WaitGroup\u003c/code\u003e lets you wait for multiple goroutines to finish some work. Under the hood,\nit\u0026rsquo;s a struct with methods like \u003ccode\u003eAdd\u003c/code\u003e, \u003ccode\u003eDone\u003c/code\u003e, and \u003ccode\u003eWait\u003c/code\u003e to sync concurrently running\ngoroutines.\u003c/p\u003e","title":"Preventing accidental struct copies in Go"},{"content":"Go 1.24 added a new tool directive that makes it easier to manage your project\u0026rsquo;s tooling.\nI used to rely on Make targets to install and run tools like stringer, mockgen, and linters like gofumpt, goimports, staticcheck, and errcheck. Problem is, these installations were global, and they\u0026rsquo;d often clash between projects.\nAnother big issue was frequent version mismatch. I ran into cases where people were formatting the same codebase differently because they had different versions of the tools installed. Then CI would yell at everyone because it was always installing the latest version of the tools before running them. Chaos!\nThe tools.go convention To avoid this mess, the Go community came up with a convention where you\u0026rsquo;d pin your tool versions in a tools.go file. I\u0026rsquo;ve written about omitting dev dependencies before. But the gist is, you\u0026rsquo;d have a tools.go file in your root directory that imports the tooling and assigns them to _:\n//go:build tools // tools.go package tools import ( _ \u0026#34;github.com/golangci/golangci-lint/cmd/golangci-lint\u0026#34; _ \u0026#34;mvdan.cc/gofumpt\u0026#34; ) Since these dependencies aren\u0026rsquo;t used directly in the codebase, the //go:build tools directive ensures they\u0026rsquo;re excluded from the main build.\nThen running go mod tidy keeps things clean and includes these dev dependencies in the go.mod and go.sum files.\nThis works, but it always felt a bit clunky. You end up polluting your main go.mod with tooling-only dependencies. And sometimes, transitive dependencies of those tools clash with your app\u0026rsquo;s dependencies.\nThe new tool directive in Go 1.24 solves some of the tools.go pain points.\nEnter the tool directive With Go 1.24, you can now add tooling with the -tool flag when using go get:\ngo get -tool github.com/golangci/golangci-lint/cmd/golangci-lint@latest This adds the dependency to your go.mod like this:\nmodule github.com/rednafi/foo go 1.24.2 tool github.com/golangci/golangci-lint/cmd/golangci-lint // ... other transitive dependencies Notice the tool directive clearly separates these from regular module dependencies.\nThen you can run the tool with:\ngo tool golangci-lint run ./... One thing to keep in mind: the first time you run a tool this way, it might take a second — Go needs to compile it before running if it isn\u0026rsquo;t already compiled. After that, it\u0026rsquo;s cached, so subsequent runs are fast.\nWhat about go generate? This also plays nicely with go generate. I\u0026rsquo;ve started replacing direct tool calls with go tool, so contributors don\u0026rsquo;t need to install tools globally. Just run go generate and you\u0026rsquo;re done:\n//go:generate go tool stringer -type=MyEnum No further setup needed, no path issues, and it\u0026rsquo;s always using the version you pinned.\nStill not perfect That said, one thing still bugs me: go get -tool adds these dev tools to the main go.mod file. That means your application and dev dependencies are still mixed together. Same problem the tools.go hack had.\nThere\u0026rsquo;s no built-in way to avoid this yet. So your options are:\nAccept that dev and app deps will live in the same go.mod file. Create a separate tools module to isolate your tooling. A bit clunky, but doable. I went with the second option.\nMy layout looks like this:\n. ├── go.mod ├── go.sum └── tools └── go.mod Then I install tools like this:\ncd tools go get -tool github.com/golangci/golangci-lint/cmd/golangci-lint@latest And run them from the root directory as follows:\ngo tool -modfile tools/go.mod golangci-lint run ./... The go tool command supports a -modfile flag that you can use to specify where to pull the tool version from. I really wish go get supported -modfile too — that way you wouldn\u0026rsquo;t need to manage the dependencies in such a wonky manner. This was close to being perfect. Well, maybe in a future release.\nOnly works with Go-native tools Another limitation is that it only works with tools written in Go. So if you\u0026rsquo;re using stuff like eslint, prettier, or jq, you\u0026rsquo;re on your own. But for most of my projects, the dev tooling is written in Go anyway, so this setup has been working okay.\n","permalink":"https://rednafi.com/go/tool-directive/","summary":"\u003cp\u003eGo 1.24 added a new \u003ccode\u003etool\u003c/code\u003e directive that makes it easier to manage your project\u0026rsquo;s tooling.\u003c/p\u003e\n\u003cp\u003eI used to rely on Make targets to install and run tools like \u003ccode\u003estringer\u003c/code\u003e, \u003ccode\u003emockgen\u003c/code\u003e, and\nlinters like \u003ccode\u003egofumpt\u003c/code\u003e, \u003ccode\u003egoimports\u003c/code\u003e, \u003ccode\u003estaticcheck\u003c/code\u003e, and \u003ccode\u003eerrcheck\u003c/code\u003e. Problem is, these\ninstallations were global, and they\u0026rsquo;d often clash between projects.\u003c/p\u003e\n\u003cp\u003eAnother big issue was frequent version mismatch. I ran into cases where people were\nformatting the same codebase differently because they had different versions of the tools\ninstalled. Then CI would yell at everyone because it was always installing the latest\nversion of the tools before running them. Chaos!\u003c/p\u003e","title":"Go 1.24's \"tool\" directive"},{"content":"Ideally, every function that writes to the stdout probably should ask for a io.Writer and write to it instead. However, it\u0026rsquo;s common to encounter functions like this:\nfunc frobnicate() { fmt.Println(\u0026#34;do something\u0026#34;) } This would be easier to test if frobnicate would ask for a writer to write to. For instance:\nfunc frobnicate(w io.Writer) { fmt.Fprintln(w, \u0026#34;do something\u0026#34;) } You could pass os.Stdout to frobnicate explicitly to write to the console:\nfunc main() { frobnicate(os.Stdout) } This behaves exactly the same way as the first version of frobnicate.\nDuring test, instead of os.Stdout, you\u0026rsquo;d just pass a bytes.Buffer and assert its content as follows:\nfunc TestFrobnicate(t *testing.T) { // Create a buffer to capture the output var buf bytes.Buffer // Call the function with the buffer frobnicate(\u0026amp;buf) // Check if the output is as expected expected := \u0026#34;do something\\n\u0026#34; if buf.String() != expected { t.Errorf(\u0026#34;Expected %q, got %q\u0026#34;, expected, buf.String()) } } This is all good. But many functions or methods that emit logs just do that directly to stdout. So we want to test the first version of frobnicate without making any changes to it.\nI found this neat pattern to test functions that write to stdout without accepting a writer.\nThe idea is to write a helper function named captureStdout that looks like this:\n// captureStdout replaces os.Stdout with a buffer and returns it. func captureStdout(f func()) string { old := os.Stdout r, w, _ := os.Pipe() os.Stdout = w f() // run the function that writes to stdout _ = w.Close() var buf bytes.Buffer _, _ = io.Copy(\u0026amp;buf, r) os.Stdout = old return buf.String() } Here\u0026rsquo;s what\u0026rsquo;s happening under the hood:\nWe use os.Pipe() to create a pipe: a connected pair of file descriptors — a reader (r) and a writer (w). Think of it like a temporary tunnel. Whatever we write to w, we can read back from r. Since both are just files as far as Go is concerned, we can temporarily replace os.Stdout with the writer end of the pipe:\nos.Stdout = w This means anything printed to stdout during the function run actually goes into our pipe. After the function runs, we close the writer to signal that we\u0026rsquo;re done writing, then read from the reader into a buffer and restore the original stdout.\nNow we can test frobnicate without touching its implementation:\nfunc TestFrobnicate(t *testing.T) { output := captureStdout(func() { frobnicate() }) expected := \u0026#34;do something\\n\u0026#34; if output != expected { t.Errorf(\u0026#34;Expected %q, got %q\u0026#34;, expected, output) } } No need to refactor frobnicate. This works great for quick tests when you don\u0026rsquo;t control the code or just want to assert some printed output.\nA more robust capture out The above version of captureStdout works fine for simple cases. But in practice, functions might also write to stderr, especially if they\u0026rsquo;re using Go\u0026rsquo;s log package or if a panic happens. For example, this would not be captured by the simple captureStdout helper:\nlog.Println(\u0026#34;something went wrong\u0026#34;) Even though it looks like a normal print statement, log writes to stderr by default. So if you want to catch that output too, or generally capture everything that\u0026rsquo;s printed to the console during a function call, we need to upgrade our helper a bit. I found this example from immudb\u0026rsquo;s captureOutput helper.\nHere\u0026rsquo;s a more complete version:\n// captureOut captures both stdout and stderr. func captureOut(f func()) string { // Create a pipe to capture stdout custReader, custWriter, err := os.Pipe() if err != nil { panic(err) } // Save the original stdout and stderr to restore later origStdout := os.Stdout origStderr := os.Stderr // Restore stdout and stderr when done defer func() { os.Stdout = origStdout os.Stderr = origStderr }() // Set the stdout and stderr to the pipe os.Stdout, os.Stderr = custWriter, custWriter log.SetOutput(custWriter) // Create a channel to read the output from the pipe out := make(chan string) // Goroutine reads from pipe and sends output to channel var wg sync.WaitGroup wg.Add(1) go func() { var buf bytes.Buffer wg.Done() io.Copy(\u0026amp;buf, custReader) out \u0026lt;- buf.String() }() wg.Wait() // Call the function that writes to stdout f() // Close the writer to signal that we\u0026#39;re done _ = custWriter.Close() // Wait for the goroutine to finish reading from the pipe return \u0026lt;-out } This version does a few more things:\nCaptures everything: It redirects both os.Stdout and os.Stderr to ensure all standard output streams are captured. It also explicitly redirects the standard log package\u0026rsquo;s output, which often bypasses os.Stderr.\nPrevents deadlocks: Output is read concurrently in a separate goroutine. This is crucial because if f generates more output than the internal pipe buffer can hold, writing would block without a concurrent reader, causing a deadlock.\nEnsure reader readiness: A sync.WaitGroup guarantees the reading goroutine is active before f starts executing. This prevents a potential race condition where initial output could be lost if f writes before the reader is ready.\nGuarantees cleanup: Using defer, the original os.Stdout and os.Stderr are always restored, even if f panics. This prevents the function from permanently altering the program\u0026rsquo;s standard output streams.\nYou\u0026rsquo;d use captureOut the same way as the naive captureStdout. This version is safer and more complete, and works well when you\u0026rsquo;re testing CLI commands, log-heavy code, or anything that might write to the terminal in unexpected ways.\nIt\u0026rsquo;s not a replacement for writing functions that accept io.Writer, but when you\u0026rsquo;re dealing with existing code or want to quickly assert on terminal output, it gets the job done.\n","permalink":"https://rednafi.com/go/capture-console-output/","summary":"\u003cp\u003eIdeally, every function that writes to the stdout probably should ask for a \u003ccode\u003eio.Writer\u003c/code\u003e and\nwrite to it instead. However, it\u0026rsquo;s common to encounter functions like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003efrobnicate\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nx\"\u003efmt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ePrintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;do something\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis would be easier to test if \u003ccode\u003efrobnicate\u003c/code\u003e would ask for a writer to write to. For\ninstance:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003efrobnicate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eio\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eWriter\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nx\"\u003efmt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eFprintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ew\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;do something\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eYou could pass \u003ccode\u003eos.Stdout\u003c/code\u003e to \u003ccode\u003efrobnicate\u003c/code\u003e explicitly to write to the console:\u003c/p\u003e","title":"Capturing console output in Go tests"},{"content":"While watching Mitchell Hashimoto\u0026rsquo;s Advanced Testing with Go talk, I came across this neat technique for deferring teardown to the caller. Let\u0026rsquo;s say you have a helper function in a test that needs to perform some cleanup afterward.\nYou can\u0026rsquo;t run the teardown inside the helper itself because the test still needs the setup. For example, in the following case, the helper runs its teardown immediately:\nfunc TestFoo(t *testing.T) { helper(t) // Test logic here: resources may already be cleaned up! } func helper(t *testing.T) { t.Helper() // Setup code here. // Teardown code here. defer func() { // Clean up something. }() } When helper is called, it defers its teardown — which executes at the end of the helper function, not the test. But the test logic still depends on whatever the helper set up. So this approach doesn\u0026rsquo;t work.\nThe next working option is to move the teardown logic into the test itself:\nfunc TestFoo(t *testing.T) { helper(t) // Run the teardown of helper. defer func() { // Clean up something. }() // Test logic here. } func helper(t *testing.T) { t.Helper() // Setup code here. // No teardown here; we move it to the caller. } This works fine if you have only one helper. But with multiple helpers, it quickly becomes messy — you now have to manage multiple teardown calls manually, like this:\nfunc TestFoo(t *testing.T) { helper1(t) helper2(t) defer func() { // Clean up helper2. }() defer func() { // Clean up helper1. }() // Test logic here. } You also need to be careful with the order: defer statements are executed in LIFO (last-in, first-out) order. So if teardown order matters, this can be a problem. Ideally, your tests shouldn\u0026rsquo;t depend on teardown order — but sometimes they do.\nSo rather than manually handling cleanup inside the test, have helpers return a teardown function that the test can defer itself. Here\u0026rsquo;s how:\nfunc TestFoo(t *testing.T) { teardown1 := helper1(t) defer teardown1() teardown2 := helper2(t) defer teardown2() // Test logic here. } func helper1(t *testing.T) func() { t.Helper() // Setup code here. // Maybe create a temp dir, start a mock server, etc. return func() { // Teardown code here. } } func helper2(t *testing.T) func() { t.Helper() // Setup code here. return func() { // Teardown code here. } } Each helper is self-contained: it sets something up and returns a function to clean up whatever resource it has spun up. The test controls when teardown happens by calling the cleanup function at the appropriate time. Another benefit is that the returned teardown closure has access to the local variables of the helper. So func() can access the helper\u0026rsquo;s *testing.T without us having to pass it explicitly as a parameter.\nHere\u0026rsquo;s how I\u0026rsquo;ve been using this pattern.\nCreating a temporary file to test file I/O The setupTempFile helper creates a temporary file, writes some content to it, and returns the file name along with a teardown function that removes the file.\nfunc setupTempFile(t *testing.T, content string) (string, func()) { t.Helper() tmpFile, err := os.CreateTemp(\u0026#34;\u0026#34;, \u0026#34;temp-*.txt\u0026#34;) if err != nil { t.Fatalf(\u0026#34;failed to create temp file: %v\u0026#34;, err) } if _, err := tmpFile.WriteString(content); err != nil { t.Fatalf(\u0026#34;failed to write to temp file: %v\u0026#34;, err) } tmpFile.Close() return tmpFile.Name(), func() { if err := os.Remove(tmpFile.Name()); err != nil { t.Errorf(\u0026#34;failed to remove temp file %s: %v\u0026#34;, tmpFile.Name(), err) } else { t.Logf(\u0026#34;cleaned up temp file: %s\u0026#34;, tmpFile.Name()) } } } In the main test:\nfunc TestReadFile(t *testing.T) { path, cleanup := setupTempFile(t, \u0026#34;hello world\u0026#34;) defer cleanup() data, err := os.ReadFile(path) if err != nil { t.Fatalf(\u0026#34;failed to read file: %v\u0026#34;, err) } t.Logf(\u0026#34;file contents: %s\u0026#34;, data) } Running the test displays:\n=== RUN TestReadFile prog_test.go:18: file contents: hello world prog_test.go:38: cleaned up temp file: /tmp/temp-30176446.txt --- PASS: TestReadFile (0.00s) PASS Starting and stopping a mock HTTP server Sometimes you want to test code that makes HTTP calls. Here\u0026rsquo;s a helper that starts an in-memory mock server and returns its URL and a cleanup function that shuts it down:\nfunc setupMockServer(t *testing.T) (string, func()) { t.Helper() handler := http.HandlerFunc( func(w http.ResponseWriter, r *http.Request) { w.WriteHeader(http.StatusOK) w.Write([]byte(\u0026#34;mock response\u0026#34;)) }, ) server := httptest.NewServer(handler) return server.URL, func() { server.Close() t.Log(\u0026#34;mock server shut down\u0026#34;) } } And in the test:\nfunc TestHTTPRequest(t *testing.T) { url, cleanup := setupMockServer(t) defer cleanup() resp, err := http.Get(url) if err != nil { t.Fatalf(\u0026#34;failed to make HTTP request: %v\u0026#34;, err) } defer resp.Body.Close() body, _ := io.ReadAll(resp.Body) t.Logf(\u0026#34;response body: %s\u0026#34;, body) } Running the test prints:\n=== RUN TestHTTPRequest prog_test.go:34: response body: mock response prog_test.go:20: mock server shut down --- PASS: TestHTTPRequest (0.00s) PASS Setting up and tearing down a database table In tests that hit a real (or test) database, you often need to create and drop tables. Here\u0026rsquo;s a helper that sets up a test table and returns a teardown function to drop it:\nfunc setupTestTable(t *testing.T, db *sql.DB) func() { t.Helper() query := `CREATE TABLE IF NOT EXISTS users ( id INTEGER PRIMARY KEY, name TEXT )` _, err := db.Exec(query) if err != nil { t.Fatalf(\u0026#34;failed to create table: %v\u0026#34;, err) } return func() { _, err := db.Exec(`DROP TABLE IF EXISTS users`) if err != nil { t.Errorf(\u0026#34;failed to drop table: %v\u0026#34;, err) } else { t.Log(\u0026#34;dropped test table\u0026#34;) } } } And the test:\nfunc TestInsertUser(t *testing.T) { db := getTestDB(t) // Opens test DB; defined elsewhere cleanup := setupTestTable(t, db) defer cleanup() _, err := db.Exec(`INSERT INTO users (name) VALUES (?)`, \u0026#34;Alice\u0026#34;) if err != nil { t.Fatalf(\u0026#34;failed to insert user: %v\u0026#34;, err) } } The t.Cleanup() method P.S. I learned about this after the blog went live.\nGo 1.14 added the t.Cleanup() method, which lets you avoid returning the teardown closures from helper functions altogether. It also runs the cleanup logic in the correct order (LIFO). So, you could rewrite the first example in this post as follows:\nfunc TestFoo(t *testing.T) { // The testing package will ensure that the cleanup runs at the end of // this test function. helper(t) // Test logic here. } func helper(t *testing.T) { t.Helper() // We register the teardown logic with t.Cleanup(). t.Cleanup(func() { // Teardown logic here. }) } Now the testing package will handle calling the cleanup logic in the correct order. You can add multiple teardown functions like this:\nt.Cleanup(func() {}) t.Cleanup(func() {}) The functions will run in LIFO order. Similarly, the database setup example can be rewritten like this:\nfunc setupTestTable(t *testing.T, db *sql.DB) func() { t.Helper() // Logic as before. // Instead of returning the teardown function, we register // it with t.Cleanup(). t.Cleanup(func() { _, err := db.Exec(`DROP TABLE IF EXISTS users`) if err != nil { t.Errorf(\u0026#34;failed to drop table: %v\u0026#34;, err) } else { t.Log(\u0026#34;dropped test table\u0026#34;) } }) } Then the helper function is used like this:\nfunc TestInsertUser(t *testing.T) { db := getTestDB(t) // Opens a test DB connection; defined elsewhere. // This sets up the DB, and t.Cleanup will execute the teardown // logic once this test function finishes. setupTestTable(t, db) // Rest of the test logic. } Fin!\n","permalink":"https://rednafi.com/go/deferred-teardown-closure/","summary":"\u003cp\u003eWhile watching \u003ca href=\"https://www.youtube.com/watch?v=8hQG7QlcLBk\"\u003eMitchell Hashimoto\u0026rsquo;s Advanced Testing with Go talk\u003c/a\u003e, I came across this neat\ntechnique for deferring teardown to the caller. Let\u0026rsquo;s say you have a helper function in a\ntest that needs to perform some cleanup afterward.\u003c/p\u003e\n\u003cp\u003eYou can\u0026rsquo;t run the teardown inside the helper itself because the test still needs the setup.\nFor example, in the following case, the \u003ccode\u003ehelper\u003c/code\u003e runs its teardown immediately:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eTestFoo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003et\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"nx\"\u003etesting\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eT\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nf\"\u003ehelper\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e// Test logic here: resources may already be cleaned up!\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003ehelper\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003et\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"nx\"\u003etesting\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eT\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nx\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eHelper\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e// Setup code here.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e// Teardown code here.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003edefer\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"c1\"\u003e// Clean up something.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}()\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWhen \u003ccode\u003ehelper\u003c/code\u003e is called, it defers its teardown — which executes at the end of the helper\nfunction, not the test. But the test logic still depends on whatever the helper set up. So\nthis approach doesn\u0026rsquo;t work.\u003c/p\u003e","title":"Deferred teardown closure in Go testing"},{"content":"There are primarily three ways of sorting slices in Go. Early on, we had the verbose but flexible method of implementing sort.Interface to sort the elements in a slice. Later, Go 1.8 introduced sort.Slice to reduce boilerplate with inline comparison functions. Most recently, Go 1.21 brought generic sorting via the slices package, which offers a concise syntax and compile-time type safety.\nThese days, I mostly use the generic sorting syntax, but I wanted to document all three approaches for posterity.\nUsing sort.Interface The oldest technique is based on sort.Interface. You create a custom type that wraps your slice and implement three methods — Len, Less, and Swap — to satisfy the interface. Then you pass this custom type to sort.Sort().\nSorting a slice of integers The following example defines an IntSlice type. Passing an IntSlice to sort.Sort arranges its integers in ascending order:\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;sort\u0026#34; ) // Define a custom IntSlice so that we can implement the sort.Interface type IntSlice []int // Len, Less, Swap are required to conform to sort.Interface func (s IntSlice) Len() int { return len(s) } func (s IntSlice) Less(i, j int) bool { return s[i] \u0026lt; s[j] } func (s IntSlice) Swap(i, j int) { s[i], s[j] = s[j], s[i] } func main() { nums := IntSlice{4, 1, 3, 2} sort.Sort(nums) fmt.Println(nums) // [1 2 3 4] } To reverse the order, invert the comparison in the Less method and define a new type:\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;sort\u0026#34; ) // Define a custom IntSlice for descending order sorting. type DescIntSlice []int func (s DescIntSlice) Len() int { return len(s) } // Inverted comparison for descending order func (s DescIntSlice) Less(i, j int) bool { return s[i] \u0026gt; s[j] } func (s DescIntSlice) Swap(i, j int) { s[i], s[j] = s[j], s[i] } func main() { nums := DescIntSlice{4, 1, 3, 2} sort.Sort(nums) fmt.Println(nums) // [4 3 2 1] } Just reversing the order requires you to define a separate type and implement the three methods again!\nLuckily, for the basic types, the sort package provides sort.IntSlice, sort.Float64Slice, and sort.StringSlice — which already implement sort.Interface. So you don\u0026rsquo;t have to do the above for sorting a slice of primitive elements. Instead, you can do this:\nints := sort.IntSlice{4, 1, 3, 2} floats := sort.Float64Slice{3.1, 2.7, 5.0} strings := sort.StringSlice{\u0026#34;banana\u0026#34;, \u0026#34;apple\u0026#34;, \u0026#34;cherry\u0026#34;} sort.Sort(ints) // ints: [1 2 3 4] sort.Sort(floats) // floats: [2.7 3.1 5] sort.Sort(strings) // strings: [apple banana cherry] To reverse the order, you can use sort.Reverse as follows:\nsort.Sort(sort.Reverse(ints)) // ints: [4 3 2 1] sort.Sort(sort.Reverse(floats)) // floats: [5 3.1 2.7] sort.Sort(sort.Reverse(strings)) // strings: [cherry banana apple] Sorting a slice of structs by age However, if you\u0026rsquo;re dealing with a slice of structs, then you do have to implement sort.Interface manually. Here, we sort by the Age field in ascending order:\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;sort\u0026#34; ) type User struct { Name string Age int } type ByAge []User func (s ByAge) Len() int { return len(s) } func (s ByAge) Less(i, j int) bool { return s[i].Age \u0026lt; s[j].Age } func (s ByAge) Swap(i, j int) { s[i], s[j] = s[j], s[i] } func main() { users := ByAge{ {\u0026#34;Alice\u0026#34;, 32}, {\u0026#34;Bob\u0026#34;, 27}, {\u0026#34;Carol\u0026#34;, 40}, } sort.Sort(users) fmt.Println(users) // [{Bob 27} {Alice 32} {Carol 40}] } We can leverage sort.Reverse to reverse the order:\nsort.Sort(sort.Reverse(users)) // [{Carol 40} {Alice 32} {Bob 27}] Although sort.Interface can handle just about any sorting logic, you must create a new custom type (or significantly modify an existing one) each time you want to sort a different slice or the same slice in a different way. It\u0026rsquo;s powerful but verbose, and can be cumbersome to maintain if you have many different sorts in your code.\nUsing sort.Slice Go 1.8 introduced sort.Slice to minimize the amount of boilerplate needed for sorting. Instead of creating a new type and implementing three methods, you provide an inline comparison function that receives the two indices you\u0026rsquo;re comparing.\nSorting a slice of float64 Here\u0026rsquo;s a simple example that sorts floats in ascending order:\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;sort\u0026#34; ) func main() { floats := []float64{2.5, 0.1, 3.9, 1.2} sort.Slice(floats, func(i, j int) bool { return floats[i] \u0026lt; floats[j] }) fmt.Println(floats) // [0.1 1.2 2.5 3.9] } Inverting the comparison sorts them in descending order:\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;sort\u0026#34; ) func main() { floats := []float64{2.5, 0.1, 3.9, 1.2} sort.Slice(floats, func(i, j int) bool { return floats[i] \u0026gt; floats[j] // Reverse the comp }) fmt.Println(floats) // [3.9 2.5 1.2 0.1] } Sorting a slice of structs by age For structs, the inline comparator can access struct fields:\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;sort\u0026#34; ) type User struct { Name string Age int } func main() { users := []User{ {\u0026#34;Alice\u0026#34;, 32}, {\u0026#34;Bob\u0026#34;, 27}, {\u0026#34;Carol\u0026#34;, 40}, } sort.Slice(users, func(i, j int) bool { return users[i].Age \u0026lt; users[j].Age }) fmt.Println(users) // [{Bob 27} {Alice 32} {Carol 40}] } Switching \u0026gt; for \u0026lt; will reverse the sort:\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;sort\u0026#34; ) type User struct { Name string Age int } func main() { users := []User{ {\u0026#34;Alice\u0026#34;, 32}, {\u0026#34;Bob\u0026#34;, 27}, {\u0026#34;Carol\u0026#34;, 40}, } sort.Slice(users, func(i, j int) bool { return users[i].Age \u0026gt; users[j].Age }) fmt.Println(users) // [{Carol 40} {Alice 32} {Bob 27}] } While sort.Slice is much simpler than sort.Interface, it\u0026rsquo;s still not strictly type-safe: the slice parameter is defined as an interface{}, and you provide a comparator that uses indices. Go won\u0026rsquo;t necessarily stop you from doing something incorrect in the comparison at compile time.\nFor example, this code compiles but will panic at runtime because other is referenced inside the comparator of a different slice ints, and the indices i or j can go out of bounds in other:\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;sort\u0026#34; ) func main() { ints := []int{3, 1, 2} other := []int{10, 20} sort.Slice(ints, func(i, j int) bool { // Using \u0026#39;other\u0026#39; here compiles, but i or j might be out of range. return other[i] \u0026lt; other[j] }) fmt.Println(ints) } You won\u0026rsquo;t find out you\u0026rsquo;ve made a mistake until runtime, when a panic occurs. There is no compiler-enforced guarantee that the func(i, j int) bool actually compares two values of the intended slice.\nNote: In sort.Slice, the comparison function parameters i and j are indices. Inside the function, you must reference slice[i] and slice[j] to get the actual elements being compared.\nUsing generics with the slices package Go 1.21 introduced the slices package, which provides generic sorting functions. These new functions combine the convenience of sort.Slice with the ability to detect type errors at compile time. For basic numeric or string slices that satisfy Go\u0026rsquo;s \u0026ldquo;ordered\u0026rdquo; constraints, you can just call slices.Sort. For more complex or custom sorting, slices.SortFunc accepts a comparator function that returns an integer (negative if a \u0026lt; b, zero if they\u0026rsquo;re equal, and positive if a \u0026gt; b).\nSorting primitive slices When you\u0026rsquo;re dealing with basic types like int, float64, or string, you can sort them immediately using slices.Sort, which arranges them in ascending order:\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;slices\u0026#34; ) func main() { ints := []int{4, 1, 3, 2} floats := []float64{2.5, 0.1, 3.9, 1.2} slices.Sort(ints) slices.Sort(floats) fmt.Println(ints) // [1 2 3 4] fmt.Println(floats) // [0.1 1.2 2.5 3.9] } For descending order, you can use slices.SortFunc and invert the usual comparison:\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;slices\u0026#34; ) func main() { ints := []int{4, 1, 3, 2} floats := []float64{2.5, 0.1, 3.9, 1.2} slices.SortFunc(ints, func(a, b int) int { switch { case a \u0026gt; b: return -1 case a \u0026lt; b: return 1 default: return 0 } }) slices.SortFunc(floats, func(a, b float64) int { switch { case a \u0026gt; b: return -1 case a \u0026lt; b: return 1 default: return 0 } }) fmt.Println(ints) // [4 3 2 1] fmt.Println(floats) // [3.9 2.5 1.2 0.1] } Sorting a slice of structs by age When dealing with more complex structures, you can define precisely how two elements should be compared:\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;slices\u0026#34; ) type User struct { Name string Age int } func main() { users := []User{ {\u0026#34;Alice\u0026#34;, 32}, {\u0026#34;Bob\u0026#34;, 27}, {\u0026#34;Carol\u0026#34;, 40}, } slices.SortFunc(users, func(a, b User) int { return a.Age - b.Age }) fmt.Println(users) // [{Bob 27} {Alice 32} {Carol 40}] } To reverse the order, invert the numerical comparison:\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;slices\u0026#34; ) type User struct { Name string Age int } func main() { users := []User{ {\u0026#34;Alice\u0026#34;, 32}, {\u0026#34;Bob\u0026#34;, 27}, {\u0026#34;Carol\u0026#34;, 40}, } slices.SortFunc(users, func(a, b User) int { switch { case a.Age \u0026gt; b.Age: return -1 case a.Age \u0026lt; b.Age: return 1 default: return 0 } }) fmt.Println(users) // [{Carol 40} {Alice 32} {Bob 27}] } Note: Unlike sort.Slice, which passes indices to the comparison function, slices.SortFunc passes the actual elements (a and b) to your comparator. Moreover, the comparator must return an int (negative, zero, or positive), rather than a boolean.\nCompile-time safety One of the major benefits of the slices package is compile-time type safety, which you don\u0026rsquo;t get with sort.Sort or sort.Slice. Those older APIs use interface{} parameters or index-based comparators and don\u0026rsquo;t strictly verify that your comparator operates on the right types.\nAs shown previously, you can accidentally reference a different slice in the comparator and your code will compile but crash at runtime. By contrast, slices.Sort and slices.SortFunc are fully generic. The compiler enforces that you pass a slice of a valid type (e.g., []int, []string, or a custom struct slice), and that your comparator\u0026rsquo;s signature matches the element type. This means you get errors at compile time instead of at runtime.\nFor instance, if you attempt to pass an array instead of a slice:\nimport \u0026#34;slices\u0026#34; func main() { arr := [4]int{10, 20, 30, 40} // compile-time error: cannot use arr (type [4]int) as []int slices.Sort(arr) } Go will refuse to compile this code because arr is not a slice. Similarly, if your comparator for slices.SortFunc returns a type other than int, the compiler will produce an error. This helps you detect mistakes immediately, rather than discovering them in runtime.\nFor a practical illustration, consider sorting a slice by a case-insensitive string field:\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;slices\u0026#34; \u0026#34;strings\u0026#34; ) type Animal struct { Name string Species string } func main() { animals := []Animal{ {\u0026#34;Bob\u0026#34;, \u0026#34;Giraffe\u0026#34;}, {\u0026#34;alice\u0026#34;, \u0026#34;Zebra\u0026#34;}, {\u0026#34;Dave\u0026#34;, \u0026#34;Elephant\u0026#34;}, } // Sort by Name, ignoring case slices.SortFunc(animals, func(a, b Animal) int { aLower := strings.ToLower(a.Name) bLower := strings.ToLower(b.Name) switch { case aLower \u0026lt; bLower: return -1 case aLower \u0026gt; bLower: return 1 default: return 0 } }) fmt.Println(animals) // Output: [{alice Zebra} {Bob Giraffe} {Dave Elephant}] } Because your comparator expects an Animal for both a and b, you can\u0026rsquo;t accidentally compare two different types or reference the wrong fields without hitting a compile-time error.\n","permalink":"https://rednafi.com/go/sort-slice/","summary":"\u003cp\u003eThere are primarily three ways of sorting slices in Go. Early on, we had the verbose but\nflexible method of implementing \u003ccode\u003esort.Interface\u003c/code\u003e to sort the elements in a slice. Later, Go\n1.8 introduced \u003ccode\u003esort.Slice\u003c/code\u003e to reduce boilerplate with inline comparison functions. Most\nrecently, Go 1.21 brought generic sorting via the \u003ccode\u003eslices\u003c/code\u003e package, which offers a concise\nsyntax and compile-time type safety.\u003c/p\u003e\n\u003cp\u003eThese days, I mostly use the generic sorting syntax, but I wanted to document all three\napproaches for posterity.\u003c/p\u003e","title":"Three flavors of sorting Go slices"},{"content":"Comparing interface values in Go has caught me off guard a few times, especially with nils. Often, I\u0026rsquo;d expect a comparison to evaluate to true but got false instead.\nMany moons ago, Russ Cox wrote a fantastic post on Go interface internals that clarified my confusion. This post is a distillation of my exploration of interfaces and nil comparisons.\nInterface internals Roughly speaking, an interface in Go has three components:\nA static type A dynamic type A dynamic value For example:\nvar n any // The static type of n is any (interface{}) n = 1 // Upon assignment, the dynamic type becomes int // And the dynamic value becomes 1 Here, the static type of n is any, which tells the compiler what operations are allowed on the variable. In the case of any, any operation is allowed. When we assign 1 to n, it adopts the dynamic type int and the dynamic value 1.\nInternally, every interface value is implemented as a two word structure:\nOne word holds a pointer to the dynamic type (i.e., a type descriptor). The other word holds the data associated with that type. This data word might directly contain the value if it\u0026rsquo;s small enough, or it might hold a pointer to the actual data. Note that this internal representation is distinct from the interface\u0026rsquo;s declared or \u0026ldquo;static\u0026rdquo; type — the type you wrote in the code (any in the example above). At runtime, what gets stored is only the pair of dynamic type and dynamic value. Here\u0026rsquo;s a crude diagram:\n+-----------------------+ | Interface | +-----------------------+ | Pointer to type info | ---\u0026gt; [Dynamic type descriptor] +-----------------------+ | Data | ---\u0026gt; [Dynamic value or pointer to the value] +-----------------------+ Comparing nils with interface variables Nil comparisons can be tricky because an interface value is considered nil only when both its dynamic type and dynamic value are nil. A few examples.\nComparing a nil pointer directly var p *int // p is a nil pointer of type *int if p == nil { fmt.Println(\u0026#34;p is nil\u0026#34;) } // Output: p is nil Here, p is a pointer to an int and is explicitly nil, so the comparison works as expected. This doesn\u0026rsquo;t have anything to do with explicit interfaces, but it\u0026rsquo;s important to demo basic nil comparison to understand how comparisons work with interfaces.\nAn interface variable explicitly set to nil var r io.Reader // The static type of r is io.Reader r = nil // The dynamic type is nil // The dynamic value is nil // Since both the dynamic type and value evaluate to nil, r == nil is true if r == nil { fmt.Println(\u0026#34;r is nil\u0026#34;) } // Output: r is nil In this case, r is directly set to nil. Since both the dynamic type and the dynamic value are nil, the interface compares equal to nil.\nAssigning a nil pointer to an interface variable var b *bytes.Buffer // b is a nil pointer of type *bytes.Buffer var r io.Reader = b // The static type of r is io.Reader. // The dynamic type of r is *bytes.Buffer. // The dynamic value of r is nil. // Although b is nil, r != nil because r holds type info (*bytes.Buffer). if r == nil { fmt.Println(\u0026#34;r is nil\u0026#34;) } else { fmt.Println(\u0026#34;r is not nil\u0026#34;) } // Output: r is not nil Even though b is nil, assigning it to the interface variable r gives r a non-nil dynamic type (*bytes.Buffer) with a nil dynamic value. Since r still holds type information, r == nil returns false, even though the underlying value is nil.\nWhen comparing an interface variable, Go checks both the dynamic type and the value. The variable evaluates to nil only if both are nil.\nUsing type assertions for reliable nil checks In cases where an interface variable might hold a nil pointer, we\u0026rsquo;ve seen that comparing the interface directly to nil may not yield the expected result.\nA type assertion can help extract the underlying value so that you can perform a more reliable nil check. This approach is especially useful when you know the expected underlying type.\nBelow, we define a simple type myReader that implements the Read method to satisfy the io.Reader interface.\ntype myReader struct{} func (mr *myReader) Read(p []byte) (int, error) { return 0, nil } Now, consider the following example:\nvar mr *myReader // mr is a nil pointer of type *myReader var r io.Reader = mr // The static type of r is io.Reader // The dynamic type of r is *myReader // The dynamic value of r is nil // Use a type assertion to extract the underlying *myReader value. if underlying, ok := r.(*myReader); ok \u0026amp;\u0026amp; underlying == nil { fmt.Println(\u0026#34;r holds a nil pointer\u0026#34;) } else { fmt.Println(\u0026#34;r does not hold a nil pointer\u0026#34;) } // Output: r holds a nil pointer Here, we assert that r holds a value of type *myReader. If the assertion succeeds (indicated by ok being true) and the underlying value is nil, we can conclude that the interface variable holds a nil pointer — even though the interface itself is not nil due to its dynamic type.\nThis type assertion trick only works when you know the underlying type of the interface value. If the type might vary, consider using the reflect package to examine the underlying value.\nWriting a generic nil checker with reflect The following function introspects any variable and checks whether it\u0026rsquo;s nil:\nfunc isNil(i any) bool { if i == nil { return true } // Arrays are not nilable, so we skip reflect.Array. switch reflect.TypeOf(i).Kind() { case reflect.Ptr, reflect.Map, reflect.Chan, reflect.Slice, reflect.Func: return reflect.ValueOf(i).IsNil() } return false } The switch on .Kind() is necessary because directly calling reflect.ValueOf().IsNil() on a non-pointer value will cause a panic.\nCalling this function on any value, including an interface, reliably checks whether it\u0026rsquo;s nil.\nFin!\n","permalink":"https://rednafi.com/go/nil-interface-comparison/","summary":"\u003cp\u003eComparing interface values in Go has caught me off guard a few times, especially with nils.\nOften, I\u0026rsquo;d expect a comparison to evaluate to \u003ccode\u003etrue\u003c/code\u003e but got \u003ccode\u003efalse\u003c/code\u003e instead.\u003c/p\u003e\n\u003cp\u003eMany moons ago, Russ Cox wrote a fantastic \u003ca href=\"https://research.swtch.com/interfaces\"\u003epost on Go interface internals\u003c/a\u003e that clarified\nmy confusion. This post is a distillation of my exploration of interfaces and nil\ncomparisons.\u003c/p\u003e\n\u003ch2 id=\"interface-internals\"\u003eInterface internals\u003c/h2\u003e\n\u003cp\u003eRoughly speaking, an interface in Go has three components:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA static type\u003c/li\u003e\n\u003cli\u003eA dynamic type\u003c/li\u003e\n\u003cli\u003eA dynamic value\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor example:\u003c/p\u003e","title":"Nil comparisons and Go interface"},{"content":"Middleware is usually the go-to pattern in Go HTTP servers for tweaking request behavior. Typically, you wrap your base handler with layers of middleware — one might log every request, while another intercepts specific routes like /special to serve a custom response.\nHowever, I often find the indirections introduced by this pattern a bit hard to read and debug. I recently came across the embedded delegation pattern while browsing Gin\u0026rsquo;s HTTP router source code. Here, I explore both patterns and explain why I usually start with delegation whenever I need to modify HTTP requests in my Go services.\nMiddleware stacking Here\u0026rsquo;s an example where the logging middleware records each request, and the special middleware intercepts requests to /special:\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) // loggingMiddleware logs incoming requests. func loggingMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { log.Println(\u0026#34;Middleware: received request for\u0026#34;, r.URL.Path) next.ServeHTTP(w, r) }) } // specialMiddleware intercepts requests for \u0026#34;/special\u0026#34; and handles them. func specialMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { if r.URL.Path == \u0026#34;/special\u0026#34; { w.Write([]byte(\u0026#34;Special middleware handling request\u0026#34;)) return } next.ServeHTTP(w, r) }) } func main() { mux := http.NewServeMux() mux.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\u0026#34;Hello, world!\u0026#34;)) }) // Middleware chain: special handling then logging. handler := loggingMiddleware(specialMiddleware(mux)) http.ListenAndServe(\u0026#34;:8080\u0026#34;, handler) } In this setup, every incoming request is first handled by the special middleware, which checks for the /special route, and then by the logging middleware that logs the request details. We\u0026rsquo;re effectively stacking the middleware functions.\nIf you hit the server with:\ncurl localhost:8080/ curl localhost:8080/special the server logs will look like this:\n2025/03/06 21:24:44 Middleware: received request for / 2025/03/06 21:24:47 Middleware: received request for /special Stacking middleware functions like middleware3(middleware2(middleware1(mux))) can get messy when you have many of them. That\u0026rsquo;s why people usually write a wrapper function to apply the middlewares to the mux:\nfunc applyMiddleware( handler http.Handler, middlewares ...func(http.Handler) http.Handler) http.Handler { // Apply middlewares in reverse order to preserve LIFO. for i := len(middlewares) - 1; i \u0026gt;= 0; i-- { handler = middlewares[i](handler) } return handler } applyMiddleware takes an http.Handler and a variadic list of middleware functions (...func(http.Handler) http.Handler). It loops over the middleware in reverse order so each one wraps the next properly. This avoids deep nesting like middleware3(middleware2(middleware1(mux))) and keeps the middleware chain tidy.\nYou\u0026rsquo;d then use it like this:\nfunc main() { mux := http.NewServeMux() mux.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\u0026#34;Hello, world!\u0026#34;)) }) // Middleware chain: special handling then logging. // specialMiddleware is applied before loggingMiddleware. handler := applyMiddleware(mux, loggingMiddleware, specialMiddleware) http.ListenAndServe(\u0026#34;:8080\u0026#34;, handler) } This behaves just like the manual middleware stacking, but it\u0026rsquo;s a bit cleaner.\nWhile this is the canonical way to handle request-response modifications in Go, it can sometimes be hard to reason about, especially when debugging or dealing with many middleware layers.\nThere\u0026rsquo;s another way to achieve the same result without dealing with a soup of nested functions. The next section talks about that.\nEmbedded delegation Embedded delegation (or the delegation pattern) means you embed the standard HTTP multiplexer inside your own struct and override its ServeHTTP method.\nIt\u0026rsquo;s a bit like inheritance — overriding a method in a subclass to add extra functionality and then delegating the call to the original method. Although Go doesn\u0026rsquo;t have a class hierarchy, you can still delegate responsibilities to the embedded type\u0026rsquo;s method.\nThe following example implements the same behavior — logging every request and intercepting the /special route — directly within a custom mux:\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) // CustomMux embeds http.ServeMux to override ServeHTTP. type CustomMux struct { *http.ServeMux } // ServeHTTP logs the request and intercepts \u0026#34;/special\u0026#34; before // delegating to the embedded mux. func (cm *CustomMux) ServeHTTP(w http.ResponseWriter, r *http.Request) { // Log all requests. log.Println(\u0026#34;CustomMux: received request for\u0026#34;, r.URL.Path) // Handle \u0026#34;/special\u0026#34; differently. if r.URL.Path == \u0026#34;/special\u0026#34; { w.Write([]byte(\u0026#34;Special handling in CustomMux\u0026#34;)) return } cm.ServeMux.ServeHTTP(w, r) } func main() { mux := http.NewServeMux() mux.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\u0026#34;Hello, world!\u0026#34;)) }) // Wrap the standard mux with our custom delegation. customMux := \u0026amp;CustomMux{ServeMux: mux} http.ListenAndServe(\u0026#34;:8080\u0026#34;, customMux) } In this example, the custom mux centralizes both logging and special-case route handling within one ServeHTTP method. This approach cuts out the extra function calls in a middleware chain and can simplify tracking the request flow. I find it a bit easier on the eyes too.\nIf you have a bunch of extra functionality to add inside cm.ServeHTTP, you can wrap them in utility functions like this:\n// logRequest logs incoming HTTP requests. func logRequest(r *http.Request) { log.Println(\u0026#34;CustomMux: received request for\u0026#34;, r.URL.Path) } // handleSpecialRequest handles requests to \u0026#34;/special\u0026#34; // and returns true if handled. func handleSpecialRequest(w http.ResponseWriter, r *http.Request) bool { if r.URL.Path != \u0026#34;/special\u0026#34; { return false // Not handled, continue processing. } w.Write([]byte(\u0026#34;Special handling in CustomMux\u0026#34;)) return true // Handled; no further processing needed. } Then, simply call these functions inside your cm.ServeHTTP method:\nfunc (cm *CustomMux) ServeHTTP(w http.ResponseWriter, r *http.Request) { logRequest(r) if handleSpecialRequest(w, r) { return } cm.ServeMux.ServeHTTP(w, r) } This keeps all the request modifications in a single ServeHTTP method.\nMixing the two approaches You can also mix both techniques. For example, you might use direct delegation for special route handling and then wrap the resulting handler with middleware for logging. Here\u0026rsquo;s how a hybrid solution might look:\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) // CustomMux embeds http.ServeMux and intercepts \u0026#34;/special\u0026#34;. type CustomMux struct { *http.ServeMux } // ServeHTTP intercepts \u0026#34;/special\u0026#34; and delegates other routes. func (cm *CustomMux) ServeHTTP(w http.ResponseWriter, r *http.Request) { if r.URL.Path == \u0026#34;/special\u0026#34; { w.Write([]byte(\u0026#34;Special handling in CustomMux\u0026#34;)) return } cm.ServeMux.ServeHTTP(w, r) } // loggingMiddleware logs incoming requests. func loggingMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { log.Println(\u0026#34;Middleware: received request for\u0026#34;, r.URL.Path) next.ServeHTTP(w, r) }) } func main() { mux := http.NewServeMux() mux.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\u0026#34;Hello, world!\u0026#34;)) }) // Use direct delegation for special routing. customMux := \u0026amp;CustomMux{ServeMux: mux} // Wrap the custom mux with logging middleware. handler := loggingMiddleware(customMux) http.ListenAndServe(\u0026#34;:8080\u0026#34;, handler) } In this hybrid approach, the specialized behavior (intercepting the /special path) is handled via direct delegation, while logging stays modular as middleware. This gives you the best of both worlds.\nI usually start with the embedded delegation and gradually introduce the middleware pattern if I need it later. It\u0026rsquo;s easier to adopt the middleware pattern if you start with delegation than the other way around.\n","permalink":"https://rednafi.com/go/middleware-vs-delegation/","summary":"\u003cp\u003eMiddleware is usually the go-to pattern in Go HTTP servers for tweaking request behavior.\nTypically, you wrap your base handler with layers of middleware — one might log every\nrequest, while another intercepts specific routes like \u003ccode\u003e/special\u003c/code\u003e to serve a custom\nresponse.\u003c/p\u003e\n\u003cp\u003eHowever, I often find the indirections introduced by this pattern a bit hard to read and\ndebug. I recently came across the embedded delegation pattern while browsing \u003ca href=\"https://github.com/gin-gonic/gin/blob/3b28645dc95d58e0df36b8aff7a6c64f7c0ca5e9/gin.go#L94\"\u003eGin\u0026rsquo;s HTTP\nrouter source code\u003c/a\u003e. Here, I explore both patterns and explain why I usually start with\ndelegation whenever I need to modify HTTP requests in my Go services.\u003c/p\u003e","title":"Stacked middleware vs embedded delegation in Go"},{"content":"I\u0026rsquo;ve always found the signature of io.Reader a bit odd:\ntype Reader interface { Read(p []byte) (n int, err error) } Why take a byte slice and write data into it? Wouldn\u0026rsquo;t it be simpler to create the slice inside Read, load the data, and return it instead?\n// Hypothetical; what I *thought* it should be Read() (p []byte, err error) This felt more intuitive to me — you call Read, and it gives you a slice filled with data, no need to pass anything.\nI found out why it\u0026rsquo;s designed this way while watching this excellent GopherCon Singapore talk on understanding allocations by Jacob Walker. It mainly boils down to two reasons.\nReducing heap allocations If Read created and returned a new slice every time, the memory would always end up on the heap.\nHeap allocations are slower because they require garbage collection, while stack allocations are faster since they are freed automatically when a function returns. By taking a caller-provided slice, Read lets the caller control memory and reuse buffers, keeping them on the stack whenever possible.\nThis matters a lot when reading large amounts of data. If each Read call created a new slice, you\u0026rsquo;d constantly be allocating memory, leading to more work for the garbage collector. Instead, the caller can allocate a buffer once and reuse it across multiple reads:\nbuf := make([]byte, 4096) // Single allocation n, err := reader.Read(buf) // Read into existing buffer Go\u0026rsquo;s escape analysis tool (go build -gcflags=-m) can confirm this. If Read returned a new slice, the tool would likely show:\nbuf escapes to heap meaning Go has to allocate it dynamically. But by reusing a preallocated slice, we avoid unnecessary heap allocations — only if the buffer is small enough to fit in the stack. How small? Only the compiler knows, and you shouldn\u0026rsquo;t depend on it. Use the escape analysis tool to see that. But most of the time, you don\u0026rsquo;t need to worry about this at all.\nReusing buffers in streaming The second issue is correctness. When reading from a stream, you usually call Read multiple times to get all the data. If Read returned a fresh slice every time, you\u0026rsquo;d have no control over memory usage across calls. Worse, you couldn\u0026rsquo;t efficiently handle partial reads, making buffer management unpredictable.\nWith the hypothetical version of Read, every call would allocate a new slice. If you needed to read a large stream of data, you\u0026rsquo;d have to manually piece everything together using append, like this:\nvar allData []byte for { buf, err := reader.Read() // New allocation every call if err != nil { break } allData = append(allData, buf...) // Growing slice, more allocs } process(allData) This is a mess. Every time append runs out of space, Go will have to allocate a larger slice and copy the existing data over, piling on unnecessary GC pressure.\nBy contrast, io.Reader\u0026rsquo;s actual design avoids this problem:\nbuf := make([]byte, 4096) // Allocate once for { n, err := reader.Read(buf) if err != nil { break } process(buf[:n]) } This avoids unnecessary allocations and produces less garbage for the GC to clean up.\n","permalink":"https://rednafi.com/go/io-reader-signature/","summary":"\u003cp\u003eI\u0026rsquo;ve always found the signature of \u003ccode\u003eio.Reader\u003c/code\u003e a bit odd:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003etype\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eReader\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003einterface\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nf\"\u003eRead\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ep\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e[]\u003c/span\u003e\u003cspan class=\"kt\"\u003ebyte\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003en\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eerr\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eerror\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWhy take a byte slice and write data into it? Wouldn\u0026rsquo;t it be simpler to create the slice\ninside \u003ccode\u003eRead\u003c/code\u003e, load the data, and return it instead?\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Hypothetical; what I *thought* it should be\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nf\"\u003eRead\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ep\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e[]\u003c/span\u003e\u003cspan class=\"kt\"\u003ebyte\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eerr\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eerror\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis felt more intuitive to me — you call \u003ccode\u003eRead\u003c/code\u003e, and it gives you a slice filled with data,\nno need to pass anything.\u003c/p\u003e","title":"Why does Go's io.Reader have such a weird signature?"},{"content":"Just like any other dynamically growable container structure, Go slices come with a few gotchas. I don\u0026rsquo;t always remember all the rules I need to be aware of. So this is an attempt to list some of the most common mistakes I\u0026rsquo;ve made at least once.\nSlices are views over arrays In Go, a slice is a lightweight wrapper around an array. Instead of storing data itself, it keeps track of three things: a pointer to an underlying array where the data is stored, the number of elements it currently holds, and the total capacity before it needs more space. The Go runtime defines it like this:\n// src/runtime/slice.go type slice struct { array unsafe.Pointer // pointer to data array len int // slice length cap int // slice capacity } When you create a slice from an array or another slice, Go doesn\u0026rsquo;t copy the data — it simply points to a section of the existing array.\nSlice Header Underlying Array +-------------+ +-------------------+ | array ------\u0026gt;|---------\u0026gt;| (data in memory) | | len | +-------------------+ | cap | +-------------+ This makes slices efficient. Passing a slice by value doesn\u0026rsquo;t mean copying all its elements — only the small slice struct gets copied, while the data stays where it is. But this behavior is also the source of much confusion. The next sections cover some common pitfalls.\nSliced slices share the underlying array Reslicing a slice doesn\u0026rsquo;t copy data. The newly created slices point to the same array. So modifying one slice will affect others.\n// Define the original slice original := []int{1, 2, 3, 4, 5} // -\u0026gt; original: [1 2 3 4 5] // Create slice1 from index 1 to 4 slice1 := original[1:4] // -\u0026gt; slice1: [2 3 4] // Create slice2 from index 2 to the end slice2 := original[2:] // -\u0026gt; slice2: [3 4 5] // Modify the first element of slice1 (affects other slices) slice1[0] = 100 // -\u0026gt; original: [1 100 3 4 5], slice1: [100 3 4], slice2: [3 4 5] Solution: To get independent slices, you need to explicitly copy the data. Use make to create a new slice and copy to transfer the elements.\n// Define the original slice original := []int{1, 2, 3, 4, 5} // -\u0026gt; [1 2 3 4 5] // Create a new slice (slice1) from original[1:4] slice1 := make([]int, len(original[1:4])) // -\u0026gt; [0 0 0] copy(slice1, original[1:4]) // -\u0026gt; [2 3 4] // Create a new slice (slice2) from original[2:] slice2 := make([]int, len(original[2:])) // -\u0026gt; [0 0 0] copy(slice2, original[2:]) // -\u0026gt; [3 4 5] // Modify the first element of slice1 (doesn\u0026#39;t affect others) // -\u0026gt; original: [1 2 3 4 5], slice1: [100 3 4], slice2: [3 4 5] slice1[0] = 100 Append may reallocate append reallocates the underlying array if capacity is insufficient, changing the backing array pointer.\nWhen passing slices to functions, reallocation inside the function won\u0026rsquo;t update the original slice header in the caller unless the slice is returned and reassigned. Modifications within the capacity are visible.\nIf you create a slice with a predefined capacity and start appending elements, everything looks fine until you exceed that capacity. Once that happens, Go reallocates memory and moves the slice to a new backing array.\n// Create a slice with length=0 and capacity=3 slice := make([]int, 0, 3) // Let\u0026#39;s say the array pointer is p1 // Append 3 elements (1,2,3) to fill up capacity slice = append(slice, 1, 2, 3) // -\u0026gt; still pointer p1, slice: [1 2 3] // Exceed capacity by appending 4 slice = append(slice, 4) // -\u0026gt; new pointer p2, slice: [1 2 3 4] The same behavior applies when passing a slice to a function. If the function modifies elements within the allocated capacity, those changes persist and are visible from outside the function. But if append triggers a reallocation inside the function, the caller\u0026rsquo;s slice remains unchanged.\n// Demonstration function that modifies and appends func modifySlice(s []int) { s[0] = 99 // modification within capacity is visible s = append(s, 100) // may trigger reallocation // s pointer might change here, but the caller won\u0026#39;t see that } // Example usage mySlice := make([]int, 1, 3) // -\u0026gt; [0], capacity=3 mySlice[0] = 1 // -\u0026gt; [1] modifySlice(mySlice) // -\u0026gt; mySlice[0] becomes 99 (within capacity) // -\u0026gt; the append inside function might reallocate, but that reallocated // version is lost // mySlice is effectively [99], capacity still = 3 // (the \u0026#34;100\u0026#34; appended is not in mySlice) Solution: If append inside a function reallocates memory, the caller won\u0026rsquo;t see the change. To make it explicit, return the modified slice and reassign it.\n// Correct approach: return the new slice func modifySliceCorrected(s []int) []int { s = append(s, 100) // may reallocate return s // return the updated slice } // Example usage mySlice := make([]int, 1, 3) // -\u0026gt; [0], cap=3 mySlice[0] = 1 // -\u0026gt; [1] mySlice = modifySliceCorrected(mySlice) // -\u0026gt; now mySlice sees the appended element [1 100] Append returns new slice append returns a new slice. If you don\u0026rsquo;t reassign the result back to the original slice variable, the slice remains unchanged after the append operation. We already saw this in the last section but I think it deserves a section of its own.\nslice := []int{1, 2, 3} // -\u0026gt; [1 2 3] // Wrong usage (no reassign): append(slice, 4) // -\u0026gt; appended result is discarded, slice remains [1 2 3] // Correct usage (assign back): slice = append(slice, 4) // -\u0026gt; slice is now [1 2 3 4] Solution: Remember to always assign the return value of append back to the slice variable you are working with.\nslice := []int{1, 2, 3} // -\u0026gt; [1 2 3] slice = append(slice, 4, 5, 6) // -\u0026gt; [1 2 3 4 5 6] Nil and empty slices differ Nil slices have nil array pointers; empty slices have initialized, non-nil pointers and zero length. While often interchangeable for emptiness checks, the distinction matters in certain contexts like JSON encoding or API interactions.\nvar nilSlice []int // -\u0026gt; nil emptySliceMake := make([]int, 0) // -\u0026gt; [] emptySliceLiteral := []int{} // -\u0026gt; [] // nilSlice == nil -\u0026gt; true // emptySliceMake == nil -\u0026gt; false // emptySliceLiteral == nil -\u0026gt; false Solution: When you need a truly empty slice (e.g., to represent an empty list in JSON), initialize it as an empty slice (e.g., []int{} or make([]int, 0)). For general emptiness checks, len(slice) == 0 works for both nil and empty slices.\nvar nilSlice []int // nil slice (pointer is nil) emptySlice := []int{} // empty slice (pointer is non-nil) nilJSON, _ := json.Marshal(nilSlice) // -\u0026gt; \u0026#34;null\u0026#34; emptyJSON, _ := json.Marshal(emptySlice) // -\u0026gt; \u0026#34;[]\u0026#34; Slicing can leak memory Small slices created from large arrays can keep the entire large array in memory.\n// Suppose we have a function returning a large slice func getLargeSlice() []int { largeSlice := make([]int, 1_000_000) // large underlying array return largeSlice } // Usage example: largeData := getLargeSlice() // -\u0026gt; slice of 1,000,000 ints smallSlice := largeData[10:20] // -\u0026gt; slice with length=10, cap=999,990 // Setting largeData to nil does not free the large array, // because smallSlice still references it. largeData = nil // The memory for the big array won\u0026#39;t be garbage collected // due to the reference from smallSlice. Solution: To avoid memory leaks, copy the data of the small slice into a new, independent slice. This allows the large underlying array to be garbage collected if no longer referenced elsewhere.\nfunc getLargeSlice() []int { largeSlice := make([]int, 1_000_000) return largeSlice } // Usage example: largeData := getLargeSlice() subset := largeData[10:20] // -\u0026gt; references big array smallSlice := make([]int, len(subset)) // -\u0026gt; new small array copy(smallSlice, subset) // -\u0026gt; copies only 10 elements largeData = nil // Now only smallSlice references a small array (cap=10) // The large array is eligible for GC. Range copies values for...range on value types iterates over copies. Modifications to the loop variable don\u0026rsquo;t change the original slice.\nslice := []int{1, 2, 3} // -\u0026gt; [1 2 3] // \u0026#34;val\u0026#34; is a copy of each element in the slice for _, val := range slice { val *= 2 // modifies only \u0026#34;val,\u0026#34; not slice } // slice remains [1 2 3] // Using an index-based loop: for i := range slice { slice[i] *= 2 // modifies the element in place } // slice is now [2 4 6] Solution: If you need to modify slice elements during iteration, use an index-based for loop. This provides direct access to each element via its index.\nslice := []int{1, 2, 3} // -\u0026gt; [1 2 3] for i := range slice { slice[i] *= 2 // modifies the original slice } // slice is now [2 4 6] Make with length initializes make([]T, length, capacity) initializes the first length elements with the zero value of T. This can be a subtle point if you expect an uninitialized slice of a certain size.\nslice := make([]int, 3, 5) // -\u0026gt; [0 0 0], cap=5 // The first 3 elements are zero-initialized slice[0] = 10 // -\u0026gt; [10 0 0] slice = append(slice, 1, 2) // -\u0026gt; [10 0 0 1 2], len=5, cap=5 emptySliceCap := make([]int, 0, 5) // -\u0026gt; [], cap=5 // This one starts with length=0, so no initial elements Solution: If you want an empty slice with a specific capacity but without initial zero values, use make([]T, 0, capacity). Or use the slice literal []T{} syntax if you don\u0026rsquo;t care about the capacity.\nIf you need a slice of a certain length initialized with zero values, make([]T, length, capacity) is the correct approach.\nemptySliceWithCap := make([]int, 0, 5) // -\u0026gt; [], cap=5 initializedSlice := make([]int, 3, 5) // -\u0026gt; [0 0 0], cap=5 Overlapping copy is tricky copy(dst, src) with overlapping slices can corrupt data when dst starts inside src.\ndata := []int{1, 2, 3, 4, 5} // -\u0026gt; [1 2 3 4 5] src := data[:] // -\u0026gt; [1 2 3 4 5] dst := data[2:] // -\u0026gt; overlap (dst starts at index 2): [3 4 5] // Copy from src to dst copy(dst, src) // Expected output: data -\u0026gt; [1 2 3 4 5] (if copied correctly) // Actual output: data -\u0026gt; [1 2 1 2 3] (corrupted) Solution: To avoid corruption, just don\u0026rsquo;t do it. If you have to, then one way to fix it is by using a temporary buffer. Even then it\u0026rsquo;s messy.\ndata := []int{1, 2, 3, 4, 5} src := data[:] dst := make([]int, len(src)-2) // New slice, shorter than src // Use a temporary buffer temp := make([]int, len(src)) // Copy from src to temp copy(temp, src) // Copy from temp to src copy(dst, temp[2:]) // Expected output: data -\u0026gt; [1 2 3 4 5] (data remains unchanged) // Actual output: data -\u0026gt; [1 2 3 4 5] // dst -\u0026gt; [3 4 5] (dst is a copy of the last part of src) Copy truncates silently copy also returns the number of elements copied, which is the smaller of len(dst) and len(src). If dst is shorter, data gets truncated.\nsrc := []int{1, 2, 3, 4, 5} // -\u0026gt; [1 2 3 4 5] dst := make([]int, 3) // -\u0026gt; [0 0 0] (length 3) copied := copy(dst, src) // Expected output: dst -\u0026gt; [1 2 3 4 5], copied -\u0026gt; 5 // Real output: dst -\u0026gt; [1 2 3], copied -\u0026gt; 3 Solution: On dst, always set the length from the src while copying.\nsrc := []int{1, 2, 3, 4, 5} // -\u0026gt; [1 2 3 4 5] dst := make([]int, len(src)) // -\u0026gt; [0 0 0 0 0] (length 5) copied := copy(dst, src) // Expected output: dst -\u0026gt; [1 2 3 4 5], copied -\u0026gt; 5 // Real output: dst -\u0026gt; [1 2 3 4 5], copied -\u0026gt; 5 I may have missed, forgotten, or not yet encountered a few other gotchas. If you\u0026rsquo;ve run into any that aren\u0026rsquo;t listed here, I\u0026rsquo;d love to hear about them.\n","permalink":"https://rednafi.com/go/slice-gotchas/","summary":"\u003cp\u003eJust like any other dynamically growable container structure, Go slices come with a few\ngotchas. I don\u0026rsquo;t always remember all the rules I need to be aware of. So this is an attempt\nto list some of the most common mistakes I\u0026rsquo;ve made at least once.\u003c/p\u003e\n\u003ch2 id=\"slices-are-views-over-arrays\"\u003eSlices are views over arrays\u003c/h2\u003e\n\u003cp\u003eIn Go, a slice is a lightweight wrapper around an array. Instead of storing data itself, it\nkeeps track of three things: a pointer to an underlying array where the data is stored, the\nnumber of elements it currently holds, and the total capacity before it needs more space.\nThe Go runtime defines it like this:\u003c/p\u003e","title":"Go slice gotchas"},{"content":"Seven years isn\u0026rsquo;t an awfully long time to work as an IC in the industry, but it\u0026rsquo;s enough to see a few cycles of change. One thing I\u0026rsquo;ve learned during this period is that, to be a key player in a business as an engineer, one of the biggest moats you can build for yourself is domain knowledge.\nWhen you know the domain well, it becomes a lot easier to weather waves of technological and managerial change. This is especially true in businesses where the tech is mostly a fleet of services communicating over some form of RPC. Doing something novel in setups like that is often hard. In situations like these, picking up the domain quickly and being able to apply a template solution is probably one of the few edges we still have over LLMs.\nTelling someone to acquire domain knowledge in a business is kind of like telling a CS grad to focus more on protocols and less on mechanisms. Protocol changes are way harder, and mechanisms shift under your feet constantly — grappling with that change is just part of the job.\nThat said, not all domain knowledge yields the same result, and it\u0026rsquo;s often not obvious which ones deserve your attention or when diving too deep into a domain can actually backfire. For example, large companies often build their own internal tooling, and knowing its quirks can be a huge advantage while you\u0026rsquo;re there. But if you switch jobs, reusing that domain knowledge can be tricky.\nOn the other hand, if you\u0026rsquo;ve worked on/with something like Bazel at Google or Cassandra/HBase at Facebook, that\u0026rsquo;s a different story. The names alone are much more marketable, and you won\u0026rsquo;t have any shortage of opportunities, regardless of whether the knowledge is reusable.\nKnowing the ins and outs of an internal feature-flagging system at your company isn\u0026rsquo;t the same as understanding the machinations of a database abstraction layer at Netflix. Not all of us work at Netflix, and finding that balance is hard. Sometimes you just have to learn enough to get by, and that\u0026rsquo;s fine — learning is part of why we get paid. But domain lock-in is real. I\u0026rsquo;ve seen extremely good engineers get stuck in super niche areas and then struggle to pivot away.\nThis is probably obvious to veterans, but it wasn\u0026rsquo;t to me when I started. I\u0026rsquo;ve seen some people get burned by over-specializing, while others pulled off moonshots by spotting opportunities that let them do fantastic, novel work. There\u0026rsquo;s a line between specialization and hyper-specialization, and most of the time, being more of a jack-of-all-trades isn\u0026rsquo;t a bad thing. At the same time, it\u0026rsquo;s neat to be able to identify those rare opportunities where getting involved early can yield outsized returns.\nP.S. Domain knowledge around a business and knowledge related to specialized tools are two different concepts. I realize this post might\u0026rsquo;ve blurred the lines, mostly for lack of a better term.\n","permalink":"https://rednafi.com/zephyr/domain-knowledge-dilemma/","summary":"\u003cp\u003eSeven years isn\u0026rsquo;t an awfully long time to work as an IC in the industry, but it\u0026rsquo;s enough to\nsee a few cycles of change. One thing I\u0026rsquo;ve learned during this period is that, to be a key\nplayer in a business as an engineer, one of the biggest moats you can build for yourself is\ndomain knowledge.\u003c/p\u003e\n\u003cp\u003eWhen you know the domain well, it becomes a lot easier to weather waves of technological and\nmanagerial change. This is especially true in businesses where the tech is mostly a fleet of\nservices communicating over some form of RPC. Doing something novel in setups like that is\noften hard. In situations like these, picking up the domain quickly and being able to apply\na template solution is probably one of the few edges we still have over LLMs.\u003c/p\u003e","title":"The domain knowledge dilemma"},{"content":"Recently at work, we ran into this problem:\nWe needed to send Slack notifications for specific events but had to enforce rate limits to avoid overwhelming the channel. Here\u0026rsquo;s how the limits worked:\nGlobal limit: Max 100 requests every 30 minutes. Category limit: Each event type (e.g., errors, warnings) capped at 10 requests per 30 minutes. Now, imagine this:\nThere are 20 event types. Each type hits its 10-notification limit in 30 minutes. That\u0026rsquo;s 200 requests total, but the global limit only allows 100. So, 100 requests must be dropped — even if some event types still have room under their individual caps. This created a hierarchy of limits:\nCategory limits keep any event type from exceeding 10 requests. The global limit ensures the combined total stays under 100. Every 30 minutes, the system resets. Here are two issues that could arise:\nIf some event types are busier, the global limit could block quieter ones. Even with room under the global limit, some event types might still hit their category caps. In our case, the event types are limited, and the category limits are both uniform and significantly smaller than the global limit, so this isn\u0026rsquo;t a concern.\nRedis sorted sets The notification sender service runs on multiple instances, each processing events and sending notifications independently. Without a shared system to enforce rate limits, these instances would maintain separate counters for global and category-specific limits. This would create inconsistencies because no instance would have a complete view of the overall activity, leading to conflicts and potential exceedance of limits.\nRedis provides a centralized state that all instances can access, ensuring they share the same counters for rate limits. This removes inconsistencies and makes rate limiting reliable, even when the notification sender scales to multiple instances.\nSorted sets in Redis track notifications within a rolling time window by using timestamps as scores, which keeps entries ordered by time. The implementation:\nMaintains a global sorted set to enforce the overall limit (e.g., 100 notifications per 30 minutes). Uses category-specific sorted sets to enforce category limits for each event type (e.g., 10 notifications per 30 minutes for errors, warnings, etc.). The limits are enforced with two Redis commands:\nZREMRANGEBYSCORE removes entries with timestamps outside the rolling time window, keeping only recent notifications. ZCARD counts the remaining entries in a set to check whether the global or category-specific limits have been reached. Lua script Instead of embedding the rate-limiting logic directly into the notification sender, we chose to implement it as a Lua script in Redis. While we could write the logic in the code and run it in a Redis pipeline, we opted not to, for the following reasons:\nA dedicated script keeps the rate-limiting logic separate and independently auditable. It saves a few TCP calls, as the entire logic runs within Redis itself. And most importantly, I wanted to write some Lua. The script is as follows:\n-- rate_limiter.lua local function check_rate_limit( global_key, category_key, global_limit, category_limit, window ) -- Get the current timestamp in seconds (including microseconds) local current_time_raw = redis.call(\u0026#39;TIME\u0026#39;) -- {secs, usecs} local current_time = current_time_raw[1] + current_time_raw[2] / 1e6 -- Step 1: Remove expired entries redis.call(\u0026#39;ZREMRANGEBYSCORE\u0026#39;, global_key, 0, current_time - window) redis.call(\u0026#39;ZREMRANGEBYSCORE\u0026#39;, category_key, 0, current_time - window) -- Step 2: Check the global limit local global_count = redis.call(\u0026#39;ZCARD\u0026#39;, global_key) if global_count \u0026gt;= global_limit then return 0 -- Reject the request if the global limit is reached end -- Step 3: Check the category-specific limit local category_count = redis.call(\u0026#39;ZCARD\u0026#39;, category_key) if category_count \u0026gt;= category_limit then return 0 -- Reject the request if the category limit is reached end -- Step 4: Add the current notification to the sorted sets redis.call(\u0026#39;ZADD\u0026#39;, global_key, current_time, current_time) redis.call(\u0026#39;ZADD\u0026#39;, category_key, current_time, current_time) return 1 -- Allow the request end -- Parameters passed to the script: -- KEYS[1]: The Redis key for the global sorted set -- KEYS[2]: The Redis key for the category-specific sorted set -- ARGV[1]: Global limit (e.g., 100) -- ARGV[2]: Category limit (e.g., 10) -- ARGV[3]: Time window in seconds (e.g., 1800 for 30 minutes) local global_key = KEYS[1] local category_key = KEYS[2] local global_limit = tonumber(ARGV[1]) local category_limit = tonumber(ARGV[2]) local window = tonumber(ARGV[3]) -- Execute the rate-limiting function and return the result return check_rate_limit( global_key, category_key, global_limit, category_limit, window ) The script performs the following operations in order:\nRemove expired entries:\nIt uses ZREMRANGEBYSCORE to remove notifications older than the time window (current_time - window). This ensures that only active notifications are considered for the limits.\nThis eliminates the need for additional bookkeeping to remove expired keys. ZREMRANGEBYSCORE is fast enough to handle the removal of a small number of keys during each invocation.\nCheck the global limit:\nZCARD counts the number of active notifications in the global sorted set. If this count equals or exceeds the global limit (e.g., 100), the request is rejected (return 0). Check the category-specific limit:\nZCARD is used again to count the active notifications for the specific category. If this count equals or exceeds the category limit (e.g., 10), the request is rejected (return 0). Add the notification:\nIf both limits are within bounds, the script uses ZADD to insert the current notification into both the global and category-specific sorted sets, using a timestamp as the score for accurate tracking. Using the script You can load the Lua script from disk, register it with Redis, and call it before invoking the notification service. If the script returns 0, drop the notification request. If it returns 1, send the notification. Here\u0026rsquo;s how to do it in Python:\nfrom redis import Redis from redis.commands.core import Script def load_lua_script(redis_client: Redis, script_path: str) -\u0026gt; Script: with open(script_path, \u0026#34;r\u0026#34;) as file: lua_script = file.read() return redis_client.register_script(lua_script) def send_notification( script: Script, global_key: str, category_key: str, global_limit: int, category_limit: int, window: int, message: str, ) -\u0026gt; None: # Check the rate limiter result: int = script( keys=[global_key, category_key], args=[global_limit, category_limit, window], ) if result == 1: # Allowed: send the notification print(f\u0026#34;Notification sent: {message}\u0026#34;) # Add actual notification-sending logic here else: # Blocked: drop the notification print(f\u0026#34;Notification dropped (rate limit exceeded): {message}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: # Connect to Redis redis_client = Redis(host=\u0026#34;localhost\u0026#34;, port=6379) # Load and register the Lua script script_path = \u0026#34;rate_limiter.lua\u0026#34; script = load_lua_script(redis_client, script_path) # Define rate limiting parameters global_key = \u0026#34;rate_limit:global\u0026#34; category_key = \u0026#34;rate_limit:category:errors\u0026#34; global_limit = 100 # Max 100 requests globally category_limit = 10 # Max 10 requests per category window = 1800 # 30-minute window # Send a single notification send_notification( script, global_key, category_key, global_limit, category_limit, window, \u0026#34;This is a single notification message\u0026#34;, ) Registering the Lua script loads it from disk once and reuses it, which is faster than repeatedly loading and evaluating it for each invocation.\nTo test this, you\u0026rsquo;ll need a running Redis instance. You can run one with Docker:\ndocker run --name redis-server -d -p 6379:6379 redis Now, running the script will print:\nNotification sent: This is a single notification message Since this sends a notification only once, the rate limiting isn\u0026rsquo;t apparent yet, but it\u0026rsquo;s working under the hood and will kick in if any limit is exceeded. To see it in action, you can attempt to send multiple notifications in a tight loop.\nTesting the rate limiter You can call the send_notification function multiple times to test the rate limiter. Below is an example that simulates several notification requests in a short loop, giving you a sense of how many will be allowed versus blocked:\nfrom redis import Redis import time def main() -\u0026gt; None: # Connect to Redis redis_client = Redis(host=\u0026#34;localhost\u0026#34;, port=6379) # Load the Lua script with open(\u0026#34;rate_limiter.lua\u0026#34;, \u0026#34;r\u0026#34;) as file: lua_script = file.read() # Register the Lua script script = redis_client.register_script(lua_script) # Example keys and arguments global_key = \u0026#34;rate_limit:global\u0026#34; category_key = \u0026#34;rate_limit:category:errors\u0026#34; global_limit = 10 category_limit = 3 window = 60 # 1 minute in seconds for this test # Run the script in a loop for i in range(10): time.sleep(0.1) result = script( keys=[global_key, category_key], args=[global_limit, category_limit, window], ) message = \u0026#34;Some notification message\u0026#34; if result == 1: # Allowed: send the notification print(f\u0026#34;{i}. Notification sent: {message}\u0026#34;) # Add actual notification-sending logic here else: # Blocked: drop the notification print( f\u0026#34;{i}. Notification dropped (rate limited): {message}\u0026#34; ) if __name__ == \u0026#34;__main__\u0026#34;: main() This code demonstrates how to test the rate limiter by simulating multiple notification requests. The Lua script is loaded, registered with Redis, and executed in a loop to evaluate whether each request is allowed or blocked based on the defined rate limits.\nRunning this will produce output similar to:\n0. Notification sent: Some notification message 1. Notification sent: Some notification message 2. Notification sent: Some notification message 3. Notification dropped (rate limit exceeded): Some notification message 4. Notification dropped (rate limit exceeded): Some notification message 5. Notification dropped (rate limit exceeded): Some notification message 6. Notification dropped (rate limit exceeded): Some notification message 7. Notification dropped (rate limit exceeded): Some notification message 8. Notification dropped (rate limit exceeded): Some notification message 9. Notification dropped (rate limit exceeded): Some notification message Here, for demonstration, we set the global rate limit to 10 and the category limit to 3 with a 60-second rolling window. After three successful category notifications (and a total of three global notifications), the rate limiter rejects additional requests in the same window, illustrating how both the global and category limits work together.\n","permalink":"https://rednafi.com/misc/hierarchical-rate-limiting/","summary":"\u003cp\u003eRecently at work, we ran into this problem:\u003c/p\u003e\n\u003cp\u003eWe needed to send Slack notifications for specific events but had to enforce rate limits to\navoid overwhelming the channel. Here\u0026rsquo;s how the limits worked:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGlobal limit\u003c/strong\u003e: Max 100 requests every 30 minutes.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCategory limit\u003c/strong\u003e: Each event type (e.g., errors, warnings) capped at 10 requests per 30\nminutes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNow, imagine this:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThere are 20 event types.\u003c/li\u003e\n\u003cli\u003eEach type hits its 10-notification limit in 30 minutes.\u003c/li\u003e\n\u003cli\u003eThat\u0026rsquo;s 200 requests total, but the global limit only allows 100. So, 100 requests must be\ndropped — even if some event types still have room under their individual caps.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis created a \u003cstrong\u003ehierarchy of limits\u003c/strong\u003e:\u003c/p\u003e","title":"Hierarchical rate limiting with Redis sorted sets"},{"content":"I came across a weird shell syntax today — dynamic shell variables. It lets you dynamically construct and access variable names in Bash scripts, which I haven\u0026rsquo;t encountered in any of the mainstream languages I juggle for work.\nIn an actual programming language, you\u0026rsquo;d usually use a hashmap to achieve the same effect, but directly templating variable names is a quirky shell feature that sometimes comes in handy.\nA primer Dynamic shell variables allow shell scripts to define and access variables based on runtime conditions. Variable indirection (${!var} syntax) lets you reference the value of a variable through another variable. This can be useful for managing environment-specific configurations and function dispatch mechanisms.\nHere\u0026rsquo;s an example:\n#!/usr/bin/env bash # script.sh config_path=\u0026#34;/etc/config\u0026#34; var=\u0026#34;config_path\u0026#34; echo \u0026#34;The value of \\$config_path is: ${!var}\u0026#34; The value of $config_path is: /etc/config Here, ${!var} resolves to the value of the variable config_path because var contains its name. This allows you to dynamically decide which variable to reference at runtime.\nContext-aware environment management A more practical use of dynamic shell variables is managing environment-specific configurations. This is particularly handy in scenarios where you have multiple environments like staging and prod, each with its own unique configuration settings.\n#!/usr/bin/env bash # script.sh # Define environment-specific configurations dynamically declare staging_URL=\u0026#34;https://staging.example.com\u0026#34; declare staging_PORT=8081 declare prod_URL=\u0026#34;https://example.com\u0026#34; declare prod_PORT=80 # Set the current environment env=$1 # Validate input if [[ \u0026#34;$env\u0026#34; != \u0026#34;staging\u0026#34; \u0026amp;\u0026amp; \u0026#34;$env\u0026#34; != \u0026#34;prod\u0026#34; ]]; then echo \u0026#34;Invalid environment. Please specify \u0026#39;staging\u0026#39; or \u0026#39;prod\u0026#39;.\u0026#34; exit 1 fi # Dynamically access the environment-specific variables URL=\u0026#34;${env}_URL\u0026#34; PORT=\u0026#34;${env}_PORT\u0026#34; echo \u0026#34;URL: ${!URL}\u0026#34; echo \u0026#34;Port: ${!PORT}\u0026#34; Run the script with an environment as the argument:\n./script.sh staging Output for env=\u0026quot;staging\u0026quot;:\nURL: https://staging.example.com Port: 8081 By passing the environment as an argument, you can switch between environments without duplicating configuration logic.\nOne gotcha to be aware of is that appending text directly to the ${!VAR} syntax (e.g., ${!env}_URL) doesn\u0026rsquo;t produce the intended results. Instead of resolving staging_URL, this line will print only _URL:\necho \u0026#34;${!env}_URL\u0026#34; Output:\n_URL This happens because ${!VAR} only resolves the value of VAR and doesn\u0026rsquo;t support direct concatenation. To avoid this, construct the full variable name (URL=\u0026quot;${env}_URL\u0026quot;) before using ${!VAR} for indirect expansion. This ensures the correct variable is accessed.\nFunction dispatch Another neat use case for dynamic variables is function dispatch — calling the appropriate function based on runtime conditions. This technique can be used to simplify scripts that need to handle multiple services or operations.\n#!/usr/bin/env bash # script.sh # Define functions for operations on different services web_start() { echo \u0026#34;Starting web service...\u0026#34; } web_stop() { echo \u0026#34;Stopping web service...\u0026#34; } db_status() { echo \u0026#34;Checking database status...\u0026#34; } # Dynamically bind operation to function declare web_start_function=\u0026#34;web_start\u0026#34; declare web_stop_function=\u0026#34;web_stop\u0026#34; declare db_status_function=\u0026#34;db_status\u0026#34; # Input variables for service and operation service=$1 operation=$2 # Build dynamic function name func=\u0026#34;${service}_${operation}_function\u0026#34; # Dispatch function dynamically if [[ $(type -t ${!func}) == \u0026#34;function\u0026#34; ]]; then ${!func} # Call the dynamically resolved function else echo \u0026#34;Unknown operation: $service $operation\u0026#34; fi Run the script with service and operation as arguments:\n./script.sh web start This returns:\nStarting web service... Similarly, running ./script.sh db status prints:\nChecking database status... Temporary file handling Dynamic variables can also help manage temporary files or logs in scripts that process multiple datasets. By dynamically generating variable names, you can track temporary file paths for each dataset without conflicts.\n#!/usr/bin/env bash # script.sh # Process multiple datasets with temporary files for dataset in data1 data2 data3; do # Dynamically declare a temporary file variable temp_file_var=\u0026#34;${dataset}_temp_file\u0026#34; declare $temp_file_var=\u0026#34;/tmp/${dataset}_processing.tmp\u0026#34; # Simulate processing and logging echo \u0026#34;Processing $dataset...\u0026#34; \u0026gt; ${!temp_file_var} cat ${!temp_file_var} # Clean up (or add a trap to make this more robust) rm -f ${!temp_file_var} done Running this prints the following:\nProcessing data1... Processing data2... Processing data3... Here, each dataset gets a unique temporary file, managed dynamically by the script. It eliminates the need for manually creating and tracking file names.\nThis works, but like everything else in shell scripts, it can quickly turn into a hairball if we\u0026rsquo;re not careful. While the syntax is nifty, I find it a bit hard to read at times!\n","permalink":"https://rednafi.com/misc/dynamic-shell-variables/","summary":"\u003cp\u003eI came across a weird shell syntax today — dynamic shell variables. It lets you dynamically\nconstruct and access variable names in Bash scripts, which I haven\u0026rsquo;t encountered in any of\nthe mainstream languages I juggle for work.\u003c/p\u003e\n\u003cp\u003eIn an actual programming language, you\u0026rsquo;d usually use a hashmap to achieve the same effect,\nbut directly templating variable names is a quirky shell feature that sometimes comes in\nhandy.\u003c/p\u003e\n\u003ch2 id=\"a-primer\"\u003eA primer\u003c/h2\u003e\n\u003cp\u003eDynamic shell variables allow shell scripts to define and access variables based on runtime\nconditions. Variable indirection (\u003ccode\u003e${!var}\u003c/code\u003e syntax) lets you reference the value of a\nvariable through another variable. This can be useful for managing environment-specific\nconfigurations and function dispatch mechanisms.\u003c/p\u003e","title":"Dynamic shell variables"},{"content":"One of my 2025 resolutions is doing things that don\u0026rsquo;t scale and doing them faster without overthinking. The idea is to focus on doing more while worrying less about scalability and sustainability in the things I do outside of work. With that in mind, I\u0026rsquo;ve been thinking for a while about tracking some of my out-of-band activities on this blog. The goal is to:\nList the things I do, books and articles I read, and talks I grok. Add some commentary or quote something I liked from the content, verbatim, for posterity. Not spam people who just want to read the regular articles. Not turn into a content junkie, churning out slop I wouldn\u0026rsquo;t want to read myself. This isn\u0026rsquo;t about getting more eyeballs on what I publish. It\u0026rsquo;s about tracking what I do so I can look back at the end of the year and enjoy a nice little lull of accomplishment. Plus, having a place to post stuff regularly nudges me to read more, explore more, and do more of the things I actually want to do.\nSocial media is usually where people do this, but digging up old posts and reviewing them later is a pain. Plus, platforms like Twitter tank your posts\u0026rsquo; visibility if they have links to other sites. A simple link blog solves all that for me.\nHaving a dynamic backend would make setting up a link blog trivial. But I\u0026rsquo;m too lazy to maintain one. Right now, this site is just a static Hugo build with some custom CSS flair. Adding a link blog without a dedicated backend is the tricky part.\nIf you don\u0026rsquo;t mind maintaining a dynamic site, Simon Willison has a great piece on running a link blog.\nHere\u0026rsquo;s what has worked for me I went with a dead simple approach that has worked for me for the past 5 months:\nDetail view: Have a single markdown page dedicated to each year and add the links there. See 2025 feed detail view. List view: Cluster the detail pages in a list view named feed. Detail view Each year gets its own detail page where I add links in reverse chronological order. The content has the following structure in markdown:\n--- title: \u0026#34;2024\u0026#34; layout: post --- ### December 26 #### [Reflecting on life – Armin Ronacher][32] ... [32]: https://lucumr.pocoo.org/2024/12/26/reflecting-on-life/ --- ### December 24 #### [How I write HTTP services in Go after 13 years – Mat Ryer][31] ... [31]: https://grafana.com/blog/2024/02/09/how-i-write-http-services/ --- Hugo renders the detail view like this:\nList view I use the Papermod theme for this site, and it automatically creates a list view from the yearly pages.\nTo avoid spamming people, the entries in the link blog are filtered out of the main RSS feed. Also, the client-side search functionality allows me to look for a particular link without having to maintain any extra infra.\nThis solution is simple and has been working well for me. I started it in August last year, and the 2024 feed page already has enough entries to make me feel good when I reviewed them at the end of the year.\n","permalink":"https://rednafi.com/misc/link-blog/","summary":"\u003cp\u003eOne of my 2025 resolutions is doing things that don\u0026rsquo;t scale and doing them faster without\noverthinking. The idea is to focus on doing more while worrying less about scalability and\nsustainability in the things I do outside of work. With that in mind, I\u0026rsquo;ve been thinking for\na while about tracking some of my out-of-band activities on this blog. The goal is to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eList the things I do, books and articles I read, and talks I grok.\u003c/li\u003e\n\u003cli\u003eAdd some commentary or quote something I liked from the content, verbatim, for posterity.\u003c/li\u003e\n\u003cli\u003eNot spam people who just want to read the regular articles.\u003c/li\u003e\n\u003cli\u003eNot turn into a content junkie, churning out slop I wouldn\u0026rsquo;t want to read myself.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis isn\u0026rsquo;t about getting more eyeballs on what I publish. It\u0026rsquo;s about tracking what I do so I\ncan look back at the end of the year and enjoy a nice little lull of accomplishment. Plus,\nhaving a place to post stuff regularly nudges me to read more, explore more, and do more of\nthe things I actually want to do.\u003c/p\u003e","title":"Link blog in a static site"},{"content":"I\u0026rsquo;ve been having a ton of fun fiddling with Tailscale over the past few days. While setting it up on a server, I came across this ufw firewall script that configures the firewall on Linux to ensure direct communication across different nodes in my tailnet. It has the following block of code that I found interesting (added comments for clarity):\n#!/usr/bin/env bash # Define PID file path using script\u0026#39;s name for uniqueness PIDFILE=\u0026#34;/tmp/$(basename \u0026#34;${BASH_SOURCE[0]%.*}.pid\u0026#34;)\u0026#34; # Open file descriptor 200 for the PID file exec 200\u0026gt;\u0026#34;${PIDFILE}\u0026#34; # Try to acquire a non-blocking lock; exit if the script is already running flock -n 200 \\ || { echo \u0026#34;${BASH_SOURCE[0]} already running. Aborting...\u0026#34;; exit 1; } # Store the current process ID (PID) in the lock file for reference PID=$$ echo \u0026#34;${PID}\u0026#34; 1\u0026gt;\u0026amp;200 # Do work (in the original script, real work happens here) sleep 999 Here, flock is a Linux command that ensures only one instance of the script runs at a time by locking a specified file (e.g., PIDFILE) through a file descriptor (e.g., 200). If another process already holds the lock, the script either waits or exits immediately. Above, it bails with an error message and exit code 1.\nIf you try running two instances of this script, the second one will exit with this message:\n\u0026lt;script-name\u0026gt; script is already running. Aborting... On most Linux distros, flock comes along with the coreutils. If not, it\u0026rsquo;s easy to install with your preferred package manager.\nA more portable version On macOS, the file locking mechanism is different, and flock doesn\u0026rsquo;t work there. To make your script portable, you can use mkdir in the following manner to achieve a similar result:\n#!/usr/bin/env bash LOCKDIR=\u0026#34;/tmp/$(basename \u0026#34;${BASH_SOURCE[0]%.*}.lock\u0026#34;)\u0026#34; # Try to create the lock directory mkdir \u0026#34;${LOCKDIR}\u0026#34; 2\u0026gt;/dev/null || { echo \u0026#34;Another instance is running. Aborting...\u0026#34; exit 1 } # Set up cleanup for the lock directory trap \u0026#34;rmdir \\\u0026#34;${LOCKDIR}\\\u0026#34;\u0026#34; EXIT # Main script logic echo \u0026#34;Acquired lock, doing important stuff...\u0026#34; # ... your script logic ... sleep 999 This works because mkdir is atomic. It creates the lock directory (LOCKDIR) in /tmp or fails if the directory already exists. This acts as a marker for the running instance. If successful, the script sets up a trap to remove the directory on exit and continues to the main logic. If mkdir fails, it means another instance of the process is running, and the script exits with a message.\nThis is almost as effective as the flock version. Since I rarely write scripts for non-Linux environments, either option is fine!\nWith Python Oftentimes, I opt for Python when I need to write larger scripts. The same can be achieved in Python like this:\nimport fcntl import os import sys import time # Use the script name to generate a unique lock file LOCKFILE = f\u0026#34;/tmp/{os.path.basename(__file__)}.lock\u0026#34; def work() -\u0026gt; None: time.sleep(999) if __name__ == \u0026#34;__main__\u0026#34;: try: # Open a file and acquire an exclusive lock with open(LOCKFILE, \u0026#34;w\u0026#34;) as lockfile: fcntl.flock(lockfile, fcntl.LOCK_EX | fcntl.LOCK_NB) print(\u0026#34;Acquired lock, running script...\u0026#34;) # Main script logic here work() except BlockingIOError: print(\u0026#34;Another instance is running. Exiting.\u0026#34;) sys.exit(1) The script uses fcntl.flock to prevent multiple instances from running. It creates a lock file (LOCKFILE) in the /tmp directory, named after the scripts filename. When the script starts, it opens the file in write mode and tries to lock it with fcntl.flock using an exclusive lock (LOCK_EX). The LOCK_NB flag makes the operation non-blocking. If another process holds the lock, the script exits with a message.\nThis approach works on both Linux and macOS, as both support fcntl for file-based locks. The lock is automatically released when the file is closed, either at the end of the script or the with block.\nWith Go I was curious about doing it in Go. It\u0026rsquo;s quite similar to Python:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;time\u0026#34; ) // Use the script name (basename) to generate a unique lock file var lockfile = fmt.Sprintf(\u0026#34;/tmp/%s.lock\u0026#34;, filepath.Base(os.Args[0])) func work() { time.Sleep(999 * time.Second) } func main() { // Open the lock file file, err := os.OpenFile(lockfile, os.O_CREATE|os.O_RDWR, 0644) if err != nil { fmt.Println(\u0026#34;Failed to open lock file:\u0026#34;, err) os.Exit(1) } defer file.Close() // Try to acquire an exclusive lock err = syscall.Flock(int(file.Fd()), syscall.LOCK_EX|syscall.LOCK_NB) if err != nil { fmt.Println(\u0026#34;Another instance is running. Exiting.\u0026#34;) os.Exit(1) } // Release the lock on exit defer syscall.Flock(int(file.Fd()), syscall.LOCK_UN) fmt.Println(\u0026#34;Acquired lock, running script...\u0026#34;) // Main script logic work() } Like the Python example, this uses syscall.Flock to prevent multiple script instances. It creates a lock file based on the script\u0026rsquo;s name using filepath.Base(os.Args[0]) and stores it in /tmp. The script tries to acquire an exclusive, non-blocking lock (LOCK_EX | LOCK_NB). If unavailable, it exits with a message. The lock is automatically released when the file is closed in the defer block.\nUnderneath, Go makes sure that syscall.Flock works on both macOS and Linux.\n","permalink":"https://rednafi.com/misc/run-single-instance/","summary":"\u003cp\u003eI\u0026rsquo;ve been having a ton of fun fiddling with \u003ca href=\"https://tailscale.com/\"\u003eTailscale\u003c/a\u003e over the past few days. While\nsetting it up on a server, I came across this \u003ca href=\"https://github.com/AT3K/Tailscale-Firewall-Setup/blob/main/update_tailscale_ufw_rules.sh\"\u003eufw firewall script\u003c/a\u003e that configures the\nfirewall on Linux to ensure direct communication across different nodes in my tailnet. It\nhas the following block of code that I found interesting (added comments for clarity):\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cp\"\u003e#!/usr/bin/env bash\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Define PID file path using script\u0026#39;s name for uniqueness\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003ePIDFILE\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;/tmp/\u003c/span\u003e\u003cspan class=\"k\"\u003e$(\u003c/span\u003ebasename \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"si\"\u003e${\u003c/span\u003e\u003cspan class=\"nv\"\u003eBASH_SOURCE\u003c/span\u003e\u003cspan class=\"p\"\u003e[0]%.*\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e.pid\u0026#34;\u003c/span\u003e\u003cspan class=\"k\"\u003e)\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Open file descriptor 200 for the PID file\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eexec\u003c/span\u003e 200\u0026gt;\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"si\"\u003e${\u003c/span\u003e\u003cspan class=\"nv\"\u003ePIDFILE\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Try to acquire a non-blocking lock; exit if the script is already running\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eflock -n \u003cspan class=\"m\"\u003e200\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"o\"\u003e||\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"nb\"\u003eecho\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"si\"\u003e${\u003c/span\u003e\u003cspan class=\"nv\"\u003eBASH_SOURCE\u003c/span\u003e\u003cspan class=\"p\"\u003e[0]\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e already running. Aborting...\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"nb\"\u003eexit\u003c/span\u003e 1\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Store the current process ID (PID) in the lock file for reference\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003ePID\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"nv\"\u003e$$\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eecho\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"si\"\u003e${\u003c/span\u003e\u003cspan class=\"nv\"\u003ePID\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e 1\u0026gt;\u003cspan class=\"p\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"m\"\u003e200\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Do work (in the original script, real work happens here)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esleep \u003cspan class=\"m\"\u003e999\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHere, \u003ccode\u003eflock\u003c/code\u003e is a Linux command that ensures only one instance of the script runs at a time\nby locking a specified file (e.g., \u003ccode\u003ePIDFILE\u003c/code\u003e) through a file descriptor (e.g., \u003ccode\u003e200\u003c/code\u003e). If\nanother process already holds the lock, the script either waits or exits immediately. Above,\nit bails with an error message and exit code 1.\u003c/p\u003e","title":"Running only a single instance of a process"},{"content":"People love single-method interfaces (SMIs) in Go. They\u0026rsquo;re simple to implement and easy to reason about. The standard library is packed with SMIs like io.Reader, io.Writer, io.Closer, io.Seeker, and more.\nOne cool thing about SMIs is that you don\u0026rsquo;t always need to create a full-blown struct with a method to satisfy the interface. You can define a function type, attach the interface method to it, and use it right away. This approach works well when there\u0026rsquo;s no state to maintain, so the extra struct becomes unnecessary. However, I find the syntax for this a bit abstruse. So, I\u0026rsquo;m jotting down a few examples here to reference later.\nUsing a struct to implement an interface This is how interfaces are typically implemented. Here, we\u0026rsquo;ll satisfy the io.Writer interface to create a writer that logs some stats before saving data to an in-memory buffer.\nThe standard library defines io.Writer like this:\ntype Writer interface { Write(p []byte) (n int, err error) } We can implement io.Writer by defining a struct type, LoggingWriter, and attaching a Write method with the required signature:\n// LoggingWriter writes data to an underlying writer and logs stats. type LoggingWriter struct { w io.Writer } func (lw *LoggingWriter) Write(data []byte) (int, error) { fmt.Printf(\u0026#34;LoggingWriter: Writing %d bytes\\n\u0026#34;, len(data)) return lw.w.Write(data) } Here\u0026rsquo;s how to use it:\nfunc main() { var buf bytes.Buffer logWriter := \u0026amp;LoggingWriter{w: \u0026amp;buf} _, err := logWriter.Write([]byte(\u0026#34;Hello, world!\u0026#34;)) if err != nil { fmt.Println(\u0026#34;Error writing data:\u0026#34;, err) return } fmt.Println(\u0026#34;Buffer content:\u0026#34;, buf.String()) } Running this will log the stats before writing to the buffer:\nLoggingWriter: Writing 13 bytes Buffer content: Hello, world! Using a function type instead Instead of defining the LoggingWriter struct, you can use a function type to satisfy io.Writer. This works well for SMIs but doesn\u0026rsquo;t make sense for interfaces with multiple methods. In those cases, we need to resort to the methods-on-struct approach.\nHere\u0026rsquo;s how it looks:\n// WriteFunc is a function type that implements io.Writer. type WriteFunc func(data []byte) (int, error) // Write makes WriteFunc satisfy io.Writer. func (wf WriteFunc) Write(data []byte) (int, error) { return wf(data) } You can use WriteFunc like this:\nfunc main() { var buf bytes.Buffer // Define a WriteFunc to log stats and write data. logWriter := WriteFunc(func(data []byte) (int, error) { fmt.Printf(\u0026#34;WriteFunc: Writing %d bytes\\n\u0026#34;, len(data)) return buf.Write(data) }) _, err := logWriter.Write([]byte(\u0026#34;Hello, world!\u0026#34;)) if err != nil { fmt.Println(\u0026#34;Error writing data:\u0026#34;, err) return } fmt.Println(\u0026#34;Buffer content:\u0026#34;, buf.String()) } WriteFunc satisfies io.Writer by defining a Write method with the expected signature. You can adapt any function to match the signature (data []byte) (int, error) using WriteFunc, so there\u0026rsquo;s no need for a struct when no state is involved.\nIn main, an anonymous function logs the number of bytes and writes the data to a buffer. Wrapping this function with WriteFunc lets it implement the io.Writer interface. The .Write method is called on the wrapped function to log stats and write data to the buffer. Finally, the buffer\u0026rsquo;s content is printed to verify everything worked.\nFor a simple example like this, using a function type to implement an interface might feel like overkill. But there are cases where it simplifies things. The next sections explore real-world examples where function types make interface implementation a bit more ergonomic.\nMocking interfaces for testing Function types let you mock interfaces without creating dedicated structs. Here\u0026rsquo;s how it works with an Authenticator interface:\ntype Authenticator interface { Authenticate(username, password string) (bool, error) } type AuthFunc func(username, password string) (bool, error) func (af AuthFunc) Authenticate(username, password string) (bool, error) { return af(username, password) } The AuthFunc type implements the Authenticate method by calling itself with the provided arguments. This lets you create mock implementations inline in your tests.\nHere\u0026rsquo;s how to use it in a test:\nfunc TestLogin(t *testing.T) { mockAuth := AuthFunc(func(u, p string) (bool, error) { fmt.Printf(\u0026#34;MockAuth called with username=%s, password=%s\\n\u0026#34;, u, p) return true, nil }) success, err := PerformLogin(\u0026#34;john_doe\u0026#34;, \u0026#34;secret\u0026#34;, mockAuth) if err != nil || !success { t.Fatalf(\u0026#34;Authentication failed\u0026#34;) } } And in application code:\nfunc main() { auth := AuthFunc(func(u, p string) (bool, error) { return u == \u0026#34;admin\u0026#34; \u0026amp;\u0026amp; p == \u0026#34;password123\u0026#34;, nil }) if success, _ := auth.Authenticate(\u0026#34;admin\u0026#34;, \u0026#34;password123\u0026#34;); success { fmt.Println(\u0026#34;Authentication successful!\u0026#34;) } } Building HTTP middlewares The standard library\u0026rsquo;s http.HandlerFunc demonstrates function types in action. Here\u0026rsquo;s how to build a logging middleware that times requests:\ntype Handler interface { ServeHTTP(http.ResponseWriter, *http.Request) } func LoggingMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { start := time.Now() fmt.Printf(\u0026#34;Started %s %s\\n\u0026#34;, r.Method, r.URL.Path) next.ServeHTTP(w, r) fmt.Printf(\u0026#34;Completed %s in %v\\n\u0026#34;, r.URL.Path, time.Since(start)) }) } http.HandlerFunc converts functions into HTTP handlers. The logging middleware wraps the next handler and adds timing and logging.\nWe use it as follows:\nfunc main() { handler := http.HandlerFunc( func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello, World!\u0026#34;) }, ) http.Handle(\u0026#34;/\u0026#34;, LoggingMiddleware(handler)) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } Adapting function types for database queries Function types can abstract database query execution for testing or supporting different database implementations:\ntype QueryExecutor interface { Execute(query string, args ...any) (Result, error) } type QueryFunc func(query string, args ...any) (Result, error) func (qf QueryFunc) Execute(query string, args ...any) (Result, error) { return qf(query, args...) } QueryFunc turns regular functions into QueryExecutor implementations, making it easy to swap implementations or create mocks.\nThis is how to use it:\nfunc main() { executor := QueryFunc(func(query string, args ...any) (Result, error) { fmt.Printf(\u0026#34;Executing query: %s with args: %v\\n\u0026#34;, query, args) return Result{RowsAffected: 1}, nil }) result, _ := executor.Execute(\u0026#34;SELECT * FROM users WHERE id = ?\u0026#34;, 1) fmt.Printf(\u0026#34;Rows affected: %d\\n\u0026#34;, result.RowsAffected) } Implementing retry logic Function types can encapsulate retry behavior without creating configuration structs:\ntype Retryer interface { Retry(fn func() error) error } type RetryFunc func(fn func() error) error func (rf RetryFunc) Retry(fn func() error) error { return rf(fn) } RetryFunc converts functions with the matching signature into a Retryer, letting you swap retry strategies or create test versions.\nHere\u0026rsquo;s how to use it:\nfunc main() { retry := RetryFunc(func(fn func() error) error { for i := 0; i \u0026lt; 3; i++ { if err := fn(); err == nil { return nil } time.Sleep(time.Second * time.Duration(i+1)) } return fmt.Errorf(\u0026#34;operation failed after 3 retries\u0026#34;) }) err := retry.Retry(func() error { return nil // Your operation here }) if err != nil { fmt.Printf(\u0026#34;Failed to execute operation: %v\\n\u0026#34;, err) } } Go lets us define methods on custom types, including function types. While this can be handy for adapting a function type to an interface, it can make the code hard to read at times. So I don\u0026rsquo;t always reach for it. It\u0026rsquo;s perfectly fine to define an empty struct with a single method if that makes the code more readable. Nonetheless, it\u0026rsquo;s a neat trick to keep in your repertoire.\n","permalink":"https://rednafi.com/go/func-types-and-smis/","summary":"\u003cp\u003ePeople love single-method interfaces (SMIs) in Go. They\u0026rsquo;re simple to implement and easy to\nreason about. The standard library is packed with SMIs like \u003ccode\u003eio.Reader\u003c/code\u003e, \u003ccode\u003eio.Writer\u003c/code\u003e,\n\u003ccode\u003eio.Closer\u003c/code\u003e, \u003ccode\u003eio.Seeker\u003c/code\u003e, and more.\u003c/p\u003e\n\u003cp\u003eOne cool thing about SMIs is that you don\u0026rsquo;t always need to create a full-blown struct with a\nmethod to satisfy the interface. You can define a function type, attach the interface method\nto it, and use it right away. This approach works well when there\u0026rsquo;s no state to maintain, so\nthe extra struct becomes unnecessary. However, I find the syntax for this a bit abstruse.\nSo, I\u0026rsquo;m jotting down a few examples here to reference later.\u003c/p\u003e","title":"Function types and single-method interfaces in Go"},{"content":"Setting up SSH access to a new VM usually follows the same routine: generate a key pair, copy it to the VM, tweak some configs, confirm the host\u0026rsquo;s identity, and maybe set up an agent to avoid typing passphrases all day. Tools like cloud-init and Ansible handle most of the setup for me now, so I rarely think about it. But I realized I don\u0026rsquo;t fully understand how all the parts work together.\nThis post attempts to give an overview of what happens when you type ssh user@host. It covers key pairs, authorized_keys, sshd_config, ~/.ssh/config, known_hosts, and how they all fit together.\nA new VM in the void You\u0026rsquo;ve provisioned a new VM and need key-based SSH access. This involves generating a key pair on your local machine, installing the public key on the remote VM, and ensuring the SSH daemon (sshd) on the VM trusts it. Done right, ssh user@host drops you into a shell without a password prompt.\nFirst, generate a key pair on your local machine:\nssh-keygen -t ed25519 -C \u0026#34;your_email@example.com\u0026#34; This creates two files: a private key (~/.ssh/id_ed25519) and a public key (~/.ssh/id_ed25519.pub). The private key stays local. The public key is shared.\nYour local fortress Your local private key proves your identity and must be protected (~/.ssh/id_ed25519). The public key (~/.ssh/id_ed25519.pub) gets copied to the VM.\nTo view the public key locally:\ncat ~/.ssh/id_ed25519.pub # Output: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIG... user@local Copy this public key to the VM.\nThe remote gatekeeper On the VM, sshd listens for connections and authenticates users. Its configuration file, /etc/ssh/sshd_config, defines policies: whether password logins are allowed, which keys are trusted, and which crypto settings to use. A hardened snippet might look like this:\n# /etc/ssh/sshd_config (on the VM) PermitRootLogin no PasswordAuthentication no PubkeyAuthentication yes AuthorizedKeysFile .ssh/authorized_keys With PasswordAuthentication no, only keys can unlock access.\nAuthorized keys and the handshake The ~/.ssh/authorized_keys file on the VM decides who gets access. Add your public key there to tell sshd that anyone holding the matching private key (you) can connect.\nOn the VM, under the user\u0026rsquo;s home directory:\nmkdir -p ~/.ssh chmod 700 ~/.ssh echo \u0026#34;ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIG... user@local\u0026#34; \\ \u0026gt;\u0026gt; ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys Now when you run ssh user@host, the server matches your key to one in authorized_keys and lets you in.\nThe client and its configs Your local SSH client can be configured via ~/.ssh/config to simplify hostnames, ports, and key paths:\n# ~/.ssh/config (on your local machine) Host myvm HostName 203.0.113.10 User ubuntu IdentityFile ~/.ssh/id_ed25519 Port 22 Now you can connect with:\nssh myvm Known hosts and server identity When you connect to the VM for the first time, SSH prompts you to confirm its identity. Accepting it adds the server\u0026rsquo;s host key to ~/.ssh/known_hosts. SSH will detect any identity changes during subsequent connections:\n203.0.113.10 ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAI... This prevents man-in-the-middle attacks.\nAgent and forwarding If your private key has a passphrase, typing it constantly is annoying. ssh-agent caches the key in memory, so you only unlock it once:\neval \u0026#34;$(ssh-agent -s)\u0026#34; ssh-add ~/.ssh/id_ed25519 # Enter passphrase once For hopping through multiple servers (local → bastion → internal server), enable agent forwarding:\n# ~/.ssh/config (local) Host myvm HostName 203.0.113.10 User ubuntu IdentityFile ~/.ssh/id_ed25519 ForwardAgent yes Now, you can connect through intermediary hosts without copying private keys around.\nConfiguring and hardening the daemon On the VM, refine /etc/ssh/sshd_config to enforce stricter security:\n# /etc/ssh/sshd_config (on the VM) PasswordAuthentication no PermitRootLogin prohibit-password AllowUsers ubuntu PubkeyAuthentication yes KexAlgorithms curve25519-sha256@libssh.org Ciphers aes256-gcm@openssh.com,chacha20-poly1305@openssh.com MACs hmac-sha2-512-etm@openssh.com These settings ensure that only trusted keys with modern crypto algorithms can connect.\nBringing it all together Here\u0026rsquo;s a quick summary of setting up SSH connection to a new machine:\nGenerate a key pair on your local machine. Copy the public key to the authorized_keys file on the VM. Configure sshd_config on the VM to allow key-based authentication. Set up ~/.ssh/config on your local machine to simplify SSH commands (e.g., ssh myvm instead of ssh \u0026lt;vm-ip\u0026gt;). Confirm the server\u0026rsquo;s identity and save it to known_hosts on your local machine. Use ssh-agent on your local machine to cache your private key and avoid typing the passphrase repeatedly. Enable agent forwarding in your SSH config to connect through intermediary servers without copying keys. Harden sshd_config on the VM to enforce modern crypto algorithms and stricter security policies. ┌──────────────────────┐ │ LOCAL │ │ ~/.ssh/config │ │ ~/.ssh/id_* │ │ ~/.ssh/known_hosts │ │ ssh-agent │ └───▲───────┬──────────┘ │ │ │ │ SSH Connection (Port 22) │ │ │ │ │ ▼ ┌────────────────────────┐ │ REMOTE │ │ /etc/ssh/sshd_config │ │ ~/.ssh/authorized_keys │ │ sshd daemon │ └────────────────────────┘ ","permalink":"https://rednafi.com/misc/ssh-saga/","summary":"\u003cp\u003eSetting up SSH access to a new VM usually follows the same routine: generate a key pair,\ncopy it to the VM, tweak some configs, confirm the host\u0026rsquo;s identity, and maybe set up an\nagent to avoid typing passphrases all day. Tools like cloud-init and Ansible handle most of\nthe setup for me now, so I rarely think about it. But I realized I don\u0026rsquo;t fully understand\nhow all the parts work together.\u003c/p\u003e","title":"SSH saga"},{"content":"Sometimes, when writing tests in Pytest, I find myself using fixtures that the test function/method doesn\u0026rsquo;t directly reference. Instead, Pytest runs the fixture, and the test function implicitly leverages its side effects. For example:\nimport os from collections.abc import Iterator from unittest.mock import Mock, patch import pytest # Define an implicit environment mock fixture that patches os.environ @pytest.fixture def mock_env() -\u0026gt; Iterator[None]: with patch.dict(\u0026#34;os.environ\u0026#34;, {\u0026#34;IMPLICIT_KEY\u0026#34;: \u0026#34;IMPLICIT_VALUE\u0026#34;}): yield # Define an explicit service mock fixture @pytest.fixture def mock_svc() -\u0026gt; Mock: service = Mock() service.process.return_value = \u0026#34;Explicit Mocked Response\u0026#34; return service # IDEs tend to dim out unused parameters like mock_env def test_stuff(mock_svc: Mock, mock_env: Mock) -\u0026gt; None: # Use the explicit mock response = mock_svc.process() assert response == \u0026#34;Explicit Mocked Response\u0026#34; mock_svc.process.assert_called_once() # Assert the environment variable patched by mock_env assert os.environ[\u0026#34;IMPLICIT_KEY\u0026#34;] == \u0026#34;IMPLICIT_VALUE\u0026#34; In the test_stuff function above, we directly use the mock_svc fixture but not mock_env. Instead, we expect Pytest to run mock_env, which modifies the environment variables. This works, but IDEs often mark mock_env as an unused parameter and dims it out.\nOne way to avoid this is by marking the mock_env fixture with @pytest.fixture(autouse=True) and omitting it from the test function\u0026rsquo;s parameters. However, I prefer not to use autouse=True because it can make reasoning about tests harder.\nTIL that you can use @pytest.mark.usefixtures to inject these implicit fixtures without cluttering the test function signature or using autouse. Here\u0026rsquo;s the same test marked with usefixtures:\n# ... same as above @pytest.mark.usefixtures(\u0026#34;mock_env\u0026#34;) def test_stuff(mock_svc: Mock) -\u0026gt; None: # Use the explicit mock response = mock_svc.process() assert response == \u0026#34;Explicit Mocked Response\u0026#34; mock_svc.process.assert_called_once() # Assert the environment variable patched by mock_env assert os.environ[\u0026#34;IMPLICIT_KEY\u0026#34;] == \u0026#34;IMPLICIT_VALUE\u0026#34; Now, the mock_env fixture is applied without cluttering the test function\u0026rsquo;s signature, and no more greyed-out unused parameter warnings! The usefixtures marker also accepts multiple fixtures as variadic arguments: @pytest.mark.usefixtures(\u0026quot;fixture_a\u0026quot;, \u0026quot;fixture_b\u0026quot;).\nOne thing to keep in mind is that it won\u0026rsquo;t work if you try to mark another fixture with the usefixtures decorator. The pytest documentation includes a warning about this.\nFin!\n","permalink":"https://rednafi.com/python/inject-pytest-fixture/","summary":"\u003cp\u003eSometimes, when writing tests in Pytest, I find myself using fixtures that the test\nfunction/method doesn\u0026rsquo;t directly reference. Instead, Pytest runs the fixture, and the test\nfunction implicitly leverages its side effects. For example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003ecollections.abc\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eIterator\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003eunittest.mock\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eMock\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003epatch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003epytest\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Define an implicit environment mock fixture that patches os.environ\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nd\"\u003e@pytest.fixture\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003emock_env\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003eIterator\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"n\"\u003epatch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;os.environ\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;IMPLICIT_KEY\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;IMPLICIT_VALUE\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e}):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eyield\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Define an explicit service mock fixture\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nd\"\u003e@pytest.fixture\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003emock_svc\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003eMock\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eservice\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eMock\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eservice\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eprocess\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ereturn_value\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Explicit Mocked Response\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eservice\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# IDEs tend to dim out unused parameters like mock_env\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003etest_stuff\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emock_svc\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eMock\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003emock_env\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eMock\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Use the explicit mock\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003emock_svc\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eprocess\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eassert\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Explicit Mocked Response\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003emock_svc\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eprocess\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eassert_called_once\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Assert the environment variable patched by mock_env\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eassert\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eenviron\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;IMPLICIT_KEY\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;IMPLICIT_VALUE\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIn the \u003ccode\u003etest_stuff\u003c/code\u003e function above, we directly use the \u003ccode\u003emock_svc\u003c/code\u003e fixture but not\n\u003ccode\u003emock_env\u003c/code\u003e. Instead, we expect Pytest to run \u003ccode\u003emock_env\u003c/code\u003e, which modifies the environment\nvariables. This works, but IDEs often mark \u003ccode\u003emock_env\u003c/code\u003e as an unused parameter and dims it\nout.\u003c/p\u003e","title":"Injecting Pytest fixtures without cluttering test signatures"},{"content":"Although I\u0026rsquo;ve been using Python 3.12 in production for nearly a year, one neat feature in the typing module that escaped me was the @override decorator. Proposed in PEP 698, it\u0026rsquo;s been hanging out in typing_extensions for a while. This is one of those small features you either don\u0026rsquo;t care about or get totally psyched over. I\u0026rsquo;m definitely in the latter camp.\nIn languages like C#, Java, and Kotlin, explicit overriding is required. For instance, in Java, you use @Override to make it clear you\u0026rsquo;re overriding a method in a sub class. If you mess up the method name or if the method doesn\u0026rsquo;t exist in the superclass, the compiler throws an error. Now, with Python\u0026rsquo;s @override decorator, we get similar benefits — though only if you\u0026rsquo;re using a static type checker.\nHere\u0026rsquo;s an example:\nfrom typing import override class Animal: def sound(self) -\u0026gt; str: return \u0026#34;Unknown\u0026#34; class Cat(Animal): @override def soud(self) -\u0026gt; str: # Notice the typo: sound -\u0026gt; soud # Your implementation here return \u0026#34;Meow\u0026#34; In this example, Cat inherits from Animal, and we intended to override the sound method. But there\u0026rsquo;s a typo in the subclass method name. Running mypy will flag it:\nerror: Method \u0026#34;soud\u0026#34; is marked as an override, but no base method was found with this name [misc] Found 1 error in 1 file (checked 1 source file) This decorator also works with class, property, or any other methods. Observe:\nfrom typing import override class Animal: @property def species(self) -\u0026gt; str: return \u0026#34;Unknown\u0026#34; class Cat(Animal): @override @property def species(self) -\u0026gt; str: return \u0026#34;Catus\u0026#34; If the overriding method isn\u0026rsquo;t marked with @property, mypy will raise an error:\nerror: Signature of \u0026#34;species\u0026#34; incompatible with supertype \u0026#34;Animal\u0026#34; [override] note: Superclass: note: str note: Subclass: note: def species(self) -\u0026gt; str Found 1 error in 1 file (checked 1 source file) The error message could be clearer here, though. You can use @override with class methods too:\nfrom typing import override class Animal: @classmethod def category(cls) -\u0026gt; str: return \u0026#34;Unknown\u0026#34; class Cat(Animal): @override @classmethod def category(cls) -\u0026gt; str: return \u0026#34;Mammal\u0026#34; In these cases, the order of @override doesn\u0026rsquo;t matter; you can put it before or after the property decorator, and it\u0026rsquo;ll still work. I personally prefer keeping it as the outermost decorator.\nI\u0026rsquo;ve been gradually adding the @override decorator to my code, as it not only prevents typos but also alerts me if an upstream method name changes.\n","permalink":"https://rednafi.com/python/typing-override/","summary":"\u003cp\u003eAlthough I\u0026rsquo;ve been using Python 3.12 in production for nearly a year, one neat feature in\nthe typing module that escaped me was the \u003ccode\u003e@override\u003c/code\u003e decorator. Proposed in \u003ca href=\"https://peps.python.org/pep-0698/\"\u003ePEP 698\u003c/a\u003e, it\u0026rsquo;s\nbeen hanging out in \u003ccode\u003etyping_extensions\u003c/code\u003e for a while. This is one of those small features you\neither don\u0026rsquo;t care about or get totally psyched over. I\u0026rsquo;m definitely in the latter camp.\u003c/p\u003e\n\u003cp\u003eIn languages like C#, Java, and Kotlin, explicit overriding is required. For instance, in\nJava, you use \u003ccode\u003e@Override\u003c/code\u003e to make it clear you\u0026rsquo;re overriding a method in a sub class. If you\nmess up the method name or if the method doesn\u0026rsquo;t exist in the superclass, the compiler\nthrows an error. Now, with Python\u0026rsquo;s \u003ccode\u003e@override\u003c/code\u003e decorator, we get similar benefits — though\nonly if you\u0026rsquo;re using a static type checker.\u003c/p\u003e","title":"Explicit method overriding with @typing.override"},{"content":"This morning, someone on Twitter pointed me to PEP 562, which introduces __getattr__ and __dir__ at the module level. While __dir__ helps control which attributes are printed when calling dir(module), __getattr__ is the more interesting addition.\nThe __getattr__ method in a module works similarly to how it does in a Python class. For example:\nclass Cat: def __getattr__(self, name: str) -\u0026gt; str: if name == \u0026#34;voice\u0026#34;: return \u0026#34;meow!!\u0026#34; raise AttributeError(f\u0026#34;Attribute {name} does not exist\u0026#34;) # Try to access \u0026#39;voice\u0026#39; on Cat cat = Cat() cat.voice # Prints \u0026#34;meow!!\u0026#34; # Raises AttributeError: Attribute something_else does not exist cat.something_else In this class, __getattr__ defines what happens when specific attributes are accessed, allowing you to manage how missing attributes behave. Since Python 3.7, you can also define __getattr__ at the module level to handle attribute access on the module itself.\nFor instance, if you have a module my_module.py:\n# my_module.py def existing_function() -\u0026gt; str: return \u0026#34;I exist!\u0026#34; def __getattr__(name: str) -\u0026gt; str: if name == \u0026#34;dynamic_attribute\u0026#34;: return \u0026#34;I was generated dynamically!\u0026#34; raise AttributeError(f\u0026#34;Module {__name__} has no attribute {name}\u0026#34;) Using this module:\n# another_module.py import my_module print(my_module.existing_function()) # Prints \u0026#34;I exist!\u0026#34; print(my_module.dynamic_attribute) # Prints \u0026#34;I was generated dynamically!\u0026#34; print(my_module.non_existent) # Raises AttributeError If an attribute isn\u0026rsquo;t found through the regular lookup (using object.__getattribute__), Python will look for __getattr__ in the module\u0026rsquo;s __dict__. If found, it calls __getattr__ with the attribute name and returns the result. But if you\u0026rsquo;re looking up a name directly as a module global, it bypasses __getattr__. This prevents performance issues that would arise from repeatedly invoking __getattr__ for built-in or common attributes.\nOne practical use for module-level __getattr__ is lazy-loading heavy dependencies to improve startup performance. Imagine you have a module that relies on a large library but don\u0026rsquo;t need it immediately at import.\n# heavy_module.py from typing import Any def __getattr__(name: str) -\u0026gt; Any: if name == \u0026#34;np\u0026#34;: import numpy as np globals()[\u0026#34;np\u0026#34;] = np # Cache it in the module\u0026#39;s namespace return np raise AttributeError(f\u0026#34;Module {__name__} has no attribute {name}\u0026#34;) With this setup, importing heavy_module doesn\u0026rsquo;t immediately import NumPy. Only when you access heavy_module.np does it trigger the import:\n# main.py import heavy_module # NumPy hasn\u0026#39;t been imported yet. # Code that doesn\u0026#39;t need NumPy... # Now we need NumPy arr = heavy_module.np.array([1, 2, 3]) print(arr) # NumPy is now imported and used The first access to heavy_module.np imports NumPy (adding ~150ns), but since we cache np with globals()['np'] = np, subsequent accesses are fast, as the module now holds the reference to NumPy.\nThis approach is handy in scenarios like CLIs where you want to keep startup quick. For example, if you need to initialize a database connection but only for specific commands, you can defer the setup until needed.\nHere\u0026rsquo;s an example with SQLite (though SQLite connections are quick, imagine a slower connection here):\n# db_module.py import sqlite3 # Caching initialized connection in the global namespace _connection: sqlite3.Connection | None = None def __getattr__(name: str) -\u0026gt; sqlite3.Connection: if name == \u0026#34;connection\u0026#34;: global _connection if _connection is None: print(\u0026#34;Initializing database connection...\u0026#34;) _connection = sqlite3.connect(\u0026#34;my_database.db\u0026#34;) return _connection raise AttributeError(f\u0026#34;Module {__name__} has no attribute {name}\u0026#34;) In this setup, nothing is instantiated when you import db_module. The connection is only initialized on the first access of db_module.connection. Later calls use the cached _connection, making subsequent access fast.\nHere\u0026rsquo;s how you might use it in a CLI:\n# cli.py import click import db_module @click.group() def cli() -\u0026gt; None: pass @cli.command() def greet() -\u0026gt; None: click.echo(\u0026#34;Hello!\u0026#34;) @cli.command() def show_data() -\u0026gt; None: conn = ( db_module.connection ) # Initializes the database connection if needed cursor = conn.cursor() cursor.execute(\u0026#34;SELECT * FROM my_table\u0026#34;) results = cursor.fetchall() click.echo(f\u0026#34;Data: {results}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: cli() When you run python cli.py greet, the CLI starts quickly since it doesn\u0026rsquo;t initialize the database connection. But running python cli.py show_data accesses db_module.connection, which triggers the connection setup.\nThis could also be achieved by defining a function that initializes the database connection and caches it for subsequent calls. However, using module-level __getattr__ can be more convenient if you have multiple global variables that require expensive calculations or initializations. Instead of writing separate functions for each variable, you can handle them all within the __getattr__ method.\nHere\u0026rsquo;s one example of using it for a non-trivial case in the wild.\n","permalink":"https://rednafi.com/python/module-getattr/","summary":"\u003cp\u003eThis morning, someone on Twitter pointed me to \u003ca href=\"https://peps.python.org/pep-0562/\"\u003ePEP 562\u003c/a\u003e, which introduces \u003ccode\u003e__getattr__\u003c/code\u003e and\n\u003ccode\u003e__dir__\u003c/code\u003e at the module level. While \u003ccode\u003e__dir__\u003c/code\u003e helps control which attributes are printed\nwhen calling \u003ccode\u003edir(module)\u003c/code\u003e, \u003ccode\u003e__getattr__\u003c/code\u003e is the more interesting addition.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003e__getattr__\u003c/code\u003e method in a module works similarly to how it does in a Python class. For\nexample:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eCat\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__getattr__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ename\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;voice\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;meow!!\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eraise\u003c/span\u003e \u003cspan class=\"ne\"\u003eAttributeError\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Attribute \u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e does not exist\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Try to access \u0026#39;voice\u0026#39; on Cat\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecat\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eCat\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecat\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evoice\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# Prints \u0026#34;meow!!\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Raises AttributeError: Attribute something_else does not exist\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecat\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esomething_else\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIn this class, \u003ccode\u003e__getattr__\u003c/code\u003e defines what happens when specific attributes are accessed,\nallowing you to manage how missing attributes behave. Since Python 3.7, you can also define\n\u003ccode\u003e__getattr__\u003c/code\u003e at the module level to handle attribute access on the module itself.\u003c/p\u003e","title":"Quicker startup with module-level __getattr__"},{"content":"I always get tripped up by Docker\u0026rsquo;s different mount types and their syntax, whether I\u0026rsquo;m stringing together some CLI commands or writing a docker-compose file. Docker\u0026rsquo;s docs cover these, but for me, the confusion often comes from how \u0026ldquo;bind\u0026rdquo; is used in various contexts and how \u0026ldquo;volume\u0026rdquo; and \u0026ldquo;bind\u0026rdquo; sometimes get mixed up in the documentation.\nHere\u0026rsquo;s my attempt to disentangle some of my most-used mount commands.\nVolume mounts Volume mounts let you store data outside the container in a location managed by Docker. The data persists even after the container stops. On non-Linux systems, volume mounts are faster than bind mounts because data doesn\u0026rsquo;t need to cross the virtualization boundary.\nThe -v option The -v flag is the older and more common way to define volume mounts in the Docker CLI. For example:\ndocker run -v myvolume:/usr/share/nginx/html:ro nginx Here\u0026rsquo;s what each part means:\nmyvolume: The name of the Docker-managed volume on the host. /usr/share/nginx/html: The mount point inside the container. :ro: Mounts the volume as read-only inside the container. The host can still write to the volume, but the container cannot. The general syntax is:\n-v [SOURCE]:[TARGET]:[OPTIONS] It can be tricky to remember which part is the host and which is the container, especially since with volumes, the SOURCE is a volume name, not a host path.\nThe --mount option To make things clearer, Docker introduced the --mount option, which uses key-value pairs. The same volume mount using --mount looks like this:\ndocker run \\ --mount \\ type=volume,source=myvolume,target=/usr/share/nginx/html,readonly nginx Or using shorthands:\ndocker run \\ --mount type=volume,src=myvolume,dst=/usr/share/nginx/html,ro nginx I find this syntax more explicit and less error-prone, even if it\u0026rsquo;s a bit more verbose.\nIn docker-compose.yml In docker-compose, volumes can be defined using both the old and new syntax. Here\u0026rsquo;s how they compare:\nOld style\nservices: app: image: nginx volumes: - myvolume:/usr/share/nginx/html:ro volumes: myvolume: New style\nservices: app: image: nginx volumes: - type: volume source: myvolume target: /usr/share/nginx/html read_only: true volumes: myvolume: I prefer the new style because it reduces ambiguity and makes the configuration clearer.\nBind mounts Bind mounts let you directly mount a file or directory from the host into the container. This is especially useful in development when you want the container to have access to your code or data.\nThe key difference between volume mounts and bind mounts is that volumes are fully managed by Docker and stored in a special location, while bind mounts rely on specific host paths. Volumes are more portable and isolated from the host, whereas bind mounts give you direct access to host files but can introduce permission issues and depend on the exact file structure of the host.\nThe -v option Using the -v syntax for bind mounts:\ndocker run -v /path/on/host:/usr/share/nginx/html:ro nginx Here:\n/path/on/host: The path on the host machine. This must be an absolute path. /usr/share/nginx/html: The mount point inside the container. :ro: Mounts the directory as read-only inside the container. The --mount option Using --mount for a bind mount:\ndocker run \\ --mount \\ type=bind,src=/path/on/host,dst=/usr/share/nginx/html,ro nginx This syntax makes it clear that you\u0026rsquo;re using a bind mount and specifies exactly which paths are involved.\nIn docker-compose.yml In docker-compose, bind mounts can be specified like this:\nOld style\nservices: app: image: nginx volumes: - ./path/on/host:/usr/share/nginx/html:ro New style\nservices: app: image: nginx volumes: - type: bind source: ./path/on/host target: /usr/share/nginx/html read_only: true Note: In docker-compose, if you specify a source that doesn\u0026rsquo;t start with / (an absolute path) or ./ (a relative path), Docker might think you\u0026rsquo;re referring to a volume. To ensure it\u0026rsquo;s interpreted as a bind mount, start the path with ./ or /.\nTmpfs mounts Tmpfs mounts store data in the host\u0026rsquo;s memory, not on disk. This makes them ideal for temporary storage that doesn\u0026rsquo;t need to persist after the container stops. They\u0026rsquo;re great for things like caches or scratch space.\nThe --tmpfs option Docker provides a --tmpfs option to create a tmpfs mount more concisely:\ndocker run --tmpfs /app/tmp:rw,size=64m nginx /app/tmp: The target directory inside the container. rw: This option allows read and write access to the tmpfs mount. size=64m: Sets the size of the tmpfs mount to 64 MB. The --mount option Alternatively, using the more flexible --mount option:\ndocker run \\ --mount type=tmpfs,target=/app/tmp,tmpfs-size=64m,tmpfs-mode=1777 nginx Here\u0026rsquo;s what each part means:\ntype=tmpfs: Specifies that this is a tmpfs mount, using the host\u0026rsquo;s memory. target=/app/tmp: The directory inside the container where the tmpfs mount is mounted. tmpfs-size=64m: Limits the size of the tmpfs mount to 64 MB. tmpfs-mode=1777: Sets permissions for the tmpfs mount (1777 grants read, write, and execute permissions to everyone). In docker-compose.yml In docker-compose, tmpfs mounts can be defined using both the old and new syntax.\nOld style\nservices: app: image: nginx tmpfs: - /app/tmp:size=64m New style\nservices: app: image: nginx volumes: - type: tmpfs target: /app/tmp tmpfs: size: 64m mode: 1777 Build cache mounts Build cache mounts help speed up Docker image builds by caching intermediate files like package downloads or compiled artifacts. They\u0026rsquo;re used during the build process and aren\u0026rsquo;t part of the final container image.\nIn a Dockerfile, you might use a build cache like this:\nRUN --mount=type=cache,target=/var/cache/apt \\ apt-get update \u0026amp;\u0026amp; apt-get install -y curl Here\u0026rsquo;s what each option does:\n--mount=type=cache: Defines a cache mount that stores the files from the apt-get commands to speed up future builds by reusing the downloaded packages. target=/var/cache/apt: Specifies the location inside the container where the cache will be stored during the build process. In my Python projects, I cache my dependencies and install them with uv like this:\nRUN --mount=type=cache,target=/root/.cache/uv \\ --mount=type=bind,source=uv.lock,target=uv.lock \\ --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\ uv sync --no-install-project --locked --no-dev Above, the cache mount at /root/.cache/uv saves dependencies so they don\u0026rsquo;t need to be re-downloaded in future builds if nothing changes. The bind mounts provide access to uv.lock and pyproject.toml from the host, allowing the container to read these config files during the build. Any changes to the files are picked up, while cached dependencies are reused unless the configurations have been updated.\n","permalink":"https://rednafi.com/misc/docker-mount/","summary":"\u003cp\u003eI always get tripped up by Docker\u0026rsquo;s different mount types and their syntax, whether I\u0026rsquo;m\nstringing together some CLI commands or writing a \u003ccode\u003edocker-compose\u003c/code\u003e file. Docker\u0026rsquo;s docs cover\nthese, but for me, the confusion often comes from how \u0026ldquo;bind\u0026rdquo; is used in various contexts and\nhow \u0026ldquo;volume\u0026rdquo; and \u0026ldquo;bind\u0026rdquo; sometimes get mixed up in the documentation.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s my attempt to disentangle some of my most-used mount commands.\u003c/p\u003e\n\u003ch2 id=\"volume-mounts\"\u003eVolume mounts\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://docs.docker.com/storage/volumes/\"\u003eVolume mounts\u003c/a\u003e let you store data outside the container in a location managed by Docker.\nThe data persists even after the container stops. On non-Linux systems, volume mounts are\nfaster than bind mounts because data doesn\u0026rsquo;t need to cross the virtualization boundary.\u003c/p\u003e","title":"Docker mount revisited"},{"content":"I was fiddling with graphlib in the Python stdlib and found it quite nifty. It processes a Directed Acyclic Graph (DAG), where tasks (nodes) are connected by directed edges (dependencies), and returns the correct execution order. The \u0026ldquo;acyclic\u0026rdquo; part ensures no circular dependencies.\nTopological sorting is useful for arranging tasks so that each one follows its dependencies. It\u0026rsquo;s widely used in scheduling, build systems, dependency resolution, and database migrations.\nFor example, consider these tasks:\nTask A must be completed before Tasks B and C. Tasks B and C must be completed before Task D. This can be represented as:\ngraph TD A --\u003e B A --\u003e C B --\u003e D C --\u003e D Here, A can start right away, B and C follow after A, and D is last, depending on both B and C.\nThe task order can be determined as:\nA B and C (in parallel, since both depend only on A) D (which depends on both B and C) This method ensures tasks are executed in the right sequence while respecting all dependencies.\nTo resolve the above-mentioned case with graphlib, you\u0026rsquo;d do the following:\nfrom graphlib import TopologicalSorter # Define the graph graph = { \u0026#34;A\u0026#34;: [], # A has no dependency \u0026#34;B\u0026#34;: [\u0026#34;A\u0026#34;], # B depends on A \u0026#34;C\u0026#34;: [\u0026#34;A\u0026#34;], # C depends on A \u0026#34;D\u0026#34;: [\u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;], # D depends on B and C } # Create a TopologicalSorter instance sorter = TopologicalSorter(graph) # Get the tasks in the correct order sorter.prepare() # Resolve the tasks in batch mode while sorter.is_active(): batch = tuple(sorter.get_ready()) print(\u0026#34;Executing:\u0026#34;, batch) sorter.done(*batch) Running this will print the following:\nExecuting: (\u0026#39;A\u0026#39;,) Executing: (\u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;) Executing: (\u0026#39;D\u0026#39;,) Since Python\u0026rsquo;s stdlib already has graphlib, I thought I\u0026rsquo;d write a sloppy one in Go to learn the mechanics of how it works.\nWriting a topological sorter in Go The API will be similar to what we\u0026rsquo;ve seen in the graphlib example.\nDefining the graph structure First, we need a graph structure to hold the tasks and their dependencies. We\u0026rsquo;ll use an adjacency list to represent the graph, and a map to track the in-degree of each node (how many tasks it depends on).\ntype Graph struct { vertices map[string][]string // Adjacency list for dependencies inDegree map[string]int // Tracks the number of incoming edges queue []string // Queue of nodes ready to process active int // Number of active tasks to process } Here:\nvertices: a list of tasks that each node points to (i.e., its dependents). inDegree: how many tasks must finish before each task can be processed. queue: tasks that can be processed because they have no unmet dependencies. active: how many tasks are currently ready for processing. Adding dependencies Next, we\u0026rsquo;ll define how one task depends on another. The AddEdge function sets up this relationship, ensuring the source task knows it must finish before the destination task can proceed.\nfunc (g *Graph) AddEdge(source, destination string) { g.vertices[source] = append(g.vertices[source], destination) g.inDegree[destination]++ // Increase destination\u0026#39;s in-degree if _, exists := g.inDegree[source]; !exists { g.inDegree[source] = 0 // Ensure the source node is tracked } } The destination task is added to the list of tasks that the source task points to, marking the dependency. The in-degree of the destination task is increased by 1 because it depends on the source task. If the source task is new, we initialize its in-degree to 0. Initializing and processing tasks in batches Now we\u0026rsquo;ll initialize the graph by identifying tasks that can be processed immediately — those with an in-degree of 0 (i.e., they have no dependencies). We then process tasks batch by batch.\nfunc (g *Graph) Prepare() { // Start by adding tasks with in-degree 0 to the queue for task, degree := range g.inDegree { if degree == 0 { g.queue = append(g.queue, task) // Ready to process } } g.active = len(g.queue) // Count how many are active } This function finds tasks with an in-degree of 0 (no dependencies) and adds them to the processing queue. The active count keeps track of how many tasks are ready to run. Processing each batch of tasks We use GetReady to retrieve the next batch of tasks that are ready for processing. These are tasks with no unmet dependencies.\nfunc (g *Graph) GetReady() []string { batch := make([]string, len(g.queue)) // Create batch copy(batch, g.queue) // Copy tasks g.queue = []string{} // Clear queue return batch // Return batch } GetReady pulls the current batch of tasks from the queue and clears it for the next batch. Tasks are returned in the order they are ready to be processed. Marking the processed tasks as done Once a batch of tasks is completed, we mark them as done and reduce the in-degree of any tasks that depend on them.\nfunc (g *Graph) Done(tasks ...string) { for _, task := range tasks { // For each completed task for _, dependent := range g.vertices[task] { g.inDegree[dependent]-- // Decrement in-degree if g.inDegree[dependent] == 0 { // If ready, enqueue g.queue = append(g.queue, dependent) } } } g.active = len(g.queue) // Update active count } For each completed task, we reduce the in-degree of any dependent tasks. If a dependent task\u0026rsquo;s in-degree reaches 0, it\u0026rsquo;s added to the queue and is now ready to be processed in the next batch. Running the full topological sort Finally, we\u0026rsquo;ll implement the TopologicalSortBatch function, which processes all tasks in batches until none are left.\nfunc TopologicalSortBatch(graph *Graph) { graph.Prepare() // Prepare the graph by loading the initial batch for graph.IsActive() { // While tasks remain to be processed batch := graph.GetReady() // Get the next batch fmt.Println(\u0026#34;Next batch:\u0026#34;, batch) // Process the batch graph.Done(batch...) // Mark the batch as done } } Prepare loads the first set of tasks that can be processed. IsActive checks if there are any tasks left to process. GetReady retrieves the next batch of tasks to process. Done marks tasks as finished, allowing dependent tasks to be processed next. Using the sorter You can use the API as follows:\ng := NewGraph() // Define task dependencies g.AddEdge(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;) // B depends on A g.AddEdge(\u0026#34;A\u0026#34;, \u0026#34;C\u0026#34;) // C depends on A g.AddEdge(\u0026#34;B\u0026#34;, \u0026#34;D\u0026#34;) // D depends on B g.AddEdge(\u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;) // D depends on C // Perform topological sort in batches TopologicalSortBatch(g) This will return:\nNext batch: [A] Next batch: [B C] Next batch: [D] Here, A needs to run first. B and C can run in parallel after A finishes, and only then can D run.\nComplete example Here\u0026rsquo;s the full implementation, heavily annotated for clarity:\npackage main import \u0026#34;fmt\u0026#34; type Graph struct { vertices map[string][]string // Task dependencies inDegree map[string]int // Number of unmet dependencies queue []string // Ready tasks active int // Active task count } func NewGraph() *Graph { return \u0026amp;Graph{ vertices: make(map[string][]string), inDegree: make(map[string]int), queue: []string{}, active: 0, } } func (g *Graph) AddEdge(source, destination string) { // Add the destination task to the source\u0026#39;s dependency list g.vertices[source] = append(g.vertices[source], destination) // Increment the in-degree of the destination task g.inDegree[destination]++ // Ensure the source task is tracked with in-degree 0 if new if _, exists := g.inDegree[source]; !exists { g.inDegree[source] = 0 } } func (g *Graph) Prepare() { // Load tasks with no unmet dependencies (in-degree 0) for task, degree := range g.inDegree { if degree == 0 { g.queue = append(g.queue, task) } } g.active = len(g.queue) // Set active task count } func (g *Graph) IsActive() bool { return g.active \u0026gt; 0 // Check if there are active tasks left } func (g *Graph) GetReady() []string { batch := make([]string, len(g.queue)) // Create batch of ready tasks copy(batch, g.queue) // Copy tasks to the batch g.queue = []string{} // Clear queue after processing return batch // Return ready tasks } func (g *Graph) Done(tasks ...string) { // For each completed task, decrement in-degree of its dependents for _, task := range tasks { for _, dependent := range g.vertices[task] { g.inDegree[dependent]-- // If dependent has no unmet dependencies, add to queue if g.inDegree[dependent] == 0 { g.queue = append(g.queue, dependent) } } } g.active = len(g.queue) // Update active task count } func TopologicalSortBatch(graph *Graph) { graph.Prepare() // Prepare initial batch of tasks for graph.IsActive() { // Process tasks while there are active ones batch := graph.GetReady() // Get the next batch fmt.Println(\u0026#34;Next batch:\u0026#34;, batch) // Output batch graph.Done(batch...) // Mark batch as done } } // Usage func main() { g := NewGraph() // Define task dependencies g.AddEdge(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;) g.AddEdge(\u0026#34;A\u0026#34;, \u0026#34;C\u0026#34;) g.AddEdge(\u0026#34;B\u0026#34;, \u0026#34;D\u0026#34;) g.AddEdge(\u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;) // Perform topological sort in batches TopologicalSortBatch(g) } You can use this to make custom task orchestrators.\nFin!\n","permalink":"https://rednafi.com/go/topological-sort/","summary":"\u003cp\u003eI was fiddling with \u003ccode\u003egraphlib\u003c/code\u003e in the Python stdlib and found it quite nifty. It processes a\n\u003cstrong\u003eDirected Acyclic Graph (DAG)\u003c/strong\u003e, where tasks (nodes) are connected by directed edges\n(dependencies), and returns the correct execution order. The \u0026ldquo;acyclic\u0026rdquo; part ensures no\ncircular dependencies.\u003c/p\u003e\n\u003cp\u003eTopological sorting is useful for arranging tasks so that each one follows its dependencies.\nIt\u0026rsquo;s widely used in scheduling, build systems, dependency resolution, and database\nmigrations.\u003c/p\u003e\n\u003cp\u003eFor example, consider these tasks:\u003c/p\u003e","title":"Topological sort"},{"content":"Besides retries, circuit breakers are probably one of the most commonly employed resilience patterns in distributed systems. While writing a retry routine is pretty simple, implementing a circuit breaker needs a little bit of work.\nI realized that I usually just go for off-the-shelf libraries for circuit breaking and haven\u0026rsquo;t written one from scratch before. So, this is an attempt to create a sloppy one in Go. I picked Go instead of Python because I didn\u0026rsquo;t want to deal with sync-async idiosyncrasies or abstract things away under a soup of decorators.\nCircuit breakers A circuit breaker acts like an automatic switch that prevents your application from repeatedly trying to execute an operation that\u0026rsquo;s likely to fail. In a distributed system, you don\u0026rsquo;t want to bombard a remote service when it\u0026rsquo;s already failing, and circuit breakers prevent that.\nIt has three states: Closed, Open, and Half-Open. Here\u0026rsquo;s a diagram that shows the state transitions:\nstateDiagram-v2 [*] --\u003e Closed: Start Closed --\u003e Open: Failure threshold reached Open --\u003e HalfOpen: Recovery period expired HalfOpen --\u003e Closed: Success threshold reached HalfOpen --\u003e Open: Request failed note right of Closed: All requests are allowed note right of Open: Requests are blocked note right of HalfOpen: Limited requests allowed to check recovery Closed: This is the healthy operating state where all requests are allowed to pass through to the service. If a certain number of consecutive requests fail (reaching a failure threshold), the circuit breaker switches to the Open state.\nOpen: In this state, all requests are immediately blocked, and an error is returned to the caller without attempting to contact the failing service. This prevents overwhelming the service and gives it time to recover. After a predefined recovery period, the circuit breaker transitions to the Half-Open state.\nHalf-Open: The circuit breaker allows a limited number of test requests to see if the service has recovered. If these requests succeed, it transitions back to the Closed state. If any of them fail, it goes back to the Open state.\nBuilding one in Go Here\u0026rsquo;s a simple circuit breaker in Go.\nDefining states First, we\u0026rsquo;ll define the constants for our states and create the circuitBreaker struct, which holds all the configurable knobs.\n// The three possible states of a circuit breaker const ( Closed = \u0026#34;closed\u0026#34; Open = \u0026#34;open\u0026#34; HalfOpen = \u0026#34;half-open\u0026#34; ) // circuitBreaker manages the state and behavior of the circuit breaker type circuitBreaker struct { mu sync.Mutex // Guards the circuit breaker state state string // Current state of the circuit breaker failureCount int // Number of consecutive failures lastFailureTime time.Time // Time of the last failure halfOpenSuccessCount int // Successful requests in half-open state failureThreshold int // Failures to trigger open state recoveryTime time.Duration // Wait time before half-open halfOpenMaxRequests int // Requests allowed in half-open state timeout time.Duration // Timeout for requests } This struct includes:\nmu: A mutex to ensure thread-safe access to the circuit breaker. state: The current state of the circuit breaker (Closed, Open, or HalfOpen). failureCount: The current count of consecutive failures. lastFailureTime: The timestamp of the last failure. halfOpenSuccessCount: The number of successful requests in the HalfOpen state. failureThreshold: The number of consecutive failures allowed before opening the circuit. recoveryTime: The cool-down period before the circuit breaker transitions from Open to HalfOpen. halfOpenMaxRequests: The maximum number of successful requests needed to close the circuit. timeout: The maximum duration to wait for a request to complete. Initializing the breaker Next, we provide a constructor function to initialize a new circuitBreaker instance.\n// NewCircuitBreaker initializes a new CircuitBreaker func NewCircuitBreaker( failureThreshold int, recoveryTime time.Duration, halfOpenMaxRequests int, timeout time.Duration, ) *circuitBreaker { return \u0026amp;circuitBreaker{ state: Closed, failureThreshold: failureThreshold, recoveryTime: recoveryTime, halfOpenMaxRequests: halfOpenMaxRequests, timeout: timeout, } } This function sets the initial state to Closed and initializes the thresholds and timeout.\nImplementing the Call method The Call method is the primary interface for executing functions through the circuit breaker. It dispatches the appropriate state handler based on the current state.\n// Call executes fn and manages state transitions func (cb *circuitBreaker) Call(fn func() (any, error)) (any, error) { cb.mu.Lock() defer cb.mu.Unlock() slog.Info(\u0026#34;Making a request\u0026#34;, \u0026#34;state\u0026#34;, cb.state) switch cb.state { case Closed: return cb.handleClosedState(fn) case Open: return cb.handleOpenState() case HalfOpen: return cb.handleHalfOpenState(fn) default: return nil, errors.New(\u0026#34;unknown circuit state\u0026#34;) } } We use a mutex to protect against concurrent access since the circuit breaker might be used by multiple goroutines. The Call method uses a switch statement to delegate the function call to the appropriate handler based on the current state.\nHandling closed states In the Closed state, all requests are allowed to pass through. We monitor the requests for failures to decide when to trip the circuit breaker.\n// handleClosedState executes the function and monitors failures func (cb *circuitBreaker) handleClosedState( fn func() (any, error), )(any, error) { result, err := cb.runWithTimeout(fn) if err != nil { slog.Warn( \u0026#34;Request failed in closed state\u0026#34;, \u0026#34;failureCount\u0026#34;, cb.failureCount+1, ) cb.failureCount++ cb.lastFailureTime = time.Now() if cb.failureCount \u0026gt;= cb.failureThreshold { cb.state = Open slog.Error(\u0026#34;Failure threshold reached, transitioning to open\u0026#34;) } return nil, err } slog.Info(\u0026#34;Request succeeded in closed state\u0026#34;) cb.resetCircuit() return result, nil } In this function:\nWe attempt to execute the provided function fn using runWithTimeout to handle possible timeouts.\nIf the function call fails, we increment the failureCount and update lastFailureTime.\nIf the failureCount reaches the failureThreshold, we transition the circuit to the Open state.\nIf the function call succeeds, we reset the circuit breaker to the Closed state by calling resetCircuit.\nResetting the breaker When a request succeeds, we reset the failure count and keep the circuit in the Closed state.\n// resetCircuit resets the circuit breaker to closed state func (cb *circuitBreaker) resetCircuit() { cb.failureCount = 0 cb.state = Closed slog.Info(\u0026#34;Circuit reset to closed state\u0026#34;) } Handling open states In the Open state, all requests are blocked to prevent further strain on the failing service. We check if the recovery period has expired before transitioning to the HalfOpen state.\n// handleOpenState blocks requests if recovery time hasn\u0026#39;t passed func (cb *circuitBreaker) handleOpenState() (any, error) { if time.Since(cb.lastFailureTime) \u0026gt; cb.recoveryTime { cb.state = HalfOpen cb.halfOpenSuccessCount = 0 cb.failureCount = 0 slog.Info(\u0026#34;Recovery period over, transitioning to half-open\u0026#34;) return nil, nil } slog.Warn(\u0026#34;Circuit is still open, blocking request\u0026#34;) return nil, errors.New(\u0026#34;circuit open, request blocked\u0026#34;) } Here:\nWe check if the recovery period (recoveryTime) has passed since the last failure. If it has, we transition to the HalfOpen state and reset the counters. If not, we block the request and return an error immediately. Handling half-open states In the HalfOpen state, we allow a limited number of requests to test if the service has recovered.\n// handleHalfOpenState executes the function and checks for recovery func (cb *circuitBreaker) handleHalfOpenState( fn func() (any, error)) (any, error) { result, err := cb.runWithTimeout(fn) if err != nil { slog.Error(\u0026#34;Failed in half-open state, transitioning to open\u0026#34;) cb.state = Open cb.lastFailureTime = time.Now() return nil, err } cb.halfOpenSuccessCount++ slog.Info(\u0026#34;Succeeded in half-open\u0026#34;, \u0026#34;successCount\u0026#34;, cb.halfOpenSuccessCount) if cb.halfOpenSuccessCount \u0026gt;= cb.halfOpenMaxRequests { slog.Info(\u0026#34;Max success, transitioning to closed\u0026#34;) cb.resetCircuit() } return result, nil } In this function:\nWe attempt to execute the provided function fn. If the function call fails, we transition back to the Open state. If the function call succeeds, we increment halfOpenSuccessCount. Once the success count reaches halfOpenMaxRequests, we reset the circuit breaker to the Closed state. Running functions with timeout To prevent the circuit breaker from hanging on slow or unresponsive functions, we implement a timeout mechanism. You probably noticed that inside each state handler we called the wrapped functions with runWithTimeout.\n// runWithTimeout executes fn with a timeout func (cb *circuitBreaker) runWithTimeout( fn func() (any, error), ) (any, error) { ctx, cancel := context.WithTimeout(context.Background(), cb.timeout) defer cancel() resultChan := make(chan struct { result any err error }, 1) go func() { result, err := fn() resultChan \u0026lt;- struct { result any err error }{result, err} }() select { case \u0026lt;-ctx.Done(): return nil, errors.New(\u0026#34;request timed out\u0026#34;) case res := \u0026lt;-resultChan: return res.result, res.err } } This function:\nCreates a context with a timeout using context.WithTimeout. Executes the provided function fn in a separate goroutine. Waits for either the result or the timeout. Returns an error if the function takes longer than the specified timeout. Taking it for a spin Let\u0026rsquo;s test our circuit breaker with an unreliable service that sometimes fails.\nfunc unreliableService() (any, error) { if time.Now().Unix()%2 == 0 { return nil, errors.New(\u0026#34;service failed\u0026#34;) } return \u0026#34;Success!\u0026#34;, nil } In the main function, we\u0026rsquo;ll create a circuit breaker and make several calls to the unreliable service.\nfunc main() { cb := cb.NewCircuitBreaker( 2, // Failure threshold 2*time.Second, // Recovery time 2, // Half-open max requests 2*time.Second, // Half-open max time ) for i := 0; i \u0026lt; 5; i++ { result, err := cb.Call(unreliableService) if err != nil { slog.Error(\u0026#34;Service request failed\u0026#34;, \u0026#34;error\u0026#34;, err) } else { slog.Info(\u0026#34;Service request succeeded\u0026#34;, \u0026#34;result\u0026#34;, result) } time.Sleep(1 * time.Second) log.Println(\u0026#34;-------------------------------------------\u0026#34;) } } This loop simulates multiple service calls, using the circuit breaker to handle failures and transitions between states.\nThis prints:\n2024/10/06 17:24:27 INFO Making a request state=closed 2024/10/06 17:24:27 INFO Request succeeded in closed state 2024/10/06 17:24:27 INFO Circuit reset to closed state 2024/10/06 17:24:27 INFO Service request succeeded result=Success! 2024/10/06 17:24:28 ----------------------------------------------- 2024/10/06 17:24:28 INFO Making a request state=closed 2024/10/06 17:24:28 WARN Request failed in closed state failureCount=1 2024/10/06 17:24:28 ERROR Service request failed error=\u0026#34;service failed\u0026#34; 2024/10/06 17:24:29 ----------------------------------------------- 2024/10/06 17:24:29 INFO Making a request state=closed 2024/10/06 17:24:29 INFO Request succeeded in closed state 2024/10/06 17:24:29 INFO Circuit reset to closed state 2024/10/06 17:24:29 INFO Service request succeeded result=Success! 2024/10/06 17:24:30 ----------------------------------------------- 2024/10/06 17:24:30 INFO Making a request state=closed 2024/10/06 17:24:30 WARN Request failed in closed state failureCount=1 2024/10/06 17:24:30 ERROR Service request failed error=\u0026#34;service failed\u0026#34; 2024/10/06 17:24:31 ----------------------------------------------- 2024/10/06 17:24:31 INFO Making a request state=closed 2024/10/06 17:24:31 INFO Request succeeded in closed state 2024/10/06 17:24:31 INFO Circuit reset to closed state 2024/10/06 17:24:31 INFO Service request succeeded result=Success! 2024/10/06 17:24:32 ----------------------------------------------- The log messages will give you a sense of what\u0026rsquo;s happening when we retry an intermittently failing function wrapped in a circuit breaker.\nThe API could be better One limitation of Go generics is that you can\u0026rsquo;t use type parameters with methods that have a receiver. This means you can\u0026rsquo;t define a method like func (cb *CircuitBreaker[T]) Call(fn func() (T, error)) (T, error).\nFor this, we have to use workarounds such as using any (an alias for interface{}) as the return type in our function signatures. While this sacrifices some type safety, it allows us to create a flexible circuit breaker that can handle functions returning different types.\nHandling incompatible function signatures What if the function you want to wrap doesn\u0026rsquo;t match the func() (any, error) signature? You can easily adapt it by wrapping your function to fit the required signature.\nSuppose you have a function like this:\nfunc fetchData(id int) (Data, error) { // ... implementation ... } You can wrap it like this:\nwrappedFunc := func() (any, error) { return fetchData(42) // Replace 42 with your desired argument } Now, wrappedFunc matches the func() (any, error) signature and can be used with our circuit breaker.\nHere\u0026rsquo;s the complete implementation on GitHub with tests.\n","permalink":"https://rednafi.com/go/circuit-breaker/","summary":"\u003cp\u003eBesides retries, \u003ca href=\"https://martinfowler.com/bliki/CircuitBreaker.html\"\u003ecircuit breakers\u003c/a\u003e are probably one of the most commonly employed\nresilience patterns in distributed systems. While writing a retry routine is pretty simple,\nimplementing a circuit breaker needs a little bit of work.\u003c/p\u003e\n\u003cp\u003eI realized that I usually just go for off-the-shelf libraries for circuit breaking and\nhaven\u0026rsquo;t written one from scratch before. So, this is an attempt to create a sloppy one in\nGo. I picked Go instead of Python because I didn\u0026rsquo;t want to deal with sync-async\nidiosyncrasies or abstract things away under a soup of decorators.\u003c/p\u003e","title":"Writing a circuit breaker in Go"},{"content":"I\u0026rsquo;m not a big fan of shims — code that messes with commands in the shell or prompt. That\u0026rsquo;s why, aside from occasional dabbling, I tend to eschew tools like asdf or pyenv and just use apt or brew for installs, depending on the OS.\nThen recently, I saw Hynek extolling direnv:\nIf you\u0026rsquo;re old-school like me, my .envrc looks like this:\nuv sync --frozen source .venv/bin/activate The sync ensures there\u0026rsquo;s always a .venv, so no memory-baking required.\nAnd Brandur doing the same:\nThis is embarrassing, but after using direnv for 10+ years, I only discovered the source_env directive yesterday.\nGame changer. I used it to improve our project\u0026rsquo;s dev configuration ergonomics so new environment variables are easily distributed via Git.\nSo I got curious and wanted to try the tool to see if it fits into my workflow, or if I\u0026rsquo;ll quickly abandon it when something goes wrong.\nWhen I first visited their landing page, I was a bit confused by the tagline:\ndirenv – unclutter your .profile\nBut I don\u0026rsquo;t have anything custom in my .profile, or more specifically, my .zprofile. Here\u0026rsquo;s what\u0026rsquo;s in it currently:\ncat ~/.zprofile eval \u0026#34;$(/opt/homebrew/bin/brew shellenv)\u0026#34; # Added by OrbStack: command-line tools and integration source ~/.orbstack/shell/init.zsh 2\u0026gt;/dev/null || : Then I realized that .profile is used here as a general term for various configuration files like .*profile, .*rc, and .*env. I have quite a bit set up in both my ~/.zshrc and ~/.zshenv — a mix of global and project-specific commands and environment variables.\nTo explain: .*profile files (like .profile or .bash_profile) are used by login shells, which are started when you log into a system, such as through SSH or a terminal login. In contrast, files like .bashrc or .zshrc are for interactive shells, meaning they run when you open a new terminal window or tab. For Zsh, .zshenv is sourced by all types of shells — both login and interactive — making it useful for global environment settings.\nWhat problem it solves Direnv solves the hassle of managing environment variables across different projects by automatically loading them when you enter a directory and unloading them when you leave. It keeps your global environment clean and avoids cluttering up your shell configuration files.\nIt checks for an .envrc (or .env) file in the current or parent directories before each prompt. If found and authorized, it loads the file into a bash sub-shell and applies the environment variables to the current shell.\nIt supports hooks for common shells like Bash, Zsh, Tcsh, and Fish, allowing you to manage project-specific environment variables without cluttering your ~/.profile. Since it\u0026rsquo;s a fast, single static executable, direnv runs seamlessly and is language-agnostic, meaning you can easily use it alongside tools like rbenv, pyenv, and phpenv.\nYou might argue that source .env works just fine, but it\u0026rsquo;s an extra step to remember. Also, being able to communicate the project-specific environment commands and variables, and having them sourced automatically, is a nice bonus.\nWhy .envrc file and not just a plain .env file This was the first question that came to my mind: why not just use a .env file? Why introduce another configuration file? Grokking the docs clarified things.\nThe .envrc file is treated like a shell script, where you can also list arbitrary shell commands that you want to be executed when you enter a project directory. You can\u0026rsquo;t do that with a plain .env file. However, direnv does support .env files too.\nIt\u0026rsquo;s such a simple idea that opens up many possibilities.\nHow I use it Here are a few things I\u0026rsquo;m using it for:\nAutomatically loading environment variables from a .env file. Loading different sets of values for the same environment keys, e.g., local vs. staging values. Activating the virtual environment when I enter the directory of a Python project. Let\u0026rsquo;s say you want to load your environment variables automatically when you cd into a directory and have them removed from the shell environment when you leave it. Suppose the project directory looks like this:\nsvc/ ├── .env ├── .env.staging └── .envrc The .env file contains environment variables for local development:\nFOO=\u0026#34;foo\u0026#34; BAR=\u0026#34;bar\u0026#34; And the .env.staging file contains the variables for staging:\nFOO=\u0026#34;foo-staging\u0026#34; BAR=\u0026#34;bar-staging\u0026#34; The .envrc file can have just one command to load the default .env file:\ndotenv Now, from the svc directory, you\u0026rsquo;ll need to allow direnv to load the environment variables into the current shell:\ndirenv allow This prints:\ndirenv: loading ~/canvas/rednafi.com/svc/.envrc direnv: export +BAR +FOO You can now print the values of the environment variables like this:\necho \u0026#34;${FOO-default}\u0026#34;; echo \u0026#34;${BAR-default}\u0026#34; This returns:\nfoo bar If you want to load different variables depending on the environment, you can add the following shell script to the .envrc file:\ncase \u0026#34;${ENVIRONMENT}\u0026#34; in \u0026#34;staging\u0026#34;) if [[ -f \u0026#34;.env.staging\u0026#34; ]]; then dotenv .env.staging fi ;; *) if [[ -f \u0026#34;.env\u0026#34; ]]; then dotenv fi ;; esac The script loads the .env.staging file if the value of $ENVIRONMENT is staging; otherwise, it loads the default .env file. From the svc root, run:\ndirenv allow This will still load the variables from .env. To load variables from .env.staging, run:\nexport ENVIRONMENT=staging \u0026amp;\u0026amp; direnv allow This time, printing the variables returns the staging values:\nfoo-staging bar-staging Oh, and when you leave the directory, the environment variables will be automatically unloaded from your working shell.\ncd .. direnv: unloading You can do a lot more with the idea, but going overboard with environment variables can be risky. You don\u0026rsquo;t want to accidentally load something into the environment you didn\u0026rsquo;t intend to. Keeping it simple with sane defaults is the way to go.\nLike Hynek, I\u0026rsquo;ve adopted uv in my Python workflow, and now my default .envrc has these two commands:\nuv sync --frozen source .venv/bin/activate The first command updates the project\u0026rsquo;s environment without changing the uv.lock file, and the second ensures I never need to remember to activate the virtual environment before running commands. Now, when I cd into a Python project and run:\necho $VIRTUAL_ENV It shows that the local .venv is active:\n/Users/rednafi/canvas/rednafi.com/.venv No more worrying about mucking up my global Python installation while running some commands.\nAnother neat directive is source_up, which lets you inherit environment variables from the parent directory. Normally, when you move into a child directory, direnv unloads the parent directory\u0026rsquo;s environment variables. But with the source_up directive in your .envrc, it\u0026rsquo;ll keep those variables around in the child directory.\nThen there\u0026rsquo;s the source_env directive, which lets you pull one .envrc file into another. So, if you\u0026rsquo;ve got some common, non-secret variables in an .envrc.local file, you can easily reuse them in your .envrc.\nHere\u0026rsquo;s an example .envrc.local file:\nexport API_URL=\u0026#34;http://localhost:5222\u0026#34; export DATABASE_URL=\u0026#34;postgres://localhost:5432/project-db\u0026#34; You can import .env.local into the .envrc file like this:\nsource_env .envrc.local # Other commands and variables go here I haven\u0026rsquo;t used source_env much yet, but I love the possibilities it unlocks.\nThe biggest reason I\u0026rsquo;ve adopted it everywhere is that it lets me share my shell environment variables and the magic commands without having anything stashed away in my ~/.zshrc or ~/.zshenv, so there\u0026rsquo;s no need for out-of-band communication.\n","permalink":"https://rednafi.com/misc/direnv/","summary":"\u003cp\u003eI\u0026rsquo;m not a big fan of shims — code that messes with commands in the shell or prompt. That\u0026rsquo;s\nwhy, aside from occasional dabbling, I tend to eschew tools like \u003ccode\u003easdf\u003c/code\u003e or \u003ccode\u003epyenv\u003c/code\u003e and just\nuse \u003ccode\u003eapt\u003c/code\u003e or \u003ccode\u003ebrew\u003c/code\u003e for installs, depending on the OS.\u003c/p\u003e\n\u003cp\u003eThen recently, I saw \u003ca href=\"https://x.com/hynek/status/1838076629249044533\"\u003eHynek extolling direnv\u003c/a\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eIf you\u0026rsquo;re old-school like me, my \u003ccode\u003e.envrc\u003c/code\u003e looks like this:\u003c/em\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003euv sync --frozen\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003esource\u003c/span\u003e .venv/bin/activate\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cem\u003eThe sync ensures there\u0026rsquo;s always a \u003ccode\u003e.venv\u003c/code\u003e, so no memory-baking required.\u003c/em\u003e\u003c/p\u003e","title":"Discovering direnv"},{"content":"I spent the evening watching this incredibly grokkable talk on event-driven services by James Eastham at NDC London 2024. Below is a cleaned-up version of my notes.\nI highly recommend watching the full talk if you\u0026rsquo;re interested before reading this distillation.\nThe curse of tightly coupled microservices Microservices often start with HTTP-based request-response communication, which seems straightforward but quickly becomes a pain as systems grow. Coupling — where one service depends on another — creates a few issues. Take the order processing service in a fictional Plant-Based Pizza company. It has to talk to the pickup service, delivery service, kitchen, and loyalty point service. They\u0026rsquo;re all tied together, so if one fails, the whole system could go down.\nThe system relies on all services being up at the same time, which causes issues when any service crashes. Even something like loyalty points can take the whole thing offline, making you wonder if the order processing service really needs to care about that.\nOn top of that, there\u0026rsquo;s semantic coupling — things like data formats. \u0026ldquo;How do you handle null values in strings? What casing is your JSON using — camelCase?\u0026rdquo; These details might seem minor, but in tightly coupled systems, they pile up, making the integrations fragile and complicated.\nWhat event-driven architecture solves Event-driven architecture offers a way to decouple services. Instead of one service needing to communicate directly with another, services react to events, giving more flexibility and scalability. \u0026ldquo;Event-driven architecture is about reversing dependencies — reversing the lines of integration.\u0026rdquo; Now, the order processing service doesn\u0026rsquo;t need to know where the downstream services are. It simply publishes an event, and the downstream services react to it.\nThis shift is powerful because it frees services from having to know about each other. In this model, the kitchen doesn\u0026rsquo;t wait for the order processing service to send a direct HTTP request. Instead, it listens for an event that triggers its response without direct integration. \u0026ldquo;You\u0026rsquo;ve removed that runtime coupling because producers and consumers no longer need to know each other exist.\u0026rdquo; By decoupling systems, event-driven architecture improves fault tolerance, scalability, and flexibility.\nThe nature of an event At the core of event-driven systems is the concept of an event. An event is \u0026ldquo;an immutable fact. It\u0026rsquo;s something that\u0026rsquo;s happened in the past. It cannot be changed.\u0026rdquo; When an event is published, it\u0026rsquo;s a record of something that has already occurred, like \u0026ldquo;order confirmed\u0026rdquo; or \u0026ldquo;pizza boxed.\u0026rdquo; Events are simple, factual, and unchangeable.\nThe analogy of a light switch brings this to life: \u0026ldquo;You hit the light switch, and that raises a light switched-on event. You can\u0026rsquo;t un-switch on a light.\u0026rdquo; To turn the light off, you generate a new event — \u0026ldquo;light switched-off\u0026rdquo; — but you don\u0026rsquo;t undo the original. This principle of immutability ensures that events in the system are reliable and unambiguous, forming the foundation for how systems react.\nEvent-driven vs. event-based systems It\u0026rsquo;s easy to confuse event-driven systems with event-based systems, but the distinction is crucial. Event-driven systems are driven by business-specific events — things that reflect real-world actions and decisions, not just technical events like a button click. \u0026ldquo;An event-driven system uses events like these: \u0026lsquo;order confirmed,\u0026rsquo; \u0026lsquo;pizza boxed,\u0026rsquo; \u0026lsquo;staff member clocked in.\u0026rsquo;\u0026rdquo; These are business-level events that reflect the narrative of the company, not just low-level system changes.\nIn contrast, event-based systems simply react to any change, such as a file being added to S3 or a button being clicked in a UI. \u0026ldquo;We\u0026rsquo;ve been building event-based systems for years\u0026hellip; that doesn\u0026rsquo;t make it event-driven.\u0026rdquo; The difference is significant because event-driven systems align technical architecture with business needs, creating a more meaningful, coherent system where the events reflect the organization\u0026rsquo;s core processes.\nFat vs. sparse events One of the critical design decisions in event-driven systems is choosing between fat events (which carry a lot of data) and sparse events (which carry minimal data). Fat events, also known as Event-Carried State Transfer (ECST), include all the information a consumer might need. For instance, \u0026ldquo;the kitchen can consume this event — it\u0026rsquo;s got the list of items on the order, so now it knows what it needs to cook.\u0026rdquo; This reduces the need for callbacks or additional requests for data back to the original system that publishes the event, making the system more robust in terms of runtime interaction.\nHowever, fat events come with risks. \u0026ldquo;The downside of that is that you get more coupling at the schema level.\u0026rdquo; Because fat events contain so much information, it becomes harder to change the event format without impacting multiple consumers. As more services depend on that data, the risk of breaking something grows.\nIn contrast, sparse events are lightweight but require callbacks to get additional information. Initially, this might seem more efficient, but as more services join the system, the number of callbacks increases exponentially. \u0026ldquo;Now you\u0026rsquo;ve got this potentially infinite number of downstream services that are all making calls back to get more information.\u0026rdquo; The result is a more tightly coupled system, albeit in a different form.\nSo which one of these is the right one to choose? The answer is: it depends. Sparse events reduce the need for frequent changes, but fat events reduce the need for constant back-and-forth communication. Often, a combination of both is necessary, depending on the use case.\nPublish-subscribe pattern and the role of the broker The core of event-driven architecture lies in the publish-subscribe pattern, facilitated by an event broker. \u0026ldquo;At its core, an event-driven architecture is made up of three parts: You have a producer, a consumer, and some kind of event broker in the middle.\u0026rdquo; The producer generates the event, the broker routes it, and the consumer processes it. The beauty of this system is that producers and consumers don\u0026rsquo;t need to know about each other\u0026rsquo;s existence.\n\u0026ldquo;The first thing you\u0026rsquo;ll notice is that the producer and the consumer here have no idea each other exists — the communication is managed by the broker.\u0026rdquo; This decoupling makes the system more flexible and scalable. A consumer can be added or removed without impacting the producer. The broker ensures that events are delivered, allowing the system to continue functioning smoothly even as it evolves.\nHowever, one responsibility remains: \u0026ldquo;The schema of your event — the format of that event — is the biggest part of the coupling that you will see in event-driven architecture.\u0026rdquo; While runtime coupling is removed, semantic coupling still exists. Producers must ensure that the event schema doesn\u0026rsquo;t change in ways that break existing consumers.\nHandling constraints and governance In event-driven systems, the responsibility for handling constraints shifts from the producer to the consumer. Producers generate events as quickly as they can, without worrying about the load on consumers. \u0026ldquo;As a producer, it\u0026rsquo;s not your responsibility to care about how your events are used\u0026hellip; that\u0026rsquo;s the subscriber\u0026rsquo;s responsibility.\u0026rdquo; Consumers must handle their own ingestion rates and ensure they don\u0026rsquo;t get overloaded.\nGovernance plays a critical role in managing these systems, particularly as they evolve. When changes are made to event schemas, it\u0026rsquo;s essential to communicate those changes to all consumers. \u0026ldquo;Governance is really important with event-driven architecture because you\u0026rsquo;ve got these systems that just don\u0026rsquo;t care about each other.\u0026rdquo; One effective method for managing this is through Request for Comments (RFCs), which allow for collaborative discussion before any changes are implemented.\n\u0026ldquo;Rather than just publishing an event and hoping for the best, introducing governance ensures that events remain consistent and understandable across teams.\u0026rdquo; This helps prevent breaking changes that could take down systems you didn\u0026rsquo;t even know were relying on your events.\nMetadata-data pattern for evolvability To enhance the evolvability of an event-driven system, East recommends using the metadata-data pattern. This pattern separates the event\u0026rsquo;s core data from its metadata, allowing for greater flexibility. \u0026ldquo;Splitting your event down into a metadata section and the data section helps you to stay evolvable.\u0026rdquo; The data contains the specifics of the event, while the metadata includes information like \u0026ldquo;event type,\u0026rdquo; \u0026ldquo;event ID,\u0026rdquo; and \u0026ldquo;version.\u0026rdquo;\nThis separation allows consumers to understand and process events more easily while providing room for schema changes. For example, \u0026ldquo;event versioning allows you to introduce breaking changes in a controlled manner.\u0026rdquo; By publishing multiple versions of an event, you can ensure backward compatibility while encouraging consumers to upgrade to the latest schema.\nEventual consistency in event-driven systems One of the trade-offs in event-driven architecture is that systems must embrace eventual consistency. In a request-response system, actions happen immediately and are reflected in real-time. But in an event-driven system, updates propagate over time. \u0026ldquo;Eventually, over time, these systems will converge on the same view of the world.\u0026rdquo; This is a shift in mindset for many developers used to strong consistency.\nTo illustrate this, consider a card payment: \u0026ldquo;When you make a card transaction, all you\u0026rsquo;re doing is making a theoretical guarantee that, at some point in the future, that money is going to move from your bank account to theirs.\u0026rdquo; While the system is eventually consistent, the end result will be correct, just not immediately. Event-driven architecture functions similarly — updates happen asynchronously, and systems eventually reach a consistent state.\nHandling HTTP communication in an event-driven world Not every system can fully adopt event-driven architecture, and many still rely on HTTP-based communication. To integrate these systems into an event-driven world, you need a middle layer. For example, if your loyalty point service is being replaced by a third-party SaaS product that only supports an HTTP API, you\u0026rsquo;d still have a service managing that integration. This service listens for events and translates them into HTTP requests for systems that aren\u0026rsquo;t event-driven.\nTo handle differences in response times and reliability between HTTP-based and event-driven systems, introducing a queue or intermediary storage is crucial. \u0026ldquo;Introducing this queue means you can keep this amount of durability\u0026hellip; you can process to the third-party API as and when you need to.\u0026rdquo; This queue adds resilience, allowing your system to continue functioning smoothly, even when interacting with external services that don\u0026rsquo;t follow event-driven principles.\nAsynchronous commands Commands in an event-driven system don\u0026rsquo;t always need to be synchronous. Instead of waiting for an immediate response, systems can issue commands asynchronously, allowing for greater flexibility and non-blocking workflows. \u0026ldquo;You want to send an email, but you might not necessarily want it to be completely request-response.\u0026rdquo;\nAn asynchronous command might still send a request to a service, but the response isn\u0026rsquo;t required to continue processing. This allows systems like the email notification service to handle requests at its own pace, rather than blocking the core order processing service. \u0026ldquo;Your email service can still expose an endpoint, but as opposed to that being an HTTP-based endpoint, that could just be a message channel.\u0026rdquo; This approach decouples the services further and ensures more efficient use of resources.\nCQRS for separating reads and writes Command Query Responsibility Segregation (CQRS) is a powerful pattern that pairs well with event-driven architecture. CQRS separates the system into two parts: one for handling commands (writes) and another for handling queries (reads). \u0026ldquo;In CQRS, you split your system into two completely independent services — one for processing commands, one for handling queries.\u0026rdquo; This allows each part of the system to be optimized for its specific workload.\nFor example, the command service focuses on writing data to the database and publishing events, while the query service listens for those events and updates a read-optimized view of the world. This separation enables more efficient scaling, as the query service can be tuned for fast reads, potentially storing data in caches like Redis or even keeping it in memory.\nI\u0026rsquo;m a bit skeptical about CQRS since I\u0026rsquo;ve worked on a system with a terrible implementation that went horribly wrong. But I intend to keep an open mind.\nHandling failure with the outbox pattern In event-driven systems, failure is inevitable, so you need strategies to handle situations where events fail to publish. The outbox pattern is one such approach. \u0026ldquo;At the point you write the data to the main database\u0026hellip; you also write the data to a secondary table.\u0026rdquo; This outbox table ensures that if the event fails to publish initially, it can be retried later.\nThis creates consistency across the system by acting as a buffer between the database and the event bus. Alternatively, systems can use change data capture to respond directly to changes in the database. \u0026ldquo;As a record is written to the database, you can stream that — you can react to that — and you can publish events off the back of that.\u0026rdquo; Both methods ensure reliability, preventing events from being lost due to temporary failures.\nThe outbox pattern sounds great in theory, but in practice, if you have a large system with many services publishing to the broker, managing an extra process for each service to read from the outbox table and publish to the event bus becomes a hassle. Instead, on the publisher side, retrying with a circuit breaker has worked better for me. Also, fun fact: I was asked about the outbox pattern in 4 of the last 5 places I interviewed for a backend role.\n","permalink":"https://rednafi.com/misc/notes-on-event-driven-systems/","summary":"\u003cp\u003eI spent the evening watching this incredibly grokkable talk on event-driven services by\nJames Eastham at NDC London 2024. Below is a cleaned-up version of my notes.\u003c/p\u003e\n\u003cp\u003eI highly recommend watching the full talk if you\u0026rsquo;re interested before reading this\ndistillation.\u003c/p\u003e\n\u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n      \u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen\" loading=\"eager\" referrerpolicy=\"strict-origin-when-cross-origin\" src=\"https://www.youtube.com/embed/qcJASFx-F5g?autoplay=0\u0026amp;controls=1\u0026amp;end=0\u0026amp;loop=0\u0026amp;mute=0\u0026amp;start=0\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" title=\"YouTube video\"\u003e\u003c/iframe\u003e\n    \u003c/div\u003e\n\n\u003ch2 id=\"the-curse-of-tightly-coupled-microservices\"\u003eThe curse of tightly coupled microservices\u003c/h2\u003e\n\u003cp\u003eMicroservices often start with HTTP-based request-response communication, which seems\nstraightforward but quickly becomes a pain as systems grow. Coupling — where one service\ndepends on another — creates a few issues. Take the \u003cem\u003eorder processing service\u003c/em\u003e in a\nfictional Plant-Based Pizza company. It has to talk to the \u003cem\u003epickup service\u003c/em\u003e, \u003cem\u003edelivery\nservice\u003c/em\u003e, \u003cem\u003ekitchen\u003c/em\u003e, and \u003cem\u003eloyalty point\u003c/em\u003e service. They\u0026rsquo;re all tied together, so if one\nfails, the whole system could go down.\u003c/p\u003e","title":"Notes on building event-driven systems"},{"content":"While going through a script at work today, I came across Bash\u0026rsquo;s nameref feature. It uses declare -n ref=\u0026quot;$1\u0026quot; to set up a variable that allows you to reference another variable by name — kind of like pass-by-reference in C. I\u0026rsquo;m pretty sure I\u0026rsquo;ve seen it before, but I probably just skimmed over it.\nAs I dug into the man pages, I realized there\u0026rsquo;s a gap in my understanding of how variable references actually work in Bash — probably because I never gave it proper attention and just got by cobbling together scripts.\nNamerefs By default, Bash variables are global unless declared as local within a function. However, when you pass variables as arguments to a function, they are accessed via positional parameters like $1, $2, etc., and any changes to these parameters inside the function do not affect the original variables outside the function.\nNamerefs allow you to essentially define a pointer to another variable. By creating a nameref, you can indirectly reference and manipulate the target variable without knowing its name beforehand. This is incredibly useful for writing generic functions that can operate on different variables based on input parameters.\nBasic usage Here\u0026rsquo;s an example:\n#!/usr/bin/env bash # Declare a variable original_var=\u0026#34;Hello, World!\u0026#34; # Function that creates a nameref to a variable create_ref() { local ref_name=$1 declare -n ref=\u0026#34;$ref_name\u0026#34; ref=\u0026#34;Hello from nameref!\u0026#34; } # Call the function with the name of the variable create_ref original_var # Print the updated variable echo \u0026#34;$original_var\u0026#34; Running this will print:\nHello from nameref! By running the create_ref function, we can dynamically update the value of $original_var, which exists outside of it. Notice that the function doesn\u0026rsquo;t even need to know about $original_var; it works on any variable name provided, making it generic.\nIn this script:\nWe declare a variable original_var with the value \u0026quot;Hello, World!\u0026quot;. The create_ref function takes the name of a variable as an argument. Inside the function, declare -n ref=\u0026quot;$ref_name\u0026quot; creates a nameref ref that points to the variable named by $ref_name. By setting ref=\u0026quot;Hello from nameref!\u0026quot;, we indirectly update original_var. Finally, we print original_var to see the updated value. Without the nameref, you could achieve the same thing with this eval (read: evil) trick:\n#!/usr/bin/env bash # Declare a variable original_var=\u0026#34;Hello, World!\u0026#34; # Function that updates a variable dynamically using eval create_ref() { local var_name=$1 local new_value=\u0026#34;Hello from eval!\u0026#34; eval \u0026#34;$var_name=\\\u0026#34;$new_value\\\u0026#34;\u0026#34; # eval 😈 } # Call the function with the name of the variable create_ref original_var # Print the updated variable echo \u0026#34;$original_var\u0026#34; This achieves the same result. The eval \u0026quot;$var_name=\\\u0026quot;$new_value\\\u0026quot;\u0026quot; dynamically updates the $original_var variable through $var_name. However, eval can be risky for security, and the nameref approach looks much cleaner syntactically.\nManaging multiple arrays Namerefs shine when you need to manage multiple arrays dynamically. Consider a scenario where you have several datasets stored in different arrays, and you want to process them using a single function.\n#!/usr/bin/env bash # Declare multiple arrays declare -a dataset1=(1 2 3 4 5) declare -a dataset2=(10 20 30 40 50) declare -a dataset3=(100 200 300 400 500) # Function to calculate the sum of an array sum_array() { local array_name=$1 declare -n arr=\u0026#34;$array_name\u0026#34; local sum=0 for num in \u0026#34;${arr[@]}\u0026#34;; do sum=$((sum + num)) done echo \u0026#34;Sum of $array_name: $sum\u0026#34; } # Process each dataset sum_array dataset1 sum_array dataset2 sum_array dataset3 This returns:\nSum of dataset1: 15 Sum of dataset2: 150 Sum of dataset3: 1500 Here:\nWe declare three arrays: dataset1, dataset2, and dataset3. The sum_array function takes the name of an array as an argument. Using declare -n arr=\u0026quot;$array_name\u0026quot;, we create a nameref arr that points to the specified array. We then iterate over the elements of arr to calculate the sum. Finally, we call sum_array for each dataset, and the function correctly processes each array based on the reference. Without the nameref, you could again use the eval trick to achieve the same thing, but this time it looks even uglier:\n#!/usr/bin/env bash # Declare multiple arrays declare -a dataset1=(1 2 3 4 5) declare -a dataset2=(10 20 30 40 50) declare -a dataset3=(100 200 300 400 500) # Function to calculate the sum of an array without namerefs sum_array() { local array_name=$1 local sum=0 local index=0 local array_length eval \u0026#34;array_length=\\${#$array_name[@]}\u0026#34; for (( index=0; index\u0026lt;array_length; index++ )); do eval \u0026#34;element=\\${$array_name[$index]}\u0026#34; sum=$((sum + element)) done echo \u0026#34;Sum of $array_name: $sum\u0026#34; } # Process each dataset sum_array dataset1 sum_array dataset2 sum_array dataset3 This approach is more complex, less secure, and harder to read in general. But the above eval example was a bit contrived to make it look bad. You can achieve the same thing without eval or nameref in this particular case like this:\n#!/usr/bin/env bash # Declare multiple arrays dataset1=(1 2 3 4 5) dataset2=(10 20 30 40 50) dataset3=(100 200 300 400 500) # Function to calculate the sum of an array sum_array() { local sum=0 for element in \u0026#34;$@\u0026#34;; do sum=$((sum + element)) done echo \u0026#34;Sum: $sum\u0026#34; } # Process each dataset sum_array \u0026#34;${dataset1[@]}\u0026#34; sum_array \u0026#34;${dataset2[@]}\u0026#34; sum_array \u0026#34;${dataset3[@]}\u0026#34; Here, instead of passing the name of the dataset arrays as strings, we pass the elements of the array to the function and add them. But I digress!\nAssociative arrays and nested references Namerefs also work with associative arrays and can be used for more complex data structures.\n#!/usr/bin/env bash # Declare an associative array declare -A user_info=( [name]=\u0026#34;Alice\u0026#34; [age]=30 [email]=\u0026#34;alice@example.com\u0026#34; ) # Function to update user information update_info() { local info_name=$1 local key=$2 local new_value=$3 declare -n info=\u0026#34;$info_name\u0026#34; info[$key]=$new_value } # Update the user\u0026#39;s email update_info user_info email \u0026#34;alice@newdomain.com\u0026#34; # Print updated information for key in \u0026#34;${!user_info[@]}\u0026#34;; do echo \u0026#34;$key: ${user_info[$key]}\u0026#34; done It prints:\nname: Alice age: 30 email: alice@newdomain.com And voilà! We have a function that can dynamically update the values in an associative array. This technique is useful for changing environments or contexts (staging/production) in shell scripts.\nIn this example:\nWe declare an associative array user_info containing user details. The update_info function takes the name of the associative array, the key to update, and the new value. Using declare -n info=\u0026quot;$info_name\u0026quot;, we create a nameref info pointing to user_info. We update the specified key in the array. Finally, we echo the updated user information. Doing this with eval isn\u0026rsquo;t pretty. I\u0026rsquo;ll leave that as an exercise for you if you like to torment yourself.\nImplementing generic setter and getter functions Building on the earlier examples, you can use namerefs to create generic setter and getter functions, making it easier to manage configuration variables or environment settings in scripts.\nHere\u0026rsquo;s an example:\n#!/usr/bin/env bash # Generic setter function set_var() { local var_name=\u0026#34;$1\u0026#34; local value=\u0026#34;$2\u0026#34; declare -n ref=\u0026#34;$var_name\u0026#34; ref=\u0026#34;$value\u0026#34; } # Generic getter function get_var() { local var_name=\u0026#34;$1\u0026#34; declare -n ref=\u0026#34;$var_name\u0026#34; echo \u0026#34;$ref\u0026#34; } # Usage example env=\u0026#34;staging\u0026#34; # Can be passed as an argument to the script # Define default variables db_host=\u0026#34;localhost\u0026#34; db_port=5432 db_user=\u0026#34;admin\u0026#34; db_pass=\u0026#34;secret\u0026#34; # Set different values based on the environment if [[ \u0026#34;$env\u0026#34; == \u0026#34;production\u0026#34; ]]; then set_var \u0026#34;db_host\u0026#34; \u0026#34;prod.db.example.com\u0026#34; set_var \u0026#34;db_user\u0026#34; \u0026#34;prod_admin\u0026#34; elif [[ \u0026#34;$env\u0026#34; == \u0026#34;staging\u0026#34; ]]; then set_var \u0026#34;db_host\u0026#34; \u0026#34;staging.db.example.com\u0026#34; set_var \u0026#34;db_user\u0026#34; \u0026#34;staging_admin\u0026#34; fi # Retrieve and display values echo \u0026#34;Using Database: $(get_var \u0026#34;db_host\u0026#34;)\u0026#34; echo \u0026#34;Database User: $(get_var \u0026#34;db_user\u0026#34;)\u0026#34; To keep things simple, the env variable isn\u0026rsquo;t a CLI argument. Based on whether env is set to staging or production, the script will print the relevant database values.\nFor staging, you\u0026rsquo;ll see:\nUsing Database: staging.db.example.com Database User: staging_admin For production:\nUsing Database: prod.db.example.com Database User: prod_admin Oh, one extra thing: nameref was introduced in Bash 4.3, so you might run into problems if you\u0026rsquo;re using an ancient version like the one shipped with macOS.\n","permalink":"https://rednafi.com/misc/bash-namerefs/","summary":"\u003cp\u003eWhile going through a script at work today, I came across Bash\u0026rsquo;s \u003ccode\u003enameref\u003c/code\u003e feature. It uses\n\u003ccode\u003edeclare -n ref=\u0026quot;$1\u0026quot;\u003c/code\u003e to set up a variable that allows you to reference another variable by\nname — kind of like pass-by-reference in C. I\u0026rsquo;m pretty sure I\u0026rsquo;ve seen it before, but I\nprobably just skimmed over it.\u003c/p\u003e\n\u003cp\u003eAs I dug into the \u003ca href=\"https://www.gnu.org/software/bash/manual/bash.html#Shell-Builtin-Commands\"\u003eman pages\u003c/a\u003e, I realized there\u0026rsquo;s a gap in my understanding of how variable\nreferences actually work in Bash — probably because I never gave it proper attention and\njust got by cobbling together scripts.\u003c/p\u003e","title":"Bash namerefs for dynamic variable referencing"},{"content":"When I started writing here about five years ago, I made a promise to myself that I wouldn\u0026rsquo;t give in to the trend of starting a blog, adding one overly enthusiastic entry about the stack behind it, and then vanishing into the ether.\nI was somewhat successful at that and wanted to write something I can link to when people are curious about the machinery that drives this site. The good thing is that the tech stack is simple and has remained stable over the years since I\u0026rsquo;ve only made changes when absolutely necessary.\nMarkdown I write plain Markdown files in my editor of choice, which has been VSCode since its launch. Once I\u0026rsquo;m finished, pre-commit runs a fleet of linters like prettier and blacken-docs to fix line length and code formatting.\nHugo Hugo is the static site generator that turns the Markdown files into HTML. I chose it because I needed something that can build the site quickly, even with lots of content. It lets me hot reload the server and check my changes as I write. Plus, I don\u0026rsquo;t get to write Go at work, so messing with Hugo templates or its source code gives me a reason to play around with Go.\nI initially tried some JS-based SSGs but dropped them pretty quickly because I couldn\u0026rsquo;t keep up with the constant tooling changes in the JavaScript universe. I use the papermod theme and have tweaked the CSS over time. Papermod handles the SEO stuff, which I like to pretend I don\u0026rsquo;t care about.\nGitHub Issues I use GitHub Issues to brainstorm ideas and keep track of my writing. I usually gather ideas throughout the week, log them in Issues, and then write something over the weekend. This workflow is heavily inspired by Simon Willison\u0026rsquo;s blog on his workflow.\nGitHub Actions and GitHub Pages Once I push content to the main branch, GitHub Actions automatically runs, checks the linter, builds the site, and deploys it to GitHub Pages. There\u0026rsquo;s nothing to maintain, and I don\u0026rsquo;t have to worry about scaling, even if one of my posts hits the front page of Hacker News. Aside from the domain, this site costs me nothing to run, and I plan to keep it that way.\nCloudflare Cache and R2 I\u0026rsquo;m a huge fan of Cloudflare and often try to shoehorn their offerings into my projects. Since my domain is registered with them, setting up their proxy with my domain\u0026rsquo;s DNS and turning on caching took just a few minutes. Their caching layer absorbs most of the traffic, and less than 10% of the requests hit the origin server. Plus, having the proxy layer gives me access to more accurate analytics.\nStatic assets like images, CSS, JS, and other files are stored on Cloudflare R2. I used to host my images with GitHub Issues and serve CSS and JS from the origin, but I recently switched everything to R2. Now I can manage it all from one place without worrying about costs. Their free plan is super generous — there\u0026rsquo;s no egress bandwidth fee, and because of caching, I barely use any of the quota. It\u0026rsquo;s fantastic!\nOxipng Oxipng is used to compress images before uploading them to the Cloudflare R2 bucket with the wrangler CLI. The Makefile in the repo has a single command called upload-static that handles everything in one go.\nupload-static: oxipng -o 6 -r static/images/ find static -type f | while read filepath; do \\ key=$$(echo \u0026#34;$$filepath\u0026#34; | sed \u0026#39;s|^|blog/|\u0026#39;); \\ wrangler r2 object put $$key --file \u0026#34;$$filepath\u0026#34;; \\ done I just drop the screenshots and images into /static/images/\u0026lt;blog-name\u0026gt;/*.png, update the references in the Markdown file, and run make upload-static before pushing the changes to the repo.\nGoogle Analytics I\u0026rsquo;m still using Google Analytics, even though I\u0026rsquo;m not a huge fan. Cloudflare already gives me better traffic insights, but the free version doesn\u0026rsquo;t show how many hits each page gets. At some point, I might just pay for Cloudflare\u0026rsquo;s upgraded plan so I can get rid of the bulky, intrusive analytics scripts for good.\nThe source code and content for this site are all publicly available on GitHub.\n","permalink":"https://rednafi.com/misc/behind-the-blog/","summary":"\u003cp\u003eWhen I started writing here about five years ago, I made a promise to myself that I wouldn\u0026rsquo;t\ngive in to the trend of starting a blog, adding one overly enthusiastic entry about the\nstack behind it, and then vanishing into the ether.\u003c/p\u003e\n\u003cp\u003eI was somewhat successful at that and wanted to write something I can link to when people\nare curious about the machinery that drives this site. The good thing is that the tech stack\nis simple and has remained stable over the years since I\u0026rsquo;ve only made changes when\nabsolutely necessary.\u003c/p\u003e","title":"Behind the blog"},{"content":"I always struggle with the syntax for redirecting multiple streams to another command or a file. LLMs do help, but beyond the most obvious cases, it takes a few prompts to get the syntax right. When I know exactly what I\u0026rsquo;m after, scanning a quick post is much faster than wrestling with a non-deterministic kraken. So, here\u0026rsquo;s a list of the redirection and piping syntax I use the most, with real examples.\nRedirecting stdout and stderr Redirect stdout to a file Standard way:\ncommand \u0026gt; file This replaces the content of file with the stdout of command. For example:\necho \u0026#34;Hello, world!\u0026#34; \u0026gt; hello.txt Print and redirect to file:\ncommand | tee file Example:\necho \u0026#34;Hello, world!\u0026#34; | tee hello.txt This prints \u0026ldquo;Hello, world!\u0026rdquo; to the terminal and also writes it to hello.txt.\nRedirect stderr to a file Standard way:\ncommand 2\u0026gt; file Sends all errors (stderr) to file. For example:\nls non_existing_file 2\u0026gt; error.log Print and redirect stderr to file:\ncommand 2\u0026gt; \u0026gt;(tee file) Example:\nls non_existing_file 2\u0026gt; \u0026gt;(tee error.log) Redirect both stdout and stderr to a file Common approach:\ncommand \u0026gt; file 2\u0026gt;\u0026amp;1 Combines stdout and stderr into one stream and saves them to file. For example:\nls non_existing_file existing_file \u0026gt; output.log 2\u0026gt;\u0026amp;1 Print and redirect both to file:\ncommand 2\u0026gt;\u0026amp;1 | tee file Example:\nls non_existing_file existing_file 2\u0026gt;\u0026amp;1 | tee output.log Convenient shorthand:\ncommand \u0026amp;\u0026gt; file Example:\nls non_existing_file existing_file \u0026amp;\u0026gt; output.log Append instead of overwriting Append stdout to a file:\ncommand \u0026gt;\u0026gt; file Example:\necho \u0026#34;Appending line\u0026#34; \u0026gt;\u0026gt; hello.txt Print and append stdout to file:\ncommand | tee -a file Example:\necho \u0026#34;Appending line\u0026#34; | tee -a hello.txt Append both stdout and stderr (explicit):\ncommand \u0026gt;\u0026gt; file 2\u0026gt;\u0026amp;1 Example:\nls non_existing_file existing_file \u0026gt;\u0026gt; output.log 2\u0026gt;\u0026amp;1 Print and append both stdout and stderr to file:\ncommand 2\u0026gt;\u0026amp;1 | tee -a file Example:\nls non_existing_file existing_file 2\u0026gt;\u0026amp;1 | tee -a output.log Convenient shorthand for appending both:\ncommand \u0026amp;\u0026gt;\u0026gt; file Example:\nls non_existing_file existing_file \u0026amp;\u0026gt;\u0026gt; output.log Piping output Pipe stdout to another command Basic usage:\ncommand1 | command2 This sends the stdout of command1 to the input of command2. For example:\necho \u0026#34;Hello, world!\u0026#34; | grep \u0026#34;Hello\u0026#34; Print and redirect piped stdout to file:\ncommand1 | tee file | command2 Example:\necho \u0026#34;Hello, world!\u0026#34; | tee output.txt | grep \u0026#34;Hello\u0026#34; Pipe both stdout and stderr Common way:\ncommand1 2\u0026gt;\u0026amp;1 | command2 Combines stdout and stderr, then pipes the combined stream to command2. For example:\nls non_existing_file existing_file 2\u0026gt;\u0026amp;1 | grep \u0026#34;No\u0026#34; Print and redirect both stdout and stderr to file:\ncommand1 2\u0026gt;\u0026amp;1 | tee file | command2 Example:\nls non_existing_file existing_file 2\u0026gt;\u0026amp;1 | tee output.txt | grep \u0026#34;No\u0026#34; Shorthand for piping both stdout and stderr (|\u0026amp;) Shorthand syntax:\ncommand1 |\u0026amp; command2 This is equivalent to command1 2\u0026gt;\u0026amp;1 | command2, combining stdout and stderr. For example:\nls non_existing_file existing_file |\u0026amp; grep \u0026#34;No\u0026#34; Print and redirect both stdout and stderr using |\u0026amp;:\ncommand1 |\u0026amp; tee file | command2 Example:\nls non_existing_file existing_file |\u0026amp; tee output.txt | grep \u0026#34;No\u0026#34; Redirecting file descriptors Custom file descriptors Create a new file descriptor (e.g., 3) and redirect stdout to it:\nexec 3\u0026gt; outputfile command \u0026gt;\u0026amp;3 This sends the stdout of command to file descriptor 3, which points to outputfile. For example:\nexec 3\u0026gt; custom_output.txt echo \u0026#34;Using FD 3\u0026#34; \u0026gt;\u0026amp;3 Print and redirect stdout to custom file descriptor:\nexec 3\u0026gt; custom_output.txt echo \u0026#34;Using FD 3\u0026#34; | tee /dev/tty \u0026gt; /dev/fd/3 This prints \u0026ldquo;Using FD 3\u0026rdquo; to the terminal and simultaneously writes it to custom_output.txt.\nRedirect stderr to a file descriptor Common case:\ncommand 2\u0026gt;\u0026amp;3 Redirects stderr to file descriptor 3. For example:\nexec 3\u0026gt; error_output.txt ls non_existing_file 2\u0026gt;\u0026amp;3 Print and redirect stderr to custom file descriptor:\ncommand 2\u0026gt; \u0026gt;(tee \u0026gt;(cat \u0026gt; /dev/fd/3)) Example:\nls non_existing_file 2\u0026gt; \u0026gt;(tee \u0026gt;(cat \u0026gt; /dev/fd/3)) Redirect both stdout and stderr to a file descriptor Common way:\ncommand \u0026gt; /dev/fd/3 2\u0026gt;\u0026amp;1 Combines stdout and stderr, and redirects them to file descriptor 3.\nNote: There\u0026rsquo;s no shorthand equivalent for redirecting both stdout and stderr to a file descriptor. You need to use the full syntax. For example:\nexec 3\u0026gt; combined_output.txt ls non_existing_file existing_file \u0026gt; /dev/fd/3 2\u0026gt;\u0026amp;1 Discarding output Send stdout and stderr to /dev/null Common:\ncommand \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 Silences all output (stdout and stderr). For example:\nls non_existing_file \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 Print and discard stdout and stderr (not sure why you\u0026rsquo;d ever need this):\ncommand | tee /dev/null Example:\nls non_existing_file | tee /dev/null Convenient shorthand:\ncommand \u0026amp;\u0026gt;/dev/null Example:\nls non_existing_file \u0026amp;\u0026gt;/dev/null At a glance Redirect stdout: command \u0026gt; file\nRedirect stderr: command 2\u0026gt; file\nRedirect both stdout and stderr:\nStandard: command \u0026gt; file 2\u0026gt;\u0026amp;1 Shorthand: command \u0026amp;\u0026gt; file Append stdout: command \u0026gt;\u0026gt; file\nAppend both stdout and stderr:\nStandard: command \u0026gt;\u0026gt; file 2\u0026gt;\u0026amp;1 Shorthand: command \u0026amp;\u0026gt;\u0026gt; file Pipe stdout: command1 | command2\nPipe both stdout and stderr:\nStandard: command1 2\u0026gt;\u0026amp;1 | command2 Shorthand: command1 |\u0026amp; command2 Custom file descriptors:\nCreate and redirect stdout: exec 3\u0026gt; file; command \u0026gt;\u0026amp;3 Redirect stderr: command 2\u0026gt;\u0026amp;3 Redirect both stdout and stderr: command \u0026gt; /dev/fd/3 2\u0026gt;\u0026amp;1 (no shorthand available) Discard stdout and stderr:\nStandard: command \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 Shorthand: command \u0026amp;\u0026gt;/dev/null ","permalink":"https://rednafi.com/misc/shell-redirection/","summary":"\u003cp\u003eI always struggle with the syntax for redirecting multiple streams to another command or a\nfile. LLMs do help, but beyond the most obvious cases, it takes a few prompts to get the\nsyntax right. When I know exactly what I\u0026rsquo;m after, scanning a quick post is much faster than\nwrestling with a non-deterministic kraken. So, here\u0026rsquo;s a list of the redirection and piping\nsyntax I use the most, with real examples.\u003c/p\u003e","title":"Shell redirection syntax soup"},{"content":"Here\u0026rsquo;s a Python snippet that makes an HTTP POST request:\n# script.py import httpx from typing import Any async def make_request(url: str) -\u0026gt; dict[str, Any]: headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} async with httpx.AsyncClient(headers=headers) as client: response = await client.post( url, json={\u0026#34;key_1\u0026#34;: \u0026#34;value_1\u0026#34;, \u0026#34;key_2\u0026#34;: \u0026#34;value_2\u0026#34;}, ) return response.json() The function make_request makes an async HTTP request with the HTTPx library. Running this with asyncio.run(make_request(\u0026quot;https://httpbin.org/post\u0026quot;)) gives us the following output:\n{ \u0026#34;args\u0026#34;: {}, \u0026#34;data\u0026#34;: \u0026#34;{\\\u0026#34;key_1\\\u0026#34;: \\\u0026#34;value_1\\\u0026#34;, \\\u0026#34;key_2\\\u0026#34;: \\\u0026#34;value_2\\\u0026#34;}\u0026#34;, \u0026#34;files\u0026#34;: {}, \u0026#34;form\u0026#34;: {}, \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, \u0026#34;Accept-Encoding\u0026#34;: \u0026#34;gzip, deflate\u0026#34;, \u0026#34;Content-Length\u0026#34;: \u0026#34;40\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;httpbin.org\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;python-httpx/0.27.2\u0026#34;, \u0026#34;X-Amzn-Trace-Id\u0026#34;: \u0026#34;Root=1-66d5f7b0-2ed0ddc57241f0960f28bc91\u0026#34; }, \u0026#34;json\u0026#34;: { \u0026#34;key_1\u0026#34;: \u0026#34;value_1\u0026#34;, \u0026#34;key_2\u0026#34;: \u0026#34;value_2\u0026#34; }, \u0026#34;origin\u0026#34;: \u0026#34;95.90.238.240\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://httpbin.org/post\u0026#34; } We\u0026rsquo;re only interested in the json field and want to assert in our test that making the HTTP call returns the expected values.\nTesting the HTTP request Now, how would you test it? One approach is by patching the httpx.AsyncClient instance to return a canned response and asserting against that. The happy path might be tested as follows:\n# test_script.py from unittest.mock import AsyncMock, patch import pytest from script import make_request @pytest.mark.asyncio async def test_make_request_ok() -\u0026gt; None: url = \u0026#34;https://httpbin.org/post\u0026#34; expected_json = {\u0026#34;key_1\u0026#34;: \u0026#34;value_1\u0026#34;, \u0026#34;key_2\u0026#34;: \u0026#34;value_2\u0026#34;} # Create a mock response object mock_response = AsyncMock() mock_response.json.return_value = expected_json mock_response.status_code = 200 # Patch the httpx.AsyncClient.post method to return the mock_response with patch( \u0026#34;script.httpx.AsyncClient.post\u0026#34;, # Don\u0026#39;t mock what you don\u0026#39;t own return_value=mock_response, ) as mock_post: response = await make_request(url) # Await the coroutine that was returned response = await response # Assertions mock_post.assert_called_once_with(url, json=expected_json) assert response == expected_json That\u0026rsquo;s quite a bit of work just to test a simple HTTP request. The mocking gets pretty hairy as the complexity of your HTTP calls increases. One way to cut down the mess is by using a library like respx that handles the patching for you.\nSimplifying mocks with respx For instance:\n# test_script.py import pytest import respx from script import make_request, httpx @pytest.mark.asyncio async def test_make_request_ok() -\u0026gt; None: url = \u0026#34;https://httpbin.org/post\u0026#34; expected_json = {\u0026#34;key_1\u0026#34;: \u0026#34;value_1\u0026#34;, \u0026#34;key_2\u0026#34;: \u0026#34;value_2\u0026#34;} # Mocking the HTTP POST request using respx with respx.mock: respx.post(url).mock( return_value=httpx.Response(200, json=expected_json) ) # Calling the function response = await make_request(url) # Assertions assert response == expected_json Much cleaner. During tests, respx intercepts HTTP requests made by httpx, allowing you to test against canned responses. The library provides a context manager that acts like an httpx client, so you can set the expected response. This removes the need to manually patch methods like post in httpx.AsyncClient.\nTesting with a stub client The previous strategy wouldn\u0026rsquo;t work if you want to change your HTTP client since respx is coupled with httpx. As an alternative, you could rewrite make_request to parametrize the HTTP client, pass a stub object during the test, and assert against it. This eliminates the need to write fragile mocking sludges or depend on an external mocking library.\nHere\u0026rsquo;s how you\u0026rsquo;d change the code:\n# script.py import httpx import asyncio from typing import Any async def make_request( url: str, client: httpx.AsyncClient ) -\u0026gt; dict[str, Any]: # We don\u0026#39;t want to initiate the ctx manager in every request # AsyncClient.__enter__ will be called once and passed here response = await client.post( url, json={\u0026#34;key_1\u0026#34;: \u0026#34;value_1\u0026#34;, \u0026#34;key_2\u0026#34;: \u0026#34;value_2\u0026#34;}, ) return response.json() async def main() -\u0026gt; None: headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} url = \u0026#34;https://httpbin.org/post\u0026#34; # Enter into the context manager and pass the instance to make_request async with httpx.AsyncClient(headers=headers) as client: response = await make_request(url, client) print(response) Now the tests would look as follows:\nimport pytest from typing import Any from httpx import Response, Request, AsyncClient from script import make_request class StubAsyncClient(AsyncClient): async def post( self, url: str, json: Any = None, **kwargs: Any ) -\u0026gt; Response: request = Request(method=\u0026#34;POST\u0026#34;, url=url, json=json, **kwargs) # Simulate the original response that matches the request response = Response( status_code=200, json={\u0026#34;key_1\u0026#34;: \u0026#34;value_1\u0026#34;, \u0026#34;key_2\u0026#34;: \u0026#34;value_2\u0026#34;}, request=request, ) return response @pytest.mark.asyncio async def test_make_request_ok() -\u0026gt; None: url = \u0026#34;https://httpbin.org/post\u0026#34; headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} async with StubAsyncClient(headers=headers) as client: response_data = await make_request(url, client) assert response_data == {\u0026#34;key_1\u0026#34;: \u0026#34;value_1\u0026#34;, \u0026#34;key_2\u0026#34;: \u0026#34;value_2\u0026#34;} Much better!\nIntegration testing with a test server One thing I\u0026rsquo;ve picked up from writing Go is that it\u0026rsquo;s often just easier to perform integration tests on these I/O-bound functions. That is, you can spin up a server that returns a canned response and then test your code against it to assert if it\u0026rsquo;s getting the expected output.\nThe test could look as follows. This assumes make_request takes in an AsyncClient instance as a parameter, as shown in the last example.\nimport pytest from starlette.applications import Starlette from starlette.responses import JSONResponse from starlette.routing import Route from starlette.requests import Request from httpx import AsyncClient from script import make_request async def test_endpoint(request: Request) -\u0026gt; JSONResponse: return JSONResponse({\u0026#34;key_1\u0026#34;: \u0026#34;value_1\u0026#34;, \u0026#34;key_2\u0026#34;: \u0026#34;value_2\u0026#34;}) app = Starlette(routes=[Route(\u0026#34;/post\u0026#34;, test_endpoint, methods=[\u0026#34;POST\u0026#34;])]) @pytest.mark.asyncio async def test_make_request() -\u0026gt; None: # Manually create the AsyncClient async with AsyncClient(app=app, base_url=\u0026#34;http://test\u0026#34;) as client: url = \u0026#34;http://testserver/post\u0026#34; response = await make_request(url, client=client) assert response == {\u0026#34;key_1\u0026#34;: \u0026#34;value_1\u0026#34;, \u0026#34;key_2\u0026#34;: \u0026#34;value_2\u0026#34;} In the above test, we\u0026rsquo;re using Starlette to define a simple ASGI server that returns our expected response. Then we set up the httpx.AsyncClient so it makes the request against the test server instead of making an external network call. Finally, we call the make_request function and assert the expected payload.\nSure, you could set up the server with the standard library\u0026rsquo;s http module, but that code doesn\u0026rsquo;t look half as pretty.\n","permalink":"https://rednafi.com/python/testing-http-requests/","summary":"\u003cp\u003eHere\u0026rsquo;s a Python snippet that makes an HTTP POST request:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# script.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ehttpx\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etyping\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003easync\u003c/span\u003e \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003emake_request\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eurl\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003edict\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eheaders\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Content-Type\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;application/json\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003easync\u003c/span\u003e \u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"n\"\u003ehttpx\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eAsyncClient\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eheaders\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eheaders\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003eclient\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003eawait\u003c/span\u003e \u003cspan class=\"n\"\u003eclient\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epost\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003eurl\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003ejson\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;key_1\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;value_1\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;key_2\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;value_2\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejson\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe function \u003ccode\u003emake_request\u003c/code\u003e makes an async HTTP request with the \u003ca href=\"https://www.python-httpx.org/\"\u003eHTTPx\u003c/a\u003e library. Running\nthis with \u003ccode\u003easyncio.run(make_request(\u0026quot;https://httpbin.org/post\u0026quot;))\u003c/code\u003e gives us the following\noutput:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026#34;args\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e{},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026#34;data\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;{\\\u0026#34;key_1\\\u0026#34;: \\\u0026#34;value_1\\\u0026#34;, \\\u0026#34;key_2\\\u0026#34;: \\\u0026#34;value_2\\\u0026#34;}\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026#34;files\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e{},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026#34;form\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e{},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026#34;headers\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nt\"\u003e\u0026#34;Accept\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;*/*\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nt\"\u003e\u0026#34;Accept-Encoding\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;gzip, deflate\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nt\"\u003e\u0026#34;Content-Length\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;40\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nt\"\u003e\u0026#34;Content-Type\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;application/json\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nt\"\u003e\u0026#34;Host\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;httpbin.org\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nt\"\u003e\u0026#34;User-Agent\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;python-httpx/0.27.2\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nt\"\u003e\u0026#34;X-Amzn-Trace-Id\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Root=1-66d5f7b0-2ed0ddc57241f0960f28bc91\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"p\"\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026#34;json\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nt\"\u003e\u0026#34;key_1\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;value_1\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nt\"\u003e\u0026#34;key_2\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;value_2\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"p\"\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026#34;origin\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;95.90.238.240\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nt\"\u003e\u0026#34;url\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;https://httpbin.org/post\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWe\u0026rsquo;re only interested in the \u003ccode\u003ejson\u003c/code\u003e field and want to assert in our test that making the\nHTTP call returns the expected values.\u003c/p\u003e","title":"Shades of testing HTTP requests in Python"},{"content":"I love pytest.mark.parametrize — so much so that I sometimes shoehorn my tests to fit into it. But the default style of writing tests with parametrize can quickly turn into an unreadable mess as the test complexity grows. For example:\nimport pytest from math import atan2 def polarify(x: float, y: float) -\u0026gt; tuple[float, float]: r = (x**2 + y**2) ** 0.5 theta = atan2(y, x) return r, theta @pytest.mark.parametrize( \u0026#34;x, y, expected\u0026#34;, [ (0, 0, (0, 0)), (1, 0, (1, 0)), (0, 1, (1, 1.5707963267948966)), (1, 1, (2**0.5, 0.7853981633974483)), (-1, -1, (2**0.5, -2.356194490192345)), ], ) def test_polarify( x: float, y: float, expected: tuple[float, float] ) -\u0026gt; None: # pytest.approx helps us ignore floating point discrepancies assert polarify(x, y) == pytest.approx(expected) The polarify function converts Cartesian coordinates to polar coordinates. We\u0026rsquo;re using @pytest.mark.parametrize in its standard form to test different conditions.\nHere, the list of nested tuples with inputs and expected values becomes hard to read as the test suite grows larger. When the function under test has a more complex signature, I find myself needing to do more mental gymnastics to parse the positional input and expected values inside parametrize.\nAlso, how do you run a specific test case within the suite? For instance, what if you want to run only the third case where x, y, expected = (0, 1, (1, 1.5707963267948966))?\nI used to set custom test IDs like below to be able to run individual test cases within parametrize:\n# ... polarify implementation hasn\u0026#39;t changed. @pytest.mark.parametrize( \u0026#34;x, y, expected\u0026#34;, [ (0, 0, (0, 0)), (1, 0, (1, 0)), (0, 1, (1, 1.5707963267948966)), (1, 1, (2**0.5, 0.7853981633974483)), (-1, -1, (2**0.5, -2.356194490192345)), ], ids=[ \u0026#34;origin\u0026#34;, \u0026#34;positive_x_axis\u0026#34;, \u0026#34;positive_y_axis\u0026#34;, \u0026#34;first_quadrant\u0026#34;, \u0026#34;third_quadrant\u0026#34;, ], ) def test_polarify( x: float, y: float, expected: tuple[float, float] ) -\u0026gt; None: # pytest.approx helps us ignore floating point discrepancies assert polarify(x, y) == pytest.approx(expected) This works, but mentally associating the IDs with the examples is cumbersome, and it doesn\u0026rsquo;t make things any easier to read.\nTIL, pytest.param gives you a better syntax and more control to achieve the same. Observe:\n# ... polarify implementation hasn\u0026#39;t changed. @pytest.mark.parametrize( \u0026#34;x, y, expected\u0026#34;, [ pytest.param(0, 0, (0, 0), id=\u0026#34;origin\u0026#34;), pytest.param(1, 0, (1, 0), id=\u0026#34;positive_x_axis\u0026#34;), pytest.param(0, 1, (1, 1.5707963267948966), id=\u0026#34;positive_y_axis\u0026#34;), pytest.param( 1, 1, (2**0.5, 0.7853981633974483), id=\u0026#34;first_quadrant\u0026#34; ), pytest.param( -1, -1, (2**0.5, -2.356194490192345), id=\u0026#34;third_quadrant\u0026#34; ), ], ) def test_polarify( x: float, y: float, expected: tuple[float, float] ) -\u0026gt; None: # pytest.approx helps us ignore floating point discrepancies assert polarify(x, y) == pytest.approx(expected) We\u0026rsquo;re setting the unique IDs inside pytest.param. Now, any test can be targeted with pytest\u0026rsquo;s -k flag like this:\npytest -k positive_x_axis This will only run the second test case on the list.\nOr,\npytest -k \u0026#39;first or third\u0026#39; This will run the last two tests.\nBut the test is still somewhat hard to read. I usually refactor mine to take a kwargs argument so that I can neatly tuck all the input and expected values associated with a test case in a single dictionary. Notice:\n# ... polarify implementation hasn\u0026#39;t changed. @pytest.mark.parametrize( \u0026#34;kwargs\u0026#34;, [ pytest.param({\u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 0, \u0026#34;expected\u0026#34;: (0, 0)}, id=\u0026#34;origin\u0026#34;), pytest.param( {\u0026#34;x\u0026#34;: 1, \u0026#34;y\u0026#34;: 0, \u0026#34;expected\u0026#34;: (1, 0)}, id=\u0026#34;positive_x_axis\u0026#34; ), pytest.param( {\u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 1, \u0026#34;expected\u0026#34;: (1, 1.5707963267948966)}, id=\u0026#34;positive_y_axis\u0026#34;, ), pytest.param( {\u0026#34;x\u0026#34;: 1, \u0026#34;y\u0026#34;: 1, \u0026#34;expected\u0026#34;: (2**0.5, 0.7853981633974483)}, id=\u0026#34;first_quadrant\u0026#34;, ), pytest.param( {\u0026#34;x\u0026#34;: -1, \u0026#34;y\u0026#34;: -1, \u0026#34;expected\u0026#34;: (2**0.5, -2.356194490192345)}, id=\u0026#34;third_quadrant\u0026#34;, ), ], ) def test_polarify(kwargs: dict[str, Any]) -\u0026gt; None: # Extract expected from kwargs expected = kwargs.pop(\u0026#34;expected\u0026#34;) # Unpack the remaining kwargs to the polarify function assert polarify(**kwargs) == pytest.approx(expected) Everything associated with a single test case is passed to pytest.param in a single dictionary, eliminating the need to guess any positional arguments.\nUsing pytest.param also allows you to set custom test execution conditionals, which I\u0026rsquo;ve started to take advantage of recently:\n# ... polarify implementation hasn\u0026#39;t changed. @pytest.mark.parametrize( \u0026#34;kwargs\u0026#34;, [ pytest.param( {\u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 1, \u0026#34;expected\u0026#34;: (1, 1.5707963267948966)}, id=\u0026#34;positive_y_axis\u0026#34;, marks=pytest.mark.xfail( reason=\u0026#34;Known issue with atan2 in this quadrant\u0026#34; ), ), pytest.param( {\u0026#34;x\u0026#34;: 1, \u0026#34;y\u0026#34;: 1, \u0026#34;expected\u0026#34;: (2**0.5, 0.7853981633974483)}, id=\u0026#34;first_quadrant\u0026#34;, ), pytest.param( { \u0026#34;x\u0026#34;: 1e10, \u0026#34;y\u0026#34;: 1e10, \u0026#34;expected\u0026#34;: (2**0.5 * 1e10, 0.7853981633974483), }, id=\u0026#34;too_large\u0026#34;, marks=pytest.mark.skipif( lambda kwargs: kwargs[\u0026#34;x\u0026#34;] \u0026gt; 1e6 or kwargs[\u0026#34;y\u0026#34;] \u0026gt; 1e6, reason=\u0026#34;Input values are too large\u0026#34;, ), ), ], ) def test_polarify(kwargs: dict[str, Any]) -\u0026gt; None: # Extract expected from kwargs expected = kwargs.pop(\u0026#34;expected\u0026#34;) # Unpack the remaining kwargs to the polarify function assert polarify(**kwargs) == pytest.approx(expected) In the last block, pytest.param bundles test data with execution conditions. We\u0026rsquo;re using xfail to mark a test as expected to fail, while skipif skips tests based on conditions. This keeps all the logic for handling test cases, including failures and skips, directly alongside the test data.\n","permalink":"https://rednafi.com/python/pytest-param/","summary":"\u003cp\u003eI love \u003ca href=\"https://docs.pytest.org/en/7.1.x/how-to/parametrize.html#parametrize-basics\"\u003epytest.mark.parametrize\u003c/a\u003e — so much so that I sometimes shoehorn my tests to fit into\nit. But the default style of writing tests with \u003ccode\u003eparametrize\u003c/code\u003e can quickly turn into an\nunreadable mess as the test complexity grows. For example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003epytest\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003emath\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eatan2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003epolarify\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003etuple\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003er\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003etheta\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eatan2\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003er\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etheta\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nd\"\u003e@pytest.mark.parametrize\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"s2\"\u003e\u0026#34;x, y, expected\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mf\"\u003e1.5707963267948966\u003c/span\u003e\u003cspan class=\"p\"\u003e)),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"mf\"\u003e0.5\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.7853981633974483\u003c/span\u003e\u003cspan class=\"p\"\u003e)),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"mf\"\u003e0.5\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mf\"\u003e2.356194490192345\u003c/span\u003e\u003cspan class=\"p\"\u003e)),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e],\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003etest_polarify\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eexpected\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003etuple\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# pytest.approx helps us ignore floating point discrepancies\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eassert\u003c/span\u003e \u003cspan class=\"n\"\u003epolarify\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"n\"\u003epytest\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eapprox\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eexpected\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe \u003ccode\u003epolarify\u003c/code\u003e function converts Cartesian coordinates to polar coordinates. We\u0026rsquo;re using\n\u003ccode\u003e@pytest.mark.parametrize\u003c/code\u003e in its standard form to test different conditions.\u003c/p\u003e","title":"Taming parametrize with pytest.param"},{"content":"I learned this neat Bash trick today where you can make a raw HTTP request using the /dev/tcp file descriptor without using tools like curl or wget. This came in handy while writing a health check script that needed to make a TCP request to a service.\nThe following script opens a TCP connection and makes a simple GET request to example.com:\n#!/bin/bash # Open TCP connection to example.com:80 and assign file descriptor 3 # exec keeps /dev/fd/3 open; 3\u0026lt;\u0026gt; enables bidirectional read-write exec 3\u0026lt;\u0026gt;/dev/tcp/example.com/80 # Send the HTTP GET request to the server (\u0026gt;\u0026amp; redirects to /dev/fd/3) echo -e \\ \u0026#34;GET / HTTP/1.1\\r\\nHost: example.com\\r\\nConnection: close\\r\\n\\r\\n\u0026#34; \u0026gt;\u0026amp;3 # Read and print the server\u0026#39;s response # \u0026lt;\u0026amp; redirects the output of /dev/fd/3 to cat cat \u0026lt;\u0026amp;3 # Close the file descriptor, terminating the TCP connection exec 3\u0026gt;\u0026amp;- Running this will print the response from the site to your console.\nThe snippet first opens a TCP connection to example.com on port 80 and assigns file descriptor 3 to manage this connection. The exec ensures that the file descriptor 3 remains open for the duration of the script, allowing multiple read and write operations without needing to reopen the connection each time. Using a file descriptor makes the code cleaner. Without it, we\u0026rsquo;d need to redirect input and output directly to /dev/tcp/example.com/80 for each read and write operation, making the script more cumbersome and harder to read.\nThen we send an HTTP GET request to the server by echoing the request to file descriptor 3. The server\u0026rsquo;s response is read and printed using cat \u0026lt;\u0026amp;3, which reads from the file descriptor and prints the output to the console. Finally, the script closes the connection by terminating file descriptor 3 with exec 3\u0026gt;\u0026amp;-.\nThis is a Bash-specific trick and won\u0026rsquo;t work in other shells like Zsh or Fish. It also allows you to open UDP connections in the same manner. The Bash manpage explains the usage like this:\n/dev/tcp/host/port If host is a valid hostname or Internet address, and port is an integer port number or service name, bash attempts to open the corresponding TCP socket. /dev/udp/host/port If host is a valid hostname or Internet address, and port is an integer port number or service name, bash attempts to open the corresponding UDP socket. I used this to write the following health check script. I didn\u0026rsquo;t want to install curl in a sidecar container that just runs a single health check process, keeping things simpler.\n#!/bin/bash # Enable bash strict mode set -euo pipefail # Constants readonly HOST=\u0026#34;example.com\u0026#34; readonly PORT=80 readonly HEALTH_PATH=\u0026#34;/\u0026#34; # Open a TCP connection to the specified host and port exec 3\u0026lt;\u0026gt;\u0026#34;/dev/tcp/${HOST}/${PORT}\u0026#34; # Send the HTTP GET request to the server echo -e \\ \u0026#34;GET ${HEALTH_PATH} HTTP/1.1\\r\\n\u0026#34;\\ \u0026#34;Host: ${HOST}\\r\\nConnection: close\\r\\n\\r\\n\u0026#34; \u0026gt;\u0026amp;3 # Read the HTTP status from the server\u0026#39;s response read -r HTTP_RESPONSE \u0026lt;\u0026amp;3 HTTP_STATUS=$( echo \u0026#34;${HTTP_RESPONSE}\u0026#34; \\ | grep -o \u0026#34;HTTP/1.1 [0-9]*\u0026#34; \\ | cut -d \u0026#39; \u0026#39; -f 2 ) if [[ \u0026#34;${HTTP_STATUS}\u0026#34; == \u0026#34;200\u0026#34; ]]; then echo \u0026#34;Service is healthy.\u0026#34; exit 0 else echo \u0026#34;Service is not healthy. HTTP status: ${HTTP_STATUS}\u0026#34; exit 1 fi # Close the file descriptor, terminating the TCP connection exec 3\u0026gt;\u0026amp;- The script makes a GET request to the service and checks that the HTTP status from the raw response is 200. If not, it exits with a non-zero status.\nNote that the script will fail if your service returns a 301 redirect code. Plus, you need to make raw textual HTTP requests, which can become cumbersome if you need to do anything beyond a simple GET call. At that point, you\u0026rsquo;re better off using curl.\n","permalink":"https://rednafi.com/misc/http-requests-via-dev-tcp/","summary":"\u003cp\u003eI learned this neat Bash trick today where you can make a raw HTTP request using the\n\u003ccode\u003e/dev/tcp\u003c/code\u003e file descriptor without using tools like \u003ccode\u003ecurl\u003c/code\u003e or \u003ccode\u003ewget\u003c/code\u003e. This came in handy\nwhile writing a health check script that needed to make a TCP request to a service.\u003c/p\u003e\n\u003cp\u003eThe following script opens a TCP connection and makes a simple GET request to \u003ccode\u003eexample.com\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cp\"\u003e#!/bin/bash\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Open TCP connection to example.com:80 and assign file descriptor 3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# exec keeps /dev/fd/3 open; 3\u0026lt;\u0026gt; enables bidirectional read-write\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eexec\u003c/span\u003e 3\u0026lt;\u0026gt;/dev/tcp/example.com/80\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Send the HTTP GET request to the server (\u0026gt;\u0026amp; redirects to /dev/fd/3)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eecho\u003c/span\u003e -e \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"s2\"\u003e\u0026#34;GET / HTTP/1.1\\r\\nHost: example.com\\r\\nConnection: close\\r\\n\\r\\n\u0026#34;\u003c/span\u003e \u0026gt;\u003cspan class=\"p\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"m\"\u003e3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Read and print the server\u0026#39;s response\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# \u0026lt;\u0026amp; redirects the output of /dev/fd/3 to cat\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ecat \u0026lt;\u003cspan class=\"p\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"m\"\u003e3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Close the file descriptor, terminating the TCP connection\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eexec\u003c/span\u003e 3\u0026gt;\u003cspan class=\"p\"\u003e\u0026amp;\u003c/span\u003e-\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRunning this will print the response from the site to your console.\u003c/p\u003e","title":"HTTP requests via /dev/tcp"},{"content":"Let\u0026rsquo;s say you have a web app that emits log messages from different layers. Your log shipper collects and sends these messages to a destination like Datadog where you can query them. One common requirement is to tag the log messages with some common attributes, which you can use later to query them.\nIn distributed tracing, this tagging is usually known as context propagation, where you\u0026rsquo;re attaching some contextual information to your log messages that you can use later for query purposes. However, if you have to collect the context at each layer of your application and pass it manually to the downstream ones, that\u0026rsquo;d make the whole process quite painful.\nSuppose you have a web view for an endpoint that calls another function to do something:\nasync def view(request: Request) -\u0026gt; JSONResponse: # Collect contextual info from the header user_id = request.headers.get(\u0026#34;Svc-User-Id\u0026#34;) platform = request.headers.get(\u0026#34;Svc-Platform\u0026#34;) # Log the request with context logger.info( \u0026#34;Request started\u0026#34;, extra={\u0026#34;user_id\u0026#34;: user_id, \u0026#34;platform\u0026#34;: platform} ) await work() # Log the response too logger.info( \u0026#34;Request ended\u0026#34;, extra={\u0026#34;user_id\u0026#34;: user_id, \u0026#34;platform\u0026#34;: platform} ) return JSONResponse({\u0026#34;message\u0026#34;: \u0026#34;Work, work work!\u0026#34;}) async def work() -\u0026gt; None: await asyncio.sleep(1) logger.info(\u0026#34;Work done\u0026#34;) I\u0026rsquo;m using starlette syntax for the above pseudocode, but this is valid for any generic ASGI web app. The view procedure collects contextual information like user_id and platform from the request headers. Then it tags the log statements before and after calling the work function using the extra fields in the logger calls. This way, the log messages have contextual info attached to them.\nHowever, the work procedure also generates a log message, and that won\u0026rsquo;t get tagged here. We may be tempted to pass the contextual information to the work subroutine and use them to tag the logs, but that\u0026rsquo;ll quickly get repetitive and cumbersome. Passing a bunch of arguments to a function just so it can tag some log messages also makes things unnecessarily verbose. Plus, it\u0026rsquo;s quite easy to forget to do so, which will leave you with orphan logs with no way to query them.\nIt turns out we can write a simple middleware to tag log statements in a way where we won\u0026rsquo;t need to manually propagate the contextual information throughout the call chain. To demonstrate that, here\u0026rsquo;s a simple get endpoint server written in Starlette that\u0026rsquo;ll just return a canned response after logging a few events. The app structure looks as follows:\nsvc ├── __init__.py ├── log.py ├── main.py ├── middleware.py └── view.py Configure the logger The first step is to configure the application logger so that it emits structured log statements in JSON where each message will look as follows:\n{ \u0026#34;message\u0026#34;: \u0026#34;Some log message\u0026#34;, \u0026#34;timestamp\u0026#34;: 1722794887376, \u0026#34;tags\u0026#34;: { \u0026#34;user_id\u0026#34;: \u0026#34;1234\u0026#34;, \u0026#34;platform\u0026#34;: \u0026#34;ios\u0026#34; } } Here\u0026rsquo;s the log configuration logic:\n# log.py import contextvars import json import logging import time # Set up the context variable with default values default_context = {\u0026#34;user_id\u0026#34;: \u0026#34;unknown\u0026#34;, \u0026#34;platform\u0026#34;: \u0026#34;unknown\u0026#34;} log_context_var = contextvars.ContextVar( \u0026#34;log_context\u0026#34;, default=default_context.copy(), ) # Custom log formatter class ContextAwareJsonFormatter(logging.Formatter): def format(self, record): log_data = { \u0026#34;message\u0026#34;: record.getMessage(), # Add millisecond precision timestamp \u0026#34;timestamp\u0026#34;: int(time.time() * 1000), # Get context from ContextVar (concurrency-safe). # Context is set in middleware; .get() returns current \u0026#34;tags\u0026#34;: log_context_var.get(), } return json.dumps(log_data) # Set up the logger logger = logging.getLogger() logger.setLevel(logging.INFO) handler = logging.StreamHandler() formatter = ContextAwareJsonFormatter() handler.setFormatter(formatter) logger.addHandler(handler) The contextvars module manages context information across asynchronous tasks, preventing context leakage between requests. We use a log_context_var context variable to store user ID and platform information, ensuring each log entry includes relevant context for the request.\nThe ContextAwareJsonFormatter formats log statements to include the message, timestamp in milliseconds, and context tags. The context is retrieved using log_context_var.get(), ensuring concurrency-safe access. The context variable is set in the middleware, so log_context_var.get() always returns the current context for each request.\nNext, we set up a StreamHandler, attach the ContextAwareJsonFormatter to it, and add the handler to the root logger.\nWrite a middleware that tags the log statements automatically With log formatting out of the way, here\u0026rsquo;s how to write the middleware to update the logger so that all the log messages within a request-response cycle get automatically tagged:\n# middleware.py import logging from collections.abc import Awaitable, Callable from starlette.middleware.base import BaseHTTPMiddleware from starlette.requests import Request from starlette.responses import Response from svc.log import default_context, log_context_var # Middleware for setting log context class LogContextMiddleware(BaseHTTPMiddleware): async def dispatch( self, request: Request, call_next: Callable[[Request], Awaitable[Response]], ) -\u0026gt; Response: # Copy the default context so that we\u0026#39;re not sharing anything # between requests context = default_context.copy() # Collect the contextual information from request headers user_id = request.headers.get(\u0026#34;Svc-User-Id\u0026#34;) platform = request.headers.get(\u0026#34;Svc-Platform\u0026#34;) # Update the context if user_id: context[\u0026#34;user_id\u0026#34;] = user_id if platform: context[\u0026#34;platform\u0026#34;] = platform # Set the log_context_var context variable token = log_context_var.set(context) try: # Log before making request logging.info(\u0026#34;From middleware: request started\u0026#34;) response = await call_next(request) # Log after making request logging.info(\u0026#34;From middleware: request ended\u0026#34;) finally: # Reset the context after the request is processed log_context_var.reset(token) return response The LogContextMiddleware class inherits from starlette.BaseHTTPMiddleware and gets initialized with the application.\nThe dispatch method is called automatically for each request. It extracts user_id and platform from the request headers and sets these values in the log_context_var to tag log messages. Then it logs the incoming request, processes it, logs the outgoing response, and then clears the context so that we don\u0026rsquo;t leak the context information across requests. This way, our view function won\u0026rsquo;t need to be peppered with repetitive log statements.\nWrite the simplified view Setting up the logger and middleware drastically simplifies our endpoint view since we won\u0026rsquo;t need to tag the logs explicitly or add request-response logs in each view. It looks like this now:\n# view.py import asyncio import logging from starlette.requests import Request from starlette.responses import JSONResponse async def view(request: Request) -\u0026gt; JSONResponse: await work() logging.info(\u0026#34;From view function: work finished\u0026#34;) return JSONResponse({\u0026#34;message\u0026#34;: f\u0026#34;Work work work!!!\u0026#34;}) async def work() -\u0026gt; None: logging.info(\u0026#34;From work function: work started\u0026#34;) await asyncio.sleep(1) Notice there\u0026rsquo;s no repetitive request-response log statements in the view function, and we\u0026rsquo;re not passing the log context anywhere explicitly. The middleware will ensure that the request and response logs are always emitted and all the logs, including the one coming out of the work function, are tagged with the contextual information.\nWire everything together The logging configuration and middleware can be wired up like this:\n# main.py import uvicorn from starlette.applications import Starlette from starlette.middleware import Middleware from starlette.routing import Route from svc.middleware import LogContextMiddleware from svc.view import view middlewares = [Middleware(LogContextMiddleware)] app = Starlette(routes=[Route(\u0026#34;/\u0026#34;, view)], middleware=middlewares) if __name__ == \u0026#34;__main__\u0026#34;: uvicorn.run(app, host=\u0026#34;0.0.0.0\u0026#34;, port=8000) To instantiate the logger config, we import log.py in the __init__.py module:\n# __init__.py from svc import log # noqa Now the application can be started with:\npython -m svc.main And then we can make a request to the server:\ncurl http://localhost:8000/ -H \u0026#39;Svc-User-Id: 1234\u0026#39; -H \u0026#39;Svc-Platform: ios\u0026#39; On the server, the request will emit the following log messages:\nINFO: Started server process [41848] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit) { \u0026#34;message\u0026#34;: \u0026#34;From middleware: request started\u0026#34;, \u0026#34;timestamp\u0026#34;: 1723166008113, \u0026#34;tags\u0026#34;: { \u0026#34;user_id\u0026#34;: \u0026#34;1234\u0026#34;, \u0026#34;platform\u0026#34;: \u0026#34;ios\u0026#34; } } { \u0026#34;message\u0026#34;: \u0026#34;From work function: work started\u0026#34;, \u0026#34;timestamp\u0026#34;: 1723166008113, \u0026#34;tags\u0026#34;: { \u0026#34;user_id\u0026#34;: \u0026#34;1234\u0026#34;, \u0026#34;platform\u0026#34;: \u0026#34;ios\u0026#34; } } { \u0026#34;message\u0026#34;: \u0026#34;From view function: work finished\u0026#34;, \u0026#34;timestamp\u0026#34;: 1723166009114, \u0026#34;tags\u0026#34;: { \u0026#34;user_id\u0026#34;: \u0026#34;1234\u0026#34;, \u0026#34;platform\u0026#34;: \u0026#34;ios\u0026#34; } } { \u0026#34;message\u0026#34;: \u0026#34;From middleware: request ended\u0026#34;, \u0026#34;timestamp\u0026#34;: 1723166009115, \u0026#34;tags\u0026#34;: { \u0026#34;user_id\u0026#34;: \u0026#34;1234\u0026#34;, \u0026#34;platform\u0026#34;: \u0026#34;ios\u0026#34; } } INFO: 127.0.0.1:54780 - \u0026#34;GET / HTTP/1.1\u0026#34; 200 OK And we\u0026rsquo;re done. You can find the fully working code in this GitHub gist.\nNote: The previous version of this example wasn\u0026rsquo;t concurrency safe and used a shared logger filter, leaking context information during concurrent requests. This was pointed out in this GitHub comment.\n","permalink":"https://rednafi.com/python/log-context-propagation/","summary":"\u003cp\u003eLet\u0026rsquo;s say you have a web app that emits log messages from different layers. Your log shipper\ncollects and sends these messages to a destination like Datadog where you can query them.\nOne common requirement is to tag the log messages with some common attributes, which you can\nuse later to query them.\u003c/p\u003e\n\u003cp\u003eIn distributed tracing, this tagging is usually known as \u003ca href=\"https://opentelemetry.io/docs/concepts/context-propagation/\"\u003econtext propagation\u003c/a\u003e, where you\u0026rsquo;re\nattaching some contextual information to your log messages that you can use later for query\npurposes. However, if you have to collect the context at each layer of your application and\npass it manually to the downstream ones, that\u0026rsquo;d make the whole process quite painful.\u003c/p\u003e","title":"Log context propagation in Python ASGI apps"},{"content":"With the recent explosion of LLM tools, I often like to kill time fiddling with different LLM client libraries and SDKs in one-off scripts. Lately, I\u0026rsquo;ve noticed that some newer tools frequently mess up the logger settings, meddling with my application logs. While it\u0026rsquo;s less common in more seasoned libraries, I guess it\u0026rsquo;s worth rehashing why hijacking the root logger isn\u0026rsquo;t a good idea when writing libraries or other forms of reusable code.\nIn Python, when I say root logger, I mean the logger instance that logging.basicConfig acts on, or the one you get back when you don\u0026rsquo;t specify a name in logging.getLogger(). The root logger is for the application code to use and if you\u0026rsquo;re a library author, you should probably steer clear from it. If not, people using your code might get into situations as follows.\nLet\u0026rsquo;s say there\u0026rsquo;s a single file library named lib.py that decides to configure the root logger:\n# lib.py import logging # Configuring the root logger here. Not a great idea! logging.basicConfig(level=logging.DEBUG) def frobnicate() -\u0026gt; None: # Using the root logger throughout the library code logging.debug(\u0026#34;This is a debug message from the library.\u0026#34;) Now, let\u0026rsquo;s say the user of lib.py imports the frobnicate function and configures the root logger in the following manner:\n# main.py import logging from lib import frobnicate # Library user attempts to reconfigure the root logger logging.basicConfig(level=logging.INFO) def main() -\u0026gt; None: # Use library code frobnicate() # Emit log message from the application code logging.info(\u0026#34;This is an info message from the application.\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Since the application code has set the log threshold to INFO, you might think that running the code snippet would only print the log message from the application. But instead, you\u0026rsquo;ll also get the DEBUG message from the library:\nDEBUG:root:This is a debug message from the library. INFO:root:This is an info message from the application. It happens because before the application code had the chance to set the log level to INFO, the library code hijacked the root logger and configured it during the import time of frobnicate. You can test it by placing the from lib import frobnicate statement after the logging.basicConfig(...) line in the main.py file. By doing so, the log configuration in the application code gets to run before the library has the chance to meddle with it.\nThis makes things confusing for the library user, and the Logging how-to doc advises against doing so:\nIt is strongly advised that you do not log to the root logger in your library. Instead, use a logger with a unique and easily identifiable name, such as the name for your library\u0026rsquo;s top-level package or module. Logging to the root logger will make it difficult or impossible for the application developer to configure the logging verbosity or handlers of your library as they wish.\nSolving this is quite straightforward. Avoid using the root logger in your library code. Instead, instantiate your own logger instance and configure it with your heart\u0026rsquo;s content. This way, your users get to keep using the root logger as they like, and they can also tap into the library\u0026rsquo;s log messages whenever they need to.\nHere\u0026rsquo;s how to achieve that in the library:\n# lib.py import logging # Create a logger object for the library logger = logging.getLogger(\u0026#34;lib\u0026#34;) def frobnicate() -\u0026gt; None: # Only use this logger object throughout the library logger.debug(\u0026#34;Debug message from the library\u0026#34;) Now the library logger no longer conflicts with the application log configuration. The application code in the main.py from the previous section can remain the same and running the snippet will only print out the INFO message this time:\nINFO:root:This is an info message from the application. This setup also lets the application code access and adjust the library\u0026rsquo;s logger to suit its needs. Here\u0026rsquo;s how it can be done in the main.py file:\n# main.py import logging from lib import frobnicate # Configure the root logger logging.basicConfig(level=logging.INFO) # Get the logger object for the library. This was already created in lib.py lib_logger = logging.getLogger(\u0026#34;lib\u0026#34;) # Set the log level for the library logger to DEBUG lib_logger.setLevel(logging.DEBUG) def main() -\u0026gt; None: frobnicate() logging.info(\u0026#34;Info message from the main\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Above, the library user sets up the root logger as usual while also reconfiguring the library\u0026rsquo;s logger. It\u0026rsquo;s the library author\u0026rsquo;s job to properly name and initialize the logger in the library code. The logger name and the default behavior should be well-documented as well.\nThis allows the application code to retrieve and customize the logger as needed. Note that calling getLogger with the same name always retrieves the same logger instance.\nAlso, you should avoid adding any handlers to your library\u0026rsquo;s logger. Doing so can complicate things for users who may want to attach their own handlers. The logging how-to guide strongly warns against this:\nIt is strongly advised that you do not add any handlers other than NullHandler to your library\u0026rsquo;s loggers. This is because the configuration of handlers is the prerogative of the application developer who uses your library. The application developer knows their target audience and what handlers are most appropriate for their application: if you add handlers ‘under the hood\u0026rsquo;, you might well interfere with their ability to carry out unit tests and deliver logs which suit their requirements.\nIf you\u0026rsquo;re looking for a real-life example of how to minimally configure your library\u0026rsquo;s logger, check out the httpx codebase. The logging behavior is well-documented there.\nYou can easily reconfigure the httpx logger in your application code while making an HTTP request like this:\n# Your application code import httpx import logging # Get the library\u0026#39;s logger instance httpx_logger = logging.getLogger(\u0026#34;httpx\u0026#34;) # Set the logger\u0026#39;s log level httpx_logger.setLevel(logging.DEBUG) # Define a handler console_handler = logging.StreamHandler() # Set the handler\u0026#39;s log level console_handler.setLevel(logging.DEBUG) # Define a formatter fmt = \u0026#34;%(name)s - %(levelname)s - %(message)s\u0026#34; console_formatter = logging.Formatter(fmt) # Add the handler to the library\u0026#39;s logger instance httpx_logger.addHandler(console_handler) # Set the formatter for the handler console_handler.setFormatter(console_formatter) # Make a request that\u0026#39;ll emit the log messages httpx.get(\u0026#34;https://httpbin.org/get\u0026#34;) Running the script will print the DEBUG messages as follows:\nhttpx - DEBUG - load_ssl_context verify=True cert=None ... httpx - DEBUG - load_verify_locations cafile=\u0026#39;...cacert.pem\u0026#39; httpx - INFO - HTTP Request: GET https://httpbin.org/get ... ","permalink":"https://rednafi.com/python/no-hijack-root-logger/","summary":"\u003cp\u003eWith the recent explosion of LLM tools, I often like to kill time fiddling with different\nLLM client libraries and SDKs in one-off scripts. Lately, I\u0026rsquo;ve noticed that some newer tools\nfrequently mess up the logger settings, meddling with my application logs. While it\u0026rsquo;s less\ncommon in more seasoned libraries, I guess it\u0026rsquo;s worth rehashing why hijacking the root\nlogger isn\u0026rsquo;t a good idea when writing libraries or other forms of reusable code.\u003c/p\u003e","title":"Please don't hijack my Python root logger"},{"content":"TIL about the install command on *nix systems. A quick GitHub search for the term brought up a ton of matches. I\u0026rsquo;m surprised I just found out about it now.\nOften, in shell scripts I need to:\nCreate a directory hierarchy Copy a config or binary file to the new directory Set permissions on the file It usually looks like this:\n# Create directory hierarchy (-p creates parent directories) mkdir -p ~/.config/app # Copy current config to the newly created directory cp conf ~/.config/app/conf # Set the file permission chmod 755 ~/.config/app/conf Turns out, the install command in GNU coreutils can do all that in one line:\ninstall -D -m 755 conf ~/.config/app/conf You can check the file status with:\nstat ~/.config/app/conf On my machine, this prints:\nFile: /Users/rednafi/.config/app Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: 1,16 Inode: 16439606 Links: 1 Access: (0755/-rwxr-xr-x) Uid: ( 501/ rednafi) Gid: ( 20/ staff) Access: 2024-07-28 20:51:42.793765043 +0200 Modify: 2024-07-28 20:51:42.793765043 +0200 Change: 2024-07-28 20:51:42.793907876 +0200 Birth: 2024-07-28 20:51:42.793765043 +0200 The -D flag directs install to create the destination directories if they don\u0026rsquo;t exist, and the -m flag sets file permissions. The result is the same as the three lines of commands before.\nIt\u0026rsquo;s common for Makefiles in C/C++ projects to install binaries like this:\ninstall -D -m 744 app_bin /usr/local/bin/app_bin It copies app_bin to /usr/local/bin, creates the parent directory hierarchy if necessary, and sets permissions on the binary so only the current user has read, write, and execute permissions, while others have read-only access.\nYou can also set directory permissions:\ninstall -d -m 600 foo/bar/bazz This creates the directory hierarchy first and then sets the permission. Here\u0026rsquo;s how they look:\ntree foo Output:\nfoo └── bar └── bazz 3 directories, 0 files Then you can copy a file to the destination and set file permissions with another install command if needed.\nYou can also set user or group ownership while copying a file:\ninstall -D -m 644 -o root -g root seed.db /var/lib/app/seed.db This command copies seed.db to the destination, creates the directory if needed, and gives access to the root user and group with the -o and -g flags, respectively.\nThere are a few other options you can read about in the man pages, but I haven\u0026rsquo;t needed anything beyond the above.\n","permalink":"https://rednafi.com/misc/install/","summary":"\u003cp\u003eTIL about the \u003ccode\u003einstall\u003c/code\u003e command on *nix systems. A quick \u003ca href=\"https://github.com/search?q=%22install+-D%22++language%3Ash+NOT+npm\u0026amp;type=code\"\u003eGitHub search for the term\u003c/a\u003e\nbrought up a ton of matches. I\u0026rsquo;m surprised I just found out about it now.\u003c/p\u003e\n\u003cp\u003eOften, in shell scripts I need to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCreate a directory hierarchy\u003c/li\u003e\n\u003cli\u003eCopy a config or binary file to the new directory\u003c/li\u003e\n\u003cli\u003eSet permissions on the file\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt usually looks like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Create directory hierarchy (-p creates parent directories)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003emkdir -p ~/.config/app\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Copy current config to the newly created directory\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ecp conf ~/.config/app/conf\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Set the file permission\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003echmod \u003cspan class=\"m\"\u003e755\u003c/span\u003e ~/.config/app/conf\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTurns out, the \u003ccode\u003einstall\u003c/code\u003e command in \u003ca href=\"https://www.gnu.org/software/coreutils/manual/html_node/install-invocation.html#install-invocation\"\u003eGNU coreutils\u003c/a\u003e can do all that in one line:\u003c/p\u003e","title":"The *nix install command"},{"content":"I was working on the deployment pipeline for a service that launches an app in a dedicated VM using GitHub Actions. In the last step of the workflow, the CI SSHs into the VM and runs several commands using a here document in bash. The simplified version looks like this:\n# SSH into the remote machine and run commands to deploy the service ssh $SSH_USER@$SSH_HOST \u0026lt;\u0026lt;EOF # Go to the work directory cd $WORK_DIR # Make a git pull git pull # Export environment variables required for the service to run export AUTH_TOKEN=$APP_AUTH_TOKEN # Start the service docker compose up -d --build EOF The fully working version can be found in the serve-init repo with here-doc.\nHere, environment variables like SSH_USER, SSH_HOST, and APP_AUTH_TOKEN are defined in the surrounding local scope of the CI. The variables then get propagated to the remote machine when we run the commands via here-doc.\nHowever, I couldn\u0026rsquo;t figure out why the Docker containers weren\u0026rsquo;t able to access the value of the AUTH_TOKEN variable. The other variables were getting through just fine.\nIt turns out, export AUTH_TOKEN=$AUTH_TOKEN within the here-doc block, doesn\u0026rsquo;t export the variable in the remote shell. So this doesn\u0026rsquo;t do what I thought it would:\ncat \u0026lt;\u0026lt;EOF export FOO=bar echo $FOO EOF I was expecting it to print:\nexport FOO=bar echo bar But instead, it just prints:\nexport FOO=bar echo So export FOO=bar in the here-doc block doesn\u0026rsquo;t set the variable in the remote shell. One solution is to set it before the block like this:\nexport FOO=bar cat \u0026lt;\u0026lt;EOF echo $FOO EOF This prints:\necho bar So, in the CI pipeline, we could do the following to propagate the environment variable from local to the remote machine:\nexport FOO=bar ssh $SSH_USER@$SSH_HOST \u0026lt;\u0026lt;EOF echo $FOO EOF This will print the value of the environment variable on the remote machine correctly. However, this doesn\u0026rsquo;t set the value in the remote shell\u0026rsquo;s environment. If you SSH into the remote machine and try to print the variable\u0026rsquo;s value, you\u0026rsquo;ll see nothing gets printed. The previous command only passes the value to the remote machine temporarily and doesn\u0026rsquo;t set it permanently in the remote shell.\nTo fix it, you could pipe the value into a file and load it in the remote shell like this:\nssh $SSH_USER@$SSH_HOST \u0026lt;\u0026lt;EOF echo \u0026#34;export FOO=$FOO\u0026#34; \u0026gt; /tmp/.env source /tmp/.env echo \\$FOO EOF Here, echo \\$FOO instead of echo $FOO ensures that the shell expansion is done on the remote machine, not on the local. This allows us to know that the environment variable has been set in the remote shell correctly.\nMaybe the behavior makes sense, but it still broke my mental model.\nSo I decided to get rid of here-doc in the pipeline altogether and went with this:\nSCRIPT=\u0026#34; # Go to the work directory cd $WORK_DIR # Make a git pull git pull # Export environment variables required for the service to run export AUTH_TOKEN=$APP_AUTH_TOKEN # Start the service docker compose up -d --build \u0026#34; # Run the script on the remote machine ssh $SSH_USER@$SSH_HOST \u0026#34;$SCRIPT\u0026#34; It works without here-doc!\nOne thing to keep in mind with the second approach is that if you need to run any expanded commands, you\u0026rsquo;ll need to defer it with a backslash so that it\u0026rsquo;s run on the remote machine, not on the local:\nSCRIPT=\u0026#34; # ... # Without backslash, shell runs this locally docker rmi -f \\$(docker compose images -q) || true \u0026#34; # Run the script on the remote machine ssh $SSH_USER@$SSH_HOST \u0026#34;$SCRIPT\u0026#34; Without the backslash, the $(...) will be expanded on the local machine, which is not desirable here. The backslash defers it so that it runs on the remote instead.\n","permalink":"https://rednafi.com/misc/heredoc-headache/","summary":"\u003cp\u003eI was working on the deployment pipeline for a service that launches an app in a dedicated\nVM using GitHub Actions. In the last step of the workflow, the CI SSHs into the VM and runs\nseveral commands using a \u003ca href=\"https://tldp.org/LDP/abs/html/here-docs.html\"\u003ehere document\u003c/a\u003e in bash. The simplified version looks like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# SSH into the remote machine and run commands to deploy the service\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003essh \u003cspan class=\"nv\"\u003e$SSH_USER\u003c/span\u003e@\u003cspan class=\"nv\"\u003e$SSH_HOST\u003c/span\u003e \u003cspan class=\"s\"\u003e\u0026lt;\u0026lt;EOF\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003e    # Go to the work directory\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003e    cd $WORK_DIR\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003e    # Make a git pull\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003e    git pull\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003e    # Export environment variables required for the service to run\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003e    export AUTH_TOKEN=$APP_AUTH_TOKEN\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003e    # Start the service\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003e    docker compose up -d --build\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003eEOF\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe fully working version can be found in the \u003ca href=\"https://github.com/rednafi/serve-init/blob/7232c55c9aa3a6c34c5da6aeb9d14afc88d9aa0e/.github/workflows/ci.yml#L86-L115\"\u003eserve-init repo with here-doc\u003c/a\u003e.\u003c/p\u003e","title":"Here-doc headache"},{"content":"One of the reasons why I\u0026rsquo;m a big advocate of rebasing and cleaning up feature branches, even when the changes get squash-merged to the mainline, is that it makes the PR reviewer\u0026rsquo;s life a little easier. I\u0026rsquo;ve written about my rebasing workflow before and learned a few new things from the Hacker News discussion around it.\nWhile there\u0026rsquo;s been no shortage of text on why and how to craft atomic commits, I often find those discussions focus too much on VCS hygiene, and the main benefit gets lost in the minutiae. When working in a team setup, I\u0026rsquo;ve discovered that individual commits matter much less than the final change list.\nAlso, I find some of the prescriptive suggestions for easier review, like keeping the PR under ~150 lines, ensuring that the tests pass in each commit, and tidying the commits to be strictly independent, quite cumbersome. Stacked PRs sometimes help to make large changes a bit more tractable, but that comes with a whole set of review-conflict-feedback challenges. So this piece will mainly focus on making large PRs a wee bit easier to work with.\nHere\u0026rsquo;s a quick rundown of the things I find useful to make reviewing the grunt work of pull requests a bit more tractable. I don\u0026rsquo;t always strictly follow them while doing personal or OSS work, but these steps have been helpful while working on a large shared repo at work.\nAvoiding the temptation to lump tangentially related changes into a PR to speed things up.\nHaving a ton of fragmented commits makes filtering useless when navigating the PR diff in a platform like GitHub. I really like to filter diffs on GitHub, but it wouldn\u0026rsquo;t be useful if the commits are all over the place.\nTo make diff filtering better, I often rebase my feature branch after a messy development workflow and divide the changes into a few commits clustered around the core implementation, tests, documentation, dependency upgrades, and occasional refactoring.\nRebasing all the changes into a single commit is okay if the change is small, but for bigger changes, this does more harm than good.\nI\u0026rsquo;ve rarely spent the time to ensure that the individual commits are perfect — in the sense that they\u0026rsquo;re complete with passing tests or documentation. As long as the complete change list makes sense as a whole, it\u0026rsquo;s good enough. YMMV. The main goal is to make sure the diff makes sense to the person reviewing the work.\nAnnotated comments from the author on the PR are great. I wish they\u0026rsquo;d take up less space and there was a way to collapse them individually.\nEach PR must be connected to either an Issue or a Jira ticket, depending on how the team works.\nAdding context, screenshots, gifs, and videos to the PR description makes things so much easier for me when I do the review. Being able to see that the changes work as intended without running the code has its benefits.\nKeeping the PR in draft state until it\u0026rsquo;s ready to be reviewed. I\u0026rsquo;m not a fan of getting a notification to review some work only to find that it\u0026rsquo;s not ready yet.\n","permalink":"https://rednafi.com/misc/sane-pull-request/","summary":"\u003cp\u003eOne of the reasons why I\u0026rsquo;m a big advocate of rebasing and cleaning up feature branches, even\nwhen the changes get squash-merged to the mainline, is that it makes the PR reviewer\u0026rsquo;s life\na little easier. I\u0026rsquo;ve written about \u003ca href=\"/misc/on-rebasing/\"\u003emy rebasing workflow\u003c/a\u003e before and learned a few new\nthings from the \u003ca href=\"https://news.ycombinator.com/item?id=40742628\"\u003eHacker News discussion\u003c/a\u003e around it.\u003c/p\u003e\n\u003cp\u003eWhile there\u0026rsquo;s been no shortage of text on why and how to craft \u003ca href=\"https://www.aleksandrhovhannisyan.com/blog/atomic-git-commits/\"\u003eatomic commits\u003c/a\u003e, I often\nfind those discussions focus too much on VCS hygiene, and the main benefit gets lost in the\nminutiae. When working in a team setup, I\u0026rsquo;ve discovered that individual commits matter much\nless than the final change list.\u003c/p\u003e","title":"The sane pull request"},{"content":"People tend to get pretty passionate about Git workflows on different online forums. Some like to rebase, while others prefer to keep the disorganized records. Some dislike the extra merge commit, while others love to preserve all the historical artifacts. There\u0026rsquo;s merit to both sides of the discussion. That being said, I kind of like rebasing because I\u0026rsquo;m a messy committer who:\nUsually doesn\u0026rsquo;t care for keeping atomic commits. Creates a lot of short commits with messages like \u0026ldquo;fix\u0026rdquo; or \u0026ldquo;wip\u0026rdquo;. Likes to clean up the untidy commits before sending the branch for peer review. Prefers a linear history over a forked one so that git log --oneline --graph tells a nice story. Git rebase allows me to squash my disordered commits into a neat little one, which bundles all the changes with passing tests and documentation. Sure, a similar result can be emulated using git merge --squash feat_branch or GitHub\u0026rsquo;s squash-merge feature, but to me, rebasing feels cleaner. Plus, over time, I\u0026rsquo;ve subconsciously picked up the tricks to work my way around rebase-related gotchas.\nJulia Evans explores the pros and cons of rebasing in detail. Also, squashing commits is just one of the many things that you can do with the rebase command. Here, I just wanted to document my daily rebasing workflow where I mostly rename, squash, or fixup commits.\nA few assumptions Broadly speaking, there are two common types of rebasing: rebasing a feature branch onto the main branch and interactive rebasing on the feature branch itself. The workflow assumes a usual web service development cadence where:\nYou\u0026rsquo;ll be working on a feature branch that\u0026rsquo;s forked off of a main branch. The main branch is protected, and you can\u0026rsquo;t directly push your changes to it. Once you\u0026rsquo;re done with your feature work, you\u0026rsquo;ll need to create a pull request against the main branch. After your PR is reviewed and merged onto the main branch, CI automatically deploys it to some staging environment. I\u0026rsquo;m aware this approach doesn\u0026rsquo;t work for some niches in software development, but it\u0026rsquo;s the one I\u0026rsquo;m most familiar with, so I\u0026rsquo;ll go with it.\nRebasing a feature branch onto the main branch Let\u0026rsquo;s say I want to start working on a new feature. Here\u0026rsquo;s how I usually go about it:\nPull in the latest main with git pull.\nFork off a new branch via git switch -c feat_branch.\nDo the work in feat_branch, and before sending the PR, do interactive rebasing if necessary, and then rebase the feat_branch onto the latest changes of main with:\ngit pull --rebase origin main Push the changes to the remote repository with git push origin HEAD and send a PR against main for review.\nHere, ...origin HEAD instructs git to push the current branch that HEAD is pointing to.\nThe 3rd step is where I often do interactive rebasing before sending the PR to make my work presentable. The next section will explain that in detail.\nOccasionally, the 4th step doesn\u0026rsquo;t go as expected, and merge conflicts occur when I run git rebase main from feat_branch. In those cases, I use my editor (VSCode) to fix the conflict, add the changes with git add ., and run git rebase --continue. This completes the rebase operation, and we\u0026rsquo;re ready to push it to the remote.\nRebasing interactively on the feature branch This is an extension of the 3rd step of the previous section. Sometimes, while working on a feature, I quickly make many messy commits and push them to the remote branch. This happens quite frequently when I\u0026rsquo;m prototyping on a feature or updating something regarding GitHub Actions. In these cases, I tend to make quick changes, commit with a message like \u0026ldquo;fix\u0026rdquo; or \u0026ldquo;ci\u0026rdquo; and push to remote to see if the CI is passing. However, once I\u0026rsquo;m done, the commit log on that branch looks like this:\ngit log @ ^main --oneline --graph This command instructs git to show only the commits that exist on feat_branch but not on main. I learned recently that in git\u0026rsquo;s context, @ indicates the current branch. Neat, this means I won\u0026rsquo;t need to remember the branch name or do a git branch and then copy the name of the current branch. Running the command returns:\n* 148934c (HEAD -\u0026gt; feat_branch) ci * e0f6152 ci * 8f4dc4c ci * bf33bf7 ci * 2e3dce6 ci I\u0026rsquo;m not too proud of the state of this feat_branch and prefer to tidy things up before making a PR against main. One common thing I do is squash all these commits into one and then add a proper commit message. Interactive rebasing allows me to do that. Let\u0026rsquo;s say you want to interactively rebase the 5 commits listed above and squash them. To do so, you can run the following command from the feat_branch:\ngit rebase -i HEAD~5 This will open a file named git-rebase-todo in your default git editor (set via git config) that looks like this:\npick 763e178 ci # empty pick 4b10faf ci # empty pick 7f7ce20 ci # empty pick 88fc529 ci # empty pick 8bc19b6 ci # empty # Rebase a2e45d3..8bc19b6 onto a2e45d3 (5 commands) # # Commands: # p, pick \u0026lt;commit\u0026gt; = use commit # r, reword \u0026lt;commit\u0026gt; = use commit, but edit the commit message # e, edit \u0026lt;commit\u0026gt; = use commit, but stop for amending # s, squash \u0026lt;commit\u0026gt; = use commit, but meld into previous commit # f, fixup [-C | -c] \u0026lt;commit\u0026gt; = like \u0026#34;squash\u0026#34; but keep only the previous # commit\u0026#39;s log message, unless -C is used, in which case # keep only this commit\u0026#39;s message; -c is same as -C but # opens the editor # x, exec \u0026lt;command\u0026gt; = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with \u0026#39;git rebase --continue\u0026#39;) # d, drop \u0026lt;commit\u0026gt; = remove commit # l, label \u0026lt;label\u0026gt; = label current HEAD with a name # t, reset \u0026lt;label\u0026gt; = reset HEAD to a label # m, merge [-C \u0026lt;commit\u0026gt; | -c \u0026lt;commit\u0026gt;] \u0026lt;label\u0026gt; [# \u0026lt;oneline\u0026gt;] # create a merge commit using the original merge commit\u0026#39;s # message (or the oneline, if no original merge commit was # specified); use -c \u0026lt;commit\u0026gt; to reword the commit message # u, update-ref \u0026lt;ref\u0026gt; = track a placeholder for the \u0026lt;ref\u0026gt; to be updated # to this position in the new commits. The \u0026lt;ref\u0026gt; is # updated at the end of the rebase # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. # Notice that the file has quite a bit of instructions that are commented out. You can perform actions like pick, reword, edit, fixup, etc. I usually use squash and edit the git-rebase-todo file like this:\npick 763e178 ci # empty s 4b10faf ci # empty # \u0026lt;- s=squash melds into prev commit s 7f7ce20 ci # empty s 88fc529 ci # empty s 8bc19b6 ci # empty # ... rest of the file remains untouched Now, if you close the previous file, git will automatically open another file like the following:\n# This is a combination of 5 commits. # This is the 1st commit message: ci # This is the commit message #2: ci # This is the commit message #3: ci # This is the commit message #4: ci # This is the commit message #5: ci After the first comment, you can put in the message for all the combined commits:\n# This is a combination of 5 commits. Add pip caching to the CI # \u0026lt;- message for the combined commits # ... you can remove rest of the content If you close this file, you\u0026rsquo;ll see a message on your console indicating that the rebase has been successful:\n[detached HEAD 28f5084] Add pip caching to the CI Date: Wed Jun 19 22:42:07 2024 +0200 Successfully rebased and updated refs/heads/feat_branch. Now running git log will show that the messy commit has been squashed into one.\ngit log @ ^main --oneline --graph This displays:\n* 28f5084 (HEAD -\u0026gt; feat_branch) Add pip caching to the CI This is just one of the many things you can do during interactive rebasing. While I do this most commonly, sometimes I also drop unnecessary commits to tidy up things and group multiple commits instead of just squashing everything into one commit. All of these actions can be done in a similar manner to squashing commits as mentioned above.\nSometimes, I don\u0026rsquo;t know how many commits I\u0026rsquo;ll need to interactively rebase. In those cases, I can get the number of all the new commits on a feature branch by counting the entries in git log as follows:\ngit log @ ^main --oneline | wc -l Then you can use the number from the output of the previous command to rebase n number of commits:\ngit rebase -i HEAD~n Another thing you can do is split a single commit into multiple commits. This is quite a bit more involved and I rarely do it during interactive rebasing.\nOne last thing I learned recently is that you can run your tests or any arbitrary command during interactive rebasing. To do so, start your rebase session with --exec cmd as follows:\ngit rebase -i --exec \u0026#34;echo hello\u0026#34; HEAD~5 In the git-rebase-todo file this time, you\u0026rsquo;ll see that the command is run after each commit as follows:\npick dffb3c1 ci # empty exec echo hello pick 4d2fa08 ci # empty exec echo hello pick 2b35e4f ci # empty exec echo hello pick 6de7a52 ci # empty exec echo hello # ... You can edit this file to run the exec command after any commit you want to. The commands will run once you save and close this file. This is a neat way to run your test suite and make sure they pass in the intermediate commits.\nFin!\nFurther reading Hackernews discussion on rebasing ","permalink":"https://rednafi.com/misc/on-rebasing/","summary":"\u003cp\u003ePeople tend to get pretty passionate about Git workflows on different online forums. Some\nlike to rebase, while others prefer to keep the disorganized records. Some dislike the extra\nmerge commit, while others love to preserve all the historical artifacts. There\u0026rsquo;s merit to\nboth sides of the discussion. That being said, I kind of like rebasing because I\u0026rsquo;m a messy\ncommitter who:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUsually doesn\u0026rsquo;t care for keeping \u003ca href=\"https://suchdevblog.com/lessons/AtomicGitCommits.html#why-should-you-write-atomic-git-commits\"\u003eatomic commits\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eCreates a lot of short commits with messages like \u0026ldquo;fix\u0026rdquo; or \u0026ldquo;wip\u0026rdquo;.\u003c/li\u003e\n\u003cli\u003eLikes to clean up the untidy commits before sending the branch for peer review.\u003c/li\u003e\n\u003cli\u003ePrefers a linear history over a forked one so that \u003ccode\u003egit log --oneline --graph\u003c/code\u003e tells a\nnice story.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eGit rebase allows me to squash my disordered commits into a neat little one, which bundles\nall the changes with passing tests and documentation. Sure, a similar result can be emulated\nusing \u003ccode\u003egit merge --squash feat_branch\u003c/code\u003e or GitHub\u0026rsquo;s squash-merge feature, but to me, rebasing\nfeels cleaner. Plus, over time, I\u0026rsquo;ve subconsciously picked up the tricks to work my way\naround rebase-related gotchas.\u003c/p\u003e","title":"I kind of like rebasing"},{"content":"People typically associate Google\u0026rsquo;s Protocol Buffer with gRPC services, and rightfully so. But things often get confusing when discussing protobufs because the term can mean different things:\nA binary protocol for efficiently serializing structured data. A language used to specify how this data should be structured. In gRPC services, you usually use both: the protobuf language in proto files defines the service interface, and then the clients use the same proto files to communicate with the services.\nHowever, protobuf can be used in non-gRPC contexts for anything that requires a strict interface. You can optionally choose to use the more compact serialization format that gRPC tools offer, or just keep using JSON if you prefer. I\u0026rsquo;ve seen this use case in several organizations over the past few years, though I haven\u0026rsquo;t given it much thought. It definitely has its benefits!\nDefining your service contracts with protobuf:\nAllows you to generate message serializers and deserializers in almost any language of your choice. You can choose from a set of serialization formats. The service contracts are self-documented, and you can simply hand over the proto files to your service users. Different parts of a service or a fleet of services can be written in different languages, as long as their communication conforms to the defined contracts. For example, consider an event-driven application that sends messages to a message broker when an event occurs. A consumer then processes these messages asynchronously. Both the producer and consumer need to agree on a message format, which is defined by a contract. The workflow usually goes as follows:\nDefine the message contract using the protobuf DSL. Generate the code for serializing/deserializing the messages in the language of your choice. On the publisher side, serialize the message using the generated code. On the consumer side, generate code from the same contract and deserialize the message with that. Define the contract You can define your service interface in a .proto file. Let\u0026rsquo;s say we want to emit some event in a search service when a user queries something. The query message structure can be defined as follows:\n// ./search/protos/message.proto syntax = \u0026#34;proto3\u0026#34;; message SearchRequest { string query = 1; int32 page_number = 2; int32 results_per_page = 3; } I\u0026rsquo;m using proto3 syntax, and you can find more about that in the official proto3 guide. Next, you can install the gRPC tools for your preferred programming language to generate the interfacing code that\u0026rsquo;ll be used to serialize and deserialize the messages.\nHere\u0026rsquo;s how it looks in Python:\nInstall grpcio-tools.\nGenerate the interface. From the directory where your proto files live, run:\npython -m grpc_tools.protoc -I. \\ --python_out=contracts \\ --grpc_python_out=contracts protos/message.proto This will generate the following files in the root directory:\nsearch ├── contracts │ └── protos │ ├── message_pb2.py │ └── message_pb2_grpc.py └── protos └── message.proto Serialize and publish Once you have the contracts in place and have generated the interfacing code, here\u0026rsquo;s how you can serialize a message payload before publishing it to an event stream:\n# ./search/services/publish.py from contracts.protos.message_pb2 import SearchRequest def serialize(query: str, page_number: int, results_per_page: int) -\u0026gt; str: search_request = SearchRequest( query=query, page_number=page_number, results_per_page=results_per_page, ) # Serialize the search request to a compact binary string return search_request.SerializeToString() def publish(serialized_message: str) -\u0026gt; None: # Publish the message to a message broker ... if __name__ == \u0026#34;__main__\u0026#34;: serialized_message = serialize(\u0026#34;foo bar\u0026#34;, 1, 5) publish(serialized_message) The code is structured in the following manner now:\nsearch ├── contracts │ ├── __init__.py │ └── protos │ ├── message_pb2.py │ └── message_pb2_grpc.py ├── protos │ └── message.proto └── services ├── __init__.py └── publish.py Deserialize and consume On the consumer side, if you have access to the proto files, you can generate the interface code again via the same commands as before and use it to deserialize the message\nas follows:\n# ./search/services/consume.py from contracts.protos.message_pb2 import SearchRequest def get_message() -\u0026gt; str: # Let\u0026#39;s say we get the message from a message broker and return it return b\u0026#34;\\n\\x04test\\x10\\x01\\x18\\x02\u0026#34; def deserialize(serialized_message: str) -\u0026gt; SearchRequest: search_request = SearchRequest() search_request.ParseFromString(serialized_message) return search_request def consume(message: SearchRequest) -\u0026gt; None: ... if __name__ == \u0026#34;__main__\u0026#34;: serialized_message = get_message() search_request = deserialize(serialized_message) consume(search_request) You can even save the proto files in a common repo, generate the interfacing code automatically for multiple languages, and package them up via CI whenever some changes are merged into the main branch. Then the services can just update those protocol packages and use the serializers and deserializers as needed.\n","permalink":"https://rednafi.com/misc/protobuffed-contracts/","summary":"\u003cp\u003ePeople typically associate Google\u0026rsquo;s \u003ca href=\"https://protobuf.dev/\"\u003eProtocol Buffer\u003c/a\u003e with \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e services, and rightfully\nso. But things often get confusing when discussing protobufs because the term can mean\ndifferent things:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA binary protocol for efficiently serializing structured data.\u003c/li\u003e\n\u003cli\u003eA language used to specify how this data should be structured.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn gRPC services, you usually use both: the protobuf language in proto files defines the\nservice interface, and then the clients use the same proto files to communicate with the\nservices.\u003c/p\u003e","title":"Protobuffed contracts"},{"content":"The handful of times I\u0026rsquo;ve reached for typing.TypeGuard in Python, I\u0026rsquo;ve always been confused by its behavior and ended up ditching it with a # type: ignore comment.\nFor the uninitiated, TypeGuard allows you to apply custom type narrowing. For example, let\u0026rsquo;s say you have a function named pretty_print that accepts a few different types and prints them differently onto the console:\nfrom typing import assert_never def pretty_print(val: int | float | str) -\u0026gt; None: if isinstance(val, int): # assert_type(val, int) print(f\u0026#34;Integer: {val}\u0026#34;) elif isinstance(val, float): # assert_type(val, float) print(f\u0026#34;Float: {val}\u0026#34;) elif isinstance(val, str): # assert_type(val, str) print(f\u0026#34;String: {val}\u0026#34;) else: assert_never(val) If you run it through mypy, in each branch, the type checker automatically narrows the type and knows exactly what the type of val is. You can test the narrowed type in each branch with the typing.assert_type function.\nThis works well for 99% of cases, but occasionally, you need to check an incoming value more thoroughly to determine its type and want to take action based on the narrowed type. In those cases, just using isinstance may not be sufficient. So, you need to factor out the complex type checking logic into a separate function and return a boolean depending on whether the inbound value satisfies all the criteria to be of the expected type. For example:\nfrom typing import TypedDict, TypeGuard class Person(TypedDict): name: str age: int def is_person(val: dict) -\u0026gt; TypeGuard[Person]: try: name, age = val[\u0026#34;name\u0026#34;], val[\u0026#34;age\u0026#34;] except KeyError: return False return len(name) \u0026gt; 1 and 0 \u0026lt; age \u0026lt; 150 def print_age(val: dict) -\u0026gt; None: if is_person(val): # assert_type(val, Person) print(f\u0026#34;Age: {val[\u0026#39;age\u0026#39;]}\u0026#34;) else: print(\u0026#34;Not a person!\u0026#34;) Here, is_person first checks that the inbound dictionary conforms to the structure of the Person typeddict and then verifies that name is at least 1 character long and age is between 0 and 150.\nThis is a bit more involved than just checking the type with isinstance and the type checker needs a little more help from the user. Although the return type of the is_person function is bool, typing it with TypeGuard[Person] signals the type checker that if the inbound value satisfies all the constraints and the checker function returns True, the underlying type of val is Person.\nYou can see more examples of TypeGuard in PEP 647.\nAll good. However, I find the behavior of TypeGuard a bit unintuitive whenever I need to couple it with union types. For example:\nfrom typing import Any, TypeGuard def is_non_zero_number(val: Any) -\u0026gt; TypeGuard[int | float]: return val != 0 def pretty_print(val: str | int | float) -\u0026gt; None: if is_non_zero_number(val): # assert_type(val, int | float) print(f\u0026#34;Non zero number: {val}\u0026#34;) else: # assert_type(val, str | int | float); wat?? print(f\u0026#34;String: {val}\u0026#34;) In the if branch, TypeGuard signals the type checker correctly that the narrowed type of the inbound value is int | float but in the else branch, I was expecting it to be str because the truthy if condition has already filtered out the int | float. But instead, we get str | int | float as the narrowed type. While there might be a valid reason behind this design choice, the resulting behavior with union types made TypeGuard fairly useless for cases I wanted to use it for.\nTypeIs has been introduced via PEP 742 to fix exactly that. The PEP agrees that people might find the current behavior of TypeGuard a bit unexpected and introducing another construct with a slightly different behavior doesn\u0026rsquo;t make things any less confusing.\nWe acknowledge that this leads to an unfortunate situation where there are two constructs with a similar purpose and similar semantics. We believe that users are more likely to want the behavior of TypeIs, the new form proposed in this PEP, and therefore we recommend that documentation emphasize TypeIs over TypeGuard as a more commonly applicable tool.\nTypeGuard and TypeIs have similar semantics, except, the latter can narrow the type in both the if and else branches of a conditional. Here\u0026rsquo;s another example with a union type where TypeIs does what I expected TypeGuard to do:\nimport sys if sys.version_info \u0026gt; (3, 13): # TypeIs is available in Python 3.13+ from typing import TypeIs else: from typing_extensions import TypeIs def is_number(value: object) -\u0026gt; TypeIs[int | float | complex]: return isinstance(value, (int, float, complex)) def pretty_print(val: str | int | float | complex) -\u0026gt; None: if is_number(val): # assert_type(val, int | float | complex) print(val) else: # assert_type(val, str) print(\u0026#34;Not a number!\u0026#34;) Notice that TypeIs has now correctly narrowed the type in the else branch as well. This would\u0026rsquo;ve also worked had we returned early from the pretty_print function in the if branch and skipped the else branch altogether. Exactly what I need!\nHere are a few typeshed stubs for the stdlib functions in the inspect module that are already taking advantage of the new TypeIs construct:\n# fmt: off def isgenerator(obj: object) -\u0026gt; TypeIs[GeneratorType[Any, Any, Any]]: ... def iscoroutine(obj: object) -\u0026gt; TypeIs[CoroutineType[Any, Any, Any]]: ... def isawaitable(obj: object) -\u0026gt; TypeIs[Awaitable[Any]]: ... def isasyncgen(object: object) -\u0026gt; TypeIs[AsyncGeneratorType[Any, Any]]: ... def istraceback(object: object) -\u0026gt; TypeIs[TracebackType]: ... # and so on and so forth ","permalink":"https://rednafi.com/python/typeguard-vs-typeis/","summary":"\u003cp\u003eThe handful of times I\u0026rsquo;ve reached for \u003ccode\u003etyping.TypeGuard\u003c/code\u003e in Python, I\u0026rsquo;ve always been\nconfused by its behavior and ended up ditching it with a \u003ccode\u003e# type: ignore\u003c/code\u003e comment.\u003c/p\u003e\n\u003cp\u003eFor the uninitiated, \u003ccode\u003eTypeGuard\u003c/code\u003e allows you to apply custom \u003ca href=\"https://mypy.readthedocs.io/en/latest/type_narrowing.html\"\u003etype narrowing\u003c/a\u003e. For example,\nlet\u0026rsquo;s say you have a function named \u003ccode\u003epretty_print\u003c/code\u003e that accepts a few different types and\nprints them differently onto the console:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etyping\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eassert_never\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003epretty_print\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eval\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e \u003cspan class=\"o\"\u003e|\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e \u003cspan class=\"o\"\u003e|\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"nb\"\u003eisinstance\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eval\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# assert_type(val, int)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Integer: \u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003eval\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eelif\u003c/span\u003e \u003cspan class=\"nb\"\u003eisinstance\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eval\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# assert_type(val, float)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Float: \u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003eval\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eelif\u003c/span\u003e \u003cspan class=\"nb\"\u003eisinstance\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eval\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# assert_type(val, str)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;String: \u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003eval\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003eassert_never\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eval\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIf you run it through \u003ccode\u003emypy\u003c/code\u003e, in each branch, the type checker automatically narrows the\ntype and knows exactly what the type of \u003ccode\u003eval\u003c/code\u003e is. You can test the narrowed type in each\nbranch with the \u003ccode\u003etyping.assert_type\u003c/code\u003e function.\u003c/p\u003e","title":"TypeIs does what I thought TypeGuard would do in Python"},{"content":"One neat use case for the HTTP ETag header is client-side HTTP caching for GET requests. Along with the ETag header, the caching workflow requires you to fiddle with other conditional HTTP headers like If-Match or If-None-Match. However, their interaction can feel a bit confusing at times.\nEvery time I need to tackle this, I end up spending some time browsing through the relevant MDN docs on ETag, If-Match, and If-None-Match to jog my memory. At this point, I\u0026rsquo;ve done it enough times to justify spending the time to write this.\nCaching the response of a GET endpoint The basic workflow goes as follows:\nThe client makes a GET request to the server. The server responds with a 200 OK status, including the content requested and an ETag header. The client caches the response and the ETag value. For subsequent requests to the same resource, the client includes the If-None-Match header with the ETag value it has cached. The server regenerates the ETag independently and checks if the ETag value sent by the client matches the generated one. If they match, the server responds with a 304 Not Modified status, indicating that the client\u0026rsquo;s cached version is still valid, and the client serves the resource from the cache. If they don\u0026rsquo;t match, the server responds with a 200 OK status, including the new content and a new ETag header, prompting the client to update its cache. Client Server | | |----- GET Request --------------------\u0026gt;| | | |\u0026lt;---- Response 200 OK + ETag ----------| | (Cache response locally) | | | |----- GET Request + If-None-Match ----\u0026gt;| (If-None-Match == previous ETag) | | | Does ETag match? | |\u0026lt;---- Yes: 304 Not Modified -----------| (No body sent; Use local cache) | No: 200 OK + New ETag ----------| (Update cached response) | | We can test this workflow with GitHub\u0026rsquo;s REST API suite via the GitHub CLI. If you\u0026rsquo;ve installed the CLI and authenticated yourself, you can make a request like this:\ngh api -i /users/rednafi This asks for the data associated with the user rednafi. The response looks as follows:\nHTTP/2.0 200 OK Etag: W/\u0026#34;b8fdfabd59aed6e0e602dd140c0a0ff48a665cac791dede458c5109bf4bf9463\u0026#34; { \u0026#34;login\u0026#34;:\u0026#34;rednafi\u0026#34;, \u0026#34;id\u0026#34;:30027932, ... } I\u0026rsquo;ve truncated the response body and omitted the headers that aren\u0026rsquo;t relevant to this discussion. You can see that the HTTP status code is 200 OK and the server has included an ETag header.\nThe W/ prefix indicates that a weak validator is used to validate the content of the cache. Using a weak validator means when the server compares the response payload to generate the hash, it doesn\u0026rsquo;t do it bit-by-bit. So, if your response is JSON, then changing the format of the JSON won\u0026rsquo;t change the value of the ETag header since two JSON payloads with the same content but with different formatting are semantically the same thing.\nLet\u0026rsquo;s see what happens if we make the same request again while passing the value of the ETag in the If-None-Match header.\ngh api -i \\ -H \u0026#39;If-None-Match: W/\u0026#34;b8fdfabd59aed6e0e602dd140c0a...\u0026#34;\u0026#39; \\ /users/rednafi This returns:\nHTTP/2.0 304 Not Modified Etag: \u0026#34;b8fdfabd59aed6e0e602dd140c0a0ff48a665cac791dede458c5109bf4bf9463\u0026#34; gh: HTTP 304 This means that the cached response in the client is still valid and it doesn\u0026rsquo;t need to refetch that from the server. So, the client can be coded to serve the previously cached data to the users when asked for.\nA few key points to keep in mind:\nAlways wrap your ETag values in double quotes when sending them with the If-None-Match header, just as the spec says for conditional header values.\nUsing the If-None-Match header to pass the ETag value means that the client request is considered successful when the ETag value from the client doesn\u0026rsquo;t match that of the server. When the values match, the server will return 304 Not Modified with no body.\nIf we\u0026rsquo;re writing a compliant server, when it comes to If-None-Match, the spec tells us to use weak comparison for ETags. This means that the client will still be able to validate the cache with weak ETags, even if there have been slight changes to the representation of the data.\nIf the client is a browser, it\u0026rsquo;ll automatically manage the cache and send conditional requests without any extra work.\nWriting a server that enables client-side caching If you\u0026rsquo;re serving static content, you can configure your load balancer to enable this caching workflow. But for dynamic GET requests, the server needs to do a bit more work to allow client-side caching.\nHere\u0026rsquo;s a simple server in Go that enables the above workflow for a dynamic GET request:\npackage main import ( \u0026#34;crypto/sha256\u0026#34; \u0026#34;encoding/hex\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strings\u0026#34; ) // calculateETag generates a weak ETag by SHA-256-hashing the content // and prefixing it with W/ to indicate a weak comparison func calculateETag(content string) string { hasher := sha256.New() hasher.Write([]byte(content)) hash := hex.EncodeToString(hasher.Sum(nil)) return fmt.Sprintf(\u0026#34;W/\\\u0026#34;%s\\\u0026#34;\u0026#34;, hash) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { // Define the content within the handler content := `{\u0026#34;message\u0026#34;: \u0026#34;Hello, world!\u0026#34;}` eTag := calculateETag(content) // Remove quotes and W/ prefix for If-None-Match header comparison ifNoneMatch := strings.TrimPrefix( strings.Trim(r.Header.Get(\u0026#34;If-None-Match\u0026#34;), \u0026#34;\\\u0026#34;\u0026#34;), \u0026#34;W/\u0026#34;) // Hash content without W/ prefix for comparison contentHash := strings.TrimPrefix(eTag, \u0026#34;W/\u0026#34;) // Check if the ETag matches; if so, return 304 Not Modified if ifNoneMatch == strings.Trim(contentHash, \u0026#34;\\\u0026#34;\u0026#34;) { w.WriteHeader(http.StatusNotModified) return } // If ETag does not match, return the content and the ETag w.Header().Set(\u0026#34;ETag\u0026#34;, eTag) // Send weak ETag w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) w.WriteHeader(http.StatusOK) fmt.Fprint(w, content) }) fmt.Println(\u0026#34;Server is running on http://localhost:8080\u0026#34;) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } The server generates a weak ETag for its content by creating a SHA-256 hash and adding W/ to the front, indicating it\u0026rsquo;s meant for weak comparison.\nYou could make the calculateETag function format-agnostic, so the hash stays the same if the JSON format changes but the content does not. The current calculateETag implementation is susceptible to format changes, and I kept it that way to keep the code shorter.\nWhen delivering content, the server includes this weak ETag in the response headers, allowing clients to cache the content along with the ETag.\nFor subsequent requests, the server checks if the client has sent an ETag in the If-None-Match header and weakly compares it with the current content\u0026rsquo;s ETag by independently generating the hash.\nIf the ETags match, indicating no significant content change, the server replies with a 304 Not Modified status. Otherwise, it sends the content again with a 200 OK status and updates the ETag. When this happens, the client knows that the existing cache is still warm and can be served without any changes to it.\nYou can spin up the server by running go run main.go and from a different console, start making requests to it like this:\ncurl -i http://localhost:8080/foo This will return the ETag header along with the JSON response:\nHTTP/1.1 200 OK Content-Type: application/json Etag: W/\u0026#34;1d3b4242cc9039faa663d7ca51a25798e91fbf7675c9007c2b0470b72c2ed2f3\u0026#34; Date: Wed, 10 Apr 2024 15:54:33 GMT Content-Length: 28 {\u0026#34;message\u0026#34;: \u0026#34;Hello, world!\u0026#34;} Now, you can make another request with the value of\nthe ETag in the If-None-Match header:\ncurl -i \\ -H \u0026#39;If-None-Match: \u0026#34;1d3b4242cc9039faa663d7ca51a2...\u0026#34;\u0026#39; \\ http://localhost:8080/foo This will return a 304 Not Modified response with no body:\nHTTP/1.1 304 Not Modified Date: Wed, 10 Apr 2024 15:57:25 GMT In a real-life scenario, you\u0026rsquo;ll probably factor out the caching part in middleware so that all of your HTTP GET requests can be cached from the client-side without repetition.\nOne thing to look out for While writing a cache-enabled server, make sure the system is set up so that the server always sends back the same ETag for the same content, even when there are multiple servers working behind a load balancer. If these servers give out different ETags for the same content, it can mess up how clients cache that content.\nClients use ETags to decide if content has changed. If the ETag value hasn\u0026rsquo;t changed, they know the content is the same and don\u0026rsquo;t download it again, saving bandwidth and speeding up access. But if ETags are inconsistent across servers, clients might download content they already have, wasting bandwidth and slowing things down.\nThis inconsistency also means servers end up dealing with more requests for content that clients could have just used from their cache if ETags were consistent.\n","permalink":"https://rednafi.com/misc/etag-and-http-caching/","summary":"\u003cp\u003eOne neat use case for the HTTP \u003ccode\u003eETag\u003c/code\u003e header is client-side HTTP caching for \u003ccode\u003eGET\u003c/code\u003e requests.\nAlong with the \u003ccode\u003eETag\u003c/code\u003e header, the caching workflow requires you to fiddle with other\nconditional HTTP headers like \u003ccode\u003eIf-Match\u003c/code\u003e or \u003ccode\u003eIf-None-Match\u003c/code\u003e. However, their interaction can\nfeel a bit confusing at times.\u003c/p\u003e\n\u003cp\u003eEvery time I need to tackle this, I end up spending some time browsing through the relevant\nMDN docs on \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/ETag\"\u003eETag\u003c/a\u003e, \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/If-Match\"\u003eIf-Match\u003c/a\u003e, and \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/If-None-Match\"\u003eIf-None-Match\u003c/a\u003e to jog my memory. At this point, I\u0026rsquo;ve\ndone it enough times to justify spending the time to write this.\u003c/p\u003e","title":"ETag and HTTP caching"},{"content":"Every once in a while, I find myself skimming through the MDN docs to jog my memory on how CORS works and which HTTP headers are associated with it. This is particularly true when a frontend app can\u0026rsquo;t talk to a backend service I manage due to a CORS error.\nMDN\u0026rsquo;s CORS documentation is excellent but can be a bit verbose for someone just looking for a way to quickly troubleshoot and fix the issue at hand.\nTypically, the CORS issue I encounter boils down to:\nA backend service that accepts requests only from a list of specified domains. A new frontend service or some other client trying to access it from a different domain that\u0026rsquo;s not on the server\u0026rsquo;s allowlist. Consequently, the server rejects it with an HTTP 4xx error. Here\u0026rsquo;s a list of some commonly found headers associated with CORS:\nRequest headers\nOrigin: indicates the origin of the request Access-Control-Request-Method: used in preflight requests to specify the method of the actual request Access-Control-Request-Headers: used in preflight to specify headers that will be used in the actual request Response headers\nAccess-Control-Allow-Origin: specifies the origins that are allowed to access the resource Access-Control-Allow-Methods: indicates the methods allowed when accessing the resource Access-Control-Allow-Headers: specifies the headers that can be included in the actual request Access-Control-Allow-Credentials: indicates whether or not the response can be exposed when the credentials flag is true Access-Control-Expose-Headers: specifies the headers that can be exposed as part of the response Access-Control-Max-Age: indicates how long the results of a preflight request can be cached In most cases, focusing on the Origin and Access-Control-Allow-Origin headers is enough to verify whether a service can be reached from a certain domain without running into a CORS error.\nTo recreate the canonical CORS issue, here\u0026rsquo;s a simple server written in Go that exposes a single POST endpoint /hello. You can make a POST request to it with the {\u0026quot;name\u0026quot;: \u0026quot;Something\u0026quot;} payload, and it will echo back with a JSON message.\nClick here ... // main.go package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) // Person struct to parse the input JSON. type Person struct { Name string `json:\u0026#34;name\u0026#34;` } // helloNameHandler responds with \u0026#34;Hello {name}\u0026#34;. func helloNameHandler(w http.ResponseWriter, r *http.Request) { if r.Method != \u0026#34;POST\u0026#34; { http.Error(w, \u0026#34;POST only\u0026#34;, http.StatusMethodNotAllowed) return } var p Person if err := json.NewDecoder(r.Body).Decode(\u0026amp;p); err != nil { http.Error(w, err.Error(), http.StatusBadRequest) return } response := fmt.Sprintf(\u0026#34;Hello %s\u0026#34;, p.Name) w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(map[string]string{\u0026#34;message\u0026#34;: response}) } // corsMiddleware adds CORS headers to the response. func corsMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { allowedOrigins := map[string]bool{ \u0026#34;http://allowed-origin-1.com\u0026#34;: true, \u0026#34;http://allowed-origin-2.com\u0026#34;: true, } origin := r.Header.Get(\u0026#34;Origin\u0026#34;) if _, ok := allowedOrigins[origin]; ok { w.Header().Set(\u0026#34;Access-Control-Allow-Origin\u0026#34;, origin) } else { // Handle not allowed origin. http.Error(w, \u0026#34;Origin not allowed\u0026#34;, http.StatusForbidden) return } w.Header().Set(\u0026#34;Access-Control-Allow-Methods\u0026#34;, \u0026#34;POST, OPTIONS\u0026#34;) w.Header().Set(\u0026#34;Access-Control-Allow-Headers\u0026#34;, \u0026#34;Content-Type\u0026#34;) // Handle preflight request. if r.Method == \u0026#34;OPTIONS\u0026#34; { w.WriteHeader(http.StatusOK) return } next.ServeHTTP(w, r) }) } func main() { mux := http.NewServeMux() handler := http.HandlerFunc(helloNameHandler) mux.Handle(\u0026#34;/hello\u0026#34;, corsMiddleware(handler)) fmt.Println(\u0026#34;Server is running on http://localhost:7676\u0026#34;) http.ListenAndServe(\u0026#34;:7676\u0026#34;, mux) } Here, the server only allows requests from two particular domains: http://allowed-origin-1.com and http://allowed-origin-2.com. A client on another host can make a preflight OPTIONS request to check if the server will permit the subsequent POST request.\nIf the client is on a domain that\u0026rsquo;s not on the allowlist, the server will reject the request.\nYou can run the server with go run main.go and then, from another console, try making a preflight request without specifying the Origin header:\ncurl -i -X OPTIONS http://localhost:7676/hello The server will reject this:\nHTTP/1.1 403 Forbidden Content-Type: text/plain; charset=utf-8 X-Content-Type-Options: nosniff Date: Tue, 12 Mar 2024 21:52:26 GMT Content-Length: 19 Origin not allowed You need to specify one of the domains expected by the server via the Origin header as follows:\ncurl -i -X OPTIONS http://localhost:7676/hello \\ -H \u0026#39;Origin: http://allowed-origin-1.com\u0026#39; This time, the preflight request will succeed:\nHTTP/1.1 200 OK Access-Control-Allow-Headers: Content-Type Access-Control-Allow-Methods: POST, OPTIONS Access-Control-Allow-Origin: http://allowed-origin-1.com Date: Tue, 12 Mar 2024 21:54:57 GMT Content-Length: 0 Notice that the Access-Control-Allow-Methods header also specifies the methods allowed on this endpoint.\nIf you make a preflight request with an origin not on the server\u0026rsquo;s allowlist, you will encounter a 4xx error again.\ncurl -i -X OPTIONS http://localhost:7676/hello \\ -H \u0026#39;Origin: http://notallowed.com\u0026#39; The return message indicates that requests from http://notallowed.com are blocked by CORS control:\nHTTP/1.1 403 Forbidden Content-Type: text/plain; charset=utf-8 X-Content-Type-Options: nosniff Date: Tue, 12 Mar 2024 21:57:06 GMT Content-Length: 19 Origin not allowed Similarly, making a POST request without sending the expected origin in the header will result in a 4xx error.\ncurl -i -X POST http://localhost:7676/hello --data \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;Foo\u0026#34;}\u0026#39; This returns:\nHTTP/1.1 403 Forbidden Content-Type: text/plain; charset=utf-8 X-Content-Type-Options: nosniff Date: Tue, 12 Mar 2024 21:59:31 GMT Content-Length: 19 Origin not allowed Like the preflight request, you need to pass the expected origin in the header.\nSo, if your frontend cannot access the backend and the browser console indicates that CORS control is blocking the request, you\u0026rsquo;ll likely need to add the new domain to your server\u0026rsquo;s allowlist. Then make sure that the client is passing the desired origin in the header. In the case of a browser, this should be automatically handled for you.\nUse the preflight request commands to test that the server is only allowing access from the whitelisted domain while blocking everything else.\n","permalink":"https://rednafi.com/misc/crossing-the-cors-crossroad/","summary":"\u003cp\u003eEvery once in a while, I find myself skimming through the MDN docs to jog my memory on how\n\u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS\"\u003eCORS\u003c/a\u003e works and which HTTP headers are associated with it. This is particularly true when a\nfrontend app can\u0026rsquo;t talk to a backend service I manage due to a \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS/Errors\"\u003eCORS error\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eMDN\u0026rsquo;s CORS documentation is excellent but can be a bit verbose for someone just looking for\na way to quickly troubleshoot and fix the issue at hand.\u003c/p\u003e","title":"Crossing the CORS crossroad"},{"content":"Ever since Rob Pike published the text on the functional options pattern, there\u0026rsquo;s been no shortage of blogs, talks, or comments on how it improves or obfuscates configuration ergonomics.\nWhile the necessity of such a pattern is quite evident in a language that lacks default arguments in functions, more often than not, it needlessly complicates things. The situation gets worse if you have to maintain a public API where multiple configurations are controlled in this manner.\nHowever, the pattern solves a valid problem and definitely has its place. Otherwise, it wouldn\u0026rsquo;t have been picked up by pretty much every other library, whether it\u0026rsquo;s Ngrok or the Elasticsearch agent.\nIf you have no idea what I\u0026rsquo;m talking about, you might want to give my previous write-up on configuring options a read.\nFunctional options pattern As a recap, here\u0026rsquo;s how the functional options pattern works. Let\u0026rsquo;s say, you need to allow the users of your API to configure something. You can expose a struct from your package that\u0026rsquo;ll be passed to some other function to tune its behavior. For example:\npackage src type Config struct { // Required Foo, Bar string // Optional Fizz, Bazz int } func Do(config *Config) { // Do something with the config values } Then the Config struct will be imported, initialized, and passed to the Do function by your API users:\npackage main import \u0026#34;.../src\u0026#34; func main() { // Initialize the config and pass it to the Do function config := \u0026amp;src.Config{ Foo: \u0026#34;hello\u0026#34;, Bar: \u0026#34;world\u0026#34;, Fizz: 0, Bazz: 42, } // Call Do with the initialized Config struct src.Do(config) } This is one way of doing that, but it\u0026rsquo;s generally discouraged since it requires you to expose the internals of your API to the users. So instead, a library usually exposes a factory function that\u0026rsquo;ll do the struct initialization while keeping the struct and fields private. For instance:\npackage src type config struct { // Notice that the struct and fields are now private // Required foo, bar string // Optional fizz, bazz int } // Public factory function func NewConfig(foo, bar string, fizz, bazz int) config { return config{foo, bar, fizz, bazz} } func Do(c *config){} The API consumers will now use NewConfig to produce a configuration and then pass the struct instance to the Do function as follows:\npackage main import \u0026#34;.../src\u0026#34; func main() { // Initialize the config with the NewConfig factory c := \u0026amp;src.NewConfig(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, 0, 42) // Call Do with the initialized Config struct src.Do(c) } This approach is better as it keeps the internal machinery hidden from users. However, it doesn\u0026rsquo;t allow for setting default values for some configuration attributes; all must be set explicitly. What if your users want to override the values of multiple attributes? This leads your configuration struct to be overloaded with options, making the NewConfig function demands numerous positional arguments.\nThis setup isn\u0026rsquo;t user-friendly, as it forces API users to explicitly pass all these options to the NewConfig factory. Ideally, you\u0026rsquo;d initialize config with some default values, offering users a chance to override them. But, Go doesn\u0026rsquo;t support default values for function arguments, which compels us to be creative and come up with different workarounds. Functional options pattern is one of them.\nHere\u0026rsquo;s how you can build your API to leverage the pattern:\npackage src type config struct { // Required foo, bar string // Optional fizz, bazz int } type option func(*config) // Each optional config attribute can be overridden with // an associated function func WithFizz(fizz int) option { return func(c *config) { c.fizz = fizz } } func WithBazz(bazz int) option { return func(c *config) { c.bazz = bazz } } func NewConfig(foo, bar string, opts ...option) config { // First fill in the options with default values c := config{foo, bar, 10, 100} // Now allow users to override the optional configuration attributes for _, opt := range opts { opt(\u0026amp;c) } return c } func Do(c *config) {} Then you\u0026rsquo;d use it as follows:\npackage main import \u0026#34;.../src\u0026#34; func main() { c := \u0026amp;src.NewConfig(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, src.WithFizz(1), src.WithBazz(2)) src.Do(c) } Functional options pattern relies on functions that modify the configuration struct\u0026rsquo;s state. These modifier functions, or option functions, are defined to accept a pointer to the configuration struct *config and then directly alter its fields. This direct manipulation is possible because the option functions are closures, which means they capture and modify the variables from their enclosing scope, in this case, the config instance.\nIn the NewConfig factory, the variadic parameter opts ...option allows for an arbitrary number of option functions to be passed. Here, opts represents the optional configurations that the users can override if they want to.\nThe NewConfig function iterates over this slice of option functions, invoking each one with the \u0026amp;c argument, which is a pointer to the newly created config instance. The config instance is created with default values, and the users can use the With* functions to override them.\nCurse of indirection That\u0026rsquo;s a fair bit of indirection just to allow API users to configure some options. I don\u0026rsquo;t know about you, but multi-layered higher-order functions hurt my brain. It\u0026rsquo;s quite slow as well.\nAll this complexity could have been avoided if Go allowed default arguments in functions. Your configuration factory could simply grab the default values from the keyword arguments and pass them to the underlying struct. The idea that supporting default arguments in functions would lead to a parameter explosion seems unfounded, especially when the alternative requires gymnastics like the functional option pattern.\nAlso, the multiple layers of indirection hinder API discoverability. Trying to discover modifier functions by hovering your cursor over the factory function\u0026rsquo;s return value in the IDE won\u0026rsquo;t be very helpful, as these functions are defined at the package level.\nSo, if you need to configure multiple structs in this manner, the explosion of their respective package-level modifiers make it even harder for the user to know which function they\u0026rsquo;ll need to use to update a certain configuration attribute.\nRecently, I\u0026rsquo;ve spontaneously stumbled upon a fluent-style API to manage configurations that don\u0026rsquo;t require so many layers of indirection and lets you expose optional configuration attributes. Let\u0026rsquo;s call it dysfunctional options pattern.\nDysfunctional options pattern The idea is quite similar to how the API with functional options pattern is constructed. Here\u0026rsquo;s the complete implementation:\npackage src type config struct { // Required foo, bar string // Optional fizz, bazz int } // Each optional configuration attribute will have its own public method func (c *config) WithFizz(fizz int) *config { c.fizz = fizz return c } func (c *config) WithBazz(bazz int) *config { c.bazz = bazz return c } // This only accepts the required options as params func NewConfig(foo, bar string) *config { // First fill in the options with default values return \u0026amp;config{foo, bar, 10, 100} } func Do(c *config) {} You\u0026rsquo;d use the API as follows:\npackage main import \u0026#34;.../src\u0026#34; func main() { // Initialize the struct with only the required options and then chain // the option methods to update the optional configuration attributes c := src.NewConfig(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;).WithFizz(0).WithBazz(42) src.Do(c) } Similar to the previous pattern, we have modifiers here too. However, instead of being higher order functions, the modifiers are methods on config and return a pointer to the struct.\nThe NewConfig factory function instantiates the config struct with some default values and returns the struct pointer like the modifiers. This enables us to chain the WithFizz and WithBazz modifiers on the returned value of NewConfig and update the values of the optional configuration attributes.\nApart from simplicity and the lack of magic, you can hover over the return type of the factory and immediately know about the supported modifier methods.\nI did a rudimentary benchmark of the two approaches and was surprised that the second one was roughly ~76x faster on Go 1.22!\nHere\u0026rsquo;s an example in fork-sweeper\u0026rsquo;s CLI code.\nP.S. This is indeed a lightweight spin on what OO languages call the builder pattern. However, I didn\u0026rsquo;t call it that because there\u0026rsquo;s no mandatory .Build() method to be called at the end of the method chain.\n","permalink":"https://rednafi.com/go/dysfunctional-options-pattern/","summary":"\u003cp\u003eEver since Rob Pike published the text on the \u003ca href=\"https://commandcenter.blogspot.com/2014/01/self-referential-functions-and-design.html\"\u003efunctional options pattern\u003c/a\u003e, there\u0026rsquo;s been no\nshortage of blogs, talks, or comments on how it improves or obfuscates configuration\nergonomics.\u003c/p\u003e\n\u003cp\u003eWhile the necessity of such a pattern is quite evident in a language that lacks default\narguments in functions, more often than not, it needlessly complicates things. The situation\ngets worse if you have to maintain a public API where multiple configurations are controlled\nin this manner.\u003c/p\u003e","title":"Dysfunctional options pattern in Go"},{"content":"In 9th grade, when I first learned about Lenz\u0026rsquo;s law in Physics class, I was fascinated by its implications. It states:\nThe direction of an induced current will always oppose the motion causing it.\nIn simpler terms, imagine you have a hoop and a magnet. If you move the magnet close to the hoop, the hoop generates a magnetic field that pushes the magnet away. Conversely, if you pull the magnet away, the hoop creates a magnetic field that pulls it back. This occurs because the hoop aims to prevent any change in the surrounding magnetic field. That\u0026rsquo;s Lenz\u0026rsquo;s Law: the hoop consistently acts to maintain the magnetic field\u0026rsquo;s status quo, reacting against the motion that\u0026rsquo;s the cause of the existence of the magnetic flux in the first place. Generators leverage this principle to convert mechanical motion into electrical energy.\nIn my experience, knowledge works like this as well. The very thing that made you good at what you do often prevents you from getting better.\nThe first decent program I ever wrote was a simulation in MATLAB during my sophomore year. Since then, I\u0026rsquo;ve switched career gears a few times and picked up a few programming languages along the way. The weird thing is, over the years, learning new paradigms hasn\u0026rsquo;t become any easier than I thought it\u0026rsquo;d be. I only got better at grokking technologies that are woefully similar to what I already know.\nSure, picking up a new tool or library in a familiar language has become second nature, but so has the tendency to keep doing the same thing I\u0026rsquo;m somewhat good at, slightly differently, and falling into the false lull of growth. This can be perilous in the sense that, unless you actively fight it, after a decade, you might find yourself in a situation where you actually have only a year\u0026rsquo;s worth of experience repeated ten times.\nI\u0026rsquo;ve been looking for a phrase to label the dilemma where the curse of knowledge, accumulated over time, prevents you from acquiring new knowledge and adopting new ways of thinking. Turns out, there\u0026rsquo;s one for that — the Einstellung effect. Einstellung is a German word for \u0026ldquo;attitude\u0026rdquo; or \u0026ldquo;setting.\u0026rdquo; It describes a situation where we stick to a familiar way of thinking or solving problems, which can stop us from seeing or seeking easier or better solutions. This only gets worse as we start growing older or becoming more experienced in a narrow slice of a highly specialized domain.\nTo put it more concretely, when I learned my first proper programming language, Python, it was much easier for me to pick up the syntax, semantics, and culture around it since all that knowledge was basically getting dumped into an empty slate. When I had to pick up some JavaScript for work, it was still relatively easy since I started to compare the features of the language with the one I knew and worked my way up. However, the time it took for me to get comfortable in the JS world wasn\u0026rsquo;t that short since I was begrudgingly resisting learning about the warts of the language and complaining about how nice Python\u0026rsquo;s type system was compared to this mess!\nIt was only when I started to learn Go that I became aware of the warts of Python and when it just wasn\u0026rsquo;t the right tool to solve the problem at hand. Despite Go\u0026rsquo;s fame for being simple, I had to read a few books, solve a few koans, and build a few tools before I got confident in solving problems with it. One reason why it took longer is because I was trying to emulate Python idioms inside Go while at the same time bashing people who\u0026rsquo;d write Go like Java, remaining blissfully unaware of my own stance. In each of these instances, my accumulated experience and acquired predispositions actively resisted adopting different ways of solving problems.\nIt\u0026rsquo;s natural for us to acquire new skills by mapping them with what we already know, and that\u0026rsquo;s true for any skill — whether it\u0026rsquo;s learning to cook a new dish or writing in an unfamiliar programming language. But at the same time, as we grow older, we actively need to strain against the tide to learn to learn like kids. Because without it, we are nothing but simulacra acting as conscious exotica, spellbound by the allure of mimicry!\n","permalink":"https://rednafi.com/zephyr/einstellung-effect/","summary":"\u003cp\u003eIn 9th grade, when I first learned about \u003ca href=\"https://www.youtube.com/watch?v=QwUq8xM_8bY\"\u003eLenz\u0026rsquo;s law\u003c/a\u003e in Physics class, I was fascinated by\nits implications. It states:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eThe direction of an induced current will always oppose the motion causing it.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn simpler terms, imagine you have a hoop and a magnet. If you move the magnet close to the\nhoop, the hoop generates a magnetic field that pushes the magnet away. Conversely, if you\npull the magnet away, the hoop creates a magnetic field that pulls it back. This occurs\nbecause the hoop aims to prevent any change in the surrounding magnetic field. That\u0026rsquo;s Lenz\u0026rsquo;s\nLaw: the hoop consistently acts to maintain the magnetic field\u0026rsquo;s status quo, reacting\nagainst the motion that\u0026rsquo;s the cause of the existence of the magnetic flux in the first\nplace. Generators leverage this principle to convert mechanical motion into electrical\nenergy.\u003c/p\u003e","title":"Einstellung effect"},{"content":"These days, I don\u0026rsquo;t build hierarchical types through inheritance even when writing languages that support it. Type composition has replaced almost all of my use cases where I would\u0026rsquo;ve reached for inheritance before.\nI\u0026rsquo;ve written about how to escape the template pattern hellscape and replace that with strategy pattern in Python before. While by default, Go saves you from shooting yourself in the foot by disallowing inheritance, it wasn\u0026rsquo;t obvious to me how I could apply the strategy pattern to make things more composable and testable.\nAlso, often the Go community exhibits a knee-jerk reaction to the word \u0026ldquo;pattern,\u0026rdquo; even when it has nothing to do with OO. However, I feel it\u0026rsquo;s important to use a specific term while explaining a concept, and I\u0026rsquo;d rather not attempt to relabel a concept when an established term already exists for it.\nJust a quick recap: the strategy pattern is a design approach where you can choose from a set of methods to solve a problem, each method wrapped in its own class. This way, you can swap out these methods easily without messing with the rest of your code, making it simple to adjust behaviors on the fly.\nLet\u0026rsquo;s say you\u0026rsquo;re writing a display service that prints a message in either plain text or JSON formats. Imperatively you could do this:\n# main.rb require \u0026#39;json\u0026#39; def display(message, format) if format == :text puts message elsif format == :json json_output = { message: message }.to_json puts json_output else puts \u0026#34;Unknown format\u0026#34; end end # Usage display(\u0026#34;Hello, World!\u0026#34;, :text) # Prints \u0026#34;Hello, World!\u0026#34; display(\u0026#34;Hello, World!\u0026#34;, :json) # Prints \u0026#34;{\u0026#34;message\u0026#34;:\u0026#34;Hello, World!\u0026#34;}\u0026#34; While this is a trivial example, you can see that adding more formats means we\u0026rsquo;ll need to extend the conditionals in the display function, and this gets out of hand pretty quickly in many real-life situations where you might have a non-trivial amount of cases.\nHowever, the biggest reason why the imperative solution isn\u0026rsquo;t ideal is because of how difficult it is to test. Imagine each of the conditionals triggers some expensive side effects when the corresponding block runs. How\u0026rsquo;d you test display then in an isolated manner without mocking the whole universe?\nStrategy pattern tells us that each conditional can be converted into a class with one method. We call these classes strategies. Then, we initialize these strategy classes at runtime and explicitly pass the instances to the display function. The function knows how to use the strategy instances and executes a specific strategy to print a message in a particular format based on a certain condition.\nHere\u0026rsquo;s how you could rewrite the previous example. In the first phase, we\u0026rsquo;ll wrap each formatter in a separate class:\n# main.rb require \u0026#39;json\u0026#39; # Formatter Interface class MessageFormatter def output(message) raise NotImplementedError, \u0026#34;This method should be overridden\u0026#34; end end # Concrete Formatter for Text class TextFormatter \u0026lt; MessageFormatter def output(message) message end end # Concrete Formatter for JSON class JsonFormatter \u0026lt; MessageFormatter def output(message) { message: message }.to_json end end Here, the TextFormatter and JsonFormatter classes implement the MessageFormatter interface. This interface requires the downstream classes to implement the output method. The output methods of the respective formatters know how to format and print the messages.\nThe display function simply takes a message and a formatter, and calls formatter.output(message) without knowing anything about what the formatter does.\n# main.rb # Display Function with direct unknown format handling def display(message, formatter) unless formatter.is_a?(MessageFormatter) puts \u0026#34;Unsupported format\u0026#34; return end output = formatter.output(message) puts output end Finally, at runtime, you can instantiate the strategy classes and pass them explicitly to the display function as necessary:\n# main.rb require_relative \u0026#39;formatter\u0026#39; text_formatter = TextFormatter.new json_formatter = JsonFormatter.new display(\u0026#34;Hello, World!\u0026#34;, text_formatter) # Prints \u0026#34;Hello, World!\u0026#34; display(\u0026#34;Hello, World!\u0026#34;, json_formatter) # {\u0026#34;message\u0026#34;:\u0026#34;Hello, World!\u0026#34;} Now whenever you need to test the display function, you can just create a fake formatter and pass that as an argument. The display function will happily accept any formatter as long as the strategy class satisfies the MessageFormatter interface.\nThe same thing can be achieved in a more functional manner and we\u0026rsquo;ll see that in the Go example.\nBut Ruby is still primarily an OO language and it has classes. How\u0026rsquo;d you model the same solution in a language like Go where there\u0026rsquo;s no concept of a class or explicit interface implementation? This wasn\u0026rsquo;t clear to me from the get-go until I started playing with the language a little more and digging through some OSS codebases.\nTurns out, in Go, you can do the same thing using interfaces and custom types, and with even fewer lines of code. Here\u0026rsquo;s how:\n// main.go // Formatter interface defines a method for outputting messages type Formatter interface { Output(message string) string } // OutputFunc is a function type that matches the signature of the Output // method in the Formatter interface type OutputFunc func(message string) string // Output method makes OutputFunc satisfy the Formatter interface func (f OutputFunc) Output(message string) string { return f(message) } Above, we\u0026rsquo;re defining a Formatter interface that contains only a single method Output. Then we define an OutputFunc type that implements the Output method on the function to satisfy the Formatter interface. We could opt in for a struct type here instead of defining a function type but since we don\u0026rsquo;t need to hold any state, a function type keeps things concise.\nThe display function will look as follows:\nfunc Display(message string, format Formatter) { fmt.Println(format.Output(message)) } Similar to the Ruby example, Display intakes a string message and an object of any type that implements the Formatter interface. Next, it calls the Output method on format without having any knowledge of what that does, achieving polymorphism.\nAlso, notice that we aren\u0026rsquo;t handling the \u0026ldquo;unknown formatter\u0026rdquo; case explicitly because now it\u0026rsquo;ll be a compile-time error if an unknown formatter is passed to the caller.\nNext, you\u0026rsquo;ll define your strategies and pass them to the Display function as follows:\nfunc main() { message := \u0026#34;Hello, World!\u0026#34; // Each strategy needs to be wrapped in OutputFunc so that the // underlying function satisfies the Formatter interface. TextFormatted := OutputFunc(func (message string) string { return message }) JSONFormatted := OutputFunc(func (message string) string { jsonData, _ := json.Marshal(map[string]string{\u0026#34;message\u0026#34;: message}) return string(jsonData) }) Display(message, TextFormatted) // Prints \u0026#34;Hello, World!\u0026#34; Display(message, JSONFormatted) // Prints \u0026#34;{\u0026#34;message\u0026#34;:\u0026#34;Hello, World!\u0026#34;}\u0026#34; } We\u0026rsquo;re defining each formatting strategy as a function and casting it to the OutputFunc so that it satisfies the Formatter interface. Then we just pass the message and the strategy instance to the Display function as before. Notice how your data and strategies are also decoupled in this case; one has no knowledge of the existence of the other.\nAnd voila, you\u0026rsquo;re done!\nUpdate: The original Go example used struct types rather than a function type to meet the Formatter interface requirements. In this particular case, the function type makes things simpler. However, if your strategy needs to do multiple things, then a struct with multiple methods is probably going to be better.\n","permalink":"https://rednafi.com/go/strategy-pattern/","summary":"\u003cp\u003eThese days, I don\u0026rsquo;t build hierarchical types through inheritance even when writing languages\nthat support it. Type composition has replaced almost all of my use cases where I would\u0026rsquo;ve\nreached for inheritance before.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve written about how to \u003ca href=\"/python/escape-template-pattern/\"\u003eescape the template pattern hellscape\u003c/a\u003e and replace that with\n\u003cem\u003estrategy pattern\u003c/em\u003e in Python before. While by default, Go saves you from shooting yourself\nin the foot by disallowing inheritance, it wasn\u0026rsquo;t obvious to me how I could apply the\nstrategy pattern to make things more composable and testable.\u003c/p\u003e","title":"Strategy pattern in Go"},{"content":"While I like Go\u0026rsquo;s approach of treating errors as values as much as the next person, it inevitably leads to a situation where there isn\u0026rsquo;t a one-size-fits-all strategy for error handling like in Python or JavaScript.\nThe usual way of dealing with errors entails returning error values from the bottom of the call chain and then handling them at the top. But it\u0026rsquo;s not universal since there are cases where you might want to handle errors as early as possible and fail catastrophically. Yet, it\u0026rsquo;s common enough that we can use it as the base of our conversation.\nThis simple but verbose error handling works okay and makes us painfully aware of all the possible error paths. Yet, the model doesn\u0026rsquo;t hold up as your program grows in scope and complexity, forcing you to devise custom patterns to add context and build thin stack traces. There\u0026rsquo;s no avoiding that.\nBut the good thing is that building an emaciated stack trace is fairly straightforward, and some of the patterns are quite portable. After reading Rob Pike\u0026rsquo;s Upspin error handling blog, I had some ideas on creating custom errors to emulate stack traces. I ended up spending a few hours this morning experimenting with some of the ideas in a more limited scope.\nLet\u0026rsquo;s say we\u0026rsquo;re building a file-copy service that will accept a src and dst path and copy the contents from source to destination.\nfunc copyFile(src, dst string) error { // Open the source file for reading srcFile, err := os.Open(src) if err != nil { return err } defer srcFile.Close() // Create the destination file for writing dstFile, err := os.Create(dst) if err != nil { return err } defer dstFile.Close() // Copy the contents from source to destination file _, err = io.Copy(dstFile, srcFile) if err != nil { return err } // Ensure that the destination file\u0026#39;s content is successfully written err = dstFile.Sync() if err != nil { return err } return nil } This typical error handling pattern involves returning error values from lower-level functions and addressing them in top-level ones. Here, the main function manages the error:\nfunc main() { // Define the source and destination file paths src := \u0026#34;path/to/source/file\u0026#34; dst := \u0026#34;path/to/destination/file\u0026#34; // Call copyFile and handle any errors err := copyFile(src, dst) if err != nil { fmt.Fprintf(os.Stderr, \u0026#34;Error copying file: %s\\n\u0026#34;, err) os.Exit(1) } fmt.Println(\u0026#34;File copied successfully.\u0026#34;) } Running this function gives us the following output:\nError copying file: open path/to/source/file: no such file or directory exit status 1 This is usually enough if you\u0026rsquo;re building a CLI or a small program. Also, squinting at the error message gives us a hint that among the 4 error-return paths, the copyFile function bailed at the first one when it couldn\u0026rsquo;t find the source file.\nA proper way to handle this in larger applications is to wrap the errors and provide them with your own context. Then, in the top-level function, you can unwrap the error message or just log it verbatim as before. So, copyFile can be rewritten as follows:\nfunc copyFile(src, dst string) error { srcFile, err := os.Open(src) if err != nil { return fmt.Errorf(\u0026#34;cannot open source file: %w\u0026#34;, err) } defer srcFile.Close() dstFile, err := os.Create(dst) if err != nil { return fmt.Errorf(\u0026#34;cannot create destination file: %w\u0026#34;, err) } defer dstFile.Close() _, err = io.Copy(dstFile, srcFile) if err != nil { return fmt.Errorf(\u0026#34;cannot copy file contents: %w\u0026#34;, err) } err = dstFile.Sync() if err != nil { return fmt.Errorf(\u0026#34;cannot sync destination file: %w\u0026#34;, err) } return nil } Notice how we\u0026rsquo;re adding extra context to the error values with the %w verb in the fmt.Errorf function.\nIf you keep the previous main function unchanged and run it, you\u0026rsquo;ll get the following output:\nError copying file: cannot open source file: open path/to/source/file: no such file or directory exit status 1 This time, since you know where you added the context, you also know which error-path the copyFile function returned from. However, even in this case, the main function just relays whatever comes out of copyFile and logs the error message.\nHow would you make the error message prettier without losing context? Also, how would you attach file names and line numbers to make debugging easier?\nThe debugging part isn\u0026rsquo;t an issue in languages that support stack traces, this is usually taken care of automatically. Now, whether that\u0026rsquo;s a good thing or a bad thing is a discussion for another day.\nWe can define a custom error struct to represent a generic error in the package that houses copyFile.\ntype Error struct { Op string Path string LineNo int FileName string Err error Debug bool } func (e *Error) Error() string { if e.Debug { return fmt.Sprintf( \u0026#34;%s: %s: %s\\n\\t%s:%d\u0026#34;, e.Op, e.Path, e.Err, e.FileName, e.LineNo, ) } msg := e.Err.Error() msgs := strings.Split(msg, \u0026#34;:\u0026#34;) msg = strings.TrimSpace(msgs[len(msgs)-1]) return fmt.Sprintf(\u0026#34;%s: %s: \\n\\t%s\u0026#34;, e.Op, e.Path, msg) } Inside the Error struct, Op represents the name of the function that the error originates from, Path is the file path, LineNo and FileName denote the precise location of the error, Err is the original error we\u0026rsquo;re wrapping, and finally the debug boolean is be used to control the verbosity of error messages.\nThen the Error() method on the struct builds either a rudimentary stack trace or a prettier error message depending on the value of the Debug flag. The Error struct can be constructed with the following constructor function:\nvar Debug bool // Flag to control output verbosity func NewError(op string, path string, err error) *Error { _, file, line, ok := runtime.Caller(1) if !ok { file = \u0026#34;???\u0026#34; line = 0 } return \u0026amp;Error{ Op: op, Path: path, LineNo: line, FileName: file, Err: err, Debug: Debug, // Populate from the global flag } } This uses the runtime package to add the location data of the caller. It\u0026rsquo;ll be called in the copyFile function as follows:\nfunc copyFile(src, dst string) error { // Open the source file for reading srcFile, err := os.Open(src) if err != nil { return NewError(\u0026#34;os.Open\u0026#34;, src, err) } defer srcFile.Close() // Create the destination file for writing dstFile, err := os.Create(dst) if err != nil { return NewError(\u0026#34;os.Create\u0026#34;, dst, err) } defer dstFile.Close() // Copy the contents from source to destination file _, err = io.Copy(dstFile, srcFile) if err != nil { return NewError(\u0026#34;io.Copy\u0026#34;, dst, err) } // Ensure that the destination file\u0026#39;s content is successfully written err = dstFile.Sync() if err != nil { return NewError(\u0026#34;dstFile.Sync\u0026#34;, dst, err) } return nil } You can turn on the Debug flag to print the stack trace in the main function:\nfunc main() { src := \u0026#34;/path/to/source/file\u0026#34; dst := \u0026#34;/path/to/destination/file\u0026#34; Debug = true // Set the Debug flag err := copyFile(src, dst) if err != nil { fmt.Fprintf(os.Stderr, \u0026#34;%v\\n\u0026#34;, err) os.Exit(1) } fmt.Println(\u0026#34;File copied successfully.\u0026#34;) } The output will be:\nos.Open: /path/to/source/file: open /path/to/source/file: no such file or directory /Users/.../main.go:54 exit status 1 Toggling Debug to false and running the snippet will return:\nos.Open: /path/to/source/file: no such file or directory exit status 1 You can add even more context to this error in different calling locations like this:\nsrcFile, err := os.Open(src) if err != nil { return fmt.Errorf( \u0026#34;more context: %w\u0026#34;, NewError(\u0026#34;os.Open\u0026#34;, src, err), ) } It\u0026rsquo;ll be pretty-printed like this when Debug is false:\nmore ctx: os.Open: /path/to/source/file: no such file or directory exit status 1 Now depending on your needs, you can customize the Error struct and NewError constructor to enable more elaborate error tracing.\nHowever, this isn\u0026rsquo;t a proper stack in the sense that it only unwinds errors one level deep. But it can be extended to recursively build the full error trace if needed. The Upspin error package demonstrates a few techniques on how to do so. But for this particular case, anything more than a level deep stack is borderline overkill.\nHere\u0026rsquo;s the complete example on GitHub.\nFin!\n","permalink":"https://rednafi.com/go/anemic-stack-traces/","summary":"\u003cp\u003eWhile I like Go\u0026rsquo;s approach of treating errors as values as much as the next person, it\ninevitably leads to a situation where there isn\u0026rsquo;t a one-size-fits-all strategy for error\nhandling like in Python or JavaScript.\u003c/p\u003e\n\u003cp\u003eThe usual way of dealing with errors entails returning error values from the bottom of the\ncall chain and then handling them at the top. But it\u0026rsquo;s not universal since there are cases\nwhere you might want to handle errors as early as possible and fail catastrophically. Yet,\nit\u0026rsquo;s common enough that we can use it as the base of our conversation.\u003c/p\u003e","title":"Anemic stack traces in Go"},{"content":"I used to reach for reflection whenever I needed a Retry function in Go. It\u0026rsquo;s fun to write, but gets messy quite quickly.\nHere\u0026rsquo;s a rudimentary Retry function that does the following:\nIt takes in another function that accepts arbitrary arguments. Then tries to execute the wrapped function. If the wrapped function returns an error after execution, Retry attempts to run the underlying function n times with some backoff. The following implementation leverages the reflect module to achieve the above goals. We\u0026rsquo;re intentionally avoiding complex retry logic for brevity:\nfunc Retry( fn any, args []any, maxRetry int, startBackoff, maxBackoff time.Duration) ([]reflect.Value, error) { fnVal := reflect.ValueOf(fn) if fnVal.Kind() != reflect.Func { return nil, errors.New(\u0026#34;retry: function type required\u0026#34;) } argVals := make([]reflect.Value, len(args)) for i, arg := range args { argVals[i] = reflect.ValueOf(arg) } for attempt := 0; attempt \u0026lt; maxRetry; attempt++ { result := fnVal.Call(argVals) errVal := result[len(result)-1] if errVal.IsNil() { return result, nil } if attempt == maxRetry-1 { return result, errVal.Interface().(error) } time.Sleep(startBackoff) if startBackoff \u0026lt; maxBackoff { startBackoff *= 2 } fmt.Printf( \u0026#34;Retrying function call, attempt: %d, error: %v\\n\u0026#34;, attempt+1, errVal, ) } return nil, fmt.Errorf(\u0026#34;retry: max retries reached without success\u0026#34;) } The Retry function uses reflection to call a function passed as any. It takes the function\u0026rsquo;s arguments as a []any slice, allowing us to run functions with varied signatures. Using reflect.ValueOf(fn).Call(argVals), it dynamically invokes the target function after converting arguments from any to reflect.Value.\nThe retry logic runs up to maxRetry times with exponential backoff. The delay starts at startBackoff, doubles after each failure, and caps at maxBackoff. If the last return value is an error and retries remain, it waits and tries again. Otherwise, it gives up.\nYou can wrap a dummy function that always returns an error to see how Retry works:\nfunc main() { someFunc := func(a, b int) (int, error) { fmt.Printf(\u0026#34;Function called with a: %d and b: %d\\n\u0026#34;, a, b) return 42, errors.New(\u0026#34;some error\u0026#34;) } result, err := Retry( someFunc, []any{42, 100}, 3, 1*time.Second, 4*time.Second, ) if err != nil { fmt.Println(\u0026#34;Function execution failed:\u0026#34;, err) return } fmt.Println(\u0026#34;Function executed successfully:\u0026#34;, result[0]) } Running it will give you the following output:\nFunction called with a: 42 and b: 100 Retrying function call, attempt: 1, error: some error Function called with a: 42 and b: 100 Retrying function call, attempt: 2, error: some error Function called with a: 42 and b: 100 Function execution failed: some error This isn\u0026rsquo;t too terrible for reflection-heavy code. But now that Go has generics, I wanted to see if I could avoid the metaprogramming. While reflection is powerful, it\u0026rsquo;s prone to runtime panics and the compiler can\u0026rsquo;t type-check dynamic code.\nTurns out, there\u0026rsquo;s a way to write the same functionality with generics if you don\u0026rsquo;t mind trading off some flexibility for shorter and more type-safe code. Here\u0026rsquo;s how:\n// Define a generic function type that can return an error type Func[T any] func() (T, error) func Retry[T any]( fn Func[T], maxRetry int, startBackoff, maxBackoff time.Duration) (T, error) { var zero T // Zero value for the function\u0026#39;s return type for attempt := 0; attempt \u0026lt; maxRetry; attempt++ { result, err := fn() if err == nil { return result, nil } if attempt == maxRetry-1 { return zero, err // Return with error after max retries } fmt.Printf( \u0026#34;Retrying function call, attempt: %d, error: %v\\n\u0026#34;, attempt+1, err, ) time.Sleep(startBackoff) if startBackoff \u0026lt; maxBackoff { startBackoff *= 2 } } return zero, fmt.Errorf(\u0026#34;retry: max retries reached without success\u0026#34;) } Functionally, the generic implementation works the same way as the previous one. However, it has a few limitations:\nThe generic Retry assumes the wrapped function returns (result, error). This fits Go\u0026rsquo;s common idiom, but the reflection version could handle varied return patterns.\nThe reflection-based Retry wraps any function via the empty interface. The generic version requires a matching signature, so you need a thin wrapper to adapt it.\nHere\u0026rsquo;s how you\u0026rsquo;d use the generic Retry function:\nfunc main() { a, b := 42, 100 someFunc := func() (int, error) { fmt.Printf(\u0026#34;Function called with a: %d and b: %d\\n\u0026#34;, a, b) return 42, errors.New(\u0026#34;some error\u0026#34;) } result, err := Retry( someFunc, 3, 1*time.Second, 4*time.Second, ) if err != nil { fmt.Println(\u0026#34;Function execution failed:\u0026#34;, err) } else { fmt.Println(\u0026#34;Function executed successfully:\u0026#34;, result) } } Running it will give you the same output as before.\nNotice how someFunc uses a closure to capture a and b rather than accepting them as arguments. This adaptation is necessary for type safety. I don\u0026rsquo;t mind it if it means avoiding reflection—plus the generic version is slightly faster.\nAfter this entry went live, Anton Zhiyanov pointed out on Twitter that there\u0026rsquo;s a closure-based approach that\u0026rsquo;s even simpler and eliminates the need for generics. The implementation looks like this:\nfunc Retry( fn func() error, maxRetry int, startBackoff, maxBackoff time.Duration) { for attempt := 0; ; attempt++ { if err := fn(); err == nil { return } if attempt == maxRetry-1 { return } fmt.Printf(\u0026#34;Retrying after %s\\n\u0026#34;, startBackoff) time.Sleep(startBackoff) if startBackoff \u0026lt; maxBackoff { startBackoff *= 2 } } } Now calling Retry is easier since the closure signature is static—you don\u0026rsquo;t need to adapt the call when the wrapped function\u0026rsquo;s signature changes:\nfunc main() { someFunc := func(a, b int) (int, error) { fmt.Printf(\u0026#34;Function called with a: %d and b: %d\\n\u0026#34;, a, b) return 42, errors.New(\u0026#34;some error\u0026#34;) } var res int var err error Retry( func() error { res, err = someFunc(42, 100) return err }, 3, 1*time.Second, 4*time.Second, ) fmt.Println(res, err) } The runtime behavior of this version is the same as the ones before.\nFin!\n","permalink":"https://rednafi.com/go/retry-function/","summary":"\u003cp\u003eI used to reach for reflection whenever I needed a \u003ccode\u003eRetry\u003c/code\u003e function in Go. It\u0026rsquo;s fun to\nwrite, but gets messy quite quickly.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s a rudimentary \u003ccode\u003eRetry\u003c/code\u003e function that does the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt takes in another function that accepts arbitrary arguments.\u003c/li\u003e\n\u003cli\u003eThen tries to execute the wrapped function.\u003c/li\u003e\n\u003cli\u003eIf the wrapped function returns an error after execution, \u003ccode\u003eRetry\u003c/code\u003e attempts to run the\nunderlying function \u003ccode\u003en\u003c/code\u003e times with some backoff.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe following implementation leverages the \u003ccode\u003ereflect\u003c/code\u003e module to achieve the above goals.\nWe\u0026rsquo;re intentionally avoiding complex retry logic for brevity:\u003c/p\u003e","title":"Retry function in Go"},{"content":"Despite moonlighting as a gopher for a while, the syntax for type assertion and type switches still trips me up every time I need to go for one of them.\nSo, to avoid digging through the docs or crafting stodgy LLM prompts multiple times, I decided to jot this down in a Go by Example style for the next run.\nType assertion Type assertion in Go allows you to access an interface variable\u0026rsquo;s underlying concrete type. After a successful assertion, the variable of interface type is converted to the concrete type to which it\u0026rsquo;s asserted.\nThe syntax is i.(T), where i is a variable of interface type and T is the type you are asserting.\nBasic usage var i interface{} = \u0026#34;Hello\u0026#34; // or use `any` as an alias for `interface{}` s := i.(string) fmt.Println(s) Here, s gets the type string, and the program outputs Hello.\nAsserting primitive types var i interface{} = 42 if v, ok := i.(int); ok { fmt.Println(\u0026#34;integer:\u0026#34;, v) } This code checks if i is an int and prints its value if so. The value of ok will be false if i isn\u0026rsquo;t an integer and nothing will be printed to the console.\nAsserting composite types var i interface{} = []string{\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;} if v, ok := i.([]string); ok { fmt.Println(\u0026#34;slice of strings:\u0026#34;, v) } This will print slice of strings: [apple banana cherry] to the console.\nSimilar to primitive types, you can also perform type assertions with composite types. In the example above, we check whether the variable i, which is of an interface type, holds a value of the type \u0026lsquo;slice of strings\u0026rsquo;.\nAsserting other interface types type fooer interface{ foo() } type barer interface{ bar() } type foobarer interface { fooer; barer } type thing struct{} func (t *thing) foo() {} func (t *thing) bar() {} var i foobarer = \u0026amp;thing{} func main() { if v, ok := i.(fooer); ok { fmt.Println(\u0026#34;i satisfies fooer:\u0026#34;, v) } } Type assertion can also be used to convert the type of an interface variable to another interface type. Here struct i implements both foo() and bar() methods; satisfying the foobarer interface.\nThen in the main function, we check whether i satisfies fooer interface and print a message if it does. Running this snippet will print i satisfies fooer: \u0026amp;{}.\nDynamically checking for certain methods type fooer interface{ foo() } type thing struct{} func (t *thing) foo() {} func (t *thing) bar() {} func main() { var i fooer = \u0026amp;thing{} if v, ok := i.(interface{ bar() }); ok { fmt.Println(\u0026#34;thing implements bar method:\u0026#34;, v) } } Type assertion can be used to dynamically check if an interface variable implements a certain method. This can come in handy when you want to know if an interface variable has a certain method right before invoking it.\nHere, the thing struct implements both foo and bar but the fooer interface only needs the foo() method to be implemented. However, we can check dynamically whether i also implements the bar() method using anonymous interface definition. Running this prints thing implements bar method: \u0026amp;{}\nHandling failures var i interface{} = \u0026#34;Hello\u0026#34; f := i.(float64) // This triggers a panic Wrong assertions, like attempting to convert a string to a float64, cause runtime panics.\nType switches Type switches let you compare an interface variable\u0026rsquo;s type against several types. It\u0026rsquo;s similar to a regular switch statement, but focuses on types.\nBasic usage var i interface{} = 7 switch i.(type) { case int: fmt.Println(\u0026#34;i is an int\u0026#34;) case string: fmt.Println(\u0026#34;i is a string\u0026#34;) default: fmt.Println(\u0026#34;unknown type\u0026#34;) } This outputs i is an int.\nUsing a variable in a type switch case var i interface{} = []byte(\u0026#34;hello\u0026#34;) switch v := i.(type) { case []byte: fmt.Println(string(v)) case string: fmt.Println(v) } Notice how we\u0026rsquo;re assinging variable v to i.(type) and then reusing the extracted value in the case statements. The snippet converts []byte to a string and prints hello.\nHandling multiple types var i interface{} = 2.5 switch i.(type) { case int, float64: fmt.Println(\u0026#34;i is a number\u0026#34;) case string: fmt.Println(\u0026#34;i is a string\u0026#34;) } The case T1, T2 syntax works like an OR relationship, outputting i is a number.\nAddressing composite types var i interface{} = map[string]bool{\u0026#34;hello\u0026#34;: true, \u0026#34;world\u0026#34;: false} switch i.(type) { case map[string]bool: fmt.Println(\u0026#34;i is a map\u0026#34;) case []string: fmt.Println(\u0026#34;i is a slice\u0026#34;) default: fmt.Println(\u0026#34;unknown type\u0026#34;) } Similar to primitive types, you can check for composite types in the case statement of a type switch. Here, we\u0026rsquo;re checking whether i is a map[string]bool or not. Running this will output i is a map.\nComparing against other interface types type fooer interface{ foo() } type barer interface{ bar() } type foobarer interface { fooer; barer } type thing struct{} func (t *thing) foo() {} func (t *thing) bar() {} var i foobarer = \u0026amp;thing{} func main() { switch v := i.(type) { case fooer: fmt.Println(\u0026#34;fooer:\u0026#34;, v) case barer: fmt.Println(\u0026#34;barer:\u0026#34;, v) case foobarer: fmt.Println(\u0026#34;foobarer:\u0026#34;, v) default: panic(\u0026#34;none of them\u0026#34;) } } Type switches can be also used to compare an interface variable with another interface type. This example is similar to the type assertion one where we\u0026rsquo;re checking whether i satisfies fooer, barer or foobarer interface. In this case, i satisfies all three of them but the case statement will stop after the first successful check. So it prints fooer: \u0026amp;{} and bails.\nDynamically checking for certain methods type fooer interface{ foo() } type thing struct{} func (t *thing) foo() {} func (t *thing) bar() {} func main() { var i fooer = \u0026amp;thing{} switch v := i.(type) { case interface{ bar() }: fmt.Println(\u0026#34;thing implements bar method:\u0026#34;, v) default: panic(\u0026#34;thing doesn\u0026#39;t implement bar method\u0026#34;) } } Similar to type assertion, within a type switch, anonymous interface definition can be used to dynamically check if an interface variable implements some method.\nThe thing struct implements both foo() and bar() methods. However, the fooer interface only requires it to implement foo(). The type switch dynamically checks whether i also implements the method bar(). Running this will print thing implements bar method: \u0026amp;{}.\nSimilarities and differences Similarities Both handle interfaces and extract their concrete types. They evaluate an interface\u0026rsquo;s dynamic type. Differences Type assertions check a single type, while type switches handle multiple types. Type assertion uses i.(T), type switch uses a switch statement with literal i.(type). Type assertions can panic or return a success boolean, type switches handle mismatches more gracefully. Type assertions are good when you\u0026rsquo;re sure of the type. Type switches are more versatile for handling various types. Type assertion can get the value and success boolean. Type switches let you access the value in each case block. Type switches can handle multiple types, including a default case, offering more flexibility for various types. Fin!\n","permalink":"https://rednafi.com/go/type-assertion-vs-type-switches/","summary":"\u003cp\u003eDespite moonlighting as a gopher for a while, the syntax for type assertion and type\nswitches still trips me up every time I need to go for one of them.\u003c/p\u003e\n\u003cp\u003eSo, to avoid digging through the docs or crafting stodgy LLM prompts multiple times, I\ndecided to jot this down in a \u003ca href=\"https://gobyexample.com/\"\u003eGo by Example\u003c/a\u003e style for the next run.\u003c/p\u003e\n\u003ch2 id=\"type-assertion\"\u003eType assertion\u003c/h2\u003e\n\u003cp\u003eType assertion in Go allows you to access an interface variable\u0026rsquo;s underlying concrete type.\nAfter a successful assertion, the variable of interface type is converted to the concrete\ntype to which it\u0026rsquo;s asserted.\u003c/p\u003e","title":"Type assertion vs type switches in Go"},{"content":"I\u0026rsquo;ve been a happy user of pydantic settings to manage all my app configurations since the 1.0 era. When pydantic 2.0 was released, the settings portion became a separate package called pydantic_settings.\nIt does two things that I love: it automatically reads the environment variables from the .env file and allows you to declaratively convert the string values to their desired types like integers, booleans, etc.\nPlus, it lets you override the variables defined in .env by exporting them in your shell.\nSo if you have a variable called FOO in your .env file like this:\nFOO=\u0026#34;some_value\u0026#34; Then you can override it via:\nexport FOO=\u0026#34;other_value\u0026#34; And pydantic settings will automatically pick up the overridden values without much fuss.\nThis is neat but can make writing deterministic unit tests tricky. If the settings instance implicitly pulls config values from both the environment file and shell, testing functions using those values can easily become flaky. Also, it\u0026rsquo;s usually frowned upon if your unit tests depend on environment variables in general.\nConsider this common instantiation workflow of the settings class. Here, we have the following app structure:\n. ├── src │ ├── __init__.py │ ├── config.py │ └── main.py ├── tests │ ├── __init__.py │ └── test_main.py └── .env In the src/config.py file, we define our settings class as follows:\nfrom pydantic_settings import BaseSettings, SettingsConfigDict class Settings(BaseSettings): # Override defaults with .env file values. model_config = SettingsConfigDict(env_file=\u0026#34;.env\u0026#34;) env_var_1: str = \u0026#34;default_value\u0026#34; env_var_2: int = 123 env_var_3: bool = False Then the corresponding values of the environment variables are defined in the .env file. Pydantic will automatically convert the upper-cased definitions to lower-case.\nENV_VAR_1=\u0026#34;overridden_value\u0026#34; ENV_VAR_2=\u0026#34;42\u0026#34; ENV_VAR_3=\u0026#34;true\u0026#34; Next, we instantiate the Settings class in the src/__init__.py file:\nfrom src.config import Settings settings = Settings() Finally, we use the config values in src/main.py:\nfrom src import settings def read_env() -\u0026gt; tuple[str, int, bool]: return settings.env_var_1, settings.env_var_2, settings.env_var_3 if __name__ == \u0026#34;__main__\u0026#34;: env_var_1, env_var_2, env_var_3 = read_env() print(f\u0026#34;{env_var_1=}\u0026#34;) print(f\u0026#34;{env_var_2=}\u0026#34;) print(f\u0026#34;{env_var_3=}\u0026#34;) From the root directory, run the main.py file with this command:\npython -m src.main This reveals that pydantic settings is doing its magic\u0026ndash;reading the .env file and overriding the default config values:\nenv_var_1=\u0026#39;overridden_value\u0026#39; env_var_2=42 env_var_3=True Fantastic! But now, testing the read_env function becomes tricky. Normally, you\u0026rsquo;d try to patch the environment variables in a pytest fixture and then test the values like this:\n# tests/test_main.py import os from collections.abc import Iterator from unittest.mock import patch import pytest from src.main import read_env @pytest.fixture def patch_env_vars() -\u0026gt; Iterator[None]: with patch.dict( os.environ, { \u0026#34;ENV_VAR_1\u0026#34;: \u0026#34;test_env_var_1\u0026#34;, \u0026#34;ENV_VAR_2\u0026#34;: \u0026#34;456\u0026#34;, \u0026#34;ENV_VAR_3\u0026#34;: \u0026#34;True\u0026#34;, }, ): yield def test_read_env(patch_env_vars: None) -\u0026gt; None: env_var_1, env_var_2, env_var_3 = read_env() assert env_var_1 == \u0026#34;test_env_var_1\u0026#34; assert env_var_2 == 456 assert env_var_3 is True But the test will fail because we\u0026rsquo;re initializing the Settings class in the src/__init__.py file and pydantic processes the environment file and variables before pytest can intervene.\nWe want our unit tests to have no dependencies on the environment variables.\nYou might say initializing a class in the __init__.py file like that is an anti-pattern and all this can be avoided through dependency injection. You\u0026rsquo;d be right but you\u0026rsquo;d also be surprised at how many apps with 7+ figure ARR initialize their config classes like that.\nSo patching the environment variables doesn\u0026rsquo;t work, what does?\nThe idea is to let pydantic do its magic and then reset the attributes of the Settings instance to their default values in a fixture. We also want the user of the fixture to be able to override the values of some or all of the environment variables if necessary.\nHere\u0026rsquo;s what has worked well for me:\nimport pytest from src.main import read_env from src import settings, Settings import pytest from collections.abc import Iterator from pytest import FixtureRequest @pytest.fixture def patch_settings(request: FixtureRequest) -\u0026gt; Iterator[Settings]: # Make a copy of the original settings original_settings = settings.model_copy() # Collect the env vars to patch env_vars_to_patch = getattr(request, \u0026#34;param\u0026#34;, {}) # Patch the settings to use the default values for k, v in settings.model_fields.items(): setattr(settings, k, v.default) # Patch the settings with the parametrized env vars for key, val in env_vars_to_patch.items(): # Raise an error if the env var is not defined in the settings if not hasattr(settings, key): raise ValueError(f\u0026#34;Unknown setting: {key}\u0026#34;) # Raise an error if the env var has an invalid type expected_type = getattr(settings, key).__class__ if not isinstance(val, expected_type): raise ValueError( f\u0026#34;Invalid type for {key}: {val.__class__} instead \u0026#34; \u0026#34;of {expected_type}\u0026#34; ) setattr(settings, key, val) yield settings # Restore the original settings settings.__dict__.update(original_settings.__dict__) Here, patch_settings is a parametrizable fixture where you can optionally pass values via pytest.mark.parametrize to override certain config attributes. If you don\u0026rsquo;t override anything, the fixture sets the attributes of the Setting instance to their default values defined in the class.\nAbove, first we make a copy of the original settings instance. Then we reset the attributes of the Setting instance to their default values. Next, we move on to override any values passed via the @parametrize decorator. While doing this, we also check for the correct type of the incoming values and raise an error accordingly.\nFinally, we yield the patched instance and reset everything back to their original values after a test ends.\nYou can use the fixture like this:\ndef test_read_env(patch_settings: Settings) -\u0026gt; None: env_var_1, env_var_2, env_var_3 = read_env() assert env_var_1 == \u0026#34;default_value\u0026#34; assert env_var_2 == 123 assert env_var_3 is False @pytest.mark.parametrize( \u0026#34;patch_settings\u0026#34;, [ {\u0026#34;env_var_1\u0026#34;: \u0026#34;patched_value\u0026#34;, \u0026#34;env_var_2\u0026#34;: 456}, {\u0026#34;env_var_2\u0026#34;: 459}, ], indirect=True, ) def test_read_env_override(patch_settings: Settings) -\u0026gt; None: env_var_1, env_var_2, env_var_3 = read_env() assert env_var_1 == patch_settings.env_var_1 assert env_var_2 == patch_settings.env_var_2 assert env_var_3 is patch_settings.env_var_3 In the first case, we\u0026rsquo;re not overriding anything. So the tests will use the Settings instance with all the default values. In the second test, we\u0026rsquo;re overriding a few values and the read_env function will use the overridden values.\nEither way, the tests don\u0026rsquo;t directly depend on the environment variables and it reduces the probability of spooky actions at a distance.\nFin!\n","permalink":"https://rednafi.com/python/patch-pydantic-settings-in-pytest/","summary":"\u003cp\u003eI\u0026rsquo;ve been a happy user of \u003ca href=\"https://docs.pydantic.dev/latest/\"\u003epydantic\u003c/a\u003e settings to manage all my app configurations since the\n1.0 era. When pydantic 2.0 was released, the settings portion became a separate package\ncalled \u003ca href=\"https://docs.pydantic.dev/latest/concepts/pydantic_settings/\"\u003epydantic_settings\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIt does two things that I love: it automatically reads the environment variables from the\n\u003ccode\u003e.env\u003c/code\u003e file and allows you to declaratively convert the string values to their desired types\nlike integers, booleans, etc.\u003c/p\u003e\n\u003cp\u003ePlus, it lets you override the variables defined in \u003ccode\u003e.env\u003c/code\u003e by exporting them in your shell.\u003c/p\u003e","title":"Patching pydantic settings in pytest"},{"content":"As of now, unlike Python or NodeJS, Go doesn\u0026rsquo;t allow you to specify your development dependencies separately from those of the application. However, I like to specify the dev dependencies explicitly for better reproducibility.\nWhile working on link-patrol, a CLI for checking dead URLs in Markdown files, I came across this neat convention: you can specify dev dependencies in a tools.go file and then exclude them while building the binary using a build tag.\nHere\u0026rsquo;s how it works. Let\u0026rsquo;s say our project foo currently has the following structure:\nfoo ├── go.mod ├── go.sum └── main.go The main.go file contains a simple hello-world function that uses a 3rd party dependency just to make a point:\npackage main import ( \u0026#34;fmt\u0026#34; // Cowsay is a 3rd party app dependency cowsay \u0026#34;github.com/Code-Hex/Neo-cowsay\u0026#34; ) func main() { fmt.Println(cowsay.Say(cowsay.Phrase(\u0026#34;Hello, World!\u0026#34;))) } Here, Neo-cowsay is our app dependency. To initialize the project, we run the following commands serially:\ngo mod init example.com/foo # creates the go.mod and go.sum files go mod tidy # installs the app dependencies Now, let\u0026rsquo;s say we want to add the following dev dependencies: golangci-lint to lint the project in the CI and gofumpt as a stricter gofmt. Since we don\u0026rsquo;t import these tools directly anywhere, they aren\u0026rsquo;t tracked by the build toolchain.\nBut we can leverage the following workflow:\nPlace a tools.go file in the root directory. Import the dev dependencies in that file. Run go mod tidy to track both app and dev dependencies via go.mod and go.sum. Specify a build tag in tools.go to exclude the dev dependencies from the binary. In this case, tools.go looks as follows:\n// go:build tools package tools import ( // Dev dependencies _ \u0026#34;github.com/golangci/golangci-lint/cmd/golangci-lint\u0026#34; _ \u0026#34;mvdan.cc/gofumpt\u0026#34; ) Above, we\u0026rsquo;re importing the dev dependencies and assigning them to underscores since we won\u0026rsquo;t be using them directly. However, now if you run go mod tidy, Go toolchain will track the dependencies via the go.mod and go.sum files. You can inspect the dependencies in go.mod:\n// go.mod module example.com/foo go 1.21.6 require ( github.com/Code-Hex/Neo-cowsay v1.0.4 // app dependency github.com/golangci/golangci-lint v1.55.2 // dev dependency mvdan.cc/gofumpt v0.5.0 // dev dependency ) // ... transient dependencies Although we\u0026rsquo;re tracking the dev dependencies along with the app ones, the build tag // go:build tools at the beginning of tools.go file will instruct the build toolchain to ignore them while creating the binary.\nFrom the root directory of foo, you can build the project by running:\ngo build main.go This will create a binary called main in the root directory. To ensure that the binary doesn\u0026rsquo;t contain the dev dependencies, run:\ngo tool nm main | grep -Ei \u0026#39;golangci-lint|gofumpt\u0026#39; This won\u0026rsquo;t return anything if the dev dependencies aren\u0026rsquo;t packed into the binary.\nBut if you do that for the app dependency, it\u0026rsquo;ll print the artifacts:\ngo tool nm main | grep -Ei \u0026#39;cowsay\u0026#39; This prints:\n1000b6d40 T github.com/Code-Hex/Neo-cowsay.(*Cow).Aurora 1000b6fb0 T github.com/Code-Hex/Neo-cowsay.(*Cow).Aurora.func1 1000b5610 T github.com/Code-Hex/Neo-cowsay.(*Cow).Balloon 1000b6020 T github.com/Code-Hex/Neo-cowsay.(*Cow).Balloon.func1 ... For some weird reason, if you want to include the dev dependencies in your binary, you can pass the tools tag while building the binary:\ngo build --tags tools main.go However, this will most likely fail if any of your dev dependencies aren\u0026rsquo;t importable.\nHere\u0026rsquo;s an example of Kubernetes\u0026rsquo;s tools.go pattern in the wild.\nWhile it works, I\u0026rsquo;d still prefer to have a proper solution instead of a hack. Fin!\n","permalink":"https://rednafi.com/go/omit-dev-dependencies-in-binaries/","summary":"\u003cp\u003eAs of now, unlike Python or NodeJS, Go doesn\u0026rsquo;t allow you to specify your development\ndependencies separately from those of the application. However, I like to specify the dev\ndependencies explicitly for better reproducibility.\u003c/p\u003e\n\u003cp\u003eWhile working on \u003ca href=\"https://github.com/rednafi/link-patrol\"\u003elink-patrol, a CLI for checking dead URLs\u003c/a\u003e in Markdown files, I came\nacross this neat convention: you can specify dev dependencies in a \u003ccode\u003etools.go\u003c/code\u003e file and then\nexclude them while building the binary using a build tag.\u003c/p\u003e","title":"Omitting dev dependencies in Go binaries"},{"content":"I love dynamically typed languages as much as the next person. They let us make ergonomic API calls like this:\nimport httpx # Sync call for simplicity r = httpx.get(\u0026#34;https://dummyjson.com/products/1\u0026#34;).json() print(r[\u0026#34;id\u0026#34;], r[\u0026#34;title\u0026#34;], r[\u0026#34;description\u0026#34;]) or this:\nfetch(\u0026#34;https://dummyjson.com/products/1\u0026#34;) .then((res) =\u0026gt; res.json()) .then((json) =\u0026gt; console.log(json.id, json.type, json.description)); In both cases, running the snippets will return:\n1 \u0026#39;iPhone 9\u0026#39; \u0026#39;An apple mobile which is nothing like apple\u0026#39; Unless you\u0026rsquo;ve worked with a statically typed language that enforces more constraints, it\u0026rsquo;s hard to appreciate how incredibly convenient it is to be able to call and use an API endpoint without having to deal with types or knowing anything about its payload structure. You can treat the API response as a black box and deal with everything in runtime.\nFor example, Go wouldn\u0026rsquo;t even allow you to do so in such a loosey-goosey way. To consume the API, you\u0026rsquo;d need to create a struct in the essence of the return payload and then unmarshal the payload into it.\nHere\u0026rsquo;s the complete response payload that curl -s https://dummyjson.com/products/1 | jq returns:\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;iPhone 9\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;An apple mobile which is nothing like apple\u0026#34;, \u0026#34;price\u0026#34;: 549, \u0026#34;discountPercentage\u0026#34;: 12.96, \u0026#34;rating\u0026#34;: 4.69, \u0026#34;stock\u0026#34;: 94, \u0026#34;brand\u0026#34;: \u0026#34;Apple\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;smartphones\u0026#34;, \u0026#34;thumbnail\u0026#34;: \u0026#34;https://cdn.dummyjson.com/product-images/1/thumbnail.jpg\u0026#34;, \u0026#34;images\u0026#34;: [ \u0026#34;https://cdn.dummyjson.com/product-images/1/1.jpg\u0026#34;, \u0026#34;https://cdn.dummyjson.com/product-images/1/2.jpg\u0026#34;, \u0026#34;https://cdn.dummyjson.com/product-images/1/3.jpg\u0026#34;, \u0026#34;https://cdn.dummyjson.com/product-images/1/4.jpg\u0026#34;, \u0026#34;https://cdn.dummyjson.com/product-images/1/thumbnail.jpg\u0026#34; ] } This is how you\u0026rsquo;d call the API endpoint in Go. I\u0026rsquo;m using the json-to-go service to generate the Go struct instead of handwriting it:\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;net/http\u0026#34; ) // Define the struct that reflects the response payload type Product struct { ID int `json:\u0026#34;id\u0026#34;` Title string `json:\u0026#34;title\u0026#34;` Description string `json:\u0026#34;description\u0026#34;` Price int `json:\u0026#34;price\u0026#34;` DiscountPercentage float64 `json:\u0026#34;discountPercentage\u0026#34;` Rating float64 `json:\u0026#34;rating\u0026#34;` Stock int `json:\u0026#34;stock\u0026#34;` Brand string `json:\u0026#34;brand\u0026#34;` Category string `json:\u0026#34;category\u0026#34;` Thumbnail string `json:\u0026#34;thumbnail\u0026#34;` Images []string `json:\u0026#34;images\u0026#34;` } func main() { // Ignore error handling for brevity var product Product response, _ := http.Get(\u0026#34;https://dummyjson.com/products/1\u0026#34;) defer response.Body.Close() body, _ := io.ReadAll(response.Body) _ = json.Unmarshal(body, \u0026amp;product) // project into struct // Do processing with product instance fmt.Println(product.ID, product.Title, product.Description) } This will give us the same output as the Python and JS code snippets:\n1 iPhone 9 An apple mobile which is nothing like apple Above, we had to create a new struct type to represent the response payload, instantiate it, and unmarshal the JSON payload into the struct before we were able to process it.\nNotice that we\u0026rsquo;re only using 3 fields and ignoring the rest. In this case, you can get away with only including those 3 fields in the struct type, and Go will do the right thing:\n// ... same as before type Product struct { ID int `json:\u0026#34;id\u0026#34;` Title string `json:\u0026#34;title\u0026#34;` Description string `json:\u0026#34;description\u0026#34;` } // ... same as before While this is less work than having to emulate the whole structure of the JSON output in the struct definition, it\u0026rsquo;s still not winning any medals for brevity against the Python and JS snippets.\nDynamically processing a JSON payload is nice as long as you\u0026rsquo;re working on a throwaway script. Anything more, it becomes a headache since the readers won\u0026rsquo;t have any idea about what the API response looks like without looking at the documentation or traces.\nAlso, type safety is an issue. Since the imperative workflow doesn\u0026rsquo;t assume the structure of the response, you\u0026rsquo;ll be surprised with a runtime error if you make an incorrect assumption about the response structure. Sure, having to write a struct is a chore, but the free documentation and the type safety are things that you don\u0026rsquo;t get with the black box API calls.\nStatically typed languages force you to maintain good hygiene while working with JSON payloads. Declaratively embedding the payload structure directly into the codebase is immensely beneficial; it reduces the out-of-band knowledge required to understand the code and adds type safety as a cherry on top. But how do you do that in a language like Python?\nIf you want to go with what\u0026rsquo;s in the standard library, you can handroll a dataclass like this and project the return payload onto it:\n# ... from dataclasses import dataclass from typing import Self @dataclass(slots=True) class Product: id: int title: str description: str price: int discount_percentage: float rating: float stock: int brand: str category: str thumbnail: str images: list[str] @classmethod def from_dict(cls, d: dict) -\u0026gt; Self: # Reconcile snake_case and camelCase variables discount_percentage = d.pop(\u0026#34;discountPercentage\u0026#34;, None) return cls(discount_percentage=discount_percentage, **d) # ... Then just call Product.from_dict and pass the output of response.json() as before.\nThis way, the API response is documented in the code, and the reader won\u0026rsquo;t have to depend on out-of-band information while reading the code. However, you can see that hand rolling data classes can quickly become hairy when you have a large JSON payload and need to reconcile the discrepancies between snake case and camel case variables. We had to add a custom from_dict class method to convert the camel case variables to their snake case counterparts in Python.\nAlso, unlike Go, you can\u0026rsquo;t define a structure to represent only a portion of the whole payload in Python without adding extra code to ignore the rest of the fields that aren\u0026rsquo;t relevant to you.\nPydantic shines here. It not only allows you to define a class to represent a partial payload structure, but also applies runtime validation to guarantee operational type safety. As a bonus, you can use the json-to-pydantic tool to generate pydantic classes from JSON:\nfrom pydantic import BaseModel, Field class Product(BaseModel): id: int title: str description: str price: int discount_percentage: float = Field(alias=\u0026#34;discountPercentage\u0026#34;) rating: float stock: int brand: str category: str thumbnail: str images: list[str] You can project your response onto the data class with Product(**response.json()) and get a rich object that also validates the incoming values. This will work the same way with partially defined classes:\n# ... from pydantic import BaseModel class Product(BaseModel): id: int title: str description: str # ... Here\u0026rsquo;s a complete example:\nimport httpx from pydantic import BaseModel import asyncio # Partially defined Pydantic model to represent the response class Product(BaseModel): id: int title: str description: str async def main() -\u0026gt; None: async with httpx.AsyncClient() as client: response = await client.get(\u0026#34;https://dummyjson.com/products/1\u0026#34;) response.raise_for_status() data = response.json() product = Product(**data) print(product.id, product.title, product.description) if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) In the JS land, you can adopt TypeScript and zod to achieve a similar result:\n// index.ts import { z } from \u0026#34;zod\u0026#34;; const ProductSchema = z.object({ id: z.number(), title: z.string(), description: z.string(), // Other fields can be added if needed }); type Product = z.infer\u0026lt;typeof ProductSchema\u0026gt;; fetch(\u0026#34;https://dummyjson.com/products/1\u0026#34;) .then((response) =\u0026gt; response.json()) .then((data) =\u0026gt; ProductSchema.parse(data)) .then((product: Product) =\u0026gt; { console.log(product.id, product.title, product.description); }); I don\u0026rsquo;t mind the added verbosity if it leads to better readability and type safety.\nFin!\n","permalink":"https://rednafi.com/misc/eschewing-black-box-api-calls/","summary":"\u003cp\u003eI love dynamically typed languages as much as the next person. They let us make ergonomic\nAPI calls like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ehttpx\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Sync call for simplicity\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003er\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ehttpx\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;https://dummyjson.com/products/1\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejson\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003er\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;id\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003er\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;title\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003er\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;description\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eor this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-js\" data-lang=\"js\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003efetch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;https://dummyjson.com/products/1\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003ethen\u003c/span\u003e\u003cspan class=\"p\"\u003e((\u003c/span\u003e\u003cspan class=\"nx\"\u003eres\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"nx\"\u003eres\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003ejson\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003ethen\u003c/span\u003e\u003cspan class=\"p\"\u003e((\u003c/span\u003e\u003cspan class=\"nx\"\u003ejson\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"nx\"\u003econsole\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003elog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ejson\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eid\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003ejson\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003etype\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003ejson\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003edescription\u003c/span\u003e\u003cspan class=\"p\"\u003e));\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIn both cases, running the snippets will return:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e1 \u0026#39;iPhone 9\u0026#39; \u0026#39;An apple mobile which is nothing like apple\u0026#39;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eUnless you\u0026rsquo;ve worked with a statically typed language that enforces more constraints, it\u0026rsquo;s\nhard to appreciate how incredibly convenient it is to be able to call and use an API\nendpoint without having to deal with types or knowing anything about its payload structure.\nYou can treat the API response as a black box and deal with everything in runtime.\u003c/p\u003e","title":"Eschewing black box API calls"},{"content":"While I tend to avoid *args and **kwargs in my function signatures, it\u0026rsquo;s not always possible to do so without hurting API ergonomics. Especially when you need to write functions that call other helper functions with the same signature.\nTyping *args and **kwargs has always been a pain since you couldn\u0026rsquo;t annotate them precisely before. For example, if all the positional and keyword arguments of a function had the same type, you could do this:\ndef foo(*args: int, **kwargs: bool) -\u0026gt; None: ... This implies that args is a tuple where all the elements are integers, and kwargs is a dictionary where the keys are strings and the values are booleans.\nOn the flip side, you couldn\u0026rsquo;t annotate *args and **kwargs properly if the values of the positional and keyword arguments had different types. In those cases, you\u0026rsquo;d have to fall back to Any, which defeats the purpose.\nConsider this example:\ndef foo( *args: tuple[int, str], **kwargs: dict[str, bool | None] ) -\u0026gt; None: ... Here, the type checker sees each positional argument as a tuple of an integer and a string. Plus, it considers each keyword argument as a dictionary where the keys are strings and the values are either booleans or None.\nWith the previous annotation, mypy will reject this:\nfoo(*(1, \u0026#34;hello\u0026#34;), **{\u0026#34;key1\u0026#34;: 1, \u0026#34;key2\u0026#34;: False}) error: Argument 1 to \u0026#34;foo\u0026#34; has incompatible type \u0026#34;*tuple[int, str]\u0026#34;; expected \u0026#34;tuple[int, str]\u0026#34; [arg-type] error: Argument 2 to \u0026#34;foo\u0026#34; has incompatible type \u0026#34;**dict[str, int]\u0026#34;; expected \u0026#34;dict[str, bool | None]\u0026#34; [arg-type] Instead, it\u0026rsquo;ll accept the following:\nfoo((1, \u0026#34;hello\u0026#34;), kw1={\u0026#34;key1\u0026#34;: 1, \u0026#34;key2\u0026#34;: False}) You probably wanted to represent the former while the type checker wants the latter.\nTo annotate the second instance correctly, you\u0026rsquo;ll need to leverage bits of PEP-589, PEP-646, PEP-655, and PEP-692. We\u0026rsquo;ll use Unpack and TypedDict from the typing module to achieve this. Here\u0026rsquo;s how:\nfrom typing import TypedDict, Unpack # Python 3.12+ # from typing_extensions import TypedDict, Unpack # \u0026lt; Python 3.12 class Kw(TypedDict): key1: int key2: bool def foo(*args: Unpack[tuple[int, str]], **kwargs: Unpack[Kw]) -\u0026gt; None: ... args = (1, \u0026#34;hello\u0026#34;) kwargs: Kw = {\u0026#34;key1\u0026#34;: 1, \u0026#34;key2\u0026#34;: False} foo(*args, **kwargs) # Ok TypedDict was introduced in Python 3.8 to allow you to annotate heterogeneous dictionaries. If all the values of a dictionary have the same type, you can simply use dict[str, T] to annotate it. However, TypedDict covers the case where all the keys of a dictionary are strings but the type of the values varies.\nThe following example shows how you might annotate a heterogeneous dictionary:\nfrom typing import TypedDict class Movie(TypedDict): name: str year: int movies: Movie = {\u0026#34;name\u0026#34;: \u0026#34;Mad Max\u0026#34;, \u0026#34;year\u0026#34;: 2015} Unpack marks an object as having been unpacked.\nUsing TypedDict with Unpack allows us to communicate with the type checker so that each positional and keyword argument isn\u0026rsquo;t mistakenly assumed as a tuple and a dictionary respectively.\nWhile the type checker is satisfied when you pass the *args and **kwargs as\nfoo(*args, **kwargs) it\u0026rsquo;ll complain if you don\u0026rsquo;t pass all the keyword arguments:\nfoo(*args, key1=1) # error: Missing named argument \u0026#34;key2\u0026#34; for \u0026#34;foo\u0026#34; To make all of the keywords optional, you could turn off the total flag in the typed-dict definition:\n# ... class Kw(TypedDict, total=False): key1: int key2: str # ... Or you could mark specific keywords as optional with typing.NotRequired:\n# ... class Kw(TypedDict): key1: int key2: NotRequired[str] # ... This will let you pass an incomplete set of optional keyword arguments without the type checker yelling at you.\nFin!\n","permalink":"https://rednafi.com/python/annotate-args-and-kwargs/","summary":"\u003cp\u003eWhile I tend to avoid \u003ccode\u003e*args\u003c/code\u003e and \u003ccode\u003e**kwargs\u003c/code\u003e in my function signatures, it\u0026rsquo;s not always\npossible to do so without hurting API ergonomics. Especially when you need to write\nfunctions that call other helper functions with the same signature.\u003c/p\u003e\n\u003cp\u003eTyping \u003ccode\u003e*args\u003c/code\u003e and \u003ccode\u003e**kwargs\u003c/code\u003e has always been a pain since you couldn\u0026rsquo;t annotate them\nprecisely before. For example, if all the positional and keyword arguments of a function had\nthe same type, you could do this:\u003c/p\u003e","title":"Annotating args and kwargs in Python"},{"content":"I needed to integrate rate limiting into a relatively small service that complements a monolith I was working on. My initial thought was to apply it at the application layer, as it seemed to be the simplest route.\nPlus, I didn\u0026rsquo;t want to muck around with load balancer configurations, and there\u0026rsquo;s no shortage of libraries that allow me to do this quickly in the app. However, this turned out to be a bad idea. In the event of a DDoS attack or thundering herd incident, even if the app rejects the influx of inbound requests, the app server workers still have to do a minimal amount of work.\nAlso, ideally, rate limiting is an infrastructure concern; your app should be oblivious to it. Implementing rate limiting in a layer in front of your app prevents rogue requests from even reaching the app server in the event of an incident. So, I decided to spend some time investigating how to do it at the load balancer layer. Nginx makes rate limiting straightforward and the system was already using it as a reverse proxy.\nFor the initial pass, I chose to go with the default Nginx settings, avoiding any additional components like a Redis layer for centralized rate limiting.\nApp structure For this demo, I\u0026rsquo;ll proceed with a simple hello-world server written in Go. Here\u0026rsquo;s the app directory:\napp ├── Dockerfile ├── docker-compose.yml ├── go.mod ├── main.go ├── main_test.go └── nginx ├── default.conf └── nginx.conf The main.go file exposes the server at the /greetings endpoint on port 8080:\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;net/http\u0026#34; ) type HelloWorldResponse struct { Message string `json:\u0026#34;message\u0026#34;` } func helloWorldHandler(w http.ResponseWriter, r *http.Request) { w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) response := HelloWorldResponse{Message: \u0026#34;Hello World\u0026#34;} json.NewEncoder(w).Encode(response) } func main() { http.HandleFunc(\u0026#34;/greetings\u0026#34;, helloWorldHandler) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } If you run the server with the go run main.go command and make a curl request to it, it\u0026rsquo;ll give you the following JSON output:\ncurl localhost:8080/greetings | jq { \u0026#34;message\u0026#34;: \u0026#34;Hello World\u0026#34; } Now, we want to set up the rate limiter in the reverse proxy layer so that it will reject requests when the inbound request rate exceeds 50 req/sec.\nNginx config The Nginx config lives in the nginx directory and consists of two config files:\napp/nginx ├── default.conf └── nginx.conf The nginx.conf file is the core configuration file. It\u0026rsquo;s where you define the server\u0026rsquo;s global settings, like how many worker processes to run, where to store log files, rate limiting policies, and overarching security protocols.\nThen there\u0026rsquo;s the default.conf file, which is typically more focused on the configuration of individual server blocks or virtual hosts. This is where you get into the specifics of each website or service you\u0026rsquo;re hosting on the server. Settings like server names, SSL certificates, and specific location directives are defined here. It\u0026rsquo;s tailored to manage the nitty-gritty of how each site or application behaves under the umbrella of the global settings set in nginx.conf.\nYou can have multiple *.conf files like default.conf and all of them are included in the nginx.conf file.\nnginx.conf Here\u0026rsquo;s how the nginx.conf looks:\nevents { worker_connections 1024; } http { # Define the rate limiting zone limit_req_zone $binary_remote_addr zone=mylimit:10m rate=50r/s; # Custom error pages should be defined within a server block # Define this in the specific server configuration files. # Include server configurations from conf.d directory include /etc/nginx/conf.d/*.conf; } In the nginx.conf file, you\u0026rsquo;ll find two main sections: events and http. Each of these serves different purposes in the setup.\nEvents block events { worker_connections 1024; } This section defines settings for the events block, specifically the worker_connections directive. It sets the maximum number of connections that each worker process can handle concurrently to 1024.\nHTTP block http { limit_req_zone $binary_remote_addr zone=mylimit:10m rate=50r/s; include /etc/nginx/conf.d/*.conf; } The http block contains directives that apply to HTTP/S traffic.\nSet the rate limiting policy (limit_req_zone directive)\nlimit_req_zone $binary_remote_addr zone=mylimit:10m rate=50r/s; This line sets up rate limiting policy using three parameters:\nKey ($binary_remote_addr): This is the client\u0026rsquo;s IP address in a binary format. It\u0026rsquo;s used as a key to apply the rate limit, meaning each unique IP address is subjected to the rate limit specified.\nZone (zone=mylimit:10m): This defines a shared memory zone named mylimit with a size of 10 megabytes. The zone stores the state of each IP address, including how often it has accessed the server. Approximately 160,000 IP addresses can be tracked with this size. If the zone is full, Nginx will start removing the oldest entries to free up space.\nRate (rate=50r/s): This parameter sets the maximum request rate to 50 requests per second for each IP address. If the rate is exceeded, additional requests may be delayed or rejected.\nInclude the default.conf file\ninclude /etc/nginx/conf.d/*.conf; This directive instructs Nginx to include additional server configurations — like default.conf — from the /etc/nginx/conf.d/ directory. This modular approach allows for better organization and management of server configurations.\ndefault.conf The default.conf file, included in the previously discussed nginx.conf, mainly configures a server block in Nginx. We\u0026rsquo;ll use the rate limiting policy defined there in the default.conf file. Here\u0026rsquo;s the content:\nserver { listen 80 default_server; # Custom JSON response for 429 errors error_page 429 = @429; location @429 { default_type application/json; return 429 \u0026#39;{\u0026#34;status\u0026#34;: 429, \u0026#34;message\u0026#34;: \u0026#34;Too Many Requests\u0026#34;}\u0026#39;; } location / { # Apply rate limiting limit_req zone=mylimit burst=10 nodelay; limit_req_status 429; # Status code for rate-limited reqs # Proxy settings - adjust as necessary for your application proxy_pass http://app:8080; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#39;upgrade\u0026#39;; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } } This file currently contains a server block where we employ the rate limiting policy and set up the reverse proxy.\nServer Block server { listen 80 default_server; ... } This section defines the server block, with Nginx listening on port 80, the default port for HTTP traffic. The default_server parameter indicates that this server block should be used if no other matches are found.\nCustom error handling error_page 429 = @429; location @429 { default_type application/json; return 429 \u0026#39;{\u0026#34;status\u0026#34;: 429, \u0026#34;message\u0026#34;: \u0026#34;Too Many Requests\u0026#34;}\u0026#39;; } By default, when a client experiences rate limiting, the server returns an HTTP 503 error with an HTML page. But we want to return 429 (Too many requests) error code with an error message in a JSON payload. This section does that.\nLocation block location / { limit_req zone=mylimit burst=10 nodelay; limit_req_status 429; ... } The location / block applies to all requests to the root URL and its subdirectories.\nApply the rate limiting policy\nlimit_req zone=mylimit burst=10 nodelay; limit_req_status 429; These directives enforce the rate limiting policy set in nginx.conf. The limit_req directive uses the previously defined mylimit zone. The burst parameter allows a burst of 10 requests above the set rate before enforcing the limit. The nodelay option ensures that excess requests within the burst limit are processed immediately without delay. limit_req_status sets the HTTP status code for rate-limited requests to 429.\nConfigure the proxy\nproxy_pass http://app:8080; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#39;upgrade\u0026#39;; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; These lines configure Nginx to act as a reverse proxy. Requests to this server are forwarded to an application server running on http://app:8080. The directives also handle HTTP headers to properly manage the connection and caching between the client, reverse proxy, and backend application server.\nContainerize everything The Dockerfile builds the hello-world service:\nFROM golang:1.21 as build WORKDIR /go/src/app COPY . . RUN go mod download RUN CGO_ENABLED=0 go build -o /go/bin/app FROM gcr.io/distroless/static-debian12 COPY --from=build /go/bin/app / CMD [\u0026#34;/app\u0026#34;] Then we orchestrate the app with reverse proxy in the docker-compose.yml file:\nversion: \u0026#34;3.8\u0026#34; services: app: build: . ports: - \u0026#34;8080:8080\u0026#34; nginx: image: nginx:alpine ports: - \u0026#34;80:80\u0026#34; volumes: - ./nginx/nginx.conf:/etc/nginx/nginx.conf - ./nginx/default.conf:/etc/nginx/conf.d/default.conf depends_on: - app The docker-compose file defines two services: app and nginx. The app service exposes port 8080, meaning the app will be accessible on this port from outside the Docker environment.\nThe nginx service sits in front of the app and is configured to expose port 80. All the external requests will hit the default port 80 where the reverse proxy will relay the request to the backend app. The custom Nginx configuration volumes are mounted in the volumes section.\nTake it for a spin Navigate to the app directory and start the system with the following command:\ndocker compose up -d Now make 200 concurrent curl requests to see the rate limiter in action:\nseq 200 | xargs -n 1 -P 100 bash -c \u0026#39;curl -s location/greetings | jq\u0026#39; This returns:\n{ \u0026#34;message\u0026#34;: \u0026#34;Hello World\u0026#34; } { \u0026#34;message\u0026#34;: \u0026#34;Hello World\u0026#34; } ... { \u0026#34;status\u0026#34;: 429, \u0026#34;message\u0026#34;: \u0026#34;Too Many Requests\u0026#34; } { \u0026#34;status\u0026#34;: 429, \u0026#34;message\u0026#34;: \u0026#34;Too Many Requests\u0026#34; } { \u0026#34;message\u0026#34;: \u0026#34;Hello World\u0026#34; } { \u0026#34;status\u0026#34;: 429, \u0026#34;message\u0026#34;: \u0026#34;Too Many Requests\u0026#34; } See the deployed service in action (might not be available later):\nseq 200 | xargs -n 1 -P 100 bash -c \u0026#39;curl -s 34.138.11.32/greetings | jq\u0026#39; This will print the same output as the local service.\nNginx uses the leaky bucket algorithm to enforce the rate limiting, where requests arrive at the bucket at various rates and leave the bucket at fixed rate.\nFind the complete implementation on GitHub.\nFin!\n","permalink":"https://rednafi.com/go/rate-limiting-via-nginx/","summary":"\u003cp\u003eI needed to integrate rate limiting into a relatively small service that complements a\nmonolith I was working on. My initial thought was to apply it at the application layer, as\nit seemed to be the simplest route.\u003c/p\u003e\n\u003cp\u003ePlus, I didn\u0026rsquo;t want to muck around with load balancer configurations, and there\u0026rsquo;s no\nshortage of libraries that allow me to do this quickly in the app. However, this turned out\nto be a bad idea. In the event of a \u003ca href=\"https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/\"\u003eDDoS attack\u003c/a\u003e or \u003ca href=\"https://nick.groenen.me/notes/thundering-herd/\"\u003ethundering herd\u003c/a\u003e incident, even if the\napp rejects the influx of inbound requests, the app server workers still have to do a\nminimal amount of work.\u003c/p\u003e","title":"Rate limiting via Nginx"},{"content":"You can use @dataclass(frozen=True) to make instances of a data class immutable during runtime. However, there\u0026rsquo;s a small caveat — instantiating a frozen data class is slightly slower than a non-frozen one. This is because, when you enable frozen=True, Python has to generate __setattr__ and __delattr__ methods during class definition time and invoke them for each instantiation.\nBelow is a quick benchmark comparing the instantiation times of a mutable dataclass and a frozen one (in Python 3.12):\nfrom dataclasses import dataclass import timeit @dataclass class NormalData: a: int b: int c: int @dataclass(frozen=True) class FrozenData: a: int b: int c: int # Measure instantiation time for NormalData normal_time = timeit.timeit(lambda: NormalData(1, 2, 3), number=1_000_000) # Measure instantiation time for FrozenData frozen_time = timeit.timeit(lambda: FrozenData(1, 2, 3), number=1_000_000) print(f\u0026#34;Normal data class: {normal_time}\u0026#34;) print(f\u0026#34;Frozen data class: {frozen_time}\u0026#34;) print(f\u0026#34;Frozen data class is {frozen_time / normal_time}x slower\u0026#34;) Running this prints:\nNormal data class: 0.13145725009962916 Frozen data class: 0.3248348340857774 Frozen data class is 2.4710301930064014x slower So, frozen data classes are approximately 2.4 times slower to instantiate than their non-frozen counterparts. This gap can widen further if you compare slotted data classes (via @dataclass(slots=True)) with frozen ones. While the cost for immutability is small, it can add up if you need to create many frozen instances.\nI was reading Tin Tvrtković\u0026rsquo;s article on zero-overhead frozen attrs on making attrs instances frozen at compile time. He mentions how to leverage mypy to enforce instance immutability statically and use mutable attr classes at runtime to avoid any instantiation cost. I wanted to see if I could do the same with standard data classes.\nHere\u0026rsquo;s how to do it:\nfrom dataclasses import dataclass from typing import TYPE_CHECKING, TypeVar, dataclass_transform if TYPE_CHECKING: T = TypeVar(\u0026#34;T\u0026#34;) @dataclass_transform(frozen_default=True) def frozen(cls: type[T]) -\u0026gt; type[T]: ... else: frozen = dataclass # or dataclass(slots=True) for faster perf @frozen class Foo: x: int y: int # Instantiate the class foo = Foo(1, 2) # Mypy will raise an error here since foo is frozen during type checking foo.x = 3 print(foo) It involves:\nUsing the type checker to ensure the data class instance is immutable. Replacing the immutable data class with a more performant mutable one at runtime. The if TYPE_CHECKING condition only executes during type-checking. In that block, we use typing.dataclass_transform, introduced in PEP-681, to create a construct similar to the dataclass function that type checkers recognize.\nThe frozen_default flag, added in Python 3.12, makes this work seamlessly via PEP-681, but the code should also function in Python 3.11 without changes, as dataclass_transform accepts any keyword arguments. In Python 3.10 and earlier, you can import dataclass_transform from typing_extensions and leave the rest of the code as is.\nThe else ... block is what runs when you actually execute the code. There, we\u0026rsquo;re just aliasing the vanilla dataclass function as frozen.\nRunning this code snippet results in:\nFoo(x=3, y=2) However, mypy will flag an error since we\u0026rsquo;re trying to mutate foo.x:\nfoo.py:24: error: Property \u0026#34;x\u0026#34; defined in \u0026#34;Foo\u0026#34; is read-only [misc] Voilà!\nI struggled to figure this one out myself, and LLMs were of no help. So, I ended up posting a question on Stack Overflow, where someone pointed out how to use dataclass_transform to achieve this.\nFin!\n","permalink":"https://rednafi.com/python/statically-enforcing-frozen-dataclasses/","summary":"\u003cp\u003eYou can use \u003ccode\u003e@dataclass(frozen=True)\u003c/code\u003e to make instances of a data class immutable during\nruntime. However, there\u0026rsquo;s a small caveat — instantiating a frozen data class is slightly\nslower than a non-frozen one. This is because, when you enable \u003ccode\u003efrozen=True\u003c/code\u003e, Python has to\ngenerate \u003ccode\u003e__setattr__\u003c/code\u003e and \u003ccode\u003e__delattr__\u003c/code\u003e methods during class definition time and invoke\nthem for each instantiation.\u003c/p\u003e\n\u003cp\u003eBelow is a quick benchmark comparing the instantiation times of a mutable dataclass and a\nfrozen one (in Python 3.12):\u003c/p\u003e","title":"Statically enforcing frozen data classes in Python"},{"content":"When I started my career in a tightly-knit team of six engineers at a small e-commerce startup, I was struck by the remarkable efficiency of having a centralized hub for all the documents used for planning. We used a single Trello board with four columns — To-do, Doing, Q/A, Done — where the tickets were grouped by feature tags. We\u0026rsquo;d leverage a dummy ticket as an epic to sketch out the full plan for a feature and link related tickets to it. The rest of the discussions took place in Slack or GitHub Issues.\nThe setup was rudimentary but stood the test of time. As we expanded into multiple teams, each unit had its own board mirroring the original structure. Managers had a clear picture of where to find stuff, everything was searchable from one spot, and communication impedance was surprisingly low.\nA few years down the line, I was fortunate enough to land gigs at larger companies with bigger teams and more corporate structures. What caught me off guard was the chaotic state of planning documents. They were scattered everywhere — RFCs, ADRs, Epics, Jira Issues, Subtasks, Design Docs, you name it. Often, a single team would juggle all these formats to plan and record their work. I\u0026rsquo;m not claiming every workplace was like this, but it\u0026rsquo;s more common than I\u0026rsquo;d like to admit.\nThe fallout? Discussing a feature or onboarding new people became a pain since explaining any part of the system required going down the rabbit hole of finding the concomitant entry point and traversing its branches. More often than not, the documents weren\u0026rsquo;t even linked properly, so figuring out which RFCs, ADRs, Epics, or Jira Issues were associated with what feature was a frustrating exercise. Also, they\u0026rsquo;d quickly go outdated since keeping all of them up to date was a full-time job itself!\nThis cultural shift doesn\u0026rsquo;t happen in a day. People, in general, love reading books, blogs, or Hacker News discussions about the engineering practices in FAANG companies and mean well when they try to slowly incorporate these insights into their current teams. But let this osmosis continue for a few years without any oversight, and you\u0026rsquo;ll end up in a labyrinth of documents, encumbered by stiff structures and other enterprise-y fluff.\nSometimes I wonder if all these theatrics are actually necessary to do the job. Hundreds of people work on OSS projects where GitHub Issues and Projects are used to coordinate work. Just pressing cmd + k lets you find anything, anywhere, allowing immediate access to feature designs without having to sift through a quagmire of documents in disparate locations. The ability to access all documentation from a single place is not just efficient; it\u0026rsquo;s empowering, especially if it\u0026rsquo;s housed alongside your code.\nAnother approach that works well in practice is having a single Jira board per team where an Epic contains all the design decisions of a feature, and individual Task tickets under that are linked to GitHub Issues. This ensures that project managers can have a bird\u0026rsquo;s eye view of everything without having to log into the code repository, and developers can easily navigate back to the corresponding Task and Epic from the GitHub Issue with a single click.\nWhatever the strategy may be, I find it incredibly hard to justify the necessity to fragment these documents behind obscure names and waste time endlessly bikeshedding about whether ADRs should be written before RFCs or vice versa.\nFin!\n","permalink":"https://rednafi.com/zephyr/planning-palooza/","summary":"\u003cp\u003eWhen I started my career in a tightly-knit team of six engineers at a small e-commerce\nstartup, I was struck by the remarkable efficiency of having a centralized hub for all the\ndocuments used for planning. We used a single Trello board with four columns — To-do, Doing,\nQ/A, Done — where the tickets were grouped by feature tags. We\u0026rsquo;d leverage a dummy ticket as\nan epic to sketch out the full plan for a feature and link related tickets to it. The rest\nof the discussions took place in Slack or GitHub Issues.\u003c/p\u003e","title":"Planning palooza"},{"content":"I\u0026rsquo;ve always had a thing for old-school web tech. By the time I joined the digital fray, CGI scripts were pretty much relics, but the term kept popping up in tech forums and discussions like ghosts from the past. So, I got curious, started reading about them, and wanted to see if I could reason about them from the first principles. Writing one from the ground up with nothing but Go\u0026rsquo;s standard library seemed like a good idea.\nTurns out, the basis of the technology is deceptively simple, but CGI scripts mostly went out of fashion because of their limitations around performance.\nWhat are those CGI scripts, or Common Gateway Interface scripts, emerged in the early 1990s as a solution for creating dynamic web content. They acted as intermediaries between the web server and external applications, allowing servers to process user input and return personalized content. This made them essential for adding interactivity to websites, such as form submissions and dynamic page updates.\nThe key function of CGI scripts was to handle data from web forms, process it, and then generate an appropriate response. The server then takes this response and displays it on a new web page. Here\u0026rsquo;s how the process might look:\nsequenceDiagram participant U as Client participant S as Server participant C as CGI Script U-\u003e\u003eS: Post request with a dynamic field value S-\u003e\u003eC: Execute the CGI script in a new process Note right of C: CGI script receives the value C--\u003e\u003eS: Process and return result S--\u003e\u003eU: Respond with result How to write one CGI scripts are usually written in dynamic scripting languages like Perl, Ruby, Python, or even Bash. However, they can also be written in a static language where the server will need execute the compiled binary. For this demo, we\u0026rsquo;re going to write the server in Go using the cgi stdlib, but the CGI script itself will be written in Bash.\nHere\u0026rsquo;s the plan:\nSet up a basic HTTP server in Go. The server will await an HTTP POST request containing a form field called name. Upon receiving the request, the server will extract the value of name. Next, it\u0026rsquo;ll set the $name environment variable for the current process. A Bash CGI script is invoked, which uses the $name environment variable to echo an HTML-formatted dynamic message. Finally, the server will then return this HTML response to the client. The server lives in a single main.go script. I\u0026rsquo;m leaving out Go\u0026rsquo;s verbose error handling for clarity.\npackage main import ( \u0026#34;net/http\u0026#34; \u0026#34;net/http/cgi\u0026#34; \u0026#34;os/exec\u0026#34; ) // Leaves out error handling for clarity func cgiHandler(w http.ResponseWriter, r *http.Request) { // Parse name from post request r.ParseForm() name := r.FormValue(\u0026#34;name\u0026#34;) // Execute the CGI script with the name as an environment variable cmd := exec.Command(\u0026#34;cgi-script.sh\u0026#34;) cmd.Env = append(cmd.Env, \u0026#34;name=\u0026#34;+name) // Serve the CGI script handler := cgi.Handler{Path: cmd.Path, Dir: cmd.Dir, Env: cmd.Env} handler.ServeHTTP(w, r) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, cgiHandler) http.ListenAndServe(\u0026#34;localhost:8080\u0026#34;, nil) } Upon every new request, the server above will execute a CGI script written in Bash. Name the shell script as cgi-script.sh and place it in the same directory as the server\u0026rsquo;s main.go file. Here\u0026rsquo;s how it looks:\n#!/bin/bash set -euo pipefail name=$name echo \u0026#34;Content-type: text/html\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#39;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026#39; echo \u0026#34;Hello $name, greetings from bash!\u0026#34; echo \u0026#39;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#39; The script just reads name from the environment variable, sets the Content-Type header, injects the value of name into the message, and echos the out the final HTML response. The server then just relays it back to the client. To test this:\nRun the server with go run main.go. Set the permission of the CGI script: sudo chmod +x cgi-script.sh Make a cURL request: curl -X POST http://localhost:8080 -d \u0026#34;name=Redowan\u0026#34; This returns the following response:\n\u0026lt;html\u0026gt;\u0026lt;body\u0026gt; Hello Redowan, greetings from bash! \u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; Why they didn\u0026rsquo;t catch on CGI scripts have fallen out of favor primarily due to concerns related to performance and security. When a CGI script is executed, it initiates a new process for each request. While this approach is straightforward, it becomes increasingly inefficient as web traffic volume grows. However, it\u0026rsquo;s worth noting that modern Linux kernels have made improvements in process spawning, and solutions like FastCGI utilize persistent process pools to reduce the overhead of creating new processes. Nevertheless, you still incur the VM startup cost for each request when using interpreted languages like Python or Ruby.\nModern application servers like Uvicorn, Gunicorn, Puma, Unicorn or even Go\u0026rsquo;s standard server have addressed these inefficiencies by maintaining persistent server processes. This, along with the advantage of not having to bear the VM startup cost, has led people to opt for these alternatives.\nAnother concern worth considering is the evident security issues associated with CGI scripts. Even in our simple example, the Bash script accepts any value for the name parameter and passes it directly to the response. This exposes a significant vulnerability to injection attacks. While it\u0026rsquo;s possible to manually sanitize the input before passing it to the next step, many of these security steps are automatically handled for you by almost any modern web framework.\nFin!\nFurther reading Apache tutorial: Dynamic content with CGI ","permalink":"https://rednafi.com/go/reminiscing-cgi-scripts/","summary":"\u003cp\u003eI\u0026rsquo;ve always had a thing for old-school web tech. By the time I joined the digital fray, CGI\nscripts were pretty much relics, but the term kept popping up in tech forums and discussions\nlike ghosts from the past. So, I got curious, started reading about them, and wanted to see\nif I could reason about them from the first principles. Writing one from the ground up with\nnothing but Go\u0026rsquo;s standard library seemed like a good idea.\u003c/p\u003e","title":"Reminiscing CGI scripts"},{"content":"Despite using VSCode as my primary editor, I never really bothered to set up the native debugger to step through application code running inside Docker containers. Configuring the debugger to work with individual files, libraries, or natively running servers is straightforward. So, I use it in those cases and just resort back to my terminal for debugging containerized apps running locally. However, after seeing a colleague\u0026rsquo;s workflow in a pair-programming session, I wanted to configure the debugger to cover this scenario too.\nI\u0026rsquo;m documenting this to save my future self from banging his head against the wall trying to figure it out again.\nDesiderata I want to start a web app with docker compose up and connect the VSCode debugger to it from the UI. For this to work, along with the webserver, we\u0026rsquo;ll need to expose a debug server from the app container which the debugger can connect to.\nApp layout For demonstration, I\u0026rsquo;ll go with a simple containerized Starlette app served with uvicorn. However, the strategy will be similar for any web app. Here\u0026rsquo;s the app\u0026rsquo;s directory structure:\nsrc ├── __init__.py ├── main.py ├── requirements.txt ├── Dockerfile └── docker-compose.yml In main.py, we\u0026rsquo;re exposing an endpoint as follows:\nfrom starlette.applications import Starlette from starlette.responses import JSONResponse from starlette.routing import Route async def homepage(request) -\u0026gt; JSONResponse: return JSONResponse({\u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34;}) app = Starlette(debug=True, routes=[Route(\u0026#34;/\u0026#34;, homepage)]) The requirement.txt lists out the runtime dependencies:\nstarlette uvicorn Then the Dockerfile builds the application:\nFROM python:3.12-slim-bookworm WORKDIR /usr/src/app COPY . /usr/src/app RUN pip install --no-cache-dir -r requirements.txt EXPOSE 8000 CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;main:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8000\u0026#34;] Finally, we orchestrate the app in a docker-compose.yml file:\n# docker-compose.yml version: \u0026#34;3.9\u0026#34; services: web: build: context: . dockerfile: ./Dockerfile # This overrides the CMD in the Dockerfile command: uvicorn main:app --host 0.0.0.0 --port 8000 ports: - 8000:8000 Add launch.json Now, in the .vscode folder of the project\u0026rsquo;s root directory, add a file named launch.json. Create the folder if it doesn\u0026rsquo;t exist. You can also do this part manually; to do so:\nClick on the debugger button and then click on create a launch.json file. Select the Python debugger. Finally, select the Remote Attach debug config. However, if you dislike clicking around, here\u0026rsquo;s the full content of launch.json for you to copy and paste (into $PWD/.vscode/launch.json):\n{ \u0026#34;version\u0026#34;:\u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;Python: Remote Attach\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;python\u0026#34;, \u0026#34;request\u0026#34;:\u0026#34;attach\u0026#34;, \u0026#34;connect\u0026#34;:{ \u0026#34;host\u0026#34;:\u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;:5678 }, \u0026#34;pathMappings\u0026#34;:[ { \u0026#34;localRoot\u0026#34;:\u0026#34;${workspaceFolder}\u0026#34;, \u0026#34;remoteRoot\u0026#34;:\u0026#34;.\u0026#34; } ], \u0026#34;justMyCode\u0026#34;:true } ] } This instructs the VSCode debugger to attach to a debug server running on localhost through port 5678. The next section will elaborate on how to run the debug server in a container.\nLaunch configuration will vary depending on your project and each project needs to be set up individually. The VSCode debugging documentation lists out all the supported application types with example configurations. To avoid having to reconfigure the same app repetitively, tracking the entire .vscode directory via source control is probably a good idea.\nAdd docker-compose.debug.yml Next up, we\u0026rsquo;ll need to update the command section of services.web in the docker-compose.yml to expose the debug server. The debugpy tool from Microsoft does that for us.\nHowever, instead of changing the docker-compose.yml file for debugging, we can add a separate file for it named docker-compose.debug.yml. Here\u0026rsquo;s the content of it:\n# docker-compose.debug.yml version: \u0026#34;3.9\u0026#34; services: web: build: context: . dockerfile: ./Dockerfile command: - \u0026#34;sh\u0026#34; - \u0026#34;-c\u0026#34; - | pip install debugpy -t /tmp \\ \u0026amp;\u0026amp; python /tmp/debugpy --wait-for-client --listen 0.0.0.0:5678 \\ -m uvicorn main:app --host 0.0.0.0 --port 8000 ports: - 8000:8000 - 5678:5678 Here:\nsh -c: Selects the shell inside the Docker container. pip install debugpy -t /tmp: Installs the debugpy tool into the /tmp directory of the container. python /tmp/debugpy --wait-for-client --listen 0.0.0.0:5678: Runs debugpy, sets it to wait for a client connection and listen on all network interfaces at port 5678. -m uvicorn main:app --host 0.0.0.0 --port 8000: Starts an uvicorn server hosting the application defined in main:app, making it accessible on all network interfaces at port 8000. Start the debugger Before starting the VScode debugger, go to the project root and run:\ndocker compose -f docker-compose.debug.yml up Now click on the debugger button and select the Python: Remote attach profile to start debugging. Hack away!\nP.S.: An altruist on Reddit brought to my attention that a more elaborate version of this, with prettier screenshots, can be found in the official documentation.\n","permalink":"https://rednafi.com/python/debug-dockerized-apps-in-vscode/","summary":"\u003cp\u003eDespite using VSCode as my primary editor, I never really bothered to set up the native\ndebugger to step through application code running inside Docker containers. Configuring the\ndebugger to work with individual files, libraries, or natively running servers is\n\u003ca href=\"https://code.visualstudio.com/docs/python/debugging#_debugging-by-attaching-over-a-network-connection\"\u003estraightforward\u003c/a\u003e. So, I use it in those cases and just resort back to my terminal for\ndebugging containerized apps running locally. However, after seeing a colleague\u0026rsquo;s workflow\nin a pair-programming session, I wanted to configure the debugger to cover this scenario\ntoo.\u003c/p\u003e","title":"Debugging dockerized Python apps in VSCode"},{"content":"Data classes are containers for your data — not behavior. The delineation is right there in the name. Yet, I see state-mutating methods getting crammed into data classes and polluting their semantics all the time. While this text will primarily talk about data classes in Python, the message remains valid for any language that supports data classes and allows you to add state-mutating methods to them, e.g., Kotlin, Swift, etc. By state-mutating method, I mean methods that change attribute values during runtime. For instance:\nfrom dataclasses import dataclass @dataclass class Person: name: str age: int def make_older(by: int = 1) -\u0026gt; None: self.age += by In this case, calling the make_older method will change the value of age in-place.\nEvery time I spot a data class decked out with such methods, I feel like I\u0026rsquo;m looking at the penguin with an elephant head from the Family Guy. Whenever I traverse down to see how the instances of the class are being used, more often than not, I find them being treated just like regular mutable class instances with fancy reprs. But if you only need a nice repr for your large OO class, adding a __repr__ to the class definition is not that difficult. Why pay the price for building heavier data class instances only for that?\nIn Python, data classes are considerably slower to define and import compared to vanilla classes. However, they serve a different purpose than your typical run-of-the-mill classes. When you decorate a class with the @dataclass decorator without changing any of the default parameters, Python automatically generates __init__, __eq__, and __repr__ methods. If you set @dataclass(order=True), it\u0026rsquo;ll also generate __lt__, __le__, __gt__, and __ge__ special methods that enable you to compare and sort the data class instances. All of this implicates that the construct was specifically designed to contain rich data that provides the means for you to create nice abstractions around lower-level primitives.\nMy gripe isn\u0026rsquo;t against using data classes because of their heavier size. If it were, Python probably wouldn\u0026rsquo;t be one of my favorite languages. I use data classes all the time and love how they often allow me to craft nicer APIs with little effort. My issue is when people add state-mutating methods to data classes. The moment you\u0026rsquo;re doing that, you\u0026rsquo;re breaking the semantics of the data structure. You probably wouldn\u0026rsquo;t use hashmaps to represent sequential data even though Python currently maintains insertion order of the keys in dicts.\nIn Kotlin, I almost always define immutable data classes and pass them around in different functions that perform transformations and calculations. In Python, however, instantiating frozen data classes (@dataclass(frozen=True)) is almost twice as slow as mutable ones. So I just set slots=True to make the instantiation quicker and call it a day. But in either case, if I need to add a method that mutates the attributes of the class instance, I reconsider whether a data class is the right abstraction for the problem at hand. The necessity to add a state-mutating method is an indicator that you need a regular OO class. You\u0026rsquo;ll signal incorrect intent to the reader if you keep using data classes in this context.\nDataclasses are also great candidates for domain modeling with types. With the help of mypy, you can leverage sum types to emulate ADTs as follows (using PEP-695 generic syntax):\nfrom dataclasses import dataclass @dataclass(slots=True) class Barcode[T: str | int]: code: T @dataclass(slots=True) class Sku[T: str | int]: # Stock Keeping Unit code: T type ProductId = Barcode | Sku | None But it only works if your data containers don\u0026rsquo;t exhibit any behavior. Here the data classes are just labels for values in a set that can contain the instances of the classes. Adding state-mutating methods to either Barcode or Sku would break the semantics of how these types can be composed.\nI still think it\u0026rsquo;s okay if you need to validate the data class attributes in a __post_init__ method or override the __eq__ or __hash__ for some reason. Read-only methods are also acceptable since they don\u0026rsquo;t do in-place state modification. Comparing two data class instances that have read-only methods is not as awkward as comparing data class instances with methods that mutate attributes. So if you need to slap a method on a data class, write a function and pass the instance as a parameter or write a normal class with a repr and add the method there. This way, the reader won\u0026rsquo;t have to wonder whether your data containers have some hidden behavior attached to them or not.\n","permalink":"https://rednafi.com/python/dataclasses-and-methods/","summary":"\u003cp\u003eData classes are containers for your data — not behavior. The delineation is right there in\nthe name. Yet, I see state-mutating methods getting crammed into data classes and polluting\ntheir semantics all the time. While this text will primarily talk about data classes in\nPython, the message remains valid for any language that supports data classes and allows you\nto add state-mutating methods to them, e.g., Kotlin, Swift, etc. By state-mutating method, I\nmean methods that change attribute values during runtime. For instance:\u003c/p\u003e","title":"Banish state-mutating methods from data classes"},{"content":"Despite being an IC for the bulk of my career, finding my groove amidst the daily torrent of meetings from the early hours has always felt like balancing on a seesaw during a never-ending earthquake. Now, pair that with the onslaught of Slack inquiries and the incessant chiming of email notifications, and you have a front-row ticket to the anxiety circus. There are days when carving out a single hour of focus time is a wild goose chase, pushing me to work after hours to get stuff done, followed by a guilt trip about screen-gazing my life away.\nMeetings in the corporate software world come in two flavors: one where you\u0026rsquo;re charting the work terrain or dissecting issues, and the other is a procession of process meetings. Daily standups, one-on-ones with the manager, monthly all hands, and a confetti of other mandatory rendezvous that often makes you question your career choice at the tail of a chaotic but unproductive day. Although the meeting marathon for an IC isn\u0026rsquo;t as grueling as what a product manager or team lead endures, it\u0026rsquo;s still a hefty list that often sidelines the real deal — doing the actual work you were hired for.\nNow, railing against the corporate sky is useless. I\u0026rsquo;m well aware that far more astute minds have written about this many times before. Also, some of these meetings are paramount in a larger organization for coordinating work among many different teams and keeping the stakeholders in the loop. But that doesn\u0026rsquo;t mean things couldn\u0026rsquo;t improve. One thing that I\u0026rsquo;ve experienced is that I work better when all the meetings are clustered together in the morning, so there\u0026rsquo;s room for a couple of hours of deep work after 1-2 PM every day. That way, I can do all the meetings, write all the emails and Slack messages, do all the busy work before lunch, and then tune into focused work until the end of the day.\nOf course, achieving this nirvana is easier said than done, especially if your manager or coworkers don\u0026rsquo;t work in the same cadence. Neither should a team adopt an immutable work style that isn\u0026rsquo;t flexible enough to cater to changes. Plus, chaos induced by the messages and meetings between different teams is often beyond your control. The best you can probably do is talk to your manager, propose a potential workflow, and see whether it works for them and your coworkers.\nI\u0026rsquo;m yet to discover the unicorn\u0026rsquo;s horn that\u0026rsquo;ll allow me to deftly toggle between meetings and in-zone work without having to dodge a few anxiety attacks during work hours. Whether you\u0026rsquo;re a manager or an IC, if you have a story of how your current or previous team tackled this issue or have a better idea, I\u0026rsquo;d love to hear about it!\n","permalink":"https://rednafi.com/zephyr/finding-flow-amid-chaos/","summary":"\u003cp\u003eDespite being an IC for the bulk of my career, finding my groove amidst the daily torrent of\nmeetings from the early hours has always felt like balancing on a seesaw during a\nnever-ending earthquake. Now, pair that with the onslaught of Slack inquiries and the\nincessant chiming of email notifications, and you have a front-row ticket to the anxiety\ncircus. There are days when carving out a single hour of focus time is a wild goose chase,\npushing me to work after hours to get stuff done, followed by a guilt trip about\nscreen-gazing my life away.\u003c/p\u003e","title":"Finding flow amid chaos"},{"content":"Ever been in a situation where you landed a software engineering job with a particular tech stack, mastered it, switched to another company with a different stack, nailed that too, and then found yourself in a third company that used the original stack? Now, you suddenly sense that your hard-earned acumen in that initial stack has not only atrophied over the years but also a portion, or all of it, has become irrelevant, making it a bit of a struggle to catch up with the latest changes.\nAfter graduation, I switched gears from electrical to software engineering. I started out as a junior data scientist at an e-commerce startup. There, I juggled tasks like training small-scale machine learning models, analyzing tabular data, and building visualizations with the Python data stack. When COVID hit, I jumped ship to another company using a different tech stack, shifting my attention to distributed system and backend engineering.\nIn my current role, I\u0026rsquo;m still mostly doing backend work, just with a different set of tools than before. Throughout this journey, I\u0026rsquo;ve been fortunate enough to dart around three different continents. While hopping between positions and tech stacks has definitely widened my horizon, I\u0026rsquo;ve been grappling with the observation that as I pick up new skills, some of the older ones depreciate at a faster pace. It\u0026rsquo;s quite difficult to keep yourself sharp with tools that you don\u0026rsquo;t get to use regularly at work.\nWith all the hype around LLMs lately, I\u0026rsquo;m feeling drawn back to my original data science roots. To rekindle that part of my brain, I\u0026rsquo;ve started dabbling in some of my old OSS work, and to my great surprise, I\u0026rsquo;m struggling quite a bit to pick up the fundamentals and the required mathematics since I\u0026rsquo;ve been out of the game for so long. While I\u0026rsquo;ve kept in touch with some part of the open-source data world to stay relevant, apparently that wasn\u0026rsquo;t enough. On top of that, the world keeps piling on new concepts and skills that I\u0026rsquo;ll need to pick up if I ever intend to get past the interview barrier and professionally work in this arena again.\nTurns out this manifestation of stochastic knowledge decay is a well-studied phenomenon. The term half-life of knowledge was coined in 1962 by economist Fritz Machlup to represent the amount of time that has to elapse before half of the knowledge or facts in a particular area is superseded or shown to be untrue. It\u0026rsquo;s named after the half-life of decay in radioactive materials. This IEEE Spectrum article dives deep into the concept and reflects upon its effect on the industry. It postulates that the half-life of engineering knowledge is shrinking faster than ever before and the only way to tackle this is through continuous learning and getting better at managing the onslaught of information.\nI don\u0026rsquo;t have a prescriptive solution for this. I wrote this text to start a discussion around a feeling I previously struggled with but didn\u0026rsquo;t know how to label. So far, engaging with the OSS community on topics I find exciting, taking meticulous notes, tracking my learning progress, adopting boring technology, and writing about them have helped me stay relevant. However, this approach isn\u0026rsquo;t bulletproof and is quite susceptible to lack of motivation at the tail end of a 40-hour workweek. If you\u0026rsquo;ve experienced something similar and found a solution that worked for you to some extent, I\u0026rsquo;d love to hear about it!\n","permalink":"https://rednafi.com/zephyr/diminishing-half-life-of-knowledge/","summary":"\u003cp\u003eEver been in a situation where you landed a software engineering job with a particular tech\nstack, mastered it, switched to another company with a different stack, nailed that too, and\nthen found yourself in a third company that used the original stack? Now, you suddenly sense\nthat your hard-earned acumen in that initial stack has not only atrophied over the years but\nalso a portion, or all of it, has become irrelevant, making it a bit of a struggle to catch\nup with the latest changes.\u003c/p\u003e","title":"The diminishing half-life of knowledge"},{"content":"Adopting existing tools that work, applying them to the business problems at hand, and quickly iterating in the business domain rather than endlessly swirling in the vortex of technobabble is woefully underrated. I\u0026rsquo;ve worked at two kinds of companies before:\nOne that only cares about measurable business outcomes, accruing technical debt and blaming engineers when no one wants to work with their codebase, ultimately hurting the product. Another that has staff engineers spending all day on linter configurations and writing seldom-read RFCs while juniors want to ditch Celery for Kafka because the latter is hip.\nWhile both are equally bad, technical people love to lambaste the former while remaining blissfully ignorant about the second type. Maybe because there\u0026rsquo;s no incentive for doing that and resume-driven development genuinely pays better. As long as companies keep making people solve obscure puzzles that has nothing to do with the job or hiring managers keep employing automated systems to look for keywords in resumes, a group of smart people will always engage in techno-maximalism to prepare for the next big opportunity; setting the underlying product up for failure.\nThe bigger and more established the company is, the more the properties of the second type start manifesting. Add middle managers with zero ideas of what the worker bees are cooking underneath, and you have the perfect recipe for disaster. Raking in absurd sums to tweak linters or buttons may not be the worst thing in the world, if it also didn\u0026rsquo;t lead these bored people to dream of becoming architecture astronauts by introducing absurdly complex tools to solve imaginary problems.\nThe situation exacerbates when companies start introducing useless metrics like LOCs, PR counts, or the number of feature tickets to quantify developer productivity. This often leads to the creation of needless tickets and kickstarts the vicious PR cycle where developers endlessly debate the best practices, micro-optimizations, gratuitous niceties, and everything else other than the core business logic. If working on the business logic isn\u0026rsquo;t rewarded, why should anyone focus on making that better? Obviously it\u0026rsquo;s more profitable to introduce a dead letter queue to the callpath of an RPC instead of just writing a retry decorator and monitoring if that works or not.\nNow that microservices are no longer in vogue, and numerous companies have been burnt by adopting the Netflix way of working, despite not having that level of revenue or manpower, there\u0026rsquo;s no shortage of articles on how bad it is to adopt SoA when a PostgreSQL-backed Django monolith would probably do the job. Also, how terrible GraphQL is when a simple denormalized secondary index would suffice, or how the high churn rate of JavaScript frontend frameworks has wasted time, effort, and money. However, few of them mention how organizational structures and policies force people to take that route.\nThere must be a middle ground where developers can focus on the core business logic that yields the most value without incurring technical debt and making the development process a nightmare. I don\u0026rsquo;t have an answer for that, nor have I worked at a company that found the perfect balance. Plus, I\u0026rsquo;m not a technical lead, manager, or business owner. So if you are one of them, I\u0026rsquo;d love to hear how you or your organization plans to tackle this.\n","permalink":"https://rednafi.com/zephyr/oh-my-poor-business-logic/","summary":"\u003cp\u003eAdopting existing tools that work, applying them to the business problems at hand, and\nquickly iterating in the business domain rather than endlessly swirling in the vortex of\ntechnobabble is woefully underrated. I\u0026rsquo;ve worked at two kinds of companies before:\u003c/p\u003e\n\u003cp\u003eOne that only cares about measurable business outcomes, accruing technical debt and blaming\nengineers when no one wants to work with their codebase, ultimately hurting the product.\nAnother that has staff engineers spending all day on linter configurations and writing\nseldom-read RFCs while juniors want to ditch Celery for Kafka because the latter is hip.\u003c/p\u003e","title":"Oh my poor business logic"},{"content":"I like writing custom scripts to automate stuff or fix repetitive headaches. Most of them are shell scripts, and a few of them are written in Python. Over the years, I\u0026rsquo;ve accumulated quite a few of them. I use Git and GNU stow to manage them across different machines, and the dotfile workflow is quite effective. However, as the list of scripts grows larger, invoking them becomes a pain because the tab completion results get cluttered with other system commands. Plus, often I even forget the initials of a script\u0026rsquo;s name and stare at my terminal while the blinking cursor facepalms at my stupidity.\nI was watching this amazing talk by Brandon Rhodes that proposes quite an elegant solution to this problem. It goes like this:\nAll your scripts should start with a character as a prefix that doesn\u0026rsquo;t have any special meaning in the shell environment. Another requirement is that no other system command should start with your chosen character.\nThat way, when you type the prefix character and hit tab, only your custom scripts should appear and nothing else. This works with your aliases too!\nThe dilemma here is picking the right character that meets both of the requirements. Luckily, Brandon did the research for us. Turns out, the shell environment uses pretty much all the characters on the keyboard as special characters other than these 6:\n@ _ + - : , Among them, the first 5 requires pressing the Shift key, which is inconvenient. But the plain old comma , is right there. You can start your script or alias names with a comma , and it\u0026rsquo;ll be golden.\nMy tab completion looks like this:\nrednafi@air:~/canvas/rednafi.com $ , ,brclr ,clear-cache ,docker-prune-containers ,redis ,brpre ,docker-nuke ,docker-prune-images ,www All my aliases start with , too so that they also appear in the list with the custom scripts. Fin!\n","permalink":"https://rednafi.com/misc/pesky-little-scripts/","summary":"\u003cp\u003eI like writing custom scripts to automate stuff or fix repetitive headaches. Most of them\nare shell scripts, and a few of them are written in Python. Over the years, I\u0026rsquo;ve accumulated\nquite a few of them. I use Git and \u003ca href=\"https://www.gnu.org/software/stow/\"\u003eGNU stow\u003c/a\u003e to manage them across different machines, and\nthe \u003ca href=\"/misc/dotfile-stewardship-for-the-indolent/\"\u003edotfile workflow\u003c/a\u003e is quite effective. However, as the list of scripts grows larger,\ninvoking them becomes a pain because the tab completion results get cluttered with other\nsystem commands. Plus, often I even forget the initials of a script\u0026rsquo;s name and stare at my\nterminal while the blinking cursor facepalms at my stupidity.\u003c/p\u003e","title":"Pesky little scripts"},{"content":"There are a few ways you can add URLs to your Markdown documents:\nInline links\n[inline link](https://example.com) This will render as inline link.\nReference links\n[reference link] Define the link destination elsewhere in the document like this:\n[reference link]: https://example.com This will render the same way as before, reference link.\nFootnote style reference links\nfootnote style reference link[^1] Define the link destination using a footnote reference:\n[^1]: https://example.com This will render a bit differently with a clickable number beside the origin text that refers to the backref at the bottom of the document. Like this: footnote style reference link (see example link).\nTry clicking on the number within the square brackets and see how the page scrolls down to the corresponding backref link that lives with other backrefs at the tail of the page.\nThe inline link approach is the most prevalent one as it\u0026rsquo;s also the easiest one to write. But it suffers from a few issues:\nLinks scattered throughout your documents can make updates cumbersome. Reusing a link elsewhere requires multiple copy-pastes. Placing several links side by side can feel awkward, and URL stylings like blue highlighting or underlining make things noisy. To add a reference section, you\u0026rsquo;ll have to create a separate segment, usually at the bottom of your page, and duplicate the URLs. On mobile devices, accidentally fat-fingering a URL can promptly redirect readers away from your content, potentially against their intention. Enforcing a line width limit can be challenging due to lengthy inlined URLs. The reference link approach solves some of these issues since you won\u0026rsquo;t have to scatter the URLs across your document or repeat them multiple times for multiple usage. This also allows you to use a Markdown formatter to enforce a maximum line width. I use prettier to cap the line width at 92 characters and the formatter works better when it doesn\u0026rsquo;t have to shimmy around multiple long inline URLs.\nThis is certainly better than using inline links, but it still suffers from all the other issues that plague the former approach. Creating a reference section still requires some repetition, and juxtaposing multiple links remains awkward. Also, accidental misclicks that take you to a different page remains an issue.\nThe footnote-style reference link comes to the rescue. It keeps the document clean by moving all URLs to the bottom in a dedicated reference section. The small superscript numbers don\u0026rsquo;t distract the reader as much but provide an easy way to navigate to the corresponding links if needed. Accidental clicks are no longer an issue since clicking on a reference superscript will bring the user down to the footnote section where they can click on the concomitant URL or jump back to the origin by tapping on the backref (↩︎) symbol. The reference section also allows you to provide more context on each link, like a title or description.\nMoreover, adding multiple links to the same origin is straightforward since you simply add the footnote numbers like this (see footnotes with extra texts and multiple footnotes). Plus, you don\u0026rsquo;t have to manually create a separate reference section; it automatically gets created for you as you start adding footnotes. See the reference section in this post and click on the backref links to go back to the origin. Most parsers like GitHub flavored Markdown now support footnotes out of the box.\nRecently, I\u0026rsquo;ve spent an entire evening converting almost all of the inline links on this site into footnote style references in a semi-automated manner. I still use reference links here and there but mostly prefer footnotes since they allow me to avoid repetition and subjectively look less distracting compared to underlined or highlighted URLs. And suddenly, prettier\u0026rsquo;s job has become easier too!\nFurther reading Checkout the raw Markdown file of this post ","permalink":"https://rednafi.com/zephyr/footnotes-for-the-win/","summary":"\u003cp\u003eThere are a few ways you can add URLs to your Markdown documents:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInline links\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-md\" data-lang=\"md\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e[\u003cspan class=\"nt\"\u003einline link\u003c/span\u003e](\u003cspan class=\"na\"\u003ehttps://example.com\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis will render as \u003ca href=\"https://example.com\"\u003einline link\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eReference links\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-md\" data-lang=\"md\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e[reference link]\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eDefine the link destination elsewhere in the document like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-md\" data-lang=\"md\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e[reference link]: https://example.com\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis will render the same way as before, \u003ca href=\"https://example.com\"\u003ereference link\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFootnote style reference links\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-md\" data-lang=\"md\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003efootnote style reference link[^1]\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eDefine the link destination using a footnote reference:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-md\" data-lang=\"md\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e[^1]: https://example.com\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis will render a bit differently with a clickable number beside the origin text that\nrefers to the backref at the bottom of the document. Like this: footnote style reference\nlink (see \u003ca href=\"https://example.com\"\u003eexample link\u003c/a\u003e).\u003c/p\u003e","title":"Footnotes for the win"},{"content":"I\u0026rsquo;m one of those people who will sit in front of a computer for hours, fiddling with algorithms or debugging performance issues, yet won\u0026rsquo;t spend 10 minutes to improve their workflows. While I usually get away with this, every now and then, my inertia slithers back to bite me. The latest episode was me realizing how tedious it is to move config files across multiple devices when I was configuring a new MacBook Air and Mac Mini at the same time.\nI dislike customizing tools and tend to use the defaults as much as possible. However, over the years, I\u0026rsquo;ve accumulated a few config files here and there, which were historically backed up in a git repository and restored manually whenever necessary. MacOS\u0026rsquo;s time machine made sure that I didn\u0026rsquo;t need to do it very often. So I never paid much attention to it.\nBut recently, I came across GNU stow and realized that people have been using it for years to manage their configs. I tried it and found that it works perfectly for what I need. It\u0026rsquo;s a nifty little tool written in perl that allows you to store all of your config files in a git repository and symlink them to the targeted directories. The tool is pretty versatile and you can do a lot more than just dotfile management. But for this purpose, only two commands will do. The workflow roughly goes like this:\n┌─────────────────┐ │git repo [source]│ └┬────────────────┘ ┌▽────────────────────────────────────────────────────────┐ │dotfiles [zsh/.zshrc, zsh/.zprofile, git/.gitconfig, ...]│ └┬────────────────────────────────────────────────────────┘ ┌▽───────────────────────┐ │gnu stow creates symlink│ └┬───────────────────────┘ ┌▽───────────────────────────┐ │home directory [destination]│ └┬───────────────────────────┘ ┌▽────────────────────────────────────────────────────────────┐ │symlinked dotfiles [~/.zshrc, ~/.zprofile, ~/.gitconfig, ...]│ └─────────────────────────────────────────────────────────────┘ All of your config files will need to live in a git repo and their directory trees will have to match the desired folder structure of the destination. That means, if you need to restore a certain config file to ~/.config/app/.conf, then in the source repo, the file needs to live in the pkg1/.config/app/.conf directory. The source\u0026rsquo;s top-level directory pkg1 is called a package and can be named anything. While invoking stow, we\u0026rsquo;ll refer to a particular dotfile by the package it lives within. Run:\nstow -v -R -t ~ pkg1 Here:\n-v (or --verbose) makes stow run in verbose mode. When you use -v, stow will list the symlinks it creates or updates, making it easier to see the changes it\u0026rsquo;s making.\n-R (or --restow) tells stow to restow the packages. It\u0026rsquo;s useful when you\u0026rsquo;ve already stowed the packages previously, and want to reapply them. The -R flag ensures that stow re-symlinks files, even if they already exist. This makes each run idempotent and you won\u0026rsquo;t have to worry about polluting your workspace with straggler links.\n-t \u0026lt;target\u0026gt; (or --target=\u0026lt;target\u0026gt;) specifies the target directory where stow should create symlinks. The default target directory is the parent of $pwd. In the above command, -t ~ is used to set the home directory as the destination.\n\u0026lt;pkg1\u0026gt; is the package name you want to stow.\nFor a more concrete example, let\u0026rsquo;s say, your source repo ~/canvas/dot has two packages named git and zsh where the former contains .gitconfig and the latter houses .zshrc and .zprofile files:\n# ~/canvas/dot zsh ├── .zprofile └── .zshrc git └── .gitconfig To symlink both of them to the home directory, you\u0026rsquo;ll need to run the following command from the root of the source directory; ~/canvas/dot in this case:\nstow -v -R -t ~ zsh git Then you can see the newly created symlinks in the home directory with this:\nls -lah ~ | grep \u0026#39;^l\u0026#39; It prints:\nlrwxr-xr-x 1 rednafi staff 25 Sep 23 19:45 .gitconfig -\u0026gt; canvas/dot/git/.gitconfig lrwxr-xr-x 1 rednafi staff 24 Sep 23 19:52 .zprofile -\u0026gt; canvas/dot/zsh/.zprofile lrwxr-xr-x 1 rednafi staff 21 Sep 23 19:45 .zshrc -\u0026gt; canvas/dot/zsh/.zshrc If you want to remove a config file, you can unstow it with:\nunstow -v -R -t ~ pkg1 or, manually remove the symlink with:\nunlink ~/pkg1 One neat side effect of managing configs in this manner is that, since symlinks are pointers to the original files living in the source repo, any changes made to the source files are automatically reflected in the destination configs.\nHere are my dotfiles and a few management scripts in all their splendor!\n","permalink":"https://rednafi.com/misc/dotfile-stewardship-for-the-indolent/","summary":"\u003cp\u003eI\u0026rsquo;m one of those people who will sit in front of a computer for hours, fiddling with\nalgorithms or debugging performance issues, yet won\u0026rsquo;t spend 10 minutes to improve their\nworkflows. While I usually get away with this, every now and then, my inertia slithers back\nto bite me. The latest episode was me realizing how tedious it is to move config files\nacross multiple devices when I was configuring a new MacBook Air and Mac Mini at the same\ntime.\u003c/p\u003e","title":"Dotfile stewardship for the indolent"},{"content":"Every once in a while, I love browsing the Wayback Machine to catch a glimpse of the early internet. I enjoy the waves of nostalgic indie hacker vibes that wash over me as I type a URL into the search box and click to see an old snapshot of the site frozen in time. Being a kid of the early \u0026rsquo;00s, I missed the spectacular cosmic genesis of the \u0026rsquo;90s internet in its entire nascent glory. However, I did briefly get a coup d\u0026rsquo;œil of the raw, untainted web 1.0 right before SEO firms, ads, pop-ups, modals, autoplays, and heavy frontend frameworks started prowling and gentrifying the whole space into an anxiety-inducing corporate circus.\nBefore social media and other corporate silos became mainstream, the web had, for the lack of a better word, more character to it. From IRC rooms to personal blogs with weird pixelated designs and sound effects, the realm was fragmented, chaotic, heterogeneous, and mostly, a lot of fun. There were no best practices around doing anything, and the notion of creativity wasn\u0026rsquo;t tied to the craft of how many keywords or interactive animations you could shove into a page to get the most eyeballs. You were the cool kid if you could just wire together a few pages with Perl, PHP, Python, or even Bash, and put something useful out there.\nNow, I don\u0026rsquo;t want to come across as one of those anachronistic hipster millennials, reeking of RMS energy, who will reject anything that\u0026rsquo;s even remotely modern. Neither am I under the illusion that web 1.0 was a blissful nirvana, and we should\u0026rsquo;ve halted the progress wheel there. I wouldn\u0026rsquo;t want to relive the past of loading jQuery-heavy pages on my crappy 2G mobile network or the security horrors of PHP CMSes. Also, I\u0026rsquo;d prefer not all the websites to adopt the user-hostile-pitch-black-text-on-blinding-white design. It wouldn\u0026rsquo;t be functional, and the UX would probably drive away most folks.\nBut there\u0026rsquo;s absolutely no denying that the raw, unfiltered chaos of the old web had a magical spark to it that the sterile, framework-stuffed UIs of today often lack. While the mighty legion of the framework overlords and their thousand minions seem unstoppable in their quest to homogenize the web, it doesn\u0026rsquo;t have to be this way everywhere. There are still pockets where the web\u0026rsquo;s wild spirit can run free, untouched by the unrelenting march of frameworks stomping out all traces of creativity.\nWhenever I see technology mughals like Peter Norvig or Rob Pike showcase their thoughts through ancient relics like Rob Pike\u0026rsquo;s blog or Peter Norvig\u0026rsquo;s blog, it does put a smile on my face. No gaudy animations, no fancy fonts, no dark mode — just strings of thoughts laid bare against a backdrop of naked HTML. If you don\u0026rsquo;t like the design, just load the content with Safari or Firefox reader and you\u0026rsquo;re golden. Of course, not all websites have to be like this but not every website has to look like Stripe\u0026rsquo;s website either.\nPeople often conflate this brutalist web design approach with user-hostile, overly minimalist, and lazy design. The opposite of UI maximalism doesn\u0026rsquo;t need to be UX belligerence. Your site can look like absolute poop but still be responsive on any device and provide a great UX, like this absolutely hilarious rant page. It wraps perfectly on your laptop, phone, or tablet, and doesn\u0026rsquo;t need to load 2 MB static assets. Plus, who cares about the layout when the content is this interesting?\nThe neo-grotesque web keeps the minimalist design of the brutalist philosophy while maintaining a neat UX and emphasizing content rather than fluff. Bring back those single-column personal blogs where writers would just list out a few heartfelt contents and call it a day. Bring back those gigachad dynamic websites like Craigslist with their unadorned buttons and blue URLs that, UI-wise, give those million-dollar react-y pages a run for their money. As long as the pages wrap on your phone, what\u0026rsquo;s there to complain about? Ugly can still be beautiful. Excrement contains nutrients. All hail the neo-grotesque web!\n","permalink":"https://rednafi.com/zephyr/an-ode-to-the-neo-grotesque-web/","summary":"\u003cp\u003eEvery once in a while, I love browsing the \u003ca href=\"http://web.archive.org/\"\u003eWayback Machine\u003c/a\u003e to catch a glimpse of the early\ninternet. I enjoy the waves of nostalgic indie hacker vibes that wash over me as I type a\nURL into the search box and click to see an old snapshot of the site frozen in time. Being a\nkid of the early \u0026rsquo;00s, I missed the spectacular cosmic genesis of the \u0026rsquo;90s internet in its\nentire nascent glory. However, I did briefly get a coup d\u0026rsquo;œil of the raw, untainted web 1.0\nright before SEO firms, ads, pop-ups, modals, autoplays, and heavy frontend frameworks\nstarted prowling and gentrifying the whole space into an anxiety-inducing corporate circus.\u003c/p\u003e","title":"An ode to the neo-grotesque web"},{"content":"This site is built with Hugo and served via GitHub Pages. Recently, I decided to change the font here to make things more consistent across different devices. However, I didn\u0026rsquo;t want to go with Google Fonts for a few reasons:\nCDN is another dependency. Hosting static assets on GitHub Pages has served me well. Google Fonts tracks users and violates GDPR in Germany. Google Analytics does that too. But since I\u0026rsquo;m using the latter anyway, this might come off a bit apocryphal. I wanted to get a few extra Lighthouse points. Turns out, it\u0026rsquo;s pretty easy to host the fonts yourself.\nDownload the fonts I found this fantastic webfont helper tool that allows you to search for any Google font and download it. You can specify the font style, thickness, and browser support status. I\u0026rsquo;ve used it to download Schibsted Grotesk for text and JetBrains Mono for code snippets, targeting only modern browsers. You might want to pick Legacy Support if you need compatibility with older browsers and Historic Support for the really old ones.\nAfter downloading, unzip the file and place the fonts in the /static/fonts folder in your root directory. If you\u0026rsquo;ve selected the Modern Browsers option, then the fonts will come in web-optimized woff2 format. Sweet!\nPaste the CSS While downloading the fonts, you may have already noticed that the helper tool also generates the CSS snippet required to link the fonts from the host storage. Here\u0026rsquo;s a sample:\n/* schibsted-grotesk-regular - latin */ @font-face { font-display: swap; font-family: \u0026#39;Schibsted Grotesk\u0026#39;; font-style: normal; font-weight: 400; src: url(\u0026#39;../fonts/schibsted-grotesk-v3-latin-regular.woff2\u0026#39;) format(\u0026#39;woff2\u0026#39;); } /* schibsted-grotesk-italic - latin */ @font-face { font-display: swap; font-family: \u0026#39;Schibsted Grotesk\u0026#39;; font-style: italic; font-weight: 400; src: url(\u0026#39;../fonts/schibsted-grotesk-v3-latin-italic.woff2\u0026#39;) format(\u0026#39;woff2\u0026#39;); } /* truncated */ Copy the generated CSS and paste it somewhere in your header.css or assets/css/extended/header-override.css file if you\u0026rsquo;re overriding a theme. Edit the src attribute to reflect your font\u0026rsquo;s path:\n@font-face { font-display: swap; font-family: \u0026#39;Schibsted Grotesk\u0026#39;; font-style: normal; font-weight: 400; /* url(\u0026#39;../fonts/schibsted-grotesk-v3-latin-regular.woff2\u0026#39;); */ src: url(\u0026#39;/fonts/schibsted-grotesk-v3-latin-regular.woff2\u0026#39;) format(\u0026#39;woff2\u0026#39;); } Here, you\u0026rsquo;ll need to change ../fonts/\u0026lt;rest\u0026gt; to /fonts/\u0026lt;rest\u0026gt;, and Hugo will take care of the rest. Notice there\u0026rsquo;s no /static prefix in the font\u0026rsquo;s path. Find this blog\u0026rsquo;s header-override.css if you\u0026rsquo;re facing any trouble while doing it. Serve your website locally and ensure that the fonts are being loaded and displayed correctly. Deploy!\n","permalink":"https://rednafi.com/misc/self-hosted-google-fonts-in-hugo/","summary":"\u003cp\u003eThis \u003ca href=\"https://github.com/rednafi/rednafi.com/\"\u003esite\u003c/a\u003e is built with \u003ca href=\"https://gohugo.io/\"\u003eHugo\u003c/a\u003e and served via \u003ca href=\"https://pages.github.com/\"\u003eGitHub Pages\u003c/a\u003e. Recently, I decided to\nchange the font here to make things more consistent across different devices. However, I\ndidn\u0026rsquo;t want to go with Google Fonts for a few reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCDN is another dependency.\u003c/li\u003e\n\u003cli\u003eHosting static assets on GitHub Pages has served me well.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://rewis.io/urteile/urteil/lhm-20-01-2022-3-o-1749320/\"\u003eGoogle Fonts tracks users and violates GDPR\u003c/a\u003e in Germany. Google Analytics does that too.\nBut since I\u0026rsquo;m using the latter anyway, this might come off a bit apocryphal.\u003c/li\u003e\n\u003cli\u003eI wanted to get a few extra Lighthouse points.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTurns out, it\u0026rsquo;s pretty easy to host the fonts yourself.\u003c/p\u003e","title":"Self-hosted Google Fonts in Hugo"},{"content":"Suppose, you have a function that takes an option struct and a message as input. Then it stylizes the message according to the option fields and prints it. What\u0026rsquo;s the most sensible API you can offer for users to configure your function? Observe:\n// app/src package src // Option struct type Style struct { Fg string // ANSI escape codes for foreground color Bg string // Background color } // Display the message according to Style func Display(s *Style, msg string) {} In the src package, the function Display takes a pointer to a Style instance and a msg string as parameters. Then it decorates the msg and prints it according to the style specified in the option struct. In the wild, I\u0026rsquo;ve seen 3 main ways to write APIs that let users configure options:\nExpose the option struct directly Use the option constructor pattern Apply functional option constructor pattern Each comes with its own pros and cons.\nExpose the option struct In this case, you\u0026rsquo;d export the Style struct with all its fields and let the user configure them directly. The previous snippet already made the struct and fields public. From another package, you could import the src package and instantiate Style like this:\npackage main import \u0026#34;app/src\u0026#34; // Users instantiate the option struct c := \u0026amp;src.Style{ \u0026#34;\\033[31m\u0026#34;, // Maroon \u0026#34;\\033[43m\u0026#34;, // Yellow } // Then pass the struct to the function Display(c, \u0026#34;Hello, World!\u0026#34;) To configure option fields, mutate the values in place:\nc.Fg = \u0026#34;\\033[35m\u0026#34; // Magenta c.Bg = \u0026#34;\\033[40m\u0026#34; // Black This works but will break users\u0026rsquo; code if new fields are added to the option struct. But your users can instantiate the struct with named parameters to avoid breakage:\nc := \u0026amp;src.Style{ Fg: \u0026#34;\\033[31m\u0026#34;, // Maroon // Bg will be implicitly set to an empty string } In this case, the field that wasn\u0026rsquo;t passed would assume the corresponding zero value. For instance, Bg will be initialized as an empty string. However, this pattern puts the responsibility of retaining API compatibility on the users\u0026rsquo; shoulders. So if your code is meant for external use, there are better ways to achieve option configurability.\nOption constructor Go standard library extensively uses this pattern. Instead of letting the users instantiate Style directly, you expose a NewStyle constructor function that constructs the struct instance for them:\npackage src // same as before // NewStyle option constructor instantiates a Style instance func NewStyle(fg, bg string) *Style { return \u0026amp;Style{fg, bg} } It\u0026rsquo;ll be used as follows:\npackage main import \u0026#34;app/src\u0026#34; // The users will now use NewStyle to instantiate Style c := src.NewStyle( \u0026#34;\\033[31m\u0026#34;, // Maroon \u0026#34;\\033[43m\u0026#34;, // Yellow ) Display(c, \u0026#34;Hello, World!\u0026#34;) If a new field is added to Style, update NewStyle to have a sensible default value for it or initialize the struct with named parameters to set the optional fields to their respective zero values. This avoids breaking users\u0026rsquo; code as long as the constructor function\u0026rsquo;s signature doesn\u0026rsquo;t change.\npackage src type Style struct { Fg string Bg string Und bool // Underline or not } // Function signature unchanged though new option field added // Set sensible default in constructor function func NewStyle(fg, bg string) *Style{ return \u0026amp;Style{ Fg: fg, Bg: bg, // Und will be implicitly set to false } } In NewStyle, we implicitly set the value of Und to false but you can be explicit there depending on your needs. The struct fields can be updated in the same manner as before:\npackage main c := src.NewStyle( \u0026#34;\\033[31m\u0026#34;, // Maroon \u0026#34;\\033[43m\u0026#34;, // Yellow ) c.Und = true // Default is false, we\u0026#39;re setting it to true src.Display(c, \u0026#34;Hello, World!\u0026#34;) This should cover most use cases. However, if you don\u0026rsquo;t want to export the underlying option struct, or your struct has tons of optional fields requiring extensibility, you\u0026rsquo;ll need an extra layer of indirection to avoid the need to accept a zillion config parameters in your option constructor.\nFunctional option constructor As mentioned at the tail of the last section, this approach works better when your struct contains many optional fields and you need your users to be able to configure them if they want. Go doesn\u0026rsquo;t allow setting non-zero default values for struct fields. So an extra level of indirection is necessary to let the users configure them. This approach also allows us to make the option struct private so that there\u0026rsquo;s no ambiguity around API usage.\nLet\u0026rsquo;s say style now has two optional fields und and zigzag that allows users to decorate the message string with underlines or zigzagged lines:\npackage src type style struct { fg string bg string und bool // Optional field zigzag bool // Optional field } Now, we\u0026rsquo;ll define a new type called styleoption like this:\n// package src type styleoption func(*style) The styleoption function accepts a pointer to the option struct and updates a particular field with a user-provided value. The implementation of this type would look as such:\nfunc (s *style) {s.fieldName = fieldValue} Next, we\u0026rsquo;ll need to define a higher order config function for each optional field in the struct where the function will accept the field value and return another function with the styleoption signature. The WithUnd and WithZigzag wrapper functions will be a part of the public API that the users will use to configure style:\n// We only define config functions for the optional fields func WithUnd(und bool) styleoption { return func(s *style) { s.und = und } } func WithZigzag(zigzag bool) styleoption { return func(s *style) { s.zigzag = zigzag } } Finally, our option constructor function needs to be updated to accept variadic options. Observe how we\u0026rsquo;re looping through the options slice and applying the field config functions to the struct pointer:\nfunc NewStyle(fg, bg string, options ...styleoption) *style { s := \u0026amp;style{fg: fg, bg: bg} // und and zigzag are set to false // Apply all the styleoption functions returned from // field config functions. for _, opt := range options { opt(s) } return s } The users will use the code like this to instantiate style and update the optional fields:\nc := src.NewStyle( \u0026#34;\\033[31m\u0026#34;, \u0026#34;\\033[43m\u0026#34;, src.WithUnd(true), // Default is false, but we\u0026#39;re setting it to true src.WithZigzag(true), // Default is false ) The required fields fg and bg must be passed while constructing the option struct. The optional fields can be configured with the field config functions like WithUnd and WithZigzag.\nThe complete snippet looks as follows:\npackage src // We can keep the option struct private type style struct { fg string bg string und bool // Optional field zigzag bool // Optional field } // This can be private too since the users won\u0026#39;t need it directly type styleoption func(*style) // We only define public config functions for the optional fields func WithUnd(und bool) styleoption { return func(s *style) { s.und = und } } func WithZigzag(zigzag bool) styleoption { return func(s *style) { s.zigzag = zigzag } } // Options are variadic but the required fiels must be passed func NewStyle(fg, bg string, options ...styleoption) *style { // You can also initialize the optional values explicitly s := \u0026amp;style{fg: fg, bg: bg} for _, opt := range options { opt(s) } return s } I first came across this pattern in Rob Pike\u0026rsquo;s post on self-referential functions.\nVerdict While the functional constructor pattern is the most intriguing one among the three, I almost never reach for it unless I need my users to be able to configure large option structs with many optional fields. It\u0026rsquo;s rare and the extra indirection makes the code inscrutable. Also, it renders the IDE suggestions useless.\nIn most cases, you can get away with exporting the option struct Stuff and a companion function NewStuff to instantiate it. For another canonical example, see bufio.Read and bufio.NewReader in the standard library.\nFurther reading Functional options for friendly APIs - Dave Cheney Functional options pattern in Go - Matt Boyle ","permalink":"https://rednafi.com/go/configure-options/","summary":"\u003cp\u003eSuppose, you have a function that takes an option struct and a message as input. Then it\nstylizes the message according to the option fields and prints it. What\u0026rsquo;s the most sensible\nAPI you can offer for users to configure your function? Observe:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// app/src\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003epackage\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003esrc\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Option struct\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003etype\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eStyle\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estruct\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nx\"\u003eFg\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003estring\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// ANSI escape codes for foreground color\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nx\"\u003eBg\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003estring\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// Background color\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Display the message according to Style\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eDisplay\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003es\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"nx\"\u003eStyle\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003emsg\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003estring\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIn the \u003ccode\u003esrc\u003c/code\u003e package, the function \u003ccode\u003eDisplay\u003c/code\u003e takes a pointer to a \u003ccode\u003eStyle\u003c/code\u003e instance and a\n\u003ccode\u003emsg\u003c/code\u003e string as parameters. Then it decorates the \u003ccode\u003emsg\u003c/code\u003e and prints it according to the style\nspecified in the option struct. In the wild, I\u0026rsquo;ve seen 3 main ways to write APIs that let\nusers configure options:\u003c/p\u003e","title":"Configuring options in Go"},{"content":"I was curious to see if I could prototype a simple load balancer in a single Go script. Go\u0026rsquo;s standard library and goroutines make this trivial. Here\u0026rsquo;s what the script needs to do:\nSpin up two backend servers that\u0026rsquo;ll handle the incoming requests. Run a reverse proxy load balancer in the foreground. The load balancer will accept client connections and round-robin them to one of the backend servers; balancing the inbound load. Once a backend responds, the load balancer will relay the response back to the client. For simplicity, we\u0026rsquo;ll only handle client\u0026rsquo;s GET requests. Obviously, this won\u0026rsquo;t have SSL termination, advanced balancing algorithms, or session persistence like you\u0026rsquo;d get with Nginx or Caddy. The point is to understand the basic workflow and show how Go makes it easy to write this sort of stuff.\nArchitecture Here\u0026rsquo;s an ASCII art that demonstrates the grossly simplified end-to-end workflow:\n+----------------------------------------+ | Load balancer (8080) | | +----------------------------------+ | | | Request from client | | | +-----------------|----------------+ | | | Forward request | | | to backend | | v | | +----------------------------------+ | | | Load balancing | | | | +----------+ +----------+ | | | | | Backend | | Backend | | | | | | 8081 | | 8082 | | | | | +----------+ +----------+ | | | +-----------------|----------------+ | | | Distribute load | | v | | +----------------------------------+ | | | Backend Servers | | | | +----------+ +----------+ | | | | | Response | | Response | | | | | | body | | body | | | | | +----------+ +----------+ | | | +----------------------------------+ | | | Send response | | v | | +----------------------------------+ | | | Client receives response | | | +----------------------------------+ | +----------------------------------------+ The diagram shows a load balancer receiving client requests on port 8080. It distributes the requests between the backends, sending each request either to a backend running on port 8081 or 8082. The selected backend processes the incoming request and returns a response through the balancer. The balancer then routes the backend\u0026rsquo;s response back to the client.\nTools we\u0026rsquo;ll need Here are the stdlib tools we\u0026rsquo;ll be using. Everything will live in the main.go script:\n// main.go package main import (\u0026#34;fmt\u0026#34;; \u0026#34;io\u0026#34;; \u0026#34;net/http\u0026#34;; \u0026#34;sync\u0026#34;) A few global variables // main.go // ... truncated previous sections var ( backends = []string{ \u0026#34;http://localhost:8081/b8081\u0026#34;, \u0026#34;http://localhost:8082/b8082\u0026#34;, } currentBackend int backendMutex sync.Mutex ) The backends slice declares a list of backend server URLs that will be load-balanced between.\nThe currentBackend integer variable keeps track of the index of the backend server that handled the most recent request. This will be used later to perform the round-robin load balancing between the backends.\nThe backendMutex lock provides mutually exclusive access to the shared variables. We\u0026rsquo;ll see how it\u0026rsquo;s used when we write the load balancing algorithm.\nWriting the backend server The backend is a simple server that\u0026rsquo;ll just write a message to the connected client, denoting which server is handling the request.\n// main.go // ... truncated previous sections // Start a backend server on the specified port func startBackend(port int, wg *sync.WaitGroup) { // Signals the lb when a backend is done processing a request defer wg.Done() http.HandleFunc(fmt.Sprintf(\u0026#34;/b%d\u0026#34;, port), func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello from backend server on :%d\\n\u0026#34;, port) }) addr := fmt.Sprintf(\u0026#34;:%d\u0026#34;, port) fmt.Printf(\u0026#34;Backend is listening on :%d \\n\u0026#34;, port) err := http.ListenAndServe(addr, nil) if err != nil { fmt.Printf(\u0026#34;Error for server on :%d; %s\\n\u0026#34;, port, err) } } The startBackend function starts a backend HTTP server listening on a given port. It takes the port number and a sync.WaitGroup. When startBackend returns, it calls Done() on the wait group to signal the load balancer that the backend has finished processing a request. The function then registers a handler that responds with the port number. It starts listening and serving on the provided port, printing any errors. We\u0026rsquo;ll run this as goroutines to spin up two backends on ports 8081 and 8082.\nSelecting backend servers in a round-robin fashion When a request from a client hits the load balancer, it\u0026rsquo;ll need a way to figure out which backend server to relay the request to. Here\u0026rsquo;s how it does that:\n// main.go // ... truncated previous sections // Get the next backend server to forward the request to // in a round-robin fashion. This function is thread-safe func getNextBackend() string { backendMutex.Lock() defer backendMutex.Unlock() backend := backends[currentBackend] currentBackend = (currentBackend + 1) % len(backends) return backend } The getNextBackend() function implements round-robin load balancing across the backends slice in a thread-safe manner. It works like this:\nAcquire a lock on backendMutex to prevent concurrent access to the shared state. Read the index of the current backend server from currentBackend. Increment currentBackend to point to the next backend server. The modulo % operation wraps around the index to the start when it reaches past the end. Release the lock on backendMutex. Return the URL of the backend at the index we read in step 2. This allows each request handling goroutine to safely get the next backend server in a round-robin fashion. The mutex prevents race conditions where two goroutines try to read/write the shared currentBackend and backends state at the same time.\nThe mutex lock synchronizes access to the shared state across concurrent goroutines. This is necessary because Go\u0026rsquo;s HTTP server handles requests concurrently by default. Without the mutex, the goroutines could overwrite each other\u0026rsquo;s changes to currentBackend, leading to incorrect load balancing behavior.\nWriting the load-balancing server The load balancer itself is a server that sits between the backends and the clients. We can write its handler function as such:\n// main.go // ... truncated previous sections // Handle incoming requests and forward them to the backend func loadBalancerHandler(w http.ResponseWriter, r *http.Request) { // Pick a backend in round-robin fashion backend := getNextBackend() // Relay the client\u0026#39;s request to the backend resp, err := http.Get(backend) if err != nil { http.Error(w, \u0026#34;Backend Error\u0026#34;, http.StatusInternalServerError) return } defer resp.Body.Close() // Copy the backend response headers and propagate them to the client for key, values := range resp.Header { for _, value := range values { w.Header().Set(key, value) } } // Copy the backend response body and propagate it to the client io.Copy(w, resp.Body) } The loadBalancerHandler() function forwards incoming requests from the clients to the backend servers. First, it calls getNextBackend() to retrieve the next backend server. It then makes an HTTP GET request to that backend using http.Get().\nIf there are any errors calling the backend, it just returns a 500 error to the client. Otherwise, it copies the backend\u0026rsquo;s headers and response body into the response writer to propagate them back to the client.\nThis allows transparently load balancing each request across the backends in a round-robin fashion. The client only sees a single load balancer endpoint. Behind the scenes, requests are distributed to the dynamic backend servers based on round-robin ordering. Copying headers and response bodies ensures clients get the proper responses from the chosen backends.\nWiring them up together Finally, the main function here just starts the backend servers on port 8081-8082 and the load balancing server on port 8080:\n// main.go // ... truncated previous sections func main() { var wg sync.WaitGroup ports := []int{8081, 8082} // Starts the backend servers in the background for _, port := range ports { wg.Add(1) go startBackend(port, \u0026amp;wg) } // Starts the load balancer server in the foreground http.HandleFunc(\u0026#34;/\u0026#34;, loadBalancerHandler) fmt.Println(\u0026#34;Load balancer is listening on :8080\u0026#34;) err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) if err != nil { fmt.Printf(\u0026#34;Error: %s\\n\u0026#34;, err) } } Taking it for a spin You can find the self-contained complete implementation in this gist. Run the server in one terminal with:\ngo run main.go It\u0026rsquo;ll print the port numbers of the backend and the load-balancing servers:\nBackend is listening on :8082 Backend is listening on :8081 Load balancer is listening on :8080 Then from another console, make a few GET requests with curl:\nfor i in {1..4}; do curl http://localhost:8080/ done This prints:\nHello from backend server on :8081 Hello from backend server on :8082 Hello from backend server on :8081 Hello from backend server on :8082 Notice how the client requests are handled by different backends in an interleaving manner.\n","permalink":"https://rednafi.com/go/dummy-load-balancer/","summary":"\u003cp\u003eI was curious to see if I could prototype a simple load balancer in a single Go script. Go\u0026rsquo;s\nstandard library and goroutines make this trivial. Here\u0026rsquo;s what the script needs to do:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSpin up two backend servers that\u0026rsquo;ll handle the incoming requests.\u003c/li\u003e\n\u003cli\u003eRun a reverse proxy load balancer in the foreground.\u003c/li\u003e\n\u003cli\u003eThe load balancer will accept client connections and round-robin them to one of the\nbackend servers; balancing the inbound load.\u003c/li\u003e\n\u003cli\u003eOnce a backend responds, the load balancer will relay the response back to the client.\u003c/li\u003e\n\u003cli\u003eFor simplicity, we\u0026rsquo;ll only handle client\u0026rsquo;s GET requests.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eObviously, this won\u0026rsquo;t have SSL termination, advanced balancing algorithms, or session\npersistence like you\u0026rsquo;d get with \u003ca href=\"https://www.nginx.com/\"\u003eNginx\u003c/a\u003e or \u003ca href=\"https://caddyserver.com/\"\u003eCaddy\u003c/a\u003e. The point is to understand the basic\nworkflow and show how Go makes it easy to write this sort of stuff.\u003c/p\u003e","title":"Dummy load balancer in a single Go script"},{"content":"I was cobbling together a long-running Go script to send webhook messages to a system when some events occur. The initial script would continuously poll a Kafka topic for events and spawn worker goroutines in a fire-and-forget manner to make HTTP requests to the destination. This had two problems:\nIt could create unlimited goroutines if many events arrived quickly (no backpressure) It might overload the destination system by making many concurrent requests (no concurrency control) In Python, I\u0026rsquo;d use just asyncio.Semaphore to limit concurrency. I\u0026rsquo;ve previously written about limiting concurrency with semaphores. Turns out, in Go, you could do the same with a buffered channel. Here\u0026rsquo;s how the naive version without any concurrency control looks:\n// go 1.24 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) // pollKafka pretends to fetch a message from Kafka func pollKafka() string { time.Sleep(200 * time.Millisecond) // emulate poll delay return fmt.Sprintf(\u0026#34;kafka-msg-%d\u0026#34;, time.Now().UnixNano()) } // worker simulates doing something with a message func worker(id int, msg string) { fmt.Printf(\u0026#34;worker %d: sending webhook for message: %s\\n\u0026#34;, id, msg) time.Sleep(200 * time.Millisecond) } func main() { for id := 0; ; id++ { // Poll a new message from Kafka before spawning the worker msg := pollKafka() // Spawn a worker goroutine for each message — fire and forget go worker(id, msg) } } Running it gives you this:\nworker 0: sending webhook for message: kafka-msg-1759579628289116000 worker 1: sending webhook for message: kafka-msg-1759579629290305000 worker 2: sending webhook for message: kafka-msg-1759579630291584000 worker 3: sending webhook for message: kafka-msg-1759579631292667000 worker 4: sending webhook for message: kafka-msg-1759579632293768000 worker 5: sending webhook for message: kafka-msg-1759579633294909000 ^Csignal: interrupt The main function runs an infinite loop where it polls the upstream message queue continuously to collect new message. Once a new message arrives, it spawns a new worker goroutine in a fire-and-forget manner that actually sends the HTTP request to the desination endpoint.\nThe problem here is that this setup creates an unbounded number of goroutines. If Kafka produces messages faster than the workers can process them, the system will keep spawning new goroutines, eventually consuming all available memory and CPU. Also, it can overwhelm the destination system by sending too many requests at once if that doesn\u0026rsquo;t have any throttling mechanism in place. So we need a way to limit how many workers can run at once.\nTo fix this, we can use a buffered channel as a semaphore. The idea is to block before launching a new worker if too many are already running. This applies backpressure naturally and prevents unbounded spawning. Observe:\n// go 1.24 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) // pollKafka pretends to fetch a message from Kafka func pollKafka() string { time.Sleep(500 * time.Millisecond) // emulate poll delay return fmt.Sprintf(\u0026#34;kafka-msg-%d\u0026#34;, time.Now().UnixNano()) } // worker simulates doing something with a message func worker(id int, msg string) { fmt.Printf(\u0026#34;worker %d: sending webhook for message: %s\\n\u0026#34;, id, msg) time.Sleep(200 * time.Millisecond) } func main() { maxConcurrency := 2 sem := make(chan struct{}, maxConcurrency) // semaphore batchInterval := 1 * time.Second for id := 0; ; id++ { msg := pollKafka() // get a message first sem \u0026lt;- struct{}{} // acquire BEFORE spawning; applies backpressure // Wrap worker in closure to keep semaphore logic separate go func() { defer func() { \u0026lt;-sem }() // release when done worker(id, msg) }() // Apply backpressure if id%maxConcurrency == 0 \u0026amp;\u0026amp; id != 0 { fmt.Printf(\u0026#34;Limit reached, waiting %s...\\n\u0026#34;, batchInterval) time.Sleep(batchInterval) } } } Here, the buffered channel sem works as a semaphore that limits concurrency. Its capacity defines how many goroutines can run at the same time. Before spawning a worker, we try to send an empty struct into the channel. If the channel is full, that line blocks until a running worker finishes and releases its spot by reading from the channel. This ensures that only maxConcurrency workers run at once and prevents goroutine buildup.\nThe closure around the worker is intentional: it keeps concurrency management out of the worker itself. The worker only focuses on processing messages, while the outer function handles synchronization and throttling. This separation allows the caller to call the worker synchronously if needed. It also makes testing the worker function much easier. In general, it\u0026rsquo;s a good practice to push concurrency to the outer edge of your system so that the caller has the choice of leveraging concurrency or not.\nThe optional batch delay isn\u0026rsquo;t required for correctness, but it helps spread out requests so the downstream system isn\u0026rsquo;t flooded. Running the script shows that even though the loop is infinite, only two workers run at once, and there\u0026rsquo;s a short pause between each batch.\nworker 0: sending webhook for message: kafka-msg-1759580074075447000 worker 1: sending webhook for message: kafka-msg-1759580074576160000 Limit reached, waiting 1s... worker 2: sending webhook for message: kafka-msg-1759580075077279000 worker 3: sending webhook for message: kafka-msg-1759580076579356000 Limit reached, waiting 1s... worker 4: sending webhook for message: kafka-msg-1759580077079956000 worker 5: sending webhook for message: kafka-msg-1759580078581780000 Limit reached, waiting 1s... worker 6: sending webhook for message: kafka-msg-1759580079082375000 ^Csignal: interrupt The workers in this version are still getting spawned in a fire-and-forget manner but without leaking goroutines. When the concurrency limit is reached, the main loop blocks instead of spawning more workers. This applies natural backpressure to the producer, keeping the system stable even under heavy load.\nNow, you might want to add extra abstractions over the core behavior to make it more ergonomic. Here\u0026rsquo;s a pointer on how to do so. Effective Go also mentions this pattern briefly.\nFurther reading How to wait until buffered channel semaphore is empty ","permalink":"https://rednafi.com/go/limit-goroutines-with-buffered-channels/","summary":"\u003cp\u003eI was cobbling together a long-running Go script to send webhook messages to a system when\nsome events occur. The initial script would continuously poll a Kafka topic for events and\nspawn worker goroutines in a fire-and-forget manner to make HTTP requests to the\ndestination. This had two problems:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt could create unlimited goroutines if many events arrived quickly (no backpressure)\u003c/li\u003e\n\u003cli\u003eIt might overload the destination system by making many concurrent requests (no\nconcurrency control)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn Python, I\u0026rsquo;d use just \u003ccode\u003easyncio.Semaphore\u003c/code\u003e to limit concurrency. I\u0026rsquo;ve previously \u003ca href=\"/python/limit-concurrency-with-semaphore/\"\u003ewritten\nabout limiting concurrency with semaphores\u003c/a\u003e. Turns out, in Go, you could do the same with a\nbuffered channel. Here\u0026rsquo;s how the naive version without any concurrency control looks:\u003c/p\u003e","title":"Limit goroutines with buffered channels"},{"content":"A TOTP based 2FA system has two parts. One is a client that generates the TOTP code. The other part is a server. The server verifies the code. If the client and the server-generated codes match, the server allows the inbound user to access the target system. The code usually expires after 30 seconds and then, you\u0026rsquo;ll have to regenerate it to be able to authenticate.\nAs per RFC-6238, the server shares a base-32 encoded secret key with the client. Using this shared secret and the current UNIX timestamp, the client generates a 6-digit code. Independently, the server also generates a 6-digit code using the same secret string and its own current timestamp. If the user-entered client code matches the server-generated code, the auth succeeds. Otherwise, it fails. The client\u0026rsquo;s and the server\u0026rsquo;s current timestamp wouldn\u0026rsquo;t be an exact match. So the algorithm usually adjusts it for ~30 seconds duration.\nI wanted to see if I could write a TOTP client and use it like Google Authenticator to log into my 2FA-enabled GitHub account. Turns out Go\u0026rsquo;s standard library lets you do that with only a couple of lines of code. Here\u0026rsquo;s the fully annotated implementation:\npackage main import ( \u0026#34;crypto/hmac\u0026#34;; \u0026#34;crypto/sha1\u0026#34;; \u0026#34;encoding/base32\u0026#34;; \u0026#34;encoding/binary\u0026#34;; \u0026#34;strings\u0026#34;; ) func generateTOTP(secretKey string, timestamp int64) uint32 { // The base32 encoded secret key string is decoded to a byte slice base32Decoder := base32.StdEncoding.WithPadding(base32.NoPadding) secretKey = strings.ToUpper(strings.TrimSpace(secretKey)) // preprocess secretBytes, _ := base32Decoder.DecodeString(secretKey) // decode // The truncated timestamp / 30 is converted to an 8-byte big-endian // unsigned integer slice timeBytes := make([]byte, 8) binary.BigEndian.PutUint64(timeBytes, uint64(timestamp) / 30) // The timestamp bytes are concatenated with the decoded secret key // bytes. Then a 20-byte SHA-1 hash is calculated from the byte slice hash := hmac.New(sha1.New, secretBytes) hash.Write(timeBytes) // Concat the timestamp byte slice h := hash.Sum(nil) // Calculate 20-byte SHA-1 digest // AND the SHA-1 with 0x0F (15) to get a single-digit offset offset := h[len(h)-1] \u0026amp; 0x0F // Truncate the SHA-1 by the offset and convert it into a 32-bit // unsigned int. AND the 32-bit int with 0x7FFFFFFF (2147483647) // to get a 31-bit unsigned int. truncatedHash := binary.BigEndian.Uint32(h[offset:]) \u0026amp; 0x7FFFFFFF // Take modulo 1_000_000 to get a 6-digit code return truncatedHash % 1_000_000 } Use it as such:\n// Import \u0026#34;time\u0026#34; and \u0026#34;fmt\u0026#34; // ... func main() { // Collect it from a TOTP server like GitHub 2FA panel secretKey := \u0026#34;6AXIS2D4ST9CXAW2\u0026#34; // This is a fake one! now := time.Now().Unix() totpCode := generateTOTP(secretKey, now) fmt.Printf(\u0026#34;Current TOTP code: %06d\\n\u0026#34;, totpCode) } This prints the following code and will keep printing the same one for the next 30 seconds if you rerun the script multiple times:\nCurrent TOTP code: 134624 Here are the detailed implementation steps:\nTrim whitespace and convert the base32 encoded secret key string to uppercase Decode the preprocessed secret key from base32 to a byte slice Get the current timestamp, divide by 30, and convert it to an 8-byte big-endian unsigned integer Concatenate the timestamp integer bytes with the decoded secret key bytes Hash the concatenated bytes to get a 20-byte SHA-1 digest Get the last byte of the SHA-1 digest and AND it with 0x0F (15) to mask off all but the last 4 bits to get an offset index from 0-15 Use the offset index to truncate the SHA-1 digest to get a 32-bit unsigned integer AND the 32-bit integer with 0x7FFFFFFF (2147483647) to mask off the most significant bit and convert to an unsigned 31-bit integer Take modulo 1_000_000 of the 31-bit integer to get a 6-digit TOTP code Return the 6-digit TOTP code To test the implementation, I collected a secret key from GitHub\u0026rsquo;s 2FA panel. Then I logged into my account by inputting a TOTP code generated by this script. Worked flawlessly!\n","permalink":"https://rednafi.com/go/totp-client/","summary":"\u003cp\u003eA \u003ca href=\"https://www.twilio.com/docs/glossary/totp\"\u003eTOTP\u003c/a\u003e based 2FA system has two parts. One is a client that generates the TOTP code. The\nother part is a server. The server verifies the code. If the client and the server-generated\ncodes match, the server allows the inbound user to access the target system. The code\nusually expires after 30 seconds and then, you\u0026rsquo;ll have to regenerate it to be able to\nauthenticate.\u003c/p\u003e\n\u003cp\u003eAs per \u003ca href=\"https://datatracker.ietf.org/doc/html/rfc6238\"\u003eRFC-6238\u003c/a\u003e, the server shares a base-32 encoded secret key with the client. Using\nthis shared secret and the current UNIX timestamp, the client generates a 6-digit code.\nIndependently, the server also generates a 6-digit code using the same secret string and its\nown current timestamp. If the user-entered client code matches the server-generated code,\nthe auth succeeds. Otherwise, it fails. The client\u0026rsquo;s and the server\u0026rsquo;s current timestamp\nwouldn\u0026rsquo;t be an exact match. So the algorithm usually adjusts it for ~30 seconds duration.\u003c/p\u003e","title":"Writing a TOTP client in Go"},{"content":"I love Go\u0026rsquo;s implicit interfaces. While convenient, they can also introduce subtle bugs unless you\u0026rsquo;re careful. Types expected to conform to certain interfaces can fluidly add or remove methods. The compiler will only complain if an identifier anticipates an interface, but is passed a type that doesn\u0026rsquo;t implement that interface. This can be problematic if you need to export types that are required to implement specific interfaces as part of their API contract.\nHowever, there\u0026rsquo;s a way you can statically check interface conformity at compile time with zero runtime overhead. Turns out, this was always buried in Effective Go. Observe:\nimport \u0026#34;io\u0026#34; // Interface guard var _ io.ReadWriter = (*T)(nil) type T struct { //... } func (t *T) Read(p []byte) (n int, err error) { // ... } func (t *T) Write(p []byte) (n int, err error) { // ... } We\u0026rsquo;re checking if struct T implements the io.ReadWriter interface. It needs to have both Read and Write methods defined. The type conformity is explicitly checked via var _ io.ReadWriter = (*T)(nil). It verifies that a nil pointer to a value of type T conforms to the io.ReadWriter interface. The code will fail to compile if the type ever stops matching the interface.\nThis is only possible because nil values in Go can assume many different types as explained in Go 101. In this case, var _ io.ReadWriter = T{} will also work, but then you\u0026rsquo;ll have to fiddle with different zero values if the type isn\u0026rsquo;t a struct. One important thing to point out is that we\u0026rsquo;re using _ because we don\u0026rsquo;t want to accidentally refer to this nil pointer anywhere in our code. Also, trying to access any method on it will cause runtime panic.\nHere\u0026rsquo;s another example borrowed from Uber\u0026rsquo;s style guide:\nNo check:\ntype Handler struct { //... } func (h *Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) { //... } Check:\ntype Handler struct { // ... } // Interface guard var _ http.Handler = (*Handler)(nil) func (h *Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) { //... } Neat, but don\u0026rsquo;t abuse this. Effective Go warns:\nDon\u0026rsquo;t do this for every type that satisfies an interface, though. By convention, such declarations are only used when there are no static conversions already present in the code, which is a rare event.\nFurther reading Interface guards - Caddy docs Tweet by Matt Boyle ","permalink":"https://rednafi.com/go/interface-guards/","summary":"\u003cp\u003eI love Go\u0026rsquo;s implicit interfaces. While convenient, they can also introduce subtle bugs\nunless you\u0026rsquo;re careful. Types expected to conform to certain interfaces can fluidly add or\nremove methods. The compiler will only complain if an identifier anticipates an interface,\nbut is passed a type that doesn\u0026rsquo;t implement that interface. This can be problematic if you\nneed to export types that are required to implement specific interfaces as part of their API\ncontract.\u003c/p\u003e","title":"Interface guards in Go"},{"content":"I enjoy writing about software — the things I learn, the tools I use, and the work I do. Owing to the constraints of the corporate software world, more often than not, you can\u0026rsquo;t showcase your work or talk about them. At least that\u0026rsquo;s how it always has been throughout my career. At the same time, as you grow older and start having a life outside of the computer screen, you realize that working on OSS at the tail of a 40+ hour workweek is hard, and maintaining consistency is even harder. On that front, how do you keep track of your progress without losing your sense of purpose as the years fly by?\nI ameliorate this by factoring out the things I learn at or outside work and writing about them publicly. Countless times I\u0026rsquo;ve found myself looking for stuff on the web only to land on my own website. But this approach isn\u0026rsquo;t bulletproof: you rarely encounter situations where you get to write about some novel concepts or one of your brilliant epiphanies. Routinely, I find myself writing about just another tool or library that I\u0026rsquo;ve figured out how to use or another book that\u0026rsquo;s already considered cliché in my area of interest. Plus, there are already a ton of more detailed or clickbaity posts out there that cover the same ground. So what good will it do if you add another drop to the ocean? Who will even read it?\nThe most recent example of this is when I spent an hour going through the docs of log/slog package of Go 1.21 and another two listing out my most common use cases in Structured logging with slog. I wrote about it despite seeing countless examples of how to use it on the internet; some of them even have the exact same title as mine. But I did that anyway because it helped me echo out my own experience with the tool that I\u0026rsquo;ll be able to relive in the future should the need arise. The goal here was not to craft the perfect post for a select audience just to get some SEO points. Rather, I wanted to write this for myself, to scratch a very particular itch. If people find it useful, great, but if I find it useful at some point, even better.\nBut occasionally, I do experience those lightbulb moments that beget more original proses like Avoid template pattern in Python, which get highly lauded by the venerable orange site citizens. However, the general trend is that the majority of these pieces go completely unnoticed. This might be one of them too and that\u0026rsquo;s perfectly okay. Internet accolades are great, but they need not be the only reason you want to explore and share your thoughts on something. For me, the aim is to uphold a meticulous record of my odyssey, my own Da Vinci\u0026rsquo;s notebook, and this post is but another page within!\n","permalink":"https://rednafi.com/zephyr/writing-on-well-trodden-topics/","summary":"\u003cp\u003eI enjoy writing about software — the things I learn, the tools I use, and the work I do.\nOwing to the constraints of the corporate software world, more often than not, you can\u0026rsquo;t\nshowcase your work or talk about them. At least that\u0026rsquo;s how it always has been throughout my\ncareer. At the same time, as you grow older and start having a life outside of the computer\nscreen, you realize that working on OSS at the tail of a 40+ hour workweek is hard, and\nmaintaining consistency is even harder. On that front, how do you keep track of your\nprogress without losing your sense of purpose as the years fly by?\u003c/p\u003e","title":"Writing on well-trodden topics"},{"content":"Before the release of version 1.21, you couldn\u0026rsquo;t set levels for your log messages in Go without either using third-party libraries or writing your own boilerplates. Coming from Python, I\u0026rsquo;ve always found this odd, considering that this capability has been in the Python standard library forever. However, it seems like the new log/slog subpackage in Go allows you to do that and a whole lot more.\nApart from being able to add levels to log messages, slog also allows you to emit JSON-structured log messages and group them by certain attributes. The ability to do all this in-house is quite neat and I wanted to take it for a spin. The official documentation on this is on the terser side but still comprehensive. So, here, instead of repeating the same information, I wanted to write something for me that mainly highlights the most common cases.\nKickoff Here\u0026rsquo;s how you\u0026rsquo;d add levels to your log messages:\npackage main import ( \u0026#34;log/slog\u0026#34; ) func main() { slog.Debug(\u0026#34;a debug message\u0026#34;) slog.Info(\u0026#34;an info message\u0026#34;) slog.Warn(\u0026#34;a warning message\u0026#34;) slog.Error(\u0026#34;an error message\u0026#34;) } Running this will print the following output.\n2023/08/10 17:10:11 INFO an info message 2023/08/10 17:10:11 WARN a warning message 2023/08/10 17:10:11 ERROR an error message Notice how the concomitant local time and level are prepended to each log message. Also, observe that the DEBUG message is missing there. That\u0026rsquo;s because the default log handler will only print messages if the log level is INFO or higher. We\u0026rsquo;ll see how we can set custom log levels shortly. But before that here\u0026rsquo;s a quick overview of how the different components of slog work together.\nMachineries The slog package lets you create Logger instances. These instances have methods like Info() and Error() that you can call to log stuff. When you call one of these methods, it creates a Record from the data you passed in and sends it to a Handler. The Handler figures out what to actually do with the log — like print it somewhere or send it over the network. You can write your own or use one of the predefined TextHandler or JSONHandler to format your log output.\nThere\u0026rsquo;s a default Logger you can use right away with functions like Info() and Error() at the top level. Underneath, the Info() function calls the Logger.Info() method. This means you don\u0026rsquo;t need to create a Logger instance by hand just to start logging. You\u0026rsquo;ve already seen how we can use these top-level functions to send different levels of logs to the stdout.\nEach log entry has an associated severity level which is represented by an integer. The more severe the log level is, the higher the value of the integer will be. The default logger only emits LevelInfo or higher levels of log messages. Predefined levels have the following values:\nconst ( LevelDebug Level = -4 LevelInfo Level = 0 LevelWarn Level = 4 LevelError Level = 8 ) Using custom log handlers You can use predefined custom handlers to change the format of your log output. The following snippet creates a new Logger instance from a TextHandler instance and then uses that to print log messages to the stdout:\n// Define a new TextHandler h := slog.NewTextHandler(os.Stdout, nil) // Update the default Logger to use the new handler slog.SetDefault(slog.New(h)) // Use the logger as usual slog.Info(\u0026#34;an info message\u0026#34;) slog.Warn(\u0026#34;a warning message\u0026#34;) Running this prints:\ntime=2023-08-10T23:57:39.914-04:00 level=INFO msg=\u0026#34;an info message\u0026#34; time=2023-08-10T23:57:39.915-04:00 level=WARN msg=\u0026#34;a warning message\u0026#34; The NewTextHandler function has two arguments: the first one takes in a type that implements the io.Writer interface and the second one accepts a HandlerOptions struct. The HandlerOptions struct can be used to customize the output format. We can pass nil for this value if we don\u0026rsquo;t need to change the handler\u0026rsquo;s default output format.\nWe\u0026rsquo;re passing os.Stdout as the first argument to direct the log messages to stdout and nil as the second argument. The NewTextHandler returns a *slog.TextHandler struct pointer which is passed to slog.New to get a new Logger instance. Then we set this newly created Logger as the default one via the slog.SetDefault() function. Finally, the updated logger is used to print an info and a warning message. Notice how the TextHandler output records are constituted as key-value attribute pairs.\nPrinting log messages in JSON format Similar to NewTextHandler, NewJSONHandler can be used to create a JSONHandler, which prints the log records as JSON objects:\n// Define a new TextHandler h := slog.NewJSONHandler(os.Stdout, nil) // Update the default Logger to use the new handler slog.SetDefault(slog.New(h)) // Use the logger as usual slog.Info(\u0026#34;an info\u0026#34;) slog.Warn(\u0026#34;a warning\u0026#34;) This prints:\n{\u0026#34;time\u0026#34;:\u0026#34;2023-08-11T00:13:44.734365-04:00\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;an info\u0026#34;} {\u0026#34;time\u0026#34;:\u0026#34;2023-08-11T00:13:44.734505-04:00\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;WARN\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;a warning\u0026#34;} Changing log levels You\u0026rsquo;ve already seen that the default logger only prints log messages of level Info and up. We\u0026rsquo;ll need to define a custom log handler to change the default log level. Here\u0026rsquo;s an example that enables printing Debug messages:\nvar programLevel = new(slog.LevelVar) // Info by default h := slog.NewTextHandler( os.Stdout, \u0026amp;slog.HandlerOptions{Level: programLevel}, ) slog.SetDefault(slog.New(h)) programLevel.Set(slog.LevelDebug) // Update log level to Debug slog.Debug(\u0026#34;a debug message\u0026#34;) slog.Info(\u0026#34;an info message\u0026#34;) It\u0026rsquo;ll print:\ntime=2023-08-10T23:53:16.654-04:00 level=DEBUG msg=\u0026#34;a debug message\u0026#34; time=2023-08-10T23:53:16.654-04:00 level=INFO msg=\u0026#34;an info message\u0026#34; First, we create an instance of slog.LevelVar with the new allocator. Next, we create a TextHandler instance and the programLevel to the slog.HandlerOptions struct pointer. Then we create a new Logger instance as before and set that as the default logger. In the last step, the programLevel is updated so that it signals the handler to allow emitting Debug messages.\nDefining custom log levels Apart from Debug, Info, Warn, and Error, you can define your own custom log levels. Here\u0026rsquo;s an example of doing that with the default Logger instance:\n// Defining a few custom levels const ( logMeh = slog.Level(2) logFatal = slog.Level(13) ) // Getting the default logger logger := slog.Default() // Use the Log method on the logger and pass the log level logger.Log(nil, logMeh, \u0026#34;a meh message\u0026#34;) logger.Log(nil, logFatal, \u0026#34;a fatal message\u0026#34;) This will return:\n2023/08/11 00:45:35 INFO+2 a meh message 2023/08/11 00:45:35 ERROR+5 a fatal message Observe that you\u0026rsquo;ll have to use Logger.Log() to pass your custom log level. Another example with a custom log handler:\n// Defining a custom log level const logPanic = slog.Level(15) // Setting up a TextHandler h := slog.NewTextHandler(os.Stderr, nil) // Setting up a logger that uses the TextHandler logger := slog.New(h) // Use the Log method on the logger and pass the log level logger.Log(nil, logPanic, \u0026#34;a panic message\u0026#34;) This prints:\ntime=2023-08-11T00:52:08.903-04:00 level=ERROR+7 msg=\u0026#34;a panic message\u0026#34; Adding or removing log attributes Log attributes are just key-value pairs. The following example appends a new key and a value to the log message:\nslog.Info(\u0026#34;an info message\u0026#34;, \u0026#34;new_key\u0026#34;, \u0026#34;new_value\u0026#34;) 2023/08/11 01:10:18 INFO an info message new_key=new_value To remove attributes from log records, you\u0026rsquo;ll need to configure your custom handler and create a logger instance from that:\nReplaceAttr := func(group []string, a slog.Attr) slog.Attr { if a.Key == \u0026#34;time\u0026#34; { return slog.Attr{} } return slog.Attr{Key: a.Key, Value: a.Value} } // Before removing the time attribute h1 := slog.NewJSONHandler(os.Stdout, nil) slog.SetDefault(slog.New(h1)) slog.Info(\u0026#34;an info message\u0026#34;) // After removing the time attribute h2 := slog.NewJSONHandler( os.Stdout, \u0026amp;slog.HandlerOptions{ReplaceAttr: ReplaceAttr}, ) slog.SetDefault(slog.New(h2)) slog.Info(\u0026#34;an info message\u0026#34;) Running this will print the following. The time key no longer exists on the second log record:\n{ \u0026#34;time\u0026#34;:\u0026#34;2023-08-11T01:23:58.936984-04:00\u0026#34;, \u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;, \u0026#34;msg\u0026#34;:\u0026#34;an info message\u0026#34; } {\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;an info message\u0026#34;} The main focus here is the ReplaceAttr function which is used to transform or remove attributes before they are processed by a handler. It accepts two arguments: a slice of group names and an Attr struct. The group name allows attributes to be qualified into different scopes, which we won\u0026rsquo;t use right now. The Attr contains the Key and Value of the attribute that\u0026rsquo;s being logged.\nIn this case, ReplaceAttr checks if the attribute key is time and if so, returns an empty Attr struct, effectively signaling the handler not to include that attribute. If the key is not time, it returns the original Attr unchanged.\nAdding sticky attributes Sometimes you want to have a few common attributes that should persist across multiple log calls. This can be done via Logger.With() method:\n// Make the attributes sticky with Logger.With method logger := slog.Default().With(\u0026#34;sticky_key\u0026#34; , \u0026#34;sticky_value\u0026#34;) // Look how we don\u0026#39;t need to repeat sticky_key and sticky_value here logger.Info(\u0026#34;an info message\u0026#34;) logger.Error(\u0026#34;an error message\u0026#34;) It prints:\n2023/08/11 01:56:44 INFO an info message sticky_key=sticky_value 2023/08/11 01:56:44 ERROR an error message sticky_key=sticky_value The Logger.With() method accepts key-value pairs of attributes. This saves you from passing the same attributes over and over again to make them persist across multiple log calls.\nGrouping log attributes You can group the log attributes for better organization. Adding a group makes the attribute keys of a log record qualified by the group name. What qualify means here can vary depending on whether you\u0026rsquo;re using a TextHandler or a JSONHandler. Here\u0026rsquo;s an example that demonstrates both:\n// The first string is the group name and the remaining // strings are key-value attribute pairs group := slog.Group(\u0026#34;group_a\u0026#34;, \u0026#34;key_a\u0026#34;, \u0026#34;value_a\u0026#34;) // Grouping for default Logger slog.Info(\u0026#34;info message\u0026#34;, group) // For TextHandler textHandler := slog.NewTextHandler(os.Stdout, nil) textLogger := slog.New(textHandler) textLogger.Warn(\u0026#34;warning message\u0026#34;, group) // For JSONHandler jsonHandler := slog.NewJSONHandler(os.Stdout, nil) jsonLogger := slog.New(jsonHandler) jsonLogger.Error(\u0026#34;error message\u0026#34;, group) This prints:\n2023/08/11 16:41:12 INFO info message group_a.key_a=value_a time=2023-08-11T16:41:12.072-04:00 level=WARN msg=\u0026#34;warning message\u0026#34; \\ group_a.key_a=value_a { \u0026#34;time\u0026#34;: \u0026#34;2023-08-11T16:41:12.072635-04:00\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;ERROR\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;error message\u0026#34;, \u0026#34;group_a\u0026#34;: { \u0026#34;key_a\u0026#34;: \u0026#34;value_a\u0026#34; } } Here, in the case of the text logger, the log attribute key is qualified by the group name as group_a.key_a. On the other hand, the JSON logger emits the log record in a way where the group name group_a is used as the key of a nested object containing the {\u0026quot;key_a\u0026quot;: \u0026quot;value_a\u0026quot;} log attributes.\nMaking log groups sticky Akin to attributes, you can also make attribute group sticky with the Logger.WithGroup() method:\n// Default logger logger := slog.Default().WithGroup(\u0026#34;group_a\u0026#34;) logger.Info(\u0026#34;info message\u0026#34;, \u0026#34;key_b\u0026#34;, \u0026#34;value_b\u0026#34;) // Text logger textHandler := slog.NewTextHandler(os.Stdout, nil) textLogger := slog.New(textHandler).WithGroup(\u0026#34;group_a\u0026#34;) textLogger.Info(\u0026#34;info message\u0026#34;, \u0026#34;key_b\u0026#34;, \u0026#34;value_b\u0026#34;) // JSON logger jsonHandler := slog.NewJSONHandler(os.Stdout, nil) jsonLogger := slog.New(jsonHandler).WithGroup(\u0026#34;group_a\u0026#34;) jsonLogger.Info(\u0026#34;info message\u0026#34;, \u0026#34;key_b\u0026#34;, \u0026#34;value_b\u0026#34;) This returns:\n2023/08/11 16:11:30 INFO info message group_a.key_b=value_b time=2023-08-11T16:11:30.913-04:00 level=INFO msg=\u0026#34;info message\u0026#34; \\ group_a.key_b=value_b { \u0026#34;time\u0026#34;: \u0026#34;2023-08-11T16:11:30.913892-04:00\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;INFO\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;info message\u0026#34;, \u0026#34;group_a\u0026#34;: { \u0026#34;key_b\u0026#34;: \u0026#34;value_b\u0026#34; } } Directing logs to different sinks The predefined TextHandler and JSONHandler takes in a type that implements the io.Writer interface as the first argument. We can leverage this aspect to change the destination of a structured logger. The following example shows how you can direct the structured log stream to both stdout and a file:\ntype TeeWriter struct { stdout *os.File file *os.File } func (t *TeeWriter) Write(p []byte) (n int, err error) { n, err = t.stdout.Write(p) if err != nil { return n, err } n, err = t.file.Write(p) return n, err } func main() { file, _ := os.Create(\u0026#34;output.txt\u0026#34;) writer := \u0026amp;TeeWriter{ stdout: os.Stdout, file: file, } h := slog.NewTextHandler(writer, nil) logger := slog.New(h) logger.Info(\u0026#34;Hello, World!\u0026#34;) } The TeeWriter struct associates stdout and a file handle. It implements a custom Write method to write to both streams, enabling teeing of output. In main(), a TeeWriter instance is created with stdout and a file. A pointer to TeeWriter is then passed to the TextHandler. Next, the TextHandler is used to create a new Logger, so when the Logger logs, the messages go through the TextHandler\u0026rsquo;s TeeWriter and are written to both the console and a file via the custom Write method.\nLeveraging Attrs and Values for performance When using a logger, you can pass in key-value pairs called Attrs instead of separate keys and values. For example:\nslog.Info(\u0026#34;info message\u0026#34;, slog.Int(\u0026#34;some_int\u0026#34;, 7)) This is the same as:\nslog.Info(\u0026#34;info message\u0026#34;, \u0026#34;some_int\u0026#34;, 7) There are helper functions like Int(), String(), and Bool() to create Attrs for common types. You can also use Any() to make an Attr for any type.\nThe real benefit is that Attrs are more efficient than separate keys and values. So for max speed, we can use the LogAttrs() instead of Log().\nFor example:\nlogger.LogAttrs( nil, slog.LevelInfo, \u0026#34;info message\u0026#34;, slog.Int(\u0026#34;some int\u0026#34;, 7), ) This avoids extra allocations while giving the same result as:\nslog.Info(\u0026#34;info message\u0026#34;, \u0026#34;some int\u0026#34;, 7) ","permalink":"https://rednafi.com/go/structured-logging-with-slog/","summary":"\u003cp\u003eBefore the release of version 1.21, you couldn\u0026rsquo;t set levels for your log messages in Go\nwithout either using third-party libraries or writing your own boilerplates. Coming from\nPython, I\u0026rsquo;ve always found this odd, considering that this capability has been in the Python\nstandard library forever. However, it seems like the new \u003ccode\u003elog/slog\u003c/code\u003e subpackage in Go allows\nyou to do that and a whole lot more.\u003c/p\u003e\n\u003cp\u003eApart from being able to add levels to log messages, \u003ccode\u003eslog\u003c/code\u003e also allows you to emit\nJSON-structured log messages and group them by certain attributes. The ability to do all\nthis in-house is quite neat and I wanted to take it for a spin. The \u003ca href=\"https://pkg.go.dev/log/slog\"\u003eofficial documentation\u003c/a\u003e\non this is on the terser side but still comprehensive. So, here, instead of repeating the\nsame information, I wanted to write something for me that mainly highlights the most common\ncases.\u003c/p\u003e","title":"Go structured logging with slog"},{"content":"If you\u0026rsquo;re a manager, then there\u0026rsquo;s no shortage of information for you on how to conduct exit interviews. But there aren\u0026rsquo;t many resources that focus on how to handle them from an employee\u0026rsquo;s perspective. I\u0026rsquo;ve been meaning to write a quick piece that isn\u0026rsquo;t biased by anyone else\u0026rsquo;s experience and is short enough so that I can quickly jog my memory in the future should the need arise. While I\u0026rsquo;ve participated in a few of them over the past five years, this text doesn\u0026rsquo;t attempt to combat the inexorable recency bias that may have seeped into the writing.\nExit interviews are trickier than your typical run-of-the-mill one-on-ones, mostly because:\nIt typically means that you\u0026rsquo;re resigning voluntarily, and not getting fired By the time it happens, there\u0026rsquo;s usually no going back There are rarely any objectives that benefit you The gains are low but the stakes can be high Instead of throwing a wall of text sectioned by a bunch of headers, I\u0026rsquo;m intentionally laying out the actionable items as aphoristic assertions.\nAvoid it if you can Ask for a shorter one if you can\u0026rsquo;t Try to exit early if you can\u0026rsquo;t do either The less work your interviewer has put into the meeting, the better No point in rambling on why you\u0026rsquo;re leaving, be vague It\u0026rsquo;s a mistake to discuss next steps and future opportunities You can\u0026rsquo;t change the culture through one last meeting, so don\u0026rsquo;t try to Your objective opinions can cause more harm than good Last-minute feedback matters less than you think, they\u0026rsquo;ve heard these before A counteroffer is usually a bad idea for both parties If you sense a trap, silence is your friend Let it be awkward Don\u0026rsquo;t be a child or throw tantrums Be neutral and don\u0026rsquo;t react to tantrums Maintain mutual respect for both yourself and your employer Remember, it\u0026rsquo;s all business at the end of the day And finally, don\u0026rsquo;t burn any bridges if you don\u0026rsquo;t have to ","permalink":"https://rednafi.com/zephyr/notes-on-exit-interviews/","summary":"\u003cp\u003eIf you\u0026rsquo;re a manager, then there\u0026rsquo;s no shortage of information for you on how to conduct exit\ninterviews. But there aren\u0026rsquo;t many resources that focus on how to handle them from an\nemployee\u0026rsquo;s perspective. I\u0026rsquo;ve been meaning to write a quick piece that isn\u0026rsquo;t biased by anyone\nelse\u0026rsquo;s experience and is short enough so that I can quickly jog my memory in the future\nshould the need arise. While I\u0026rsquo;ve participated in a few of them over the past five years,\nthis text doesn\u0026rsquo;t attempt to combat the inexorable recency bias that may have seeped into\nthe writing.\u003c/p\u003e","title":"Notes on exit interviews"},{"content":"The 100k context window of Claude 2 has been a huge boon for me since now I can paste a moderately complex problem to the chat window and ask questions about it. In that spirit, it recently refactored some pretty gnarly conditional logic for me in such an elegant manner that it absolutely blew me away. Now, I know how bitmasks work and am aware of the existence of enum.Flag in Python. However, it never crossed my mind that flags can be leveraged to trim conditional branches in such a clever manner that Claude illustrated. But once I looked at the proposed solution, the whole thing immediately clicked for me.\nThe conundrum Here\u0026rsquo;s a problem that\u0026rsquo;s similar to what I was trying to solve. Let\u0026rsquo;s say we have instances of a Client entity that need to be notified when some special event occurs in our system. The notification can happen in three ways: email, webhook, and postal mail. These are the three attributes on the Client class that determine which notification method will be used:\n@dataclass class Client: email: str url: str address: str The business logic requires that the system must abide by the following rules while sending notifications:\nIf only email is populated, send an email. If only url is populated, send a webhook. If only address is populated, send a postal mail. If email and url are populated, send an email and a webhook. If email and address are populated, only send an email. If url and address are populated, only send a webhook. If all three are populated, send both an email and a webhook. At least one attribute must be populated, or it\u0026rsquo;s an error. Notice how the business logic wants to minimize sending notifications via postal mail. Postal mails are expensive and will only be sent if address is the only attribute on the Client instance. In any other cases, emails and webhooks are preferred.\nFirst shot The notify function takes in a Client object and sprouts a few conditional branches to send notifications while maintaining the business constraints.\ndef notify(client: Client) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Apply business logic and invoke notification handlers.\u0026#34;\u0026#34;\u0026#34; if client.email and not client.url and not client.address: send_email() elif client.url and not client.email and not client.address: send_webhook() elif client.address and not client.email and not client.url: send_mail() elif client.email and client.url and not client.address: send_email() send_webhook() elif client.email and client.address and not client.url: send_email() elif client.url and client.address and not client.email: send_webhook() elif client.email and client.url and client.address: send_email() send_webhook() else: raise ValueError(\u0026#34;at least one attribute must be populated\u0026#34;) Whoa! Lots of if-else branches for such a simple scenario. Since there are 3 attributes in the complete set, we have to make sure we\u0026rsquo;re writing 2^3=8 branches to cover all the possible subsets. For 4, 5, 6 \u0026hellip; attributes, the number of branches will increase as powers of 2: 2^4=16, 2^5=32, 2^6=64 \u0026hellip; and so on. Then our tests will need to be able to verify each of these branches. We can try to apply De Morgan\u0026rsquo;s law to simplify some of the negation logic.\nDe Morgan\u0026rsquo;s laws allow us to take the negation of a conditional statement and distribute it across the operators, changing ANDs to ORs and vice versa, and flipping the negation of each component. This can help simplify complex boolean logic statements.\nSo this:\nif client.email and not client.url and not client.address: ... Can become:\nif client.email and not (client.url or client.address): ... However, that still doesn\u0026rsquo;t reduce the number of branches. Bitmasks can help us to get out of this pothole.\nA quick primer on bitwise operations \u0026amp; bitmasking Bitwise operations allow manipulating numbers at the individual bit level. This is useful for compactly storing and accessing data, performing fast calculations, and implementing low-level algorithms. Here\u0026rsquo;s a list of bitwise operations:\nBitwise AND (\u0026amp;): Takes two numbers and performs the logical AND operation on each pair of corresponding bits. Returns a number where a bit is 1 only if that bit is 1 in both input numbers.\nBitwise OR (|): Takes two numbers and performs the logical OR operation on each pair of corresponding bits. Returns a number where a bit is 1 if that bit is 1 in either or both input numbers.\nBitwise XOR (^): Takes two numbers and performs the logical XOR (exclusive OR) operation on each pair of corresponding bits. Returns a number where a bit is 1 if that bit is 1 in exactly one of the input numbers (but not both).\nBitwise NOT (~): Takes a single number and flips all its bits.\nLeft shift (\u0026laquo;): Shifts the bits of a number to the left by a specified number of positions. Zeros are shifted in on the right. Equivalent to multiplying by 2^n where n is the number of positions shifted.\nRight shift (\u0026raquo;): Shifts the bits of a number to the right by a specified number of positions. Zeros are shifted in on the left. Equivalent to integer division by 2^n.\nHere\u0026rsquo;s an example displaying these operators:\na = 60 # 60 = 0011 1100 b = 13 # 13 = 0000 1101 print(a \u0026amp; b) # 12 = 0000 1100 (0011 1100 \u0026amp; 0000 1101 = 0000 1100) print(a | b) # 61 = 0011 1101 (0011 1100 | 0000 1101 = 0011 1101) print(a ^ b) # 49 = 0011 0001 (0011 1100 ^ 0000 1101 = 0011 0001) print(~a) # -61 = 1100 0011 (~0011 1100 = 1100 0011) print(a \u0026lt;\u0026lt; 2) # 240 = 1111 0000 (0011 1100 \u0026lt;\u0026lt; 2 = 1111 0000) print(a \u0026gt;\u0026gt; 2) # 15 = 0000 1111 (0011 1100 \u0026gt;\u0026gt; 2 = 0000 1111) Bitmasks are integers that represent a set of flags using bits as boolean values. Bitmasking uses bitwise operators to manipulate and access these flags. A common use of bitmasks is to compactly store multiple boolean values or options in a single integer, where each bit position has a specific meaning if it is 1. In the next section, we\u0026rsquo;ll use this capability to clip the conditional statements in the notify function.\nFor example, here\u0026rsquo;s a bitmask representing text style options:\n# Flags BOLD = 1 # 0000 0001 ITALIC = 2 # 0000 0010 UNDERLINE = 4 # 0000 0100 # Bitmask STYLE = BOLD | ITALIC # 0000 0011 - bold and italic We use powers of 2 (1, 2, 4, 8, etc.) for the flag values so that each bit position corresponds to a single flag, and the flags can be combined using bitwise OR without overlapping. This allows testing and accessing each flag independently:\nhas_bold = STYLE \u0026amp; BOLD == BOLD # True has_italic = STYLE \u0026amp; ITALIC == ITALIC # True has_underline = STYLE \u0026amp; UNDERLINE == UNDERLINE # False And toggle an option on or off using XOR:\nSTYLE ^= BOLD # Toggles BOLD bit on/off You can do a ton of other cool stuff with bitwise operations and bitmasks. However, this is pretty much all we need to know to curtail the twisted conditional branching necessitated by the business logic. Check out this incredibly in-depth Python bitwise operators article from Real Python if you want to dig deeper.\nPruning conditional branches with flags With all the intros and primers out of the way, we can now start working towards making the notify function more tractable and testable. We\u0026rsquo;ll do that in 3 phases:\nFirst, we\u0026rsquo;re gonna define a flag-type enum called NotifyStatus which will house all the valid states our notification system can be in. Any state that\u0026rsquo;s not explicitly defined as an enum variant is invalid.\nSecond, we\u0026rsquo;ll write a function named get_notify_status that\u0026rsquo;ll take in a Client object as input, apply the business logic and return the appropriate NotifyStatus enum variant. This function won\u0026rsquo;t be responsible for dispatching the actual notification handlers; rather, it\u0026rsquo;ll just map the attribute values of the Client instance to a fitting enum variant. We do this to keep the core business logic devoid of any external dependencies — following Gary Bernhardt\u0026rsquo;s functional core, imperative shell ethos.\nFinally, we\u0026rsquo;ll define the notify function that\u0026rsquo;ll just accept the enum variant returned by the previous function and invoke the desired notification handlers.\nThe NotifyStatus enum is defined as follows:\nclass NotifyStatus(Flag): # Valid primary variants (flags) EMAIL = 1 URL = 2 ADDRESS = 4 # Valid composite variants (bitmasks) EMAIL_URL = EMAIL | URL EMAIL_ADDRESS = EMAIL | ADDRESS URL_ADDRESS = URL | ADDRESS EMAIL_URL_ADDRESS = EMAIL | URL | ADDRESS Here, the EMAIL, URL, and ADDRESS variants correspond to the eponymous attributes on the Client instance. Then we define the composite variants (bitmasks) to compactly represent the valid states the system can be in. For example, EMAIL_URL = EMAIL | URL means that on the Client instance, email and url attributes are populated but address isn\u0026rsquo;t. Likewise, EMAIL_URL_ADDRESS denotes that all the attributes are populated. The biggest benefit we get from this is that we don\u0026rsquo;t need to write the negation logic explicitly; the bitmasks encode that information inherently. This representation will grossly simplify the implementation of the business logic.\nNow, let\u0026rsquo;s write the get_notify_status function that\u0026rsquo;ll take in an instance of Client and return the appropriate NotifyStatus variant based on our business logic:\ndef get_notify_status(client: Client) -\u0026gt; NotifyStatus: status = 0 if client.email: status |= NotifyStatus.EMAIL.value if client.url: status |= NotifyStatus.URL.value if client.address: status |= NotifyStatus.ADDRESS.value if status == 0: raise ValueError(\u0026#34;Invalid status\u0026#34;) return NotifyStatus(status) This is the full implementation of our business logic in its entirety. It checks which of the notification attributes among email, url, and address are populated on the Client object. For each one that is populated, it picks the corresponding variant from the NotifyStatus enum and sets the variant bit in the status integer using bitwise OR. If all three attributes are empty, it raises a ValueError. The final value of status is then used to return the correct NotifyStatus enum variant.\nOn the last step, the notify function can take the NotifyStatus variant returned by the get_notify_status function and dispatch the correct notification handlers like this:\ndef notify(notify_status: NotifyStatus) -\u0026gt; None: # Mapping between enum variants and notification handlers actions = { NotifyStatus.EMAIL: [send_email], NotifyStatus.URL: [send_webhook], NotifyStatus.ADDRESS: [send_mail], NotifyStatus.EMAIL_URL: [send_email, send_webhook], NotifyStatus.EMAIL_ADDRESS: [send_email], NotifyStatus.URL_ADDRESS: [send_webhook], NotifyStatus.EMAIL_URL_ADDRESS: [send_email, send_webhook], } if notify_status not in actions: raise ValueError(\u0026#34;invalid notify status\u0026#34;) for action in actions[notify_status]: action() Observe how we\u0026rsquo;ve totally eliminated conditional statements from the notify function. The key takeaway here is that the program flow is now flatter and easier to follow. The core business logic is neatly tucked inside the get_notify_status routine, and the NotifyStatus enum explicitly defines all the valid states that the system can be in. This also means that if a new notification channel pops up, all we\u0026rsquo;ll need to do is update three flat constructs and write the corresponding tests instead of battling with the twisted conditional statements that we started with. Not too shabby, eh?\n","permalink":"https://rednafi.com/python/tame-conditionals-with-bitmasks/","summary":"\u003cp\u003eThe 100k context window of \u003ca href=\"https://www.anthropic.com/index/claude-2\"\u003eClaude 2\u003c/a\u003e has been a huge boon for me since now I can paste a\nmoderately complex problem to the chat window and ask questions about it. In that spirit, it\nrecently refactored some pretty gnarly conditional logic for me in such an elegant manner\nthat it absolutely blew me away. Now, I know how \u003ca href=\"https://stackoverflow.com/questions/10493411/what-is-bit-masking\"\u003ebitmasks\u003c/a\u003e work and am aware of the\nexistence of \u003ca href=\"https://docs.python.org/3/library/enum.html#enum.Flag\"\u003eenum.Flag\u003c/a\u003e in Python. However, it never crossed my mind that flags can be\nleveraged to trim conditional branches in such a clever manner that Claude illustrated. But\nonce I looked at the proposed solution, the whole thing immediately clicked for me.\u003c/p\u003e","title":"Taming conditionals with bitmasks"},{"content":"This morning, while browsing Hacker News, I came across a neat trick for sharing data via DNS TXT records. It can be useful for propagating a small amount of data in environments that restrict IP but allow DNS queries, or to bypass censorship.\nTo test this out, I opened my domain registrar\u0026rsquo;s panel and created a new TXT type DNS entry with a base64 encoded message containing the poem A Poison Tree by William Blake. The message can now be queried and decoded with the following shell command:\ndig +short _poem.rednafi.com TXT | sed \u0026#39;s/[\\\u0026#34; ]//g\u0026#39; | base64 -d The command uses dig to query a TXT DNS record for _poem.rednafi.com, removes any double quotes and spaces from the record value via sed, and then decodes the base64-encoded value via base64 to retrieve the original plaintext message that was stored in the TXT record. Running this will return the decoded content of the record:\nI was angry with my friend; I told my wrath, my wrath did end. I was angry with my foe: I told it not, my wrath did grow. And I watered it in fears, Night \u0026amp; morning with my tears: And I sunned it with smiles, And with soft deceitful wiles. And it grew both day and night. Till it bore an apple bright. And my foe beheld it shine, And he knew that it was mine. And into my garden stole, When the night had veiled the pole; In the morning glad I see; My foe outstretched beneath the tree. You can also encode image data and retrieve it in a similar manner. If your data is too large to fit in a single record, you can split it into multiple records and concatenate them on the receiving end.\nHowever, there are some limitations to this approach. RFC 1035 says that the total size of a DNS resource record cannot exceed 65535 bytes. Also, the maximum length of the actual text value in a single TXT record is 255 bytes or characters. This doesn\u0026rsquo;t give us much room to tunnel large amounts of data. Plus, DNS has well-known vulnerabilities like MITM attacks, injection issues, cache poisoning, and DoS. So I\u0026rsquo;d refrain from transferring any data in this manner that requires a layer of security. Protocols like DANE and DNSSEC aim to address some of these concerns but their adoption is spotty at best. Still, I found the idea of using DNS records as a simple database quite clever!\n","permalink":"https://rednafi.com/misc/dns-record-to-share-text/","summary":"\u003cp\u003eThis morning, while browsing Hacker News, I came across a neat trick for \u003ca href=\"https://news.ycombinator.com/item?id=36754366\"\u003esharing data via\nDNS TXT records\u003c/a\u003e. It can be useful for propagating a small amount of data in environments\nthat restrict IP but allow DNS queries, or to bypass censorship.\u003c/p\u003e\n\u003cp\u003eTo test this out, I opened my domain registrar\u0026rsquo;s panel and created a new TXT type DNS entry\nwith a base64 encoded message containing the poem \u003cstrong\u003eA Poison Tree\u003c/strong\u003e by William Blake. The\nmessage can now be queried and decoded with the following shell command:\u003c/p\u003e","title":"Using DNS record to share text data"},{"content":"Unless I\u0026rsquo;m hand rolling my own ORM-like feature or validation logic, I rarely need to write custom descriptors in Python. The built-in descriptor magics like @classmethod, @property, @staticmethod, and vanilla instance methods usually get the job done. However, every time I need to dig my teeth into descriptors, I reach for this fantastic Descriptor how-to guide by Raymond Hettinger. You should definitely set aside the time to read it if you haven\u0026rsquo;t already. It has helped me immensely to deepen my understanding of how many of the fundamental language constructs are wired together underneath.\nDescriptors are considered fairly advanced Python features and can easily turn into footguns if used carelessly. Recently, while working on an app with a descriptor-based data validator, I discovered a subtle but obvious bug that was hemorrhaging memory all across the app. The app was using a descriptor to validate class variables while simultaneously tracking instances where validation occurred. This validator was being used all over the codebase, so it slowly started blowing up memory usage in the background. The problem is that it was keeping hard references to everything it validated, so none of those objects could get garbage collected. But the really sneaky thing was how slowly and secretly the problem happened — the leakage built up bit by bit over time even when people used the validator in totally innocuous ways.\nHere\u0026rsquo;s a simpler example of a validation descriptor that tracks the instances it\u0026rsquo;s applied to:\nclass Within: # The instances are tracked here _seen = {} def __init__(self, min, max): self.min = min self.max = max def __set_name__(self, instance, name): self.name = name def __get__(self, instance, instance_type): return instance.__dict__[self.name] def __set__(self, instance, value): if not self.min \u0026lt;= value \u0026lt;= self.max: raise ValueError( f\u0026#34;{value} is not within {self.min} and {self.max}\u0026#34; ) instance.__dict__[self.name] = value # Track the instances that have been seen. # This is the memory leak. self._seen[instance] = value The Within descriptor validates that the values assigned to instance attributes are within a specified min and max range. It does this by implementing the __set__ and __get__ dunder methods. When the descriptor is accessed via instance.attrname, the __get__ method is called which returns the value from the instance\u0026rsquo;s dict. When a value is assigned via instance.attrname = value, the __set__ method is called which validates the value is within the min/max bounds before setting it on the instance. A memory leak occurs because the _seen dict keeps a reference to every instance the descriptor has been accessed on. This prevents the instances from being garbage collected even if there are no other references to them. You can use the descriptor and observe the memory leakage like this:\nimport gc class Exam: math = Within(0, 100) physics = Within(0, 100) chemistry = Within(0, 100) def __init__(self, math, physics, chemistry): self.math = math self.physics = physics self.chemistry = chemistry if __name__ == \u0026#34;__main__\u0026#34;: exam = Exam(30, 50, 40) exam.math = 60 # Delete the exam instance del exam # Force garbage collection gc.collect() # Check the strong reference to the deleted instance print(tuple(Within._seen.items())) Here, we\u0026rsquo;re defining an Exam class that uses the Within descriptor to apply constraints on the values of the math, physics, and chemistry class variables. Then we initialize the class instance and mutate the math attribute to demonstrate that the validator is working as expected. The instance of the Exam class is saved to the _seen dictionary of the descriptor when the __set__ method is called. Next, we delete the Exam instance and force garbage collection. However, when you run the snippet, you\u0026rsquo;ll see that it prints the following:\n((\u0026lt;__main__.Exam object at 0x10466ca10\u0026gt;, 60),) This indicates that although we\u0026rsquo;ve deleted the Exam instance, it can\u0026rsquo;t be fully garbage collected since the Within descriptor\u0026rsquo;s _seen dictionary holds a strong reference to it.\nDispel the malady Once I spotted the bug, the solution was fairly simple. Don\u0026rsquo;t keep strong references to the class instances if you don\u0026rsquo;t need to. Also, use a more robust tool like Pydantic to perform validation but I digress here! Using a weakref.WeakKeyDictionary instead of a regular dict for _seen would prevent the memory leakage by avoiding strong references to the deleted instances. Since WeakKeyDictionary holds weak references to the keys, if all other strong references to an instance are deleted, the garbage collector can reclaim it. The weak reference in WeakKeyDictionary won\u0026rsquo;t keep the instance alive. Here\u0026rsquo;s how you\u0026rsquo;d modify Within to fix the issue:\nfrom weakref import WeakKeyDictionary class Within: _seen = WeakKeyDictionary() # Drop in dict replacement def __init__(self, min, max): ... def __set_name__(self, instance, name): ... def __get__(self, instance, instance_type): ... def __set__(self, instance, value): ... The modified descriptor is a drop-in replacement for the previous one — minus the memory leakage issue. So in the last snippet, when exam is deleted and the gc is called, weakref allows the instance to be garbage collected correctly instead of remaining in memory due to the strong reference in _seen. The weak reference doesn\u0026rsquo;t interfere with gc freeing up the memory as desired. If you run the demonstration snippet again, this time you\u0026rsquo;ll see that once we force the gc to collect the garbage, the _seen container gets emptied out.\nexam = Exam(30, 50, 40) exam.math = 60 # Delete the exam instance del exam # Force garbage collection gc.collect() # Check the strong reference to the deleted instance print(tuple(Within._seen.items())) This will print an empty tuple:\n() This also means that now Within will only keep track of instances that are alive in memory.\n","permalink":"https://rednafi.com/python/memory-leakage-in-descriptors/","summary":"\u003cp\u003eUnless I\u0026rsquo;m hand rolling my own ORM-like feature or validation logic, I rarely need to write\ncustom descriptors in Python. The built-in descriptor magics like \u003ccode\u003e@classmethod\u003c/code\u003e,\n\u003ccode\u003e@property\u003c/code\u003e, \u003ccode\u003e@staticmethod\u003c/code\u003e, and vanilla instance methods usually get the job done.\nHowever, every time I need to dig my teeth into descriptors, I reach for this fantastic\n\u003ca href=\"https://docs.python.org/3/howto/descriptor.html\"\u003eDescriptor how-to guide\u003c/a\u003e by Raymond Hettinger. You should definitely set aside the time to\nread it if you haven\u0026rsquo;t already. It has helped me immensely to deepen my understanding of how\nmany of the fundamental language constructs are wired together underneath.\u003c/p\u003e","title":"Memory leakage in Python descriptors"},{"content":"Python offers a ton of ways like os.system or os.spawn* to create new processes and run arbitrary commands in your system. However, the documentation usually encourages you to use the subprocess module for creating and managing child processes. The subprocess module exposes a high-level run() function that provides a simple interface for running a subprocess and waiting for it to complete. It accepts the command to run as a list of strings, starts the subprocess, waits for it to finish, and then returns a CompletedProcess object with information about the result. For example:\nimport subprocess # Here, result is an instance of CompletedProcess result = subprocess.run( [\u0026#34;ls\u0026#34;, \u0026#34;-lah\u0026#34;], capture_output=True, encoding=\u0026#34;utf-8\u0026#34; ) # No exception means clean exit result.check_returncode() print(result.stdout) This prints:\ndrwxr-xr-x 4 rednafi staff 128B Jul 8 12:10 .. -rw-r--r--@ 1 rednafi staff 250B Jul 8 12:10 .editorconfig drwxr-xr-x@ 16 rednafi staff 512B Jul 13 14:47 .git drwxr-xr-x@ 4 rednafi staff 128B Jul 8 12:10 .github ... This works great when you\u0026rsquo;re carrying out simple and synchronous workflows, but it doesn\u0026rsquo;t offer enough flexibility when you need to fork multiple processes and want the processes to run in parallel. I was working on a project where I wanted to glue a bunch of programs together with Python and needed a way to run composite shell commands with pipes, e.g. echo 'foo\\nbar' | grep 'foo'. So I got curious to see how I could emulate that in Python.\nTurns out you can do that easily with subprocess.Popen. This function allows for more control over the subprocess. It starts the process and returns a Popen object immediately, without waiting for the command to complete. This allows you to continue executing code while the subprocess runs in parallel. Popen has methods like poll() to check if the process has finished, wait() to wait for completion, and communicate() for interacting with stdin/stdout/stderr. For example:\nimport subprocess import time procs = [] for ip in (\u0026#34;1.1.1.1\u0026#34;, \u0026#34;8.8.8.8\u0026#34;): print(f\u0026#34;Pinging {ip}...\u0026#34;) proc = subprocess.Popen([\u0026#34;ping\u0026#34;, \u0026#34;-c1\u0026#34;, ip]) procs.append(proc) print(f\u0026#34;Process {proc.pid} started\u0026#34;) # Do other stuff here while ping is running print(\u0026#34;Napping for a second...\u0026#34;) time.sleep(1) # Wait for the processes to finish for proc in procs: proc.communicate() print(f\u0026#34;Process {proc.pid} finished with code {proc.returncode}\u0026#34;) The above example shows how you can fire off subprocess tasks to run in parallel, let them chug along in the background, do other stuff, and then collect the results at the end when you need them. The goal here is to ping a couple of IP addresses in parallel using the subprocess module. First, it creates an empty list to store the processes. Then it loops through the IPs, printing a message and kicking off a ping for each one using Popen() so they run asynchronously in the background. The Popen objects get appended to the procs list.\nAfter starting the pings, it simulates doing other work by sleeping for a second. Then it loops through the processes again, waits for each one to finish with communicate(), and prints out the process ID and return code for each ping. Running the script will give you the following result (truncated for brevity):\nPinging 1.1.1.1... Process 76242 started Pinging 8.8.8.8... Process 76243 started Napping for a second... 64 bytes from 8.8.8.8: icmp_seq=0 ttl=56 time=26.305 ms 64 bytes from 1.1.1.1: icmp_seq=0 ttl=54 time=27.365 ms --- 8.8.8.8 ping statistics --- 1 packets transmitted, 1 packets received, 0.0% packet loss --- 1.1.1.1 ping statistics --- round-trip min/avg/max/stddev = 26.305/26.305/26.305/0.000 ms Process 76242 finished with code 0 Process 76243 finished with code 0 Now that we can run processes asynchronously and gather results, I\u0026rsquo;ll demonstrate how I emulated a composite UNIX command using that technique.\nEmulating UNIX pipes Say you want to emulate the following shell command:\nps -ef | head -5 I\u0026rsquo;m running MacOS. So this returns:\nUID PID PPID C STIME TTY TIME CMD 0 1 0 0 Fri04PM ?? 23:45.79 /sbin/launchd 0 353 1 0 Fri04PM ?? 3:26.62 /usr/libexec/logd 0 354 1 0 Fri04PM ?? 0:00.09 /usr/libexec/smd 0 355 1 0 Fri04PM ?? 0:25.56 /usr/libexec/UserEventAgent (System) The ps -ef command outputs a full list of running processes, then the pipe symbol sends that output as input to the head -5 command, which reads the first 5 lines from that input and prints just those, essentially slicing off the top 5 processes. We can emulate this in Python as follows:\nimport subprocess # Run \u0026#39;ps -ef\u0026#39; and pipe the output to \u0026#39;head -n 5\u0026#39; ps_cmd = subprocess.Popen([\u0026#34;ps\u0026#34;, \u0026#34;-ef\u0026#34;], stdout=subprocess.PIPE) # Run \u0026#39;head -n 5\u0026#39; and pipe the output of \u0026#39;ps -ef\u0026#39; to it head_cmd = subprocess.Popen( [\u0026#34;head\u0026#34;, \u0026#34;-n\u0026#34;, \u0026#34;5\u0026#34;], stdin=ps_cmd.stdout, stdout=subprocess.PIPE, encoding=\u0026#34;utf-8\u0026#34;, ) stdout, stderr = head_cmd.communicate() print(stdout) This snippet uses the subprocess.Popen to run shell commands and pipe the outputs between them. First, ps_cmd executes ps -ef and sends the full output to the subprocess.PIPE buffer. Next, head_cmd runs head -n 5. The stdin of head_cmd is set to the stdout of ps_cmd. This pipes the stdout from ps_cmd as input to head_cmd. Finally, head_cmd.communicate() runs the composite command and waits for the whole thing to finish. The final output of this snippet is the same as the ps -ef | head -5 command.\nHere\u0026rsquo;s another example where we\u0026rsquo;ll emulate the sha256sum \u0026lt; \u0026lt;(echo 'foo') command. On the left side, sha256sum computes the SHA-256 cryptographic hash of an input. The construct \u0026lt;(echo 'foo') creates a temporary file descriptor containing the output \u0026lsquo;foo\u0026rsquo; from echo, which is then redirected via \u0026lt; as standard input to sha256sum. Together this computes and prints the SHA-256 hash of the input string without needing an actual file. In this particular case, we want to compute the hash of 3 different inputs in parallel by spawning three separate processes.\nimport subprocess import os def calculate_hash(plaintext: bytes) -\u0026gt; subprocess.Popen: # Create a new process for each pass of hashing proc = subprocess.Popen( [\u0026#34;sha256sum\u0026#34;], stdin=subprocess.PIPE, stdout=subprocess.PIPE ) # Send the plaintext to the stdin of the child process proc.stdin.write(plaintext) # Ensure that the child gets input proc.stdin.flush() return proc procs = [] for _ in range(3): proc = calculate_hash(os.urandom(10)) procs.append(proc) for proc in procs: stdout, _ = proc.communicate() print(stdout.decode(\u0026#34;utf8\u0026#34;).strip()) Running this snippet will display the 3 hashes:\n3db9d86f16a60907f261f97f6b9b3dce97416056dc65b9608921ee80c71885a3 - 1ee3de4990a3bca56454d6b3fb94cba1275c8c2f19a8ce6dca5cb2779b5152a7 - f9aa6903c454c70f037328fa1504bf66700e6fdb20407fe1830223e3acec2028 - First, we define a function called calculate_hash that accepts a bytes plaintext input and returns a subprocess.Popen object. This function will spawn a new child process running the sha256sum command. The stdin and stdout of the child process are configured as subprocess.PIPE using the Popen constructor. This enables data to be piped between the parent and child processes. Inside calculate_hash, the plaintext input is written to the stdin pipe of the child process using proc.stdin.write(). This pipes the data into the child\u0026rsquo;s standard input stream. Next, proc.stdin.flush() method is called to ensure the child process actually receives the input.\nThe main logic begins by initializing an empty list called procs. Then a loop runs 3 times, each time generating a random 10-byte string using os.urandom. This string is passed to calculate_hash, which spawns a new sha256sum child process, pipes the random data to it, and returns the Popen object representing the child. Each Popen is appended to the procs list, so now there are 3 child processes running in parallel.\nFinally, the procs list is iterated through and proc.communicate() is called on each Popen instance to read back the stdout pipe from the child. This contains the output of sha256sum, which is the hash of the random input string. The hash is then decoded, stripped, and printed to the console.\nFurther reading Effective Python - Item 52 ","permalink":"https://rednafi.com/python/unix-style-pipeline-with-subprocess/","summary":"\u003cp\u003ePython offers a ton of ways like \u003ccode\u003eos.system\u003c/code\u003e or \u003ccode\u003eos.spawn*\u003c/code\u003e to create new processes and run\narbitrary commands in your system. However, the documentation usually encourages you to use\nthe \u003ca href=\"https://docs.python.org/3/library/subprocess\"\u003esubprocess\u003c/a\u003e module for creating and managing child processes. The \u003ccode\u003esubprocess\u003c/code\u003e module\nexposes a high-level \u003ccode\u003erun()\u003c/code\u003e function that provides a simple interface for running a\nsubprocess and waiting for it to complete. It accepts the command to run as a list of\nstrings, starts the subprocess, waits for it to finish, and then returns a\n\u003ccode\u003eCompletedProcess\u003c/code\u003e object with information about the result. For example:\u003c/p\u003e","title":"Unix-style pipelining with Python's subprocess module"},{"content":"The current title of this post is probably incorrect and may even be misleading. I had a hard time coming up with a suitable name for it. But the idea goes like this: sometimes you might find yourself in a situation where you need to iterate through a generator more than once. Sure, you can use an iterable like a tuple or list to allow multiple iterations, but if the number of elements is large, that\u0026rsquo;ll cause an OOM error. On the other hand, once you\u0026rsquo;ve already consumed a generator, you\u0026rsquo;ll need to restart it if you want to go through it again. This behavior is common in pretty much every programming language that supports the generator construct.\nSo, in the case where a function returns a generator and you\u0026rsquo;ve already consumed its values, you\u0026rsquo;ll need to call the function again to generate a new instance of the generator that you can use. Observe:\nfrom __future__ import annotations from collections.abc import Generator def get_numbers( start: int, end: int, step: int ) -\u0026gt; Generator[int, None, None]: yield from range(start, end, step) This can be used like this:\nnumbers = get_numbers(1, 10, 2) for number in numbers: print(number) It\u0026rsquo;ll return:\n1 3 5 7 9 Now, if you try to consume the iterable again, you\u0026rsquo;ll get empty value. Run this again:\nfor number in numbers: print(number) It won\u0026rsquo;t print anything since the previous loop has exhausted the generator. This is expected and if you want to loop through the same elements again, you\u0026rsquo;ll have to call the function again to produce another generator that you can consume. So, the following will always work:\nfor number in get_numbers(): print(number) If you run this snippet multiple times, on each pass, the get_numbers() function will be called again and that\u0026rsquo;ll return a new generator for you to iterate through. Calling the generator function like this works but here\u0026rsquo;s another thing that I learned today while reading Effective Python by Brett Slatkin. You can create a class with the __iter__ method and yield numbers from it just like the function. Then when you initiate the class, the instance of the class will allow you to loop through it multiple times; each time creating a new generator.\nI knew that you could create an iterable class by adding __iter__ to a class and yielding values from it. But I wasn\u0026rsquo;t aware that the you could also iterate through the instance of the class multiple times and the class will run __iter__ on each pass and produce a new generator for you to consume.\nFor example:\nfrom __future__ import annotations from collections.abc import Generator class NumberGen: def __init__(self, start: int, end: int, step: int) -\u0026gt; None: self.start = start self.end = end self.step = step def __iter__(self) -\u0026gt; Generator[int, None, None]: yield from range(self.start, self.end, self.step) Now use the class as such:\nnumbers = NumberGen() for number in numbers: print(number) This prints:\n1 3 5 7 9 If you run the for-loop again on the number instance, you\u0026rsquo;ll see that the snippet will print the same numbers again. Here, instantiating the NumberGen class creates a NumberGen instance that is not a generator per se, but can return a generator if you call the iter() function on the instance. When you run the for loop on the instance, it runs the underlying __iter__ method to produce a new generator that the loop can iterate through. This allows you to run the for-loop multiple times on the instance, since each run creates a new generator that the loop can consume.\nA generator can still only be consumed once but each time you\u0026rsquo;re running a new for-loop on the above instance, the __iter__ method on it gets called and the method returns a new generator for you to iterate through.\nThis is more convenient than having to repeatedly call a generator function if your API needs to consume a generator multiple times.\n","permalink":"https://rednafi.com/python/enable-repeatable-lazy-iterations/","summary":"\u003cp\u003eThe current title of this post is probably incorrect and may even be misleading. I had a\nhard time coming up with a suitable name for it. But the idea goes like this: sometimes you\nmight find yourself in a situation where you need to iterate through a generator more than\nonce. Sure, you can use an iterable like a tuple or list to allow multiple iterations, but\nif the number of elements is large, that\u0026rsquo;ll cause an OOM error. On the other hand, once\nyou\u0026rsquo;ve already consumed a generator, you\u0026rsquo;ll need to restart it if you want to go through it\nagain. This behavior is common in pretty much every programming language that supports the\ngenerator construct.\u003c/p\u003e","title":"Enabling repeatable lazy iterations in Python"},{"content":" Around a year ago, I ditched my fancy Linux rig for a beefed-up 16\u0026quot; MacBook Pro and ever since, it\u0026rsquo;s been my primary machine for both personal and work stuff. I love how this machine strikes a decent balance between power and portability. However, I often joke that this chonky boy is just a pound shy of being an ENIAC. It\u0026rsquo;s a beast of a machine when you need all that power, but certainly isn\u0026rsquo;t the most convenient contraption to lug around while flying. I work fully remote, but can\u0026rsquo;t get any work done while traveling and rarely ever need to tap into the full power this thing offers.\nSo I wanted to find an excuse to have separate machines for work and when I\u0026rsquo;m out and about. Sure, I could\u0026rsquo;ve gone for a 14\u0026quot; Pro to make it more portable and all, but here\u0026rsquo;s the deal: I absolutely detest working on anything that has a screen smaller than 15\u0026quot;. So when Apple dropped the new 15\u0026quot; Air, I just knew I had to get my hands on that. Plus, I\u0026rsquo;m gonna stick with my 16\u0026quot; machine for work anyway, so I\u0026rsquo;m totally cool with grabbing a less powerful device that still sports a larger screen and doesn\u0026rsquo;t weigh 5 pounds.\nBut here\u0026rsquo;s the thing, if you\u0026rsquo;re not down with the base model and don\u0026rsquo;t mind rolling with a 14\u0026quot; screen, the 14\u0026quot; Pro is actually a better deal. It\u0026rsquo;s got better I/O, a slightly better screen, and definitely better speakers. Personally, I already have all those perks with my 16\u0026quot; machine, so my sights were strictly set on the aesthetics and portabitly aspects of the device. And boy, it didn\u0026rsquo;t disappoint. I went for the 16/256 config and snagged it for around 1500 USD.\nThis isn\u0026rsquo;t a product review, and I honestly don\u0026rsquo;t know a squat about reviewing things. It\u0026rsquo;s simply a brief piece sharing my thoughts on a product I purchased with my own money. You won\u0026rsquo;t find any affiliated links here — just my genius opinions. Also, this comes from the perspective of a person who won\u0026rsquo;t be using this as their main work machine.\nI was worried that I\u0026rsquo;d have a hard time adjusting to the smaller 15\u0026quot; screen and wouldn\u0026rsquo;t find the keyboard as spacious. However, the good thing is that the differences were barely noticeable, and the Air still rocks a larger-than-life trackpad. The screen gets a bit less bright than its big brother, but that isn\u0026rsquo;t a problem since I mostly work indoors. For the price, the screen is terrific, and I don\u0026rsquo;t have any complaints about it. Oddly enough, I found the lack of external speaker grills aesthetically pleasing, and it still gets plenty loud; noticeably so than the 13\u0026quot; Air.\nIn terms of appearance, hands down, the Air looks much better than the Pros because of its significantly slimmer body. The 15\u0026quot; Air is barely half as thick and weighs half as much as the Pros. Did I mention it weighs half as much as the bigger Pro machine? Now, since there\u0026rsquo;s no fan and the whole SoC is passively cooled, the performance does take a hit when compared with an actively cooled machine. My workflow on this device includes writing stuff on VSCode, running 3-10 Docker containers, developing web applications, and the usual lightweight browsing. Turns out, the passively cooled 8-core M2 can handle all of those like a champ and some more. Also, the 16 GB memory gives me enough leeway to do serious development work every now and then should I need to.\nSo far, performance hasn\u0026rsquo;t been a bottleneck at all, and I can always resort to the 16\u0026quot; apparatus if I need to. However, for the work that I usually do, the device is holding up surprisingly well, and I knew exactly what I\u0026rsquo;d be getting when I picked the Air over the 14\u0026quot; Pro. One big caveat is that the I/O situation is less than ideal as it only has two USB-C ports, a 3.5 mm headphone jack, and a MagSafe power port — that\u0026rsquo;s it. Also, it only supports a single monitor, but that\u0026rsquo;s rarely an issue because where would I even get a monitor in an Airbnb?\nFinally, I won\u0026rsquo;t even talk about the insanely good battery life, as the Airs have been the reigning champion in that department for years. The 16\u0026quot; MBP already has great battery life, and the Air matches that with a smaller battery due to having less powerful but more efficient internals. Overall, even considering my recency bias, this is certainly one of — if not the most — prudent tech purchases that I made this year!\n","permalink":"https://rednafi.com/zephyr/descending-into-the-aether/","summary":"\u003cp\u003e\u003cimg\nsrc=\"https://user-images.githubusercontent.com/\n30027932/252213261-01adc640-3bcf-46d8-8f40-dc506e0cb493.jpg\"\nalt=\"Macbook Air M2 15 inch\"\nwidth=\"800px\"\u003e \u003c/img\u003e\u003c/p\u003e\n\u003cp\u003eAround a year ago, I ditched my fancy Linux rig for a beefed-up 16\u0026quot; MacBook Pro and ever\nsince, it\u0026rsquo;s been my primary machine for both personal and work stuff. I love how this\nmachine strikes a decent balance between power and portability. However, I often joke that\nthis chonky boy is just a pound shy of being an \u003ca href=\"https://en.wikipedia.org/wiki/ENIAC\"\u003eENIAC\u003c/a\u003e. It\u0026rsquo;s a beast of a machine when you\nneed all that power, but certainly isn\u0026rsquo;t the most convenient contraption to lug around while\nflying. I work fully remote, but can\u0026rsquo;t get any work done while traveling and rarely ever\nneed to tap into the full power this thing offers.\u003c/p\u003e","title":"Descending into the aether"},{"content":"Over the years, I\u0026rsquo;ve used the template pattern across multiple OO languages with varying degrees of success. It was one of the first patterns I learned in the primordial hours of my software engineering career, and for some reason, it just feels like the natural way to tackle many real-world code-sharing problems. Yet, even before I jumped on board with the composition over inheritance camp, I couldn\u0026rsquo;t help but notice how using this particular inheritance technique spawns all sorts of design and maintenance headaches as the codebase starts to grow.\nAn epiphany This isn\u0026rsquo;t an attempt to explain why you should prefer composition over inheritance (although you should), as it\u0026rsquo;s a vast topic and much has been said regarding this. Also, only after a few years of reading concomitant literatures and making enough mistakes in real-life codebases, it dawned on me that opting for inheritance as the default option leads to a fragile design. So I won\u0026rsquo;t even attempt to tackle that in a single post and will refer to a few fantastic prior arts that proselytized me to the composition cult.\nThe goal of this article is not to focus on the wider spectrum of how to transform subclass-based APIs to use composition but rather to zoom in specifically on the template pattern and propose an alternative way to solve a problem where this pattern most naturally manifests. In the first portion, the post will explain what the template pattern is and how it gradually leads to an intractable mess as the code grows. In the latter segments, I\u0026rsquo;ll demonstrate how I\u0026rsquo;ve designed a real-world service by adopting the obvious and natural path of inheritance-driven architecture. Then, I\u0026rsquo;ll explain how the service can be refactored to escape the quagmire that I\u0026rsquo;ve now started to refer to as the template pattern hellscape.\nOnly a few moons ago, while watching Hynek Schlawack\u0026rsquo;s Python 2023 talk aptly titled Subclassing, Composition, Python, and You and reading his fantastic blog post Subclassing in Python Redux, the concept of adopting composition to gradually phase out subclass-oriented design from my code finally clicked for me. However, it\u0026rsquo;s not always obvious to me how to locate inheritance metastasis and exactly where to intervene to make the design better. This post is my attempt to distill some of my learning from those resources and focus on improving only a small part of the gamut.\nThe infectious template pattern You\u0026rsquo;re consciously or subconsciously implementing the template pattern when your API design follows these steps:\nYou have an Abstract Base Class (ABC) with abstract methods. The ABC also includes one or more concrete methods. The concrete methods in the ABC depend on the concrete implementation of the abstract methods. API users are expected to inherit from the ABC and provide concrete implementations for the abstract methods. Users then utilize the concrete methods defined in the ABC class. This pattern enables the sharing of concrete method implementations with subclasses. However, the concrete methods of the baseclass are only valid when the user inherits from the base and implements the abstract methods. Attempting to instantiate the baseclass without implementing the abstract methods will result in a TypeError. Only the subclass can be initialized once all the abstract methods have been implemented.\nObserve this example:\nfrom abc import ABC, abstractmethod class Base(ABC): def concrete_method(self) -\u0026gt; None: # This depends on abstract_method. The user is expected to create # a subclass from Base and implement abstract_method. return self.abstract_method() @abstractmethod def abstract_method(self) -\u0026gt; None: raise NotImplementedError class Sub(Base): def abstract_method(self) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Providing concrete implementation for \u0026#39;abstract_method\u0026#39;.\u0026#34;\u0026#34;\u0026#34; print( \u0026#34;I\u0026#39;m concrete implementation of \u0026#39;abstract_method\u0026#39; of Base.\u0026#34; ) This is how you use it:\nsub = Sub() # Using only the \u0026#39;concrete_method\u0026#39; defined in the Base class sub.concrete_method() Here, the abstract Base class is defined by inheriting from the abc.ABC class. Inside Base, there\u0026rsquo;s a concrete_method that relies on an abstract_method. The concrete_method is defined to call abstract_method, expecting that subclasses will provide their own implementation of abstract_method. If a subclass of Base fails to implement abstract_method, calling concrete_method on an instance of that subclass will raise a NotImplementedError.\nThe snippet also provides an example subclass called Sub, which inherits from Base. Sub overrides the abstract_method and provides its own implementation. In this case, it just prints a statement. By subclassing Base and implementing abstract_method, Sub becomes a concrete class that can be instantiated. The purpose of this design pattern is to define a common interface through the Base class, with the expectation that subclasses will implement specific behavior by overriding the abstract methods, while still providing a way to call those methods through the concrete methods defined in the baseclass. This seemingly innocuous and often convenient bi-directional relationship between the base and sub class tends to become infectious and introduces complexity into all the subclasses that inherit from the base.\nThe dark side of the moon Template pattern seems like the obvious way of sharing code and it almost always is one of the first things that people learn while familiarizing themselves with how OO works in Python. Plus, it\u0026rsquo;s used extensively in the standard library. For example, in the collections.abc module, there are a few ABCs that you can subclass to build your own containers. I wrote about this a few years back. Here\u0026rsquo;s how you can subclass collections.abc.Sequence to implement a tuple-like immutable datastructure:\nfrom typing import Any from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, *args: Any) -\u0026gt; None: self._data = list(args) def __getitem__(self, index: int) -\u0026gt; Any: return self._data[index] def __len__(self) -\u0026gt; int: return len(self._data) You\u0026rsquo;d use the class as such:\nseq = CustomSequence(1, 2, 3, 4) assert seq[0] == 1 assert len(seq) == 4 We\u0026rsquo;re inheriting from the Sequence ABC and implementing the required abstract methods. Here\u0026rsquo;s the first issue: how do we even know which methods to implement and which methods we get for free? You can consult the Sequence documentation and learn that __getitem__ and __len__ are the abstract methods that subclasses are expected to implement. In return, the base Sequence class gives you __contains__, __iter__, __reversed__, index, and count as mixin methods. You can also print out the abstract methods by accessing the Sequence.__abstractmethod__ attribute. Sure, you\u0026rsquo;re getting a lot of concrete methods for free, but suddenly you\u0026rsquo;re dependent on some out-of-band information to learn about the behavior of your specialized CustomSequence class.\nThe following three sections will briefly explore the issues that deceptively creep up on your codebase when you opt for the template pattern.\nElusive public API You\u0026rsquo;ve already seen the manifestation of this issue in the CustomSequence example. The subclass-oriented code-sharing pattern like this makes it difficult to discover the public API of your specialized class because many of its functionalities come from the concrete mixin methods provided by the base Sequence class. Now, this isn\u0026rsquo;t too terrible for a tool in the standard library since they\u0026rsquo;re usually quite well-documented, and you can always resort to inspecting the subclass instance to learn about the abstract and concrete methods.\nNot all subclass-driven design is bad, and the standard library makes judicious use of the template pattern. However, in an application codebase that you might be writing, this elusive nature of the public API can start becoming recalcitrant. Your code may not be as well-documented as the standard library, or instantiating the subclass may be expensive, making introspection difficult. You\u0026rsquo;re basically trading off readability for writing ergonomics. There\u0026rsquo;s nothing wrong in doing that as long as you\u0026rsquo;re aware of the tradeoffs. All I\u0026rsquo;m trying to say is that it\u0026rsquo;s a non-ideal default.\nNamespace pollution If you introspect the previously defined subclass with dir(CustomSequence), you\u0026rsquo;ll get the following result. I\u0026rsquo;ve removed the common attributes that every class inherits from object for brevity and annotated the abstract and mixin method names for clarity.\n[ \u0026#34;__abstractmethods__\u0026#34;, # Allows you to list out the abstract methods \u0026#34;__class_getitem__\u0026#34;, # Used for generic typing \u0026#34;__contains__\u0026#34;, # Provided by the base \u0026#34;__getitem__\u0026#34;, # You implement \u0026#34;__iter__\u0026#34;, # Provided by the base \u0026#34;__len__\u0026#34;, # You implement \u0026#34;__reversed__\u0026#34;, # Provided by the base \u0026#34;count\u0026#34;, # Provided by the base \u0026#34;index\u0026#34;, # Provided by the base ] From the above list, it\u0026rsquo;s evident that all the methods from the baseclass and the subclass live in the same namespace. The moment you\u0026rsquo;re inheriting from some baseclass, you have no control over what that class is bringing over to your subclass\u0026rsquo;s namespace and effectively polluting it. It\u0026rsquo;s like a more sneaky version of from foo import *.\nThis flat namespacing makes it hard to understand which method is coming from where. In the above case, without the annotations, you\u0026rsquo;d have a hard time discerning between the methods that you implemented and the alien methods from the baseclass. This isn\u0026rsquo;t a cardinal sin in the Python realm if that\u0026rsquo;s what you want, but it\u0026rsquo;s certainly a suboptimal default.\nSRP violation \u0026amp; goose chase program flow The biggest complaint I have against the template pattern is how it encourages the baseclass to do too many things at once. I can endure poor discoverability of public APIs and namespace pollution to some extent, but when a class tries to do too many things simultaneously, it eventually exhibits the tendency to give birth to God Objects; breaching the SRP (Single Responsibility Principle).\nIntentionally violating the SRP rarely fosters good results, and in this case, the baseclass defines both concrete and abstract methods. Not only that, the base expects the subclasses to implement those abstract methods so that it can use them in its concrete method implementation. Just reading back this sentence is giving me a headache. If you design your APIs in this manner, you\u0026rsquo;ll have to carefully read through both the sub and the base class implementations to understand how this intricate bi-directional thread is woven into your program flow. This seems easy enough in a simple example where you can see both the base and the sub class in a single snippet, but it quickly gets out of hand when large base and sub classes are scattered across multiple modules. You\u0026rsquo;ll need to perform the mental gymnastics of tracking this back-and-forth logic, aka the abominable goose chase program flow.\nThe disease and the cure Let\u0026rsquo;s examine a specific design problem and observe how it can be modeled using the template pattern. Then, we\u0026rsquo;ll explore an alternative solution that replaces the inheritance-driven design with composition.\nDesigning with template pattern The following code snippet mimics a real-world webhook dispatcher that takes a message and posts it to a callback URL via HTTP POST request. First, we\u0026rsquo;ll commit the cardinal sin of modeling the domain with the template pattern and then we\u0026rsquo;ll try to find a way out of the quandary. Here it goes:\nfrom dataclasses import dataclass, field, asdict from uuid import uuid4 from abc import ABC, abstractmethod @dataclass(frozen=True) class Message: ref: str = field(default_factory=lambda: str(uuid4())) body: str = \u0026#34;\u0026#34; class BaseWebhook(ABC): def send(self) -\u0026gt; None: url = self.get_url() data = self.get_message() print(f\u0026#34;sending {data} to {url}\u0026#34;) @abstractmethod def get_message(self) -\u0026gt; dict[str, str]: raise NotImplementedError @abstractmethod def get_url(self) -\u0026gt; str: raise NotImplementedError class Webhook(BaseWebhook): def __init__(self, message: Message) -\u0026gt; None: self.message = message def get_message(self) -\u0026gt; dict[str, str]: # Assume we\u0026#39;re doing side effects and adding data in runtime return asdict(self.message) def get_url(self) -\u0026gt; str: return \u0026#34;https://webhook.site/foo\u0026#34; Here\u0026rsquo;s how you\u0026rsquo;ll orchestrate the classes:\nmessage = Message(body=\u0026#34;Hello World\u0026#34;) webhook = Webhook(message) # This just prints: # sending # { # \u0026#39;ref\u0026#39;: \u0026#39;4635cfe0-825e-4f40-9c7b-04275b1c809e\u0026#39;, # \u0026#39;body\u0026#39;: \u0026#39;Hello World\u0026#39; # } to https://webhook.site/foo webhook.send() We start by defining an immutable Message container to store our webhook message. Next, we write an abstract BaseWebhook class that inherits from abc.ABC. This class serves as a template for the webhook functionality and declares two abstract methods: get_message() and get_url(). Type annotations are used to indicate the return types of these methods. Any subclasses derived from BaseWebhook must implement these abstract methods. The send() method, implemented in the baseclass, uses the concrete implementations of the abstract methods to perform webhook dispatching. In this case, we simulate the HTTP POST functionality by printing the message and destination URL.\nThe Webhook class is a subclass of BaseWebhook and provides concrete implementations of the abstract methods. It accepts a single Message object as a parameter in its constructor. The get_url() method returns a fixed URL, while the get_message() method converts the Message object into a serializable dictionary representation using dataclasses.asdict().\nIn this structure, the user of the Webhook class only needs to initialize the class and call the send() method on the instance. The send() method, however, lives in the BaseWebhook class, not the specialized Webhook subclass. It utilizes the concrete implementations of abstract methods to deliver the send() functionality. In the following section, we\u0026rsquo;ll explore a method to avoid this weird back-and-forth program flow.\nFinding salvation in strategy pattern There are multiple ways and conflicting opinions on how to get out of the hole we\u0026rsquo;ve dug for ourselves. Some even like to spend more time prattling around the philosophy of how OO is terrible and how, if it weren\u0026rsquo;t for Java\u0026rsquo;s huge influence on Python, we wouldn\u0026rsquo;t be in this mess, rather than attempting to solve the actual problem. So instead of trying to cover every possible solution under the sun, I\u0026rsquo;ll go through the one that has worked for me fairly well.\nWe\u0026rsquo;ll refactor the code in the previous section to take advantage of composition and structural subtyping support in Python. Long story short, structural subtyping refers to the ability to ensure type safety based on the structure or shape of an object rather than its explicit inheritance hierarchy. This allows us to define and enforce contracts based on the presence of specific attributes or methods, rather than relying on a specific class or inheritance relationship.\nThis is achieved through the use of the typing.Protocol class introduced in Python 3.8. By defining a protocol using the typing.Protocol class, we can specify the expected attributes and methods that an object should have to satisfy the protocol. Any object that matches the structure defined by the protocol can be treated as if it conforms to that protocol, enabling more flexible and dynamic type-checking in Python. This conformity is usually checked by a type-checking tool like mypy. If you want to learn more, check out Glyph\u0026rsquo;s post titled I Want a New Duck. Here\u0026rsquo;s how I refactored it:\nfrom dataclasses import dataclass, field, asdict from uuid import uuid4 from typing import Protocol @dataclass(frozen=True) class Message: ref: str = field(default_factory=lambda: str(uuid4())) body: str = \u0026#34;\u0026#34; class Retriever(Protocol): def get_message(self, message: Message) -\u0026gt; dict[str, str]: ... def get_url(self) -\u0026gt; str: ... class Dispatcher(Protocol): def dispatch(self, url: str, data: dict[str, str]) -\u0026gt; None: ... class HookRetriever: def get_message(self, message: Message) -\u0026gt; dict[str, str]: # Assume we\u0026#39;re doing side effects and adding data in runtime return asdict(message) def get_url(self) -\u0026gt; str: return \u0026#34;https://webhook.site/foo\u0026#34; class HookDispatcher: def dispatch(self, url: str, data: dict[str, str]) -\u0026gt; None: print(f\u0026#34;Sending {data} to {url}\u0026#34;) @dataclass class Webhook: message: Message retriever: Retriever dispatcher: Dispatcher def send(self) -\u0026gt; None: url = self.retriever.get_url() data = self.retriever.get_message(self.message) return self.dispatcher.dispatch(url, data) The classes can be wired together as follows:\nmessage = Message(body=\u0026#34;Hello World\u0026#34;) retriever = HookRetriever() dispatcher = HookDispatcher() webhook = Webhook(message, retriever, dispatcher) # This prints the same thing as before: # sending # { # \u0026#39;ref\u0026#39;: \u0026#39;4635cfe0-825e-4f40-9c7b-04275b1c809e\u0026#39;, # \u0026#39;body\u0026#39;: \u0026#39;Hello World\u0026#39; # } to https://webhook.site/foo webhook.send() We\u0026rsquo;ve agreed that the BaseWebhook class tries to do too many things at once. The first step to disentangling a class is to identify its responsibilities and create multiple component classes where each new class will only have one responsibility. Here, the base class retrieves the necessary data and dispatches the webhook using that data at the same time. The Retriever and Dispatcher protocol classes will formalize the shape and structure of those component classes. These protocols work like the ABCs, but you don\u0026rsquo;t need to inherit from them to ensure interface conformity; the type checker will do it for you.\nThe Retriever class has two methods: get_message and get_url, which fetch message and URL data respectively. Similarly, the Dispatcher protocol has only a dispatch method that sends the webhook. In either case, the protocol methods don\u0026rsquo;t implement anything; they work just like the abstract methods of the ABCs, and the protocol classes themselves can\u0026rsquo;t be instantiated. Then the HookRetriever and HookDispatcher components implicitly implement the protocol classes. Notice that neither of the components inherits from the protocol classes. The type checker will ensure that they conform to the defined protocols.\nThe question is, how does the type checker know which class is supposed to conform to which protocol? The answer lies in the final Webhook class. We define a final dataclass that takes instances of the Message, Retriever, and Dispatcher classes in the constructor. Notice that while adding type hints to the retriever and dispatcher parameters of the dataclass constructor, we\u0026rsquo;re using the protocol classes instead of the concrete ones. This is how the type checker knows that whatever instance is passed to the retriever and dispatcher parameters must conform to the Retriever and Dispatcher protocols, respectively. Note that we\u0026rsquo;ve completely eliminated subclassing from our public API. Injecting dependencies in this manner is also known as the strategy pattern.\nThe Webhook class now has a hierarchical namespace instead of a flat one, unlike our inheritance-based friend. You\u0026rsquo;ll have to be explicit about where a method is coming from when calling it. So if you need to access the fetched URL, you\u0026rsquo;ll need to explicitly call self.retriever.get_url(). The self namespace has only one user-defined public method, .send(), which can be called to dispatch the webhook from a Webhook instance. This also means you no longer have to deal with goose chase program flow since all the dependencies flow towards the final Webhook class.\nOn the flip side, you\u0026rsquo;ll need to do more work while initializing the Webhook class. The Message, HookRetriever, and HookDispatcher classes need to be instantiated first and then passed explicitly to the constructor of the Webhook class to instantiate it. You\u0026rsquo;re basically trading writing ergonomics for readability. Instantiating the template subclass was a lot easier for sure.\nTradeoffs Opting in for composition isn\u0026rsquo;t free, as it usually leads to more verbose code orchestration. If you\u0026rsquo;re passing all the dependencies explicitly, as shown above, wiring the code together will be more complex. However, in return, you get a more readable and testable design substrate. So, I\u0026rsquo;m more than happy to make the tradeoff. Additionally, avoiding namespace pollution means that one attribute access has now turned into two or more attribute accesses, which can cause performance issues in tight conditions.\nMoreover, you can\u0026rsquo;t just take your inheritance-heavy API and suddenly turn it into a composable one. It usually requires planning and designing from the ground up, where you might decide that the ROI isn\u0026rsquo;t good enough to justify the effort of refactoring. Plus, in a language like Python, you can\u0026rsquo;t always escape inheritance, nor should you try to do so.\nYet behold, it need not be the customary stratagem that thou graspest at each moment thine heart yearns to commune code amidst classes.\nFurther reading End of object inheritance - Augie Fackler, Nathaniel Manista ","permalink":"https://rednafi.com/python/escape-template-pattern/","summary":"\u003cp\u003eOver the years, I\u0026rsquo;ve used the \u003ca href=\"https://refactoring.guru/design-patterns/template-method/python/example\"\u003etemplate pattern\u003c/a\u003e across multiple OO languages with varying\ndegrees of success. It was one of the first patterns I learned in the primordial hours of my\nsoftware engineering career, and for some reason, it just feels like the natural way to\ntackle many real-world code-sharing problems. Yet, even before I jumped on board with the\n\u003ca href=\"https://python-patterns.guide/gang-of-four/composition-over-inheritance/\"\u003ecomposition over inheritance\u003c/a\u003e camp, I couldn\u0026rsquo;t help but notice how using this particular\ninheritance technique spawns all sorts of design and maintenance headaches as the codebase\nstarts to grow.\u003c/p\u003e","title":"Escaping the template pattern hellscape in Python"},{"content":"One major drawback of Python\u0026rsquo;s huge ecosystem is the significant variances in workflows among people trying to accomplish different things. This holds true for dependency management as well. Depending on what you\u0026rsquo;re doing with Python — whether it\u0026rsquo;s building reusable libraries, writing web apps, or diving into data science and machine learning — your workflow can look completely different from someone else\u0026rsquo;s. That being said, my usual approach to any development process is to pick a method and give it a shot to see if it works for my specific needs. Once a process works, I usually automate it and rarely revisit it unless something breaks.\nAlso, I actively try to abstain from picking up tools that haven\u0026rsquo;t stood the test of time. If the workflow laid out here doesn\u0026rsquo;t work for you and something else does, that\u0026rsquo;s fantastic! I just wanted to document a more modern approach to the dependency management workflow that has reliably worked for me over the years. Plus, I don\u0026rsquo;t want to be the person who still uses distutils in their package management workflow and gets reprehended by pip for doing so.\nDefining the scope Since the dependency management story in Python is a huge mess for whatever reason, to avoid getting yelled at by the most diligent gatekeepers of the internet, I\u0026rsquo;d like to clarify the scope of this piece. I mainly write web applications in Python and dabble in data science and machine learning every now and then. So yeah, I\u0026rsquo;m well aware of how great conda is when you need to deal with libraries with C dependencies. However, that\u0026rsquo;s not typically my day-to-day focus. Here, I\u0026rsquo;ll primarily delve into how I manage dependencies when developing large-scale web apps and reusable libraries.\nIn applications, I manage my dependencies with pip and pip-tools, and for libraries, my preferred build backend is hatch. PEP-621 attempts to standardize the process of storing project metadata in a pyproject.toml file, and I absolutely love the fact that now, I\u0026rsquo;ll mostly be able to define all my configurations and dependencies in a single file. This made me want to rethink how I wanted to manage the dependencies without sailing against the current recommended standard while also not getting swallowed into the vortex of conflicting opinions in this space.\nIn applications Whether I\u0026rsquo;m working on a large Django monolith or exposing a microservice via FastAPI or Flask, while packaging an application, I want to be able to:\nStore all project metadata, linter configs, and top-level dependencies in a pyproject.toml file following the PEP-621 conventions. Separate the top-level application and development dependencies. Generate requirements.txt and requirements-dev.txt files from the requirements specified in the TOML file, where the top-level and their transient dependencies will be pinned to specific versions. Use vanilla pip to build the application hermetically from the locked dependencies specified in the requirements*.txt files. The goal is to simply be able to run the following command to install all the pinned dependencies in a reproducible manner:\npip install -r requirements.txt -r requirements-dev.txt pip-tools allows me to do exactly that. Suppose, you have an app where you\u0026rsquo;re defining the top-level dependencies in a canonical pyproject.toml file like this:\n[project] requires-python = \u0026#34;\u0026gt;=3.8\u0026#34; name = \u0026#34;foo-app\u0026#34; version = \u0026#34;0.1.0\u0026#34; dependencies = [ \u0026#34;fastapi==0.97.0\u0026#34;, \u0026#34;uvicorn==0.22.0\u0026#34;, ] [project.optional-dependencies] dev = [ \u0026#34;black\u0026gt;=23.3.0\u0026#34;, \u0026#34;mypy\u0026gt;=1.2.0\u0026#34;, \u0026#34;pip-tools\u0026gt;=6.13.0\u0026#34;, \u0026#34;pytest\u0026gt;=7.3.2\u0026#34;, \u0026#34;pytest-cov\u0026gt;=4.1.0\u0026#34;, \u0026#34;ruff\u0026gt;=0.0.272\u0026#34; ] # Even for an application, specifying a build backend is required. # Otherwise, pip-compile command will give you an obscure error. [tool.setuptools.packages.find] where = [\u0026#34;app\u0026#34;] # [\u0026#34;.\u0026#34;] by default Here, following PEP-621 conventions, we\u0026rsquo;ve specified the app and dev dependencies in the project.dependencies and project.optional-dependencies.dev sections respectively. Now in a virtual environment, install pip-tools and run the following commands:\n# This will pin the app and dev deps to requirements*.txt files and # generate hashes for hermetic builds # Pin the app deps along with their build hashes pip-compile -o requirements.txt pyproject.toml \\ --generate-hashes --strip-extras # Use the app deps as a constraint while pinning the dev deps so that the # dev deps don\u0026#39;t install anything that conflicts with the app deps echo \u0026#34;--constraint $(PWD)/requirements.txt\u0026#34; \\ | pip-compile --generate-hashes --output-file requirements-dev.txt \\ --extra dev - pyproject.toml Running the commands will create two lock files requirements.txt and requirements-dev.txt where all the pinned top-level and transient dependencies will be listed out. The contents of the requirements.txt file looks like this (truncated):\n# # This file is autogenerated by pip-compile with Python 3.11 # by the following command: # # pip-compile --generate-hashes --output-file=requirements.txt # --strip-extras pyproject.toml # anyio==3.7.0 \\ --hash=sha256:275d9973793619a5374e1c89a4f4ad3f4b0a5510a2b5b939444bee8f4c4d37ce \\ --hash=sha256:eddca883c4175f14df8aedce21054bfca3adb70ffe76a9f607aef9d7fa2ea7f0 # via starlette click==8.1.3 \\ --hash=sha256:7682dc8afb30297001674575ea00d1814d808d6a36af415a82bd481d37ba7b8e \\ --hash=sha256:bb4d8133cb15a609f44e8213d9b391b0809795062913b383c62be0ee95b1db48 # via uvicorn ... Similarly, the content of requirements-dev.txt file goes as follows (truncated):\n# # This file is autogenerated by pip-compile with Python 3.11 # by the following command: # # pip-compile --extra=dev --generate-hashes --output-file=requirements-dev.txt # - pyproject.toml # anyio==3.7.0 \\ --hash=sha256:275d9973793619a5374e1c89a4f4ad3f4b0a5510a2b5b939444bee8f4c4d37ce \\ --hash=sha256:eddca883c4175f14df8aedce21054bfca3adb70ffe76a9f607aef9d7fa2ea7f0 # via # -r - # starlette black==23.3.0 \\ --hash=sha256:064101748afa12ad2291c2b91c960be28b817c0c7eaa35bec09cc63aa56493c5 \\ --hash=sha256:0945e13506be58bf7db93ee5853243eb368ace1c08a24c65ce108986eac65915 \\ ... Once the lock files are generated, you\u0026rsquo;re free to build the application in however way you see fit and the build process doesn\u0026rsquo;t even need to be aware of the existence of pip-tools. In the simplest case, you can just run pip install to build the application. Check out this fastapi-nano example that uses the workflow explained in this section.\nIn libraries While packaging libraries, I pretty much want the same things mentioned in the application section. However, the story of dependency management in reusable libraries is a bit more hairy. Currently, there\u0026rsquo;s no standard around a lock file and I\u0026rsquo;m not aware of a way to build artifacts from a plain requirements.txt file. For this purpose, my preferred build backend is hatch. Mostly because it follows the latest standards formalized by the associated PEPs. From the FAQ section of the hatch docs:\nQ: What is the risk of lock-in?\nA: Not much! Other than the plugin system, everything uses Python\u0026rsquo;s established standards by default. Project metadata is based entirely on PEP-621/PEP-631, the build system is compatible with PEP-517/PEP-660, versioning uses the scheme specified by PEP-440, dependencies are defined with PEP-508 strings, and environments use virtualenv.\nHowever, it doesn\u0026rsquo;t support lock files yet:\nThe only caveat is that currently there is no support for re-creating an environment given a set of dependencies in a reproducible manner. Although a standard lock file format may be far off since PEP-665 was rejected, resolving capabilities are coming to pip. When that is stabilized, Hatch will add locking functionality and dedicated documentation for managing applications.\nIn my experience, I haven\u0026rsquo;t faced many issues regarding the lack of support for lock files while building reusable libraries. Your mileage may vary.\nNow let\u0026rsquo;s say we\u0026rsquo;re trying to package up a CLI that has the following source structure:\nsrc ├── __init__.py └── cli.py The content of cli.py looks like this:\nimport click @click.command() @click.version_option() def cli() -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Simple cli command to show the version of the package\u0026#34;\u0026#34;\u0026#34; click.echo(\u0026#34;Hello from foo-cli!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: cli() The corresponding pyproject.toml file looks as follows:\n[project] requires-python = \u0026#34;\u0026gt;=3.8\u0026#34; name = \u0026#34;foo-cli\u0026#34; dependencies = [ \u0026#34;click\u0026gt;=8.1.3\u0026#34;, ] version = \u0026#34;0.0.1\u0026#34; [project.optional-dependencies] dev = [ \u0026#34;hatch\u0026gt;=1.7.0\u0026#34;, \u0026#34;black\u0026gt;=23.3.0\u0026#34;, \u0026#34;mypy\u0026gt;=1.2.0\u0026#34;, \u0026#34;pip-tools\u0026gt;=6.13.0\u0026#34;, \u0026#34;pytest\u0026gt;=7.3.2\u0026#34;, \u0026#34;pytest-cov\u0026gt;=4.1.0\u0026#34;, \u0026#34;ruff\u0026gt;=0.0.272\u0026#34; ] [project.scripts] foo-cli = \u0026#34;src:cli.cli\u0026#34; [build-system] requires = [\u0026#34;hatchling \u0026gt;= 1.7.0\u0026#34;] build-backend = \u0026#34;hatchling.build\u0026#34; # We\u0026#39;re using setuptools as the build backend [tool.setuptools.packages.find] where = [\u0026#34;src\u0026#34;] # [\u0026#34;.\u0026#34;] by default Now install hatch in your virtualenv and run the following command to create the build artifacts:\nhatch build src This will create the build artifacts in the src directory:\nsrc ├── __init__.py ├── cli.py ├── foo_cli-0.0.1-py3-none-any.whl └── foo_cli-0.0.1.tar.gz You can now install the local wheel file to test the build:\npip install foo_cli-0.0.1-py3-none-any.whl Once you\u0026rsquo;ve installed the CLI locally, you can test it by running foo-cli from your console:\nfoo-cli This returns:\nHello from foo-cli! You can also build and install the CLI with:\npip install \u0026#34;.[dev]\u0026#34; Hatch also provides a hatch publish command to upload the package to PyPI. For a complete reference, check out how I shipped rubric following this workflow.\nFurther reading Using pyproject.toml in your Django project - Peter Baumgartner TIL: pip-tools Supports pyproject.toml - Hynek Schlawack ","permalink":"https://rednafi.com/python/dependency-management-redux/","summary":"\u003cp\u003eOne major drawback of Python\u0026rsquo;s huge ecosystem is the significant variances in workflows\namong people trying to accomplish different things. This holds true for dependency\nmanagement as well. Depending on what you\u0026rsquo;re doing with Python — whether it\u0026rsquo;s building\nreusable libraries, writing web apps, or diving into data science and machine learning —\nyour workflow can look completely different from someone else\u0026rsquo;s. That being said, my usual\napproach to any development process is to pick a method and give it a shot to see if it\nworks for my specific needs. Once a process works, I usually automate it and rarely revisit\nit unless something breaks.\u003c/p\u003e","title":"Python dependency management redux"},{"content":"I was watching Storytelling with traceroute lightning talk by Karla Burnett and wanted to understand how traceroute works in Unix. Traceroute is a tool that shows the route of a network packet from your computer to another computer on the internet. It also tells you how long it takes for the packet to reach each stop along the way.\nIt\u0026rsquo;s useful when you want to know more about how your computer connects to other computers on the internet. For example, if you want to visit a website, your computer sends a request to the website\u0026rsquo;s server, which is another computer that hosts the website. But the request doesn\u0026rsquo;t go directly from your computer to the server. It has to pass through several other devices, such as routers, that help direct the traffic on the internet. These devices are called hops. Traceroute shows you the list of hops that your request goes through, and how long it takes for each hop to respond. This can help you troubleshoot network problems, such as slow connections or unreachable websites.\nThis is how you usually use traceroute:\ntraceroute example.com This returns:\ntraceroute to example.com (93.184.216.34), 64 hops max, 52 byte packets 1 192.168.1.1 (192.168.1.1) 2.386 ms 1.976 ms 1.703 ms 2 142-254-158-201.inf.spectrum.com (142.254.158.201) 9.970 ms 9.463 ms 9.867 ms 3 lag-63.uparohgd02h.netops.charter.com (65.25.145.149) 52.340 ms 26.224 ms 18.094 ms 4 lag-31.clmcohib01r.netops.charter.com (24.33.161.216) 24.277 ms 10.391 ms 16.529 ms 5 lag-27.rcr01clevohek.netops.charter.com (65.29.1.38) 16.485 ms 16.258 ms 16.999 ms 6 lag-416.vinnva0510w-bcr00.netops.charter.com (66.109.6.164) 23.478 ms 24.685 ms lag-415.vinnva0510w-bcr00.netops.charter.com (66.109.6.12) 25.211 ms 7 lag-11.asbnva1611w-bcr00.netops.charter.com (66.109.6.30) 24.541 ms lag-21.asbnva1611w-bcr00.netops.charter.com (66.109.3.24) 24.574 ms lag-31.asbnva1611w-bcr00.netops.charter.com (107.14.18.82) 24.253 ms 8 xe-7-3-1.cr0.chi10.tbone.rr.com (209.18.36.1) 24.283 ms 26.479 ms 45.171 ms 9 ae-65.core1.dcb.edgecastcdn.net (152.195.64.129) 24.550 ms 24.753 ms 25.007 ms 10 93.184.216.34 (93.184.216.34) 23.998 ms 24.086 ms 24.180 ms 11 93.184.216.34 (93.184.216.34) 23.627 ms 24.238 ms 24.271 ms This traceroute output draws the path of a network packet from my computer to example.com\u0026rsquo;s server, which has an IP address of 93.184.216.34. It shows that the packet goes through 11 hops before reaching the destination. The first hop is my router (192.168.1.1), the second hop is my ISP\u0026rsquo;s router (142.254.158.201), and so on. The last column shows the time it takes for each hop to respond in milliseconds (ms). The lower the time, the faster the connection.\nSome hops have multiple lines with different names or IP addresses. This means that there are multiple routers at that hop that can handle the traffic, and traceroute randomly picks one of them for each packet. For example, hop 7 has three routers with names starting with lag-11, lag-21, and lag-31. These are probably load-balancing routers that distribute the traffic among them.\nThe last hop (93.184.216.34) appears twice in the output. This is because traceroute sends three packets to each hop by default, and sometimes the last hop responds to all three packets instead of discarding them. This is not a problem and does not affect the accuracy of the traceroute.\nThis is all good and dandy but I wanted to understand how traceroute can find out what route a packet takes and how long it takes between each hop. So I started reading blogs like How traceroute works that does an awesome job at explaining what\u0026rsquo;s going on behind the scene. The gist of it goes as follows.\nUnderneath traceroute Traceroute works by sending a series of ICMP (Internet Control Message Protocol) echo request packets, which are also known as pings, to the target IP address or URL that you want to reach. Each packet has an associated time-to-live (TTL) value, which is a number that indicates how many hops (or intermediate devices) the packet can pass through before it expires and is discarded by a router. Yeah, strangely, TTL doesn\u0026rsquo;t denote any time duration here.\nTraceroute starts by sending a packet with a low TTL value, usually 1. This means that the packet can only make one hop before it expires. When a router receives this packet, it decreases its TTL value by 1 and checks if it is 0. If it is 0, the router discards the packet and sends back an ICMP time exceeded message to the source of the packet. This message contains the IP address of the router that discarded the packet. This is how the sender knows the IP address of the first hop (router, computer, or whatsoever).\nTraceroute records the IP address and round-trip time (RTT) of each ICMP time exceeded message it receives. The RTT is the time it takes for a packet to travel from the source to the destination and back. It reflects the latency (or delay) between each hop.\nTraceroute then increases the TTL value by 1 and sends another packet. This packet can make 2 hops before it expires. The process repeats until traceroute reaches the destination or a maximum TTL value, usually 30. When the returned IP is the same as the initial destination IP, traceroute knows that the packet has completed the whole journey. By doing this, traceroute can trace the route that your packets take to reach the target IP address or URL and measure the latency between each hop. The tool prints out the associated IPs and latencies as it jumps through different hops.\nI snagged this photo from an SFU traceroute machinery slide that I think explains the machinery of traceroute quite well:\nWriting a crappier version of traceroute in Python After getting a rough idea of what\u0026rsquo;s going on underneath, I wanted to write a simpler and crappier version of traceroute in Python. This version would roughly perform the following steps:\nEstablish a UDP socket connection that\u0026rsquo;d be used to send empty packets to the hops. Create an ICMP socket that\u0026rsquo;d receive ICMP time exceeded messages. Start a loop and use the UDP socket to send an empty byte with a TTL of 1 to the first hop. The TTL value of the packet would be decremented by 1 at the first hop. Once the TTL reaches 0, the packet would be discarded, and an ICMP time exceeded message would be returned to the sender through the ICMP socket. The sender would also receive the address of the first hop. Calculate the time delta between sending a packet and receiving the ICMP time exceeded message. Also, capture the address of the first hop and log the time delta and address to the console. In the subsequent iterations, the TTL value will be incremented by 1 (2, 3, 4, \u0026hellip;) and the steps from 1 through 5 will be repeated until it reaches the max_hops value, which is set at 64. Here\u0026rsquo;s the complete self-contained implementation. I tested it on Python 3.11:\n# script.py from __future__ import annotations import socket import sys import time from collections.abc import Generator from contextlib import ExitStack def traceroute( dest_addr: str, max_hops: int = 64, timeout: float = 2 ) -\u0026gt; Generator[tuple[str, float], None, None]: \u0026#34;\u0026#34;\u0026#34;Traceroute implementation using UDP packets. Args: dest_addr (str): The destination address. max_hops (int, optional): The maximum number of hops. Defaults to 64. timeout (float, optional): The timeout for receiving packets. Defaults to 2. Yields: Generator[tuple[str, float], None, None]: A generator that yields the current address and elapsed time for each hop. \u0026#34;\u0026#34;\u0026#34; # ExitStack allows us to avoid multiple nested contextmanagers with ExitStack() as stack: # Create an ICMP socket connection for receiving packets rx = stack.enter_context( socket.socket( socket.AF_INET, socket.SOCK_RAW, socket.IPPROTO_ICMP ) ) # Create a UDP socket connection for sending packets tx = stack.enter_context( socket.socket( socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP ) ) # Set the timeout for receiving packets rx.settimeout(timeout) # Bind the receiver socket to any available port rx.bind((\u0026#34;\u0026#34;, 0)) # Iterate over the TTL values for ttl in range(1, max_hops + 1): # Set the TTL value in the sender socket tx.setsockopt(socket.IPPROTO_IP, socket.IP_TTL, ttl) # Send an empty UDP packet to the destination address tx.sendto(b\u0026#34;\u0026#34;, (dest_addr, 33434)) try: # Start the timer start_time = time.perf_counter_ns() # Receive response packet and extract source address _, curr_addr = rx.recvfrom(512) curr_addr = curr_addr[0] # Stop the timer and calculate the elapsed time end_time = time.perf_counter_ns() elapsed_time = (end_time - start_time) / 1e6 except socket.error: # If an error occurs while receiving the packet, set the # address and elapsed time as None curr_addr = None elapsed_time = None # Yield the current address and elapsed time yield curr_addr, elapsed_time # Break the loop if the destination address is reached if curr_addr == dest_addr: break def main() -\u0026gt; None: # Get the destination address from command-line argument dest_name = sys.argv[1] dest_addr = socket.gethostbyname(dest_name) # Print the traceroute header print(f\u0026#34;Traceroute to {dest_name} ({dest_addr})\u0026#34;) print(f\u0026#34;{\u0026#39;Hop\u0026#39;:\u0026lt;5s}{\u0026#39;IP Address\u0026#39;:\u0026lt;20s}{\u0026#39;Hostname\u0026#39;:\u0026lt;50s}{\u0026#39;Time\u0026#39;:\u0026lt;10s}\u0026#34;) print(\u0026#34;-\u0026#34; * 90) # Iterate over the traceroute results and print each hop information for i, (addr, elapsed_time) in enumerate(traceroute(dest_addr)): if addr is not None: try: # Get the hostname corresponding to the IP address host = socket.gethostbyaddr(addr)[0] except socket.error: host = \u0026#34;\u0026#34; # Print the hop information print(f\u0026#34;{i+1:\u0026lt;5d}{addr:\u0026lt;20s}{host:\u0026lt;50s}{elapsed_time:.3f}ms\u0026#34;) else: # Print \u0026#34;*\u0026#34; for hops with no response print(f\u0026#34;{i+1:\u0026lt;5d}{\u0026#39;*\u0026#39;:\u0026lt;20s}{\u0026#39;*\u0026#39;:\u0026lt;50s}{\u0026#39;*\u0026#39;:\u0026lt;10s}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Running the script will give you the following nicely formatted output:\nsudo python script.py example.com Traceroute to example.com (93.184.216.34) Hop IP Address Hostname Time ---------------------------------------------------------------------------------- 1 192.168.1.1 1.420ms 2 142.254.158.201 142-254-158-201.inf.spectrum.com 9.669ms 3 65.25.145.149 lag-63.uparohgd02h.netops.charter.com 139.603ms 4 24.33.161.216 lag-31.clmcohib01r.netops.charter.com 14.493ms 5 65.29.1.38 lag-27.rcr01clevohek.netops.charter.com 19.221ms 6 66.109.6.70 lag-17.vinnva0510w-bcr00.netops.charter.com 25.803ms 7 66.109.3.24 lag-21.asbnva1611w-bcr00.netops.charter.com 24.969ms 8 209.18.36.1 xe-7-3-1.cr0.chi10.tbone.rr.com 24.351ms 9 152.195.64.129 ae-65.core1.dcb.edgecastcdn.net 25.114ms 10 93.184.216.34 23.546ms ","permalink":"https://rednafi.com/python/implement-traceroute/","summary":"\u003cp\u003eI was watching \u003ca href=\"https://www.youtube.com/watch?v=xW_ALxfop7Y\"\u003eStorytelling with traceroute\u003c/a\u003e lightning talk by Karla Burnett and wanted to\nunderstand how \u003ccode\u003etraceroute\u003c/code\u003e works in Unix. Traceroute is a tool that shows the route of a\nnetwork packet from your computer to another computer on the internet. It also tells you how\nlong it takes for the packet to reach each stop along the way.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s useful when you want to know more about how your computer connects to other computers\non the internet. For example, if you want to visit a website, your computer sends a request\nto the website\u0026rsquo;s server, which is another computer that hosts the website. But the request\ndoesn\u0026rsquo;t go directly from your computer to the server. It has to pass through several other\ndevices, such as routers, that help direct the traffic on the internet. These devices are\ncalled hops. Traceroute shows you the list of hops that your request goes through, and how\nlong it takes for each hop to respond. This can help you troubleshoot network problems, such\nas slow connections or unreachable websites.\u003c/p\u003e","title":"Implementing a simple traceroute clone in Python"},{"content":"Recently, I purchased a domain for this blog and migrated the content from rednafi.github.io to rednafi.com. This turned out to be a much bigger hassle than I originally thought it\u0026rsquo;d be, mostly because, despite setting redirection for almost all the URLs from the previous domain to the new one and submitting the new sitemap.xml to the Search Console, Google kept indexing the older domain. To make things worse, the search engine selected the previous domain as canonical, and no amount of manual requests were changing the status in the last 30 days. Strangely, I didn\u0026rsquo;t encounter this issue with Bing, as it reindexed the new site within a week after I submitted the sitemap file via their webmaster panel.\nWhile researching this, one potential solution suggested that along with submitting the sitemap via Google Search Console, I\u0026rsquo;d have to make individual indexing requests for each URL to encourage faster indexing. The problem is, I\u0026rsquo;ve got quite a bit of content on this site, and it\u0026rsquo;ll take forever for me to click through all the links and request indexing that way. Naturally, I looked for a way to do this programmatically. Luckily, I found out that there\u0026rsquo;s an indexing API that allows you to make bulk indexing requests programmatically. This has one big advantage — Google responds to API requests faster than indexing requests with sitemap submission.\nAll you\u0026rsquo;ve to do is:\nList out the URLs that need to be indexed.\nFulfill the prerequisites and download the private key JSON file required to make requests to the API. From the docs:\nEvery call to the Indexing API must be authenticated with an OAuth token that you get in exchange for your private key. Each token is good for a span of time. Google provides API client libraries to get OAuth tokens for a number of languages.\nThe private key file will look like this:\n{ \u0026#34;type\u0026#34;: \u0026#34;service_account\u0026#34;, \u0026#34;project_id\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;private_key_id\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;private_key\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;client_email\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;auth_uri\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;token_uri\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;auth_provider_x509_cert_url\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;client_x509_cert_url\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;universe_domain\u0026#34;: \u0026#34;...\u0026#34; } Use an API client to make the requests.\nIn my case, this site\u0026rsquo;s sitemap.xml lists out all the URLs as follows:\n\u0026lt;urlset xmlns=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#34; xmlns:xhtml=\u0026#34;http://www.w3.org/1999/xhtml\u0026#34;\u0026gt; \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;https://rednafi.com/tags/github/\u0026lt;/loc\u0026gt; \u0026lt;lastmod\u0026gt;2023-05-21T00:00:00+00:00\u0026lt;/lastmod\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;https://rednafi.com/tags/javascript/\u0026lt;/loc\u0026gt; \u0026lt;lastmod\u0026gt;2023-05-21T00:00:00+00:00\u0026lt;/lastmod\u0026gt; \u0026lt;/url\u0026gt; ... Here\u0026rsquo;s a NodeJS script that collects the URLs from sitemap.xml and makes requests to the indexing API:\n// ES6 import import { google } from \u0026#34;googleapis\u0026#34;; import { parseString } from \u0026#34;xml2js\u0026#34;; import fetch from \u0026#34;node-fetch\u0026#34;; import pkey from \u0026#34;./google-api-pkey.json\u0026#34; assert { type: \u0026#34;json\u0026#34; }; // Parse the sitemap.xml file and extract the URLs. async function getUrls(url) { try { const response = await fetch(url); const xml = await response.text(); let urls; parseString(xml, (err, result) =\u0026gt; { if (err) { console.error(\u0026#34;Error parsing XML:\u0026#34;, err); return; } urls = result.urlset.url.map((url) =\u0026gt; url.loc[0]); }); return urls; } catch (error) { console.error(\u0026#34;Error fetching sitemap:\u0026#34;, error); } } // Initialize auth client const jwtClient = new google.auth.JWT( pkey.client_email, null, pkey.private_key, [\u0026#34;https://www.googleapis.com/auth/indexing\u0026#34;], null ); // Perfrom auth and make multiple API calls jwtClient.authorize(async function (err, tokens) { if (err) { console.log(err); return; } const options = { url: \u0026#34;https://indexing.googleapis.com/v3/urlNotifications:publish\u0026#34;, method: \u0026#34;POST\u0026#34;, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, Authorization: `Bearer ${tokens.access_token}`, }, json: { url: \u0026#34;\u0026#34;, type: \u0026#34;URL_UPDATED\u0026#34;, // Means we want to request indexing }, }; try { const urls = await getUrls(\u0026#34;https://www.rednafi.com/sitemap.xml\u0026#34;); // There\u0026#39;s a bulk endpoint but looping through the list // and making multiple requests is just as easy for (const url of urls) { options.json.url = url; const response = await fetch(options.url, { method: options.method, headers: options.headers, body: JSON.stringify(options.json), }); const body = await response.json(); console.log(body); } } catch (error) { console.error(\u0026#34;Error:\u0026#34;, error); } }); Before executing the script, npm install googleapis and xml2js. Now running the script will give you an output similar to this:\n{ urlNotificationMetadata: { url: \u0026#39;https://rednafi.com/categories/\u0026#39;, latestUpdate: { url: \u0026#39;https://rednafi.com/categories/\u0026#39;, type: \u0026#39;URL_UPDATED\u0026#39;, notifyTime: \u0026#39;2023-05-27T01:02:35.537421311Z\u0026#39; } } } { urlNotificationMetadata: { url: \u0026#39;https://rednafi.com/search/\u0026#39;, latestUpdate: { url: \u0026#39;https://rednafi.com/search/\u0026#39;, type: \u0026#39;URL_UPDATED\u0026#39;, notifyTime: \u0026#39;2023-05-27T01:02:35.789809492Z\u0026#39; } } }, ... Here, the getUrls function is defined to fetch the sitemap content from a specified URL, parse the XML content and extract the URLs. It uses the fetch function to retrieve the file, then uses xml2js to parse the XML and extract the URLs from the result.\nThe script then initializes an authentication client using the imported private key and specifies the required API scope. The authorize function is called to authenticate the client and obtain access tokens. Inside the authorization callback, the script prepares the necessary options for making API requests to the Google indexing API. It then calls the getUrls function to fetch the URLs from the sitemap.xml file. For each URL, it updates the options with the URL and makes a POST request to the Indexing API to request indexing. The response from the API is then logged into the console.\nOne thing to keep in mind is that by default, the daily request quota per project is 200. But you can request more quota if you need it.\n","permalink":"https://rednafi.com/javascript/bulk-request-google-search-index/","summary":"\u003cp\u003eRecently, I purchased a domain for this blog and migrated the content from\n\u003ca href=\"https://rednafi.github.io\"\u003erednafi.github.io\u003c/a\u003e to \u003ca href=\"/\"\u003erednafi.com\u003c/a\u003e. This turned out to be a much bigger hassle than I\noriginally thought it\u0026rsquo;d be, mostly because, despite setting redirection for almost all the\nURLs from the previous domain to the new one and submitting the new \u003ca href=\"/sitemap.xml\"\u003esitemap.xml\u003c/a\u003e to the\nSearch Console, Google kept indexing the older domain. To make things worse, the search\nengine selected the previous domain as canonical, and no amount of manual requests were\nchanging the status in the last 30 days. Strangely, I didn\u0026rsquo;t encounter this issue with Bing,\nas it reindexed the new site within a week after I submitted the sitemap file via their\nwebmaster panel.\u003c/p\u003e","title":"Bulk request Google search indexing with API"},{"content":"Cloudflare absolutely nailed the serverless function DX with Cloudflare Workers. However, I feel like it\u0026rsquo;s yet to receive widespread popularity like AWS Lambda since as of now, the service only offers a single runtime — JavaScript. But if you can look past that big folly, it\u0026rsquo;s a delightful piece of tech to work with. I\u0026rsquo;ve been building small tools with it for a couple of years but never got around to writing about the immense productivity boost it usually gives me whenever I need to quickly build and deploy a self-contained service.\nRecently, I was doing some lightweight frontend work and needed to make some AJAX calls from one domain to another. Usually, browser\u0026rsquo;s CORS (Cross-Origin Resource Sharing) policy will get in your way if you try this. While you\u0026rsquo;re reading this piece, open the dev console and paste the following fetch snippet:\nfetch(\u0026#34;https://mozilla.org\u0026#34;) .then((response) =\u0026gt; response.text()) .then((data) =\u0026gt; { // Do something with the received data console.log(data); }) .catch((error) =\u0026gt; { // Handle any errors that occurred during the request console.error(\u0026#34;Error:\u0026#34;, error); }); This snippet will attempt to make a GET request from https://rednafi.com to https://mozilla.org. However, the client\u0026rsquo;s CORS policy won\u0026rsquo;t allow you to make an AJAX request like this and load external resources into the current site. On your console, you\u0026rsquo;ll see an error message like this:\nAccess to fetch at \u0026#39;https://mozilla.org/\u0026#39; from origin \u0026#39;https://rednafi.com\u0026#39; has been blocked by CORS policy: No \u0026#39;Access-Control-Allow-Origin\u0026#39; header is present on the requested resource. If an opaque response serves your needs, set the request\u0026#39;s mode to \u0026#39;no-cors\u0026#39; to fetch the resource with CORS disabled. This is a good security measure. Without CORS, a malicious script could make a request to a server in another domain and access the resources that the user of the page is not intended to have access to. So much has been said and written about CORS that I won\u0026rsquo;t even attempt to explain it here. Here\u0026rsquo;s another high-level introduction to CORS.\nCORS proxy While CORS is generally a good thing, it can be quite annoying when you\u0026rsquo;re trying to build something that needs access to external resources. In those cases, you\u0026rsquo;ll have to mess around with the origin server and add a few headers that the browser can understand before it allows you to load those external resources. But sometimes you don\u0026rsquo;t have access to the origin server or simply don\u0026rsquo;t want to deal with modifying the server\u0026rsquo;s response headers every time you need to access external resources. That\u0026rsquo;s where CORS proxies can come in handy.\nA CORS proxy server acts as a bridge between your client and the target server. It receives your request and forwards it to the target server with a modified origin header so that the target server thinks the request is coming from the same origin as itself.\nThis way, you can bypass the same-origin policy of browsers and access resources from different domains. I usually use free proxies like cors.sh to bypass CORS restrictions. You can drop this snippet to your browser\u0026rsquo;s console and this time it\u0026rsquo;ll allow you to load the contents of https://mozilla.org from https://rednafi.com:\n// Notice how we\u0026#39;re prepending CORS URL before the target URL fetch(\u0026#34;https://proxy.cors.sh/https://mozilla.org\u0026#34;) .then((response) =\u0026gt; response.text()) .then((data) =\u0026gt; { // Do something with the received data console.log(data); }) .catch((error) =\u0026gt; { // Handle any errors that occurred during the request console.error(\u0026#34;Error:\u0026#34;, error); }); The target server\u0026rsquo;s response looks somewhat like this:\n\u0026lt;!doctype html\u0026gt; \u0026lt;html class=\u0026#34;windows no-js\u0026#34; lang=\u0026#34;en\u0026#34; dir=\u0026#34;ltr\u0026#34; data-country-code=\u0026#34;US\u0026#34; data-latest-firefox=\u0026#34;113.0.1\u0026#34; data-esr-versions=\u0026#34;102.11.0\u0026#34; data-gtm-container-id=\u0026#34;GTM-MW3R8V\u0026#34; data-gtm-page-id=\u0026#34;Homepage\u0026#34; data-stub-attribution-rate=\u0026#34;1.0\u0026#34; data-convert-project-id=\u0026#34;10039-1003343\u0026#34;\u0026gt; \u0026lt;head\u0026gt; ... If you want to learn more about how CORS proxies work, here\u0026rsquo;s a fantastic article on CORS proxies that explains the inner machinery in more detail.\nFree proxy servers can be pernicious Using a free CORS proxy server can be dangerous as it might spill the beans on your requests and data to some random service you don\u0026rsquo;t know or trust. Since it plays as a middleman between your app and the resource you\u0026rsquo;re after, it could potentially snoop on, mess with, or keep tabs on your requests and data. Plus, some of those free CORS proxy servers might have restrictions on the size, type, or number of requests they can handle, or they might not even support HTTPS or other security bells and whistles.\nBuild your own CORS proxy with Cloudflare Workers With all the intros out of the way, here\u0026rsquo;s how CloudFlare Workers afforded me to prop up a CORS proxy in less than half an hour. If you\u0026rsquo;re impatient and just want to take a look at the service in its full glory then head over to the cors-proxy repo. GitHub Actions deploys the service automatically to CloudFlare Workers every time a change is pushed to the main branch.\nInstalling the prerequisites Assuming you have node installed on your system, you can fetch the wrangler CLI with the following command:\nnpm install -g wrangler This will allow us to develop and test the service locally.\nBootstrapping the service Create a new directory where you want to develop your service and bring it under source control. Now, run:\nnpm create cloudflare@latest The CLI will guide you through the entire bootstrapping process interactively. You\u0026rsquo;ll have to create a Cloudflare account (if you don\u0026rsquo;t have one already) and log into the dashboard. Then it\u0026rsquo;ll prompt you to deploy your first hello-world API endpoint that you can immediately start to play with without doing anything else. Being able to see the serverless function in action within like 5 minutes gave me a huge dopamine boost that AWS Lambda never could. You can see the interactive bootstrapping section here:\nComplete CLI output... using create-cloudflare version 2.0.7 ╭ Create an application with Cloudflare Step 1 of 3 │ ├ Where do you want to create your application? │ dir cors-proxy │ ├ What type of application do you want to create? │ type \u0026#34;Hello World\u0026#34; script │ ├ Do you want to use TypeScript? │ typescript no │ ├ Copying files from \u0026#34;simple\u0026#34; template │ ╰ Application created ╭ Installing dependencies Step 2 of 3 │ ├ Installing dependencies │ installed via `npm install` │ ╰ Dependencies Installed ╭ Deploy with Cloudflare Step 3 of 3 │ ├ Do you want to deploy your application? │ yes deploying via `npm run deploy` │ ├ Logging into Cloudflare This will open a browser window │ allowed via `wrangler login` │ ├ Deploying your application │ deployed via `npm run deploy` │ ├ SUCCESS View your deployed application at │ https://cors-proxy.rednafi.workers.dev (this may take a few mins) │ │ Run the development server npm run dev │ Deploy your application npm run deploy │ Read the documentation https://developers.cloudflare.com/workers │ Stuck? Join us at https://discord.gg/cloudflaredev │ ╰ See you again soon! Running the interactive session will create the following directory structure:\n├── src │ └── worker.js ├── package-lock.json ├── package.json └── wrangler.toml Developing the CORS proxy We\u0026rsquo;ll write our proxy server in src/worker.js file. Copy the following JS snippet and paste it to the file:\nexport default { async fetch(request, env, ctx) { // Extract method, url and headers from the incoming request object. const { method, url, headers } = request; // Extract destination url from the query string. const destUrl = new URL(url).searchParams.get(\u0026#34;url\u0026#34;); // If the destination url is not present, return 400. if (!destUrl) { return new Response(\u0026#34;Missing destination URL.\u0026#34;, { status: 400 }); } // If the request method is OPTIONS, return CORS headers. if ( method === \u0026#34;OPTIONS\u0026#34; \u0026amp;\u0026amp; headers.has(\u0026#34;Origin\u0026#34;) \u0026amp;\u0026amp; headers.has(\u0026#34;Access-Control-Request-Method\u0026#34;) ) { const responseHeaders = { \u0026#34;Access-Control-Allow-Origin\u0026#34;: headers.get(\u0026#34;Origin\u0026#34;), \u0026#34;Access-Control-Allow-Methods\u0026#34;: \u0026#34;*\u0026#34;, // Allow all methods \u0026#34;Access-Control-Allow-Headers\u0026#34;: headers.get( \u0026#34;Access-Control-Request-Headers\u0026#34; ), \u0026#34;Access-Control-Max-Age\u0026#34;: \u0026#34;86400\u0026#34;, }; return new Response(null, { headers: responseHeaders }); } const proxyRequest = new Request(destUrl, { method, headers: { ...headers, Origin: \u0026#34;\u0026#34;, }, }); try { const response = await fetch(proxyRequest); const responseHeaders = new Headers(response.headers); responseHeaders.set(\u0026#34;Access-Control-Allow-Origin\u0026#34;, \u0026#34;*\u0026#34;); responseHeaders.set(\u0026#34;Access-Control-Allow-Credentials\u0026#34;, \u0026#34;true\u0026#34;); responseHeaders.set(\u0026#34;Access-Control-Allow-Methods\u0026#34;, \u0026#34;*\u0026#34;); return new Response(response.body, { status: response.status, statusText: response.statusText, headers: responseHeaders, }); } catch (error) { return new Response(\u0026#34;Error occurred while fetching the resource.\u0026#34;, { status: 500, }); } }, }; The first section of the code deals with extracting relevant information from the incoming request. It destructures the method, url, and headers properties from the request object, which represents the client\u0026rsquo;s request.\nNext, it extracts the destination URL from the query string. It extracts the URL parameter using the searchParams.get() method. If the destination URL isn\u0026rsquo;t provided, the function returns a Response object with an error message and a status code of 400 (Bad Request).\nThe code then checks if the request method is OPTIONS. The OPTIONS method is used in CORS preflight requests to determine if the actual request is safe to send. If the request is an OPTIONS request and contains specific headers indicating a CORS preflight request (Origin and Access-Control-Request-Method), the function generates a response with appropriate CORS headers. The response headers include Access-Control-Allow-Origin to reflect the client\u0026rsquo;s origin, Access-Control-Allow-Methods set to *, allowing any HTTP method, Access-Control-Allow-Headers based on the requested headers, and Access-Control-Max-Age set to 86400 seconds (one day) to cache the preflight response.\nIf the request is not an OPTIONS request or doesn\u0026rsquo;t meet the CORS preflight conditions, the code continues execution. It creates a new Request object named proxyRequest uses the extracted destination URL and sets the method and headers of the original request. The Origin header is removed to prevent CORS restrictions when forwarding the request.\nThe subsequent code performs the actual request forwarding. It uses fetch to send the proxyRequest to the destination URL. If the fetch is successful, the code proceeds to process the response. It creates a new Headers object from the response\u0026rsquo;s headers and modifies them to include the necessary CORS headers.\nFinally, the function constructs a Response object using the response body, status, statusText, and modified headers. If an error occurs during the fetch operation, the code catches the error and returns a Response object with an error message and a status code of 500 (Internal Server Error).\nOnce you\u0026rsquo;ve pasted the snippet, you can redeploy the service from your local machine with:\nwrangler deploy This will deploy the service immediately:\n⛅️ wrangler 3.0.0 ------------------ Total Upload: 1.52 KiB / gzip: 0.57 KiB Uploaded cors-proxy (0.51 sec) Published cors-proxy (0.38 sec) https://\u0026lt;your-deployed-service\u0026gt; Current Deployment ID: f300ac99-c15e-4e30-a910-a56d81c10b95 I\u0026rsquo;ve removed my root domain from the above output since I\u0026rsquo;m using the free version of Workers and don\u0026rsquo;t want people to exhaust my free request quota. Haha, security by obscurity! But once you\u0026rsquo;ve deployed your proxy server, you can go to the following URL from your browser:\nhttps://\u0026lt;your-deployed-service\u0026gt;/?url=https://mozilla.com This will send you to the Mozilla website through the deployed function. Now you can use it just like the free CORS proxy. Try it out by dropping the following snippet to your browser console. This is exactly the same as the previous fetch snippet but the only difference is this time, we\u0026rsquo;re using our own proxy server that we control:\n// Notice how we\u0026#39;re prepending CORS URL before the target URL fetch(\u0026#34;https://\u0026lt;your-deployed-service\u0026gt;?url=https://mozilla.org\u0026#34;) .then((response) =\u0026gt; response.text()) .then((data) =\u0026gt; { // Do something with the received data console.log(data); }) .catch((error) =\u0026gt; { // Handle any errors that occurred during the request console.error(\u0026#34;Error:\u0026#34;, error); }); Don\u0026rsquo;t forget to replace the \u0026lt;your-deployed-service\u0026gt; URL with your own service. This will result in a successful request. You can also interactively send requests to the destination URLs via the Cloudflare Workers dashboard. Go to your Cloudflare dashboard, head over to the Workers section, and select your deployed serverless function:\nDeploying the service with GitHub Actions For one-off services, wrangler deploy in the local machine works perfectly but I usually don\u0026rsquo;t consider a project fully done until I\u0026rsquo;ve automated away the whole process. So, I wrote a quick GitHub Actions workflow to run the linters and deploy the service automatically when a new commit is pushed to the main branch. Here\u0026rsquo;s how it looks:\n# .github/workers/ci.yml name: Deploy on: push: branches: - main # Allow running this workflow manually. workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-node@v3 with: node-version: \u0026#34;lts/*\u0026#34; cache: npm cache-dependency-path: cors-proxy/package-lock.json - name: Install dependencies working-directory: ./cors-proxy run: | npm install - name: Run linter working-directory: ./cors-proxy run: | npx prettier --check . deploy: runs-on: ubuntu-latest needs: build steps: - uses: actions/checkout@v3 - name: Publish uses: cloudflare/wrangler-action@2.0.0 with: apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }} workingDirectory: \u0026#34;cors-proxy\u0026#34; For this to work, you\u0026rsquo;ll need to create a Cloudflare API token and add it to the GitHub Secrets of your proxy server\u0026rsquo;s repository. Here\u0026rsquo;s the complete CI workflow file.\n","permalink":"https://rednafi.com/javascript/cors-proxy-with-cloudflare-workers/","summary":"\u003cp\u003eCloudflare absolutely nailed the serverless function DX with \u003ca href=\"https://workers.cloudflare.com/\"\u003eCloudflare Workers\u003c/a\u003e. However,\nI feel like it\u0026rsquo;s yet to receive widespread popularity like AWS Lambda since as of now, the\nservice only offers a single runtime — JavaScript. But if you can look past that big folly,\nit\u0026rsquo;s a delightful piece of tech to work with. I\u0026rsquo;ve been building small tools with it for a\ncouple of years but never got around to writing about the immense productivity boost it\nusually gives me whenever I need to quickly build and deploy a self-contained service.\u003c/p\u003e","title":"Building a CORS proxy with Cloudflare Workers"},{"content":"This weekend, I was working on a fun project that required a fixed-time job scheduler to run a curl command at a future timestamp. I was aiming to find the simplest solution that could just get the job done. I\u0026rsquo;ve also been exploring Google Bard recently and wanted to see how it stacks up against other LLM tools like ChatGPT, BingChat, or Anthropic\u0026rsquo;s Claude in terms of resolving programming queries.\nSo, I asked Bard:\nWhat\u0026rsquo;s the simplest solution I could get away with to run a shell command at a future datetime?\nIt introduced me to the UNIX at command that does exactly what I needed. Cron wouldn\u0026rsquo;t be a good fit for this particular use case, and I wasn\u0026rsquo;t aware of the existence of at before. So I started probing the model and wanted to document my findings for future reference. Also, the final hacky solution that allowed me to schedule jobs remotely can be found at the end of this post.\nThe insipid definition The command at in UNIX is used to schedule one-time jobs or commands to be executed at a specific time in the future. Internally, the system maintains a queue that adds a new entry when a job is scheduled, and once it gets executed, the job is removed from the queue.\nNOTE: By default, the jobs will be scheduled using the targeted machine\u0026rsquo;s local timezone.\nPrerequisites The command isn\u0026rsquo;t included in GNU coreutils, so you might have to install it separately on your machine.\nDebian-ish On a Debian-flavored Linux machine, run:\napt install at Then check the status of atd daemon. This daemon executes the scheduled jobs.\nservice atd status * atd is running If the service isn\u0026rsquo;t running, then you can start the daemon with this command:\nservice atd start * Starting deferred execution scheduler atd [OK] MacOS On MacOS, scheduled jobs are carried out by atrun and it\u0026rsquo;s disabled by default. I had to fiddle around quite a bit to make it work on my MacBook Pro running MacOS Ventura. First, you\u0026rsquo;ll need to launch the daemon with the following command:\nsudo launchctl load -w /System/Library/LaunchDaemons/com.apple.atrun.plist This will start the atrun daemon. Or enable it for future bootups by modifying /System/Library/LaunchDaemons/com.apple.atrun.plist to have:\n... \u0026lt;key\u0026gt;Enabled\u0026lt;/key\u0026gt; ... On modern MacOS like Ventura, unfortunately, this requires disabling SIP. Next, you\u0026rsquo;ll need to provide full disk access to atrun. To do so:\nOpen Spotlight and type in Allow full disk access. On the left panel, click on Allow applications to access all user files. On the right panel, add /usr/libexec/atrun to the list of allowed apps. Press cmd + shift + g and type in the full path of atrun. You can learn more about making atrun work on MacOS. Although I\u0026rsquo;m using MacOS for development, In my particular case, making at work on MacOS wasn\u0026rsquo;t the first priority because I deployed the final solution to an Ubuntu container.\nA few examples The following sections demonstrates some examples of scheduling commands to be executed in a few different scenarios.\nSchedule at a specific time To schedule a command to be executed at a specific time, use this command syntax:\nat \u0026lt;time\u0026gt; \u0026lt;command\u0026gt; For example, to schedule the command ls -lah \u0026gt;\u0026gt; foo.txt to be executed at 3:00 PM local time, you\u0026rsquo;d use the following command:\nat 3pm at\u0026gt; ls -lah \u0026gt;\u0026gt; foo.txt at\u0026gt; \u0026lt;Ctrl-D\u0026gt; Pressing \u0026lt;Ctrl-D\u0026gt; tells at that you have finished entering the command, and it should schedule the job to run at the specified time. You\u0026rsquo;ll see that at 3.00PM local time, a file named foo.txt containing the output of ls -l will be created.\nSchedule after a certain period of time To schedule a command to run in a specific amount of time from now, use:\nat now + \u0026lt;time\u0026gt; \u0026lt;command\u0026gt; For example, to schedule ps aux \u0026gt;\u0026gt; foo.txt to run in 2 minutes from now, you\u0026rsquo;d use the following command:\nat now + 2 minutes at\u0026gt; ps aux \u0026gt;\u0026gt; foo.txt at\u0026gt; \u0026lt;Ctrl-D\u0026gt; This will schedule the command to run in two minutes in the current local time.\nSchedule a script run You can also run a script containing multiple commands at a specific time. To do this, create a script file that houses the commands you want to run, and then use at to schedule the script to be executed at the desired time.\nFor example, suppose you have a script file called script.sh that contains a curl command which makes an API call and saves the output to a file. You can schedule it as such:\n#!/usr/bin/env bash curl -X GET https://httpbin.org/get \u0026gt;\u0026gt; foo.json at -f script.sh now + 1 minute The script will be executed in a minute from now. You can check the content of foo.json 1 minute later:\n{ \u0026#34;args\u0026#34;: {}, \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;httpbin.org\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/7.85.0\u0026#34;, \u0026#34;X-Amzn-Trace-Id\u0026#34;: \u0026#34;Root=1-646162a8-71a232d563e0c16a4a497acf\u0026#34; }, \u0026#34;origin\u0026#34;: \u0026#34;74.140.2.169\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://httpbin.org/get\u0026#34; } Schedule in a non-interactive manner What if you don\u0026rsquo;t want to create a new script file and also don\u0026rsquo;t want to schedule a command interactively as shown before? You can echo the desired command and pipe it to at like this:\necho \u0026#34;dig +short rednafi.com \u0026gt;\u0026gt; foo.txt\u0026#34; | at now + 1 minute We can also run multi-line commands in a single go by taking advantage of the heredoc format:\nat now + 1 minute \u0026lt;\u0026lt;EOF dig +short rednafi.com \u0026gt;\u0026gt; foo.txt EOF In either case, 1 minute later, you\u0026rsquo;ll see that a foo.txt file will be created in your local directory with the following content:\n185.199.111.153 185.199.108.153 185.199.109.153 185.199.110.153 This command above uses at to schedule the execution of a dig command for the domain name rednafi.com. In this case, dig performs a DNS lookup, and the scheduled time is set to be 1 minute from now in the current local time. The output of the command is then appended to the file foo.txt. The \u0026lt;\u0026lt;EOF syntax is used for input redirection, which allows the command to be specified in a heredoc format without requiring you to enter the command in interactive mode as before.\nSchedule with UNIX timestamp You can schedule jobs using a UNIX timestamp with the -t flag. The at command requires a timestamp in the format [[[mm]dd]HH]MM[[cc]yy][.ss]]. Here\u0026rsquo;s an example that uses the date command to generate the current datetime, adds a 30-second offset to it, formats it to the at\u0026rsquo;s expected format, and schedules a job.\nOn Linux, run:\nat -t $(date -d \u0026#34;+30 seconds\u0026#34; +\u0026#34;%Y%m%d%H%M.%S\u0026#34;) \u0026lt;\u0026lt;EOF ping -c 5 rednafi.com \u0026gt;\u0026gt; foo.txt EOF On MacOS, run:\nat -t $(date -v \u0026#34;+30S\u0026#34; +\u0026#34;%Y%m%d%H%M.%S\u0026#34;) \u0026lt;\u0026lt;EOF ping -c 5 rednafi.com \u0026gt;\u0026gt; foo.txt EOF View and manage scheduled jobs To view a list of scheduled jobs, use the following command:\natq This will display a list of all the tasks that are currently scheduled.\n36 Sun May 14 18:42:00 2023 37 Sun May 14 18:42:00 2023 To remove a scheduled task, use the following command:\natrm \u0026lt;job number\u0026gt; The job number is the number assigned to the task when it was scheduled. You can find the job number by running the atq command. If you need to clear all the pending jobs, use this:\natrm $(atq | cut -f 1) A hacky way to schedule jobs remotely This is a hacky and probably dangerous way to do remote job scheduling. However, the beauty of side projects is that nobody\u0026rsquo;s here to tell you what to do and it\u0026rsquo;s a fun way to play with hazmats.\nI needed a way to quickly prop up a service that\u0026rsquo;d allow me to schedule webhook API calls at a fixed point in time in the future. So I exposed a simple NodeJS server that\u0026rsquo;d allow me to schedule an API call with at and execute the command at the desired datetime. Here\u0026rsquo;s the complete server:\n// server.js import express, { json } from \u0026#34;express\u0026#34;; import { exec } from \u0026#34;child_process\u0026#34;; const app = express(); const port = 3000; const authToken = \u0026#34;some-token\u0026#34;; app.use(json()); app.post(\u0026#34;/run-command\u0026#34;, (req, res) =\u0026gt; { const { command } = req.body; if (!command) { return res.status(400).json({ error: \u0026#34;Command not provided.\u0026#34; }); } const authHeader = req.headers.authorization; if (!authHeader || authHeader !== `Bearer ${authToken}`) { return res.status(401).json({ error: \u0026#34;Unauthorized.\u0026#34; }); } exec(command, (error, stdout, stderr) =\u0026gt; { if (error) { return res .status(500) .json({ msg: \u0026#34;Command execution failed.\u0026#34;, error: stderr }); } res.json({ msg: \u0026#34;Command execution successful.\u0026#34;, output: stdout }); }); }); app.listen(port, () =\u0026gt; { console.log(`Server listening on port ${port}`); }); This endpoint takes in a shell command and just runs it on the server; bad idea, right? But the endpoint is secured behind a Bearer token and I\u0026rsquo;m the only one who\u0026rsquo;s going to use this. Security by obscurity!\nBefore running the server, you\u0026rsquo;ll need to install express and once you\u0026rsquo;ve done it, you can start the server with the following command:\nnode server.js Now, from a different console panel, you can schedule a remote task as follows:\ncurl -X POST -H \u0026#34;Authorization: Bearer some-token\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ --data \u0026#39;{\u0026#34;command\u0026#34;:\u0026#34;echo ping -c 5 google.com | at now +1min\u0026#34;}\u0026#39; \\ http://localhost:3000/run-command This will return:\n{\u0026#34;msg\u0026#34;:\u0026#34;Command execution successful.\u0026#34;,\u0026#34;output\u0026#34;:\u0026#34;\u0026#34;} In my case, I needed to POST a payload at a certain time in the future:\ncurl -X POST -H \u0026#34;Authorization: Bearer some-token\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ --data \u0026#34;{\\\u0026#34;command\\\u0026#34;:\\\u0026#34;echo \\\\\\\u0026#34;curl -X POST \\ https://webhook.site/d667acd3-477f-453c-9375-0dcbb51703bd \\ -H \u0026#39;Content-Type: application/json\u0026#39; --data \\ \u0026#39;{\\\\\\\u0026#34;hello\\\\\\\u0026#34;: \\\\\\\u0026#34;world\\\\\\\u0026#34;}\u0026#39;\\\\\\\u0026#34; | at now +1min\\\u0026#34;}\u0026#34; \\ http://localhost:3000/run-command Further reading at command in Linux ","permalink":"https://rednafi.com/misc/fixed-time-task-scheduling-with-at/","summary":"\u003cp\u003eThis weekend, I was working on a fun project that required a fixed-time job scheduler to run\na \u003ccode\u003ecurl\u003c/code\u003e command at a future timestamp. I was aiming to find the simplest solution that\ncould just get the job done. I\u0026rsquo;ve also been exploring \u003ca href=\"https://bard.google.com/\"\u003eGoogle Bard\u003c/a\u003e recently and wanted to\nsee how it stacks up against other LLM tools like ChatGPT, BingChat, or Anthropic\u0026rsquo;s Claude\nin terms of resolving programming queries.\u003c/p\u003e\n\u003cp\u003eSo, I asked Bard:\u003c/p\u003e","title":"Fixed-time job scheduling with UNIX 'at' command"},{"content":"I needed a way to sort a Django queryset based on a custom sequence of an attribute. Typically, Django allows sorting a queryset by any attribute on the model or related to it in either ascending or descending order. However, what if you need to sort the queryset following a custom sequence of attribute values?\nSuppose, you\u0026rsquo;re working with a model called Product where you want to sort the rows of the table based on a list of product ids that are already sorted in a particular order. Here\u0026rsquo;s how it might look:\n# List of product ids id_list = [3, 1, 2, 4, 8, 7, 5, 6] # We want to sort the products queryset in such a way that the records # appear in the same order specified in the id_list. products = Product.objects.all() Turns out, this is a great case where Django\u0026rsquo;s Case and When can come in handy. With these, Django exposes the underlying SQL\u0026rsquo;s way of performing conditional logic through CASE and WHEN statements. They allow you to return different values or expressions based on some criteria. Think of them as similar to IF-THEN-ELSE statements in other programming languages. Primarily, there are two types of CASE expressions: simple and searched.\nSimple CASE expression A simple CASE expression compares an input expression to a list of values and returns the corresponding result. Here\u0026rsquo;s the syntax:\nCASE input_expression WHEN value1 THEN result1 WHEN value2 THEN result2 ... ELSE default_result END The input_expression can be any valid SQL expression. The data types of the input_expression and each value must be the same or must be an implicit conversion.\nThe WHEN clauses are evaluated in order, from top to bottom. The first one that matches the input_expression determines the result of the CASE expression. If none of the values match, the ELSE clause is executed. If the ELSE clause is omitted and no values match, the CASE expression returns NULL. For example, suppose we have a table called products with the following data:\n| id | name | price | category | | -- | ---- | ----- | -------- | | 1 | A | 10 | X | | 2 | B | 20 | Y | | 3 | C | 30 | Z | | 4 | D | 40 | X | | 5 | E | 50 | Y | We can use a simple CASE expression to assign a label to each product based on its category:\nSELECT id, name, price, CASE category WHEN \u0026#39;X\u0026#39; THEN \u0026#39;Low\u0026#39; WHEN \u0026#39;Y\u0026#39; THEN \u0026#39;Medium\u0026#39; WHEN \u0026#39;Z\u0026#39; THEN \u0026#39;High\u0026#39; END AS label FROM products; The output would be:\n| id | name | price | label | | -- | ---- | ----- | ------ | | 1 | A | 10 | Low | | 2 | B | 20 | Medium | | 3 | C | 30 | High | | 4 | D | 40 | Low | | 5 | E | 50 | Medium | Searched CASE expression A searched CASE expression evaluates a list of Boolean expressions and returns the corresponding result. The syntax looks as follows:\nCASE WHEN condition1 THEN result1 WHEN condition2 THEN result2 ... ELSE default_result END The conditions can be any valid Boolean expressions. Just like simple CASE expressions, here also, the data types of each result must be the same or must be an implicit conversion.\nAs before, the WHEN clauses are evaluated in order, from top to bottom. The first one that evaluates to TRUE determines the result of the CASE expression. If none of the conditions are TRUE, the ELSE clause is executed. If the ELSE clause is omitted and no conditions are TRUE, the CASE expression returns NULL.\nFor example, we can use a searched CASE expression to calculate a discount for each product based on its price:\nSELECT id, name, price, CASE WHEN price \u0026lt; 20 THEN price * 0.9 --10% discount WHEN price BETWEEN 20 AND 40 THEN price *0.8 --20% discount ELSE price *0.7 --30% discount END AS discounted_price FROM products; The output would be:\n| id | name | price | discounted_price | | -- | ---- | ----- | ---------------- | | 1 | A | 10 | 9 | | 2 | B | 20 | 16 | | 3 | C | 30 | 24 | | 4 | D | 40 | 32 | | 5 | E | 50 | 35 | Using searched CASE expressions to order querysets With the intro explanations out of the way, here\u0026rsquo;s how you can sort the products table introduced in the previous section by a list of product ids:\nfrom django.db.models import Case, When from .models import Product product_ids = [4, 2, 1, 3, 5] products = Product.objects.all() preferred = Case( *(When(id=id, then=pos) for pos, id in enumerate(product_ids, start=1)) ) products_sorted = products.filter(id__in=product_ids).order_by(preferred) Printing the queryset will return the following output:\n\u0026lt;QuerySet [ \u0026lt;Product: Product object (4)\u0026gt;, \u0026lt;Product: Product object (2)\u0026gt;, \u0026lt;Product: Product object (1)\u0026gt;, \u0026lt;Product: Product object (3)\u0026gt;, \u0026lt;Product: Product object (5)\u0026gt; ]\u0026gt; Here, we\u0026rsquo;re trying to sort the products queryset by the product ids in the same order specified in the product_ids list. First, we build a Case expression where we\u0026rsquo;re iterating through the product ids and defining the designated positions of the ids based on their positions in the list. Then we filter the products queryset by the ids and pass the preferred expression to the .order_by method. To see the underlying SQL generated by Django, you can print the result of products_sorted.query:\nSELECT blog_product.id, blog_product.name, blog_product.price, blog_product.category FROM blog_product WHERE blog_product.id IN (4, 2, 1, 3, 5) ORDER BY CASE WHEN blog_product.id = 4 THEN 1 WHEN blog_product.id = 2 THEN 2 WHEN blog_product.id = 1 THEN 3 WHEN blog_product.id = 3 THEN 4 WHEN blog_product.id = 5 THEN 5 ELSE NULL END ASC; You can directly run this query against the database and get the same result. Notice how Django is taking advantage of a searched CASE expression to sort the queryset in the desired way. This also allows sorting by a custom sequence of an attribute related to the target model. So you can do this:\nfrom django.db.models import Case, When from .models import Product, Order # Notice how we want to sort the products by the ids of the orders order_ids = [4, 2, 1, 3, 5] products = Product.objects.all() preferred = Case( *( When(order__id=id, then=pos) for pos, id in enumerate(order_ids, start=1) ) ) products_sorted = products.filter( order__id__in=order_ids ).order_by(preferred) Here\u0026rsquo;s what the Product and Order models look like:\nfrom django.db import models class Product(models.Model): name = models.CharField(max_length=200) price = models.FloatField() category = models.CharField(max_length=200) class Order(models.Model): product = models.ForeignKey(Product, on_delete=models.CASCADE) quantity = models.IntegerField() date = models.DateField() Order has a foreign key relationship with Product and we\u0026rsquo;re sorting the product queryset based on a custom sequence of order ids. The query generates the SQL below:\nSELECT blog_product.id, blog_product.name, blog_product.price, blog_product.category FROM blog_product LEFT OUTER JOIN blog_order ON (blog_product.id = blog_order.product_id) WHERE blog_order.id IN (4, 2, 1, 3, 5) ORDER BY CASE WHEN blog_order.id = 4 THEN 1 WHEN blog_order.id = 2 THEN 2 WHEN blog_order.id = 1 THEN 3 WHEN blog_order.id = 3 THEN 4 WHEN blog_order.id = 5 THEN 5 ELSE NULL END ASC Running the query gives us the following output:\n| id | name | price | category | |----|-----------|-------|------------| | 9 | Product 9 | 46.0 | Category 9 | | 6 | Product 6 | 83.0 | Category 6 | | 4 | Product 4 | 59.0 | Category 4 | | 11 | Product 1 | 51.0 | Category 1 | Further reading Django get a queryset from an array of id\u0026rsquo;s in a specific order - Stack Overflow ","permalink":"https://rednafi.com/python/sort-by-a-custom-sequence-in-django/","summary":"\u003cp\u003eI needed a way to sort a Django queryset based on a custom sequence of an attribute.\nTypically, Django allows sorting a queryset by any attribute on the model or related to it\nin either ascending or descending order. However, what if you need to sort the queryset\nfollowing a custom sequence of attribute values?\u003c/p\u003e\n\u003cp\u003eSuppose, you\u0026rsquo;re working with a model called \u003ccode\u003eProduct\u003c/code\u003e where you want to sort the rows of the\ntable based on a list of product ids that are already sorted in a particular order. Here\u0026rsquo;s\nhow it might look:\u003c/p\u003e","title":"Sorting a Django queryset by a custom sequence of an attribute"},{"content":"I recently gave my personal blog a fresh new look and decided it was time to spruce up my GitHub profile\u0026rsquo;s landing page as well. GitHub has a special way of treating the README.md file of your repo, displaying its content as the landing page for your profile. My goal was to showcase a brief introduction about myself and my work, along with a list of the five most recent articles on my blog. Additionally, I wanted to ensure that the article list stayed up to date.\nThere are plenty of fancy GitHub Action workflows like the blog-post-workflow that allow you to add your site\u0026rsquo;s URL to the CI file and it\u0026rsquo;ll periodically fetch the most recent content from the source and update the readme file. However, I wanted to make a simpler version of it from scratch which can be extended for periodically updating any Markdown file in any repo, just not the profile readme. So, here\u0026rsquo;s the plan:\nA custom GitHub Action workflow will periodically run a nodejs script. The script will then: Grab the XML index of this blog that you\u0026rsquo;re reading. Parse the XML content and extract the URLs and publication dates of 5 most recent articles. Update the associated Markdown table with the extracted content on the profile\u0026rsquo;s README.md file. Finally, the workflow will commit the changes and push them to the profile repo. You can see the final outcome in the profile repository. Here\u0026rsquo;s the script that performs the above steps:\n// importBlogs.js /* Import the latest 5 blog posts from rss feed */ import fetch from \u0026#34;node-fetch\u0026#34;; import { Parser } from \u0026#34;xml2js\u0026#34;; import { promises } from \u0026#34;fs\u0026#34;; const rssUrl = \u0026#34;https://rednafi.com/index.xml\u0026#34;; const header = `\u0026lt;div align=\u0026#34;center\u0026#34;\u0026gt; Introducing myself... \u0026lt;div\u0026gt;\\n\\n`; const outputFile = \u0026#34;README.md\u0026#34;; const parser = new Parser(); // Define an async function to get and parse the rss data async function getRssData() { try { const res = await fetch(rssUrl); const data = await res.text(); return await parser.parseStringPromise(data); } catch (err) { console.error(err); } } // Define an async function to write the output file async function writeOutputFile(output) { try { await promises.writeFile(outputFile, output); console.log(`Saved ${outputFile}`); } catch (err) { console.error(err); } } // Call the async functions getRssData() .then((result) =\u0026gt; { // Get the first five posts from the result object const posts = result.rss.channel[0].item.slice(0, 5); // Initialize an empty output string let output = \u0026#34;\u0026#34;; // Add a title to the output string output += header; // Add a header row to the output string output += `#### Recent articles\\n\\n`; output += \u0026#34;| Title | Published On |\\n\u0026#34;; output += \u0026#34;| ----- | ------------ |\\n\u0026#34;; // Loop through posts and add a row for each to output for (let post of posts) { // Strip the time from the pubDate const date = post.pubDate[0].slice(0, 16); output += `| [${post.title}](${post.link}) | ${date} |\\n`; } // Call the writeOutputFile function with the output string writeOutputFile(output); }) .catch((err) =\u0026gt; { // Handle the error console.error(err); }); The snippet above utilizes node-fetch to make HTTP calls,xml2js for XML parsing, and the built-in fs module\u0026rsquo;s promises for handling file system operations.\nNext, it defines an async function getRssData responsible for fetching the XML data from the [https://rednafi.com/index.xml] URL. It extracts the blog URLs and publication dates, and returns the parsed data as a list of objects. Another async function, writeOutputFile, writes the parsed XML content as a Markdown table and saves it to the README.md file.\nThe script is executed by the following GitHub Action workflow every day at 0:00 UTC. Before the CI runs, make sure you create a new Action Secret that houses a personal access token with write access to the repo where the CI runs.\n# Run a bash script to randomly generate empty commit to this repo. name: CI on: # Don\u0026#39;t run on push; this CI pushes (would cause infinite loop) # push: [ main ] # Add a schedule to run the job every day at 0:00 UTC schedule: - cron: \u0026#34;0 0 * * *\u0026#34; # Allow running this workflow manually workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - name: Checkout repo uses: actions/checkout@v4 with: # Required for pushing refs to destination repository fetch-depth: 0 ref: ${{ github.head_ref }} token: ${{ secrets.ACCESS_TOKEN }} - uses: actions/setup-node@v3 with: node-version: \u0026#34;lts/*\u0026#34; cache: npm cache-dependency-path: package-lock.json - name: Install dependencies run: | npm install - name: Run linter run: | npx prettier --write . - name: Run script run: | node scripts/importBlogs.js - name: Commit changes run: | git config --local user.name \\ \u0026#34;github-actions[bot]\u0026#34; git config --local user.email \\ \u0026#34;41898282+github-actions[bot]@users.noreply.github.com\u0026#34; git add . git diff-index --quiet HEAD \\ || git commit -m \u0026#34;Autocommit: updated at $(date -u)\u0026#34; - name: Push changes uses: ad-m/github-push-action@master with: force_with_lease: true In the first four steps, the workflow checks out the codebase, sets up nodejs, installs the dependencies, and then runs prettier on the scripts. Next, it executes the importBlogs.js script. The script updates the README and the subsequent shell commands commit the changes to the repo. The following line ensures that we\u0026rsquo;re only trying to commit when there\u0026rsquo;s a change in the tracked files.\ngit diff-index --quiet HEAD \\ || git commit -m \u0026#34;Autocommit: updated at $(date -u)\u0026#34; Then in the last step, we use an off-the-shelf workflow to push our changes to the repo. Check out the workflow directory of my profile\u0026rsquo;s repo to see the whole setup in action. I\u0026rsquo;m quite satisfied with the final output:\n","permalink":"https://rednafi.com/javascript/periodic-readme-updates-with-gh-actions/","summary":"\u003cp\u003eI recently gave my \u003ca href=\"https://rednafi.com/\"\u003epersonal blog\u003c/a\u003e a fresh new look and decided it was time to spruce up my\n\u003ca href=\"https://github.com/rednafi/\"\u003eGitHub profile\u003c/a\u003e\u0026rsquo;s landing page as well. GitHub has a \u003ca href=\"https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-github-profile/customizing-your-profile/managing-your-profile-readme\"\u003especial way of treating the README.md\nfile\u003c/a\u003e of your \u003cyour-username\u003e repo, displaying its content as the landing page for your\nprofile. My goal was to showcase a brief introduction about myself and my work, along with a\nlist of the five most recent articles on my blog. Additionally, I wanted to ensure that the\narticle list stayed up to date.\u003c/p\u003e","title":"Periodic readme updates with GitHub Actions"},{"content":"One of my favorite pastimes these days is to set BingChat to creative mode, ask it to teach me a trick about topic X, and then write a short blog post about it to reinforce my understanding. Some of the things it comes up with are absolutely delightful. In the spirit of that, I asked it to teach me a Shell trick that I can use to mimic maps or dictionaries in a shell environment. I didn\u0026rsquo;t even know what I was expecting.\nIt didn\u0026rsquo;t disappoint and introduced me to the idea of associative arrays in Bash. This data structure is basically the Bash equivalent of a map.\nFirst, we have our usual arrays which are containers that can store multiple values, indexed by numbers. Associative arrays are similar, but they use strings as keys instead of numbers. For example, if you want to store the names of some fruits in a regular array, you can use:\nfruits=(apple banana cherry) This will create an array called fruits with three elements. You can access the elements by using the index number inside brackets, such as:\necho ${fruits[0]} This prints:\napple You can also use a range of indices to get a slice of the array, such as:\necho ${fruits[@]:1:2} This will print:\nbanana cherry Moreover, you can use * or @ to get all the elements of the array, such as:\necho ${fruits[*]} This returns:\napple banana cherry Associative arrays are declared with the declare -A command, and then assigned values using the = operator and brackets. For example, if you want to store the prices of some fruits in an associative array, you can use:\ndeclare -A prices prices=([apple]=1.00 [banana]=0.50 [cherry]=2.00) Or you can create the key-value pairs in place like this:\ndeclare -A prices prices[apple]=1.00 prices[banana]=0.50 prices[cherry]=2.00 This will create an associative array called prices with three key-value pairs. You can access the values by using the keys inside brackets, such as:\necho ${prices[apple]} This will print:\n1.00 Similar to regular arrays, you can use * or @ to get all the keys or values of the associative array. Run the following command to get all the keys of the prices associative array:\necho ${!prices[*]} This will print:\napple banana cherry To get the values, run:\necho ${prices[@]} This returns:\n1.00 0.50 2.00 Arrays and associative arrays can be useful when you want to store and manipulate complex data structures in bash. You can use them to perform arithmetic operations, string operations, or loop over them with for or while commands. For example, you can use:\nfor fruit in ${!prices[*]}; do echo \u0026#34;$fruit costs ${prices[$fruit]}\u0026#34;; done In the above snippet, we iterate through the keys of prices in a for loop. The ${!prices[*]} notation expands to a list of all the keys in the prices array. Inside the loop, we print the key-value pairs, where $fruit represents the current key and ${prices[$fruit]} represents the corresponding value. So in each iteration, the snippet will output the name of each fruit along with its corresponding price.\nRunning the snippet will print:\napple costs 1.00 banana costs 0.50 cherry costs 2.00 A more practical example Here\u0026rsquo;s a script that downloads three famous RFCs using cURL. We\u0026rsquo;re using an associative array for bookkeeping purposes.\n#!/usr/bin/env bash set -euo pipefail declare -A rfc_urls=( [http-error]=\u0026#34;https://www.rfc-editor.org/rfc/rfc7808.txt\u0026#34; [http-one]=\u0026#34;https://www.rfc-editor.org/rfc/rfc7231.txt\u0026#34; [datetime-format]=\u0026#34;https://www.rfc-editor.org/rfc/rfc3339.txt\u0026#34; ) echo \u0026#34;======================\u0026#34; echo \u0026#34;start downloading rfcs\u0026#34; echo \u0026#34;======================\u0026#34; echo \u0026#34;\u0026#34; for key in \u0026#34;${!rfc_urls[@]}\u0026#34;; do value=${rfc_urls[$key]} echo \u0026#34;Downloading rfcs ${key}: ${value}\u0026#34; curl -OJLs \u0026#34;${value}\u0026#34; done echo \u0026#34;\u0026#34; echo \u0026#34;======================\u0026#34; echo \u0026#34;done downloading rfcs\u0026#34; echo \u0026#34;======================\u0026#34; Running this will download the RFCs in the current directory:\n====================== start downloading rfcs ====================== Downloading rfcs http-error: https://www.rfc-editor.org/rfc/rfc7808.txt Downloading rfcs datetime-format: https://www.rfc-editor.org/rfc/rfc3339.txt Downloading rfcs http-one: https://www.rfc-editor.org/rfc/rfc7231.txt ====================== done downloading rfcs ====================== The script begins by declaring an associative array called rfc_urls. This array serves as a convenient way to keep track of the RFCs we want to download. Each key in the array represents a unique identifier for an RFC, while the corresponding value holds the complete URL to download that specific RFC.\nInside a loop that iterates over the keys of the rfc_urls array, we retrieve the URL value associated with each key. To provide a progress update, we echo a message indicating the RFC being downloaded.\nUsing the curl command with the options -OJLs, we initiate the download process. The -O flag ensures that the remote file is saved with its original filename, while the -J flag takes advantage of the Content-Disposition header in the HTTP response to determine the filename. We include the -L flag to follow redirects, and the -s flag to silence curl\u0026rsquo;s progress output.\nFurther reading Advanced Bash scripting guide - devconnected\nAdvanced Bash scripting techniques for Linux administrators\nUseful Bash command line tips and tricks\nLearning Bash with command line games\n","permalink":"https://rednafi.com/misc/associative-arrays-in-bash/","summary":"\u003cp\u003eOne of my favorite pastimes these days is to set BingChat to creative mode, ask it to teach\nme a trick about topic X, and then write a short blog post about it to reinforce my\nunderstanding. Some of the things it comes up with are absolutely delightful. In the spirit\nof that, I asked it \u003cem\u003eto teach me a Shell trick that I can use to mimic maps or dictionaries\nin a shell environment\u003c/em\u003e. I didn\u0026rsquo;t even know what I was expecting.\u003c/p\u003e","title":"Associative arrays in Bash"},{"content":"Whenever I need to deduplicate the items of an iterable in Python, my usual approach is to create a set from the iterable and then convert it back into a list or tuple. However, this approach doesn\u0026rsquo;t preserve the original order of the items, which can be a problem if you need to keep the order unscathed. Here\u0026rsquo;s a naive approach that works:\nfrom __future__ import annotations from collections.abc import Iterable # Python \u0026gt;3.9 def dedup(it: Iterable) -\u0026gt; list: seen = set() result = [] for item in it: if item not in seen: seen.add(item) result.append(item) return result it = (2, 1, 3, 4, 66, 0, 1, 1, 1) deduped_it = dedup(it) # Gives you [2, 1, 3, 4, 66, 0] This code snippet defines a function dedup that takes an iterable it as input and returns a new list containing the unique items of the input iterable in their original order. The function uses a set seen to keep track of the items that have already been seen, and a list result to store the unique items.\nThen it iterates over all the items of the input iterable using a for loop. For each item, the function checks if it has already been seen (i.e., if it\u0026rsquo;s in the seen set). If the item hasn\u0026rsquo;t been seen, it\u0026rsquo;s added to both the seen set and the result list. The final result list contains the unique items of it in their original order.\nThis can be made a little terser by using listcomp as follows:\nfrom __future__ import annotations from collections.abc import Iterable # Python \u0026gt;3.9 def dedup(it: Iterable) -\u0026gt; list: seen = set() # Binding seen.add to a variable reduces the cost of attribute # fetching within a tight loop seen_add = seen.add # Here, \u0026#39;or\u0026#39; allows us to add the item to \u0026#39;seen\u0026#39; when it doesn\u0026#39;t # already exist there in a single line. return [item for item in it if not (item in seen or seen_add(item))] Dedup with ordered dict from collections import OrderedDict dedup = lambda it: list(OrderedDict.fromkeys(seq)) it = (2, 1, 3, 4, 66, 0, 1, 1, 1) deduped_it = dedup(it) # Gives you [2, 1, 3, 4, 66, 0] Similar to the first snippet, this also defines dedup that takes an iterable it as input and returns a new list containing the unique items of it in their original order. The function uses the OrderedDict.fromkeys() method to create a new ordered dict with the items of it as keys and None as values.\nSince an ordered dict maintains the insertion order of its keys, this effectively removes any duplicate items from the iterable without affecting the order of the remaining ones. The iterable containing the keys of the resulting ordered dict is then converted into a list using the list() function to obtain a list of the unique items in their original order.\nWhile this is quite terse and does the job with O(1) complexity, neither this nor the previous solution would work for compound iterables as follows:\n# Here the dedup function will have to remove the duplicate items by # nested items. So the desired output will be ((1,1), (2, 1), (3, 1)) it = ((1, 1), (2, 1), (3, 1), (1, 1), (1, 3)) The next solution works on one-level nested iterables.\nDedup by any element of an item in a nested iterable Consider this one-level nested iterable:\n# Here, (1,1), (2, 1) are items of the iterable \u0026#39;it\u0026#39; and 1 is an element # of the first item (1,1). We\u0026#39;re referring to items of items as elements. it = ((1, 1), (2, 1), (3, 1), (1, 1), (1, 3)) We want to write a dedup function that\u0026rsquo;ll allow us to deduplicate the iterable based on a particular element of an item. Here, (1,1), (2, 1) are items of the iterable it and 1 is the second element of item (2, 1). Here\u0026rsquo;s how we can modify the first dedup to allow deduplication by nested elements.\nfrom __future__ import annotations from collections.abc import Iterable from typing import Any, Generator def dedup( it: Iterable[tuple[Any, ...]], index: int, lazy: bool = True ) -\u0026gt; list[Any] | Generator[Any, None, None]: seen = set() # type: set[Any] seen_add = seen.add expr = ( item for item in it if not ((elem := item[index]) in seen or seen_add(elem)) ) return expr if lazy else list(expr) it = ((1, 1), (2, 1), (3, 1), (1, 1), (1, 3)) # We\u0026#39;re deduplicating by the second element of the items. dedup(it, 2, False) # Returns [(1,1), (1,3)] This time, the dedup function takes in an iterable of tuples it, an element index index, and a boolean lazy (defaulting to True) as arguments. The function returns a list or generator of the unique items from the input iterable based on the specified element index.\nJust as before, the function first creates an empty set seen and binds its add method to a variable seen_add. It then creates a generator expression that iterates over it and yields each item if its element at the specified index isn\u0026rsquo;t already present in the seen set. If item[index] isn\u0026rsquo;t present in seen, it\u0026rsquo;s added to it using the seen_add method.\nIf lazy is True, the function returns the generator expression verbatim. Otherwise, it returns a list created from the generator expression.\nIn the example provided, the function is called with arguments it, 1, and False. This means that it will deduplicate the input iterable based on the second element of each tuple and return a list. The result is [(1,1), (1,3)].\nFurther reading How do I remove duplicates from a list while preserving order ","permalink":"https://rednafi.com/python/deduplicate-iterables-while-preserving-order/","summary":"\u003cp\u003eWhenever I need to deduplicate the items of an iterable in Python, my usual approach is to\ncreate a set from the iterable and then convert it back into a list or tuple. However, this\napproach doesn\u0026rsquo;t preserve the original order of the items, which can be a problem if you\nneed to keep the order unscathed. Here\u0026rsquo;s a naive approach that works:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003e__future__\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eannotations\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003ecollections.abc\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eIterable\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# Python \u0026gt;3.9\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ededup\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eIterable\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003elist\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eseen\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003eset\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eresult\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003eitem\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eitem\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003eseen\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003eseen\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eadd\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eitem\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003eresult\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eappend\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eitem\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eresult\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eit\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e66\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ededuped_it\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ededup\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# Gives you [2, 1, 3, 4, 66, 0]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis code snippet defines a function \u003ccode\u003ededup\u003c/code\u003e that takes an iterable \u003ccode\u003eit\u003c/code\u003e as input and\nreturns a new list containing the unique items of the input iterable in their original\norder. The function uses a set \u003ccode\u003eseen\u003c/code\u003e to keep track of the items that have already been\nseen, and a list \u003ccode\u003eresult\u003c/code\u003e to store the unique items.\u003c/p\u003e","title":"Deduplicating iterables while preserving order in Python"},{"content":"I needed to compare two large directories with thousands of similarly named PDF files and find the differing filenames between them. In the first pass, this is what I did:\nListed out the content of the first directory and saved it in a file:\nls dir1 \u0026gt; dir1.txt Did the same for the second directory:\nls dir2 \u0026gt; dir2.txt Compared the difference between the two outputs:\ndiff dir1.txt dir2.txt This returned the name of the differing files likes this:\n3c3,4 \u0026lt; f3.pdf --- \u0026gt; f4.pdf \u0026gt; f5.pdf It does the job, but I asked BingChat if there\u0026rsquo;s a better way to accomplish the task without creating intermediate files, and it didn\u0026rsquo;t let me down. Turns out that in Bash, process substitution allows you to do just that. Instead of running three commands, you can achieve the same result with a simple one-liner:\ndiff \u0026lt;(ls dir1) \u0026lt;(ls dir2) Process substitution In Bash, process substitution is a feature that allows you to treat the output of a command or commands as if it were a file. It enables you to use the output of a command as an input to another command or perform other operations that expect file input or output.\nOne important thing to point out is that process substitution is specific to Bash, Zsh, and certain versions of Ksh. Other shells and Bash in POSIX mode don\u0026rsquo;t understand it. Bash, Zsh, and Ksh (88,93) support process substitution, but pdksh derivatives like mksh don\u0026rsquo;t currently have this capability.* The syntax for process substitution is as follows:\n\u0026lt;(command): This form allows you to use the output of a command as a file-like input. \u0026gt;(command): This form allows you to use the output of a command as a file-like output. When using process substitution, Bash creates a named pipe (FIFO) or a special file descriptor /dev/fd/\u0026lt;n\u0026gt; behind the scenes. The command within the parentheses is executed, and its output is redirected to the named pipe or file descriptor. Then, the path to the named pipe or file descriptor is substituted into the original command line.\nThis is different from the plain-old stdin or stdout redirection. Here\u0026rsquo;s how:\nInput\nPlain redirection: When using plain stdin redirection (\u0026lt;), you can redirect input from a file, for example, \u0026lt; input.txt. The command reads the content of the file as standard input (stdin). Process substitution: With process substitution, you can use the output of a command as input. For example, command \u0026lt; \u0026lt;(echo \u0026quot;input\u0026quot;). Here, the output of the echo command is treated as a file-like object and used as the input to command. Output\nPlain redirection: Using plain stdout redirection (\u0026gt; or \u0026gt;\u0026gt;), you can redirect the output (stdout) of a command to a file, for example, command \u0026gt; output.txt. The command\u0026rsquo;s output is written to the specified file. Process substitution: With process substitution, you can use the output of a command as output. For example, command \u0026gt;(process_output). Here, the output of command is treated as a file-like output, and it is passed as input to the process_output command or operation. By using process substitution, the output of a command can be seamlessly integrated into other commands as if it were a file, even if the command doesn\u0026rsquo;t explicitly support stdin or stdout redirection. This allows for greater compatibility and enables the use of the output in situations where direct piping or redirection may not be possible.\nA few practical examples Inspecting the descriptors involved in process substitution You can inspect the descriptor used by a process substitution like this:\necho \u0026gt;(true) \u0026lt;(false) This returns:\n/dev/fd/13 /dev/fd/11 Here, the expression \u0026gt;(true) creates a temporary file-like object, and the true command serves as a placeholder for its input. Similarly, \u0026lt;(false) creates another temporary file-like object with the false command serving as a placeholder for its output. When the echo command is executed, it displays the filenames associated with these temporary file-like objects, which are /dev/fd/13 and /dev/fd/11 in this specific scenario. These filenames represent the underlying descriptors of the respective process substitutions, indicating the file descriptors associated with the temporary objects created during the process substitution.\nCalculating the total number of lines in a file wc -l \u0026lt; \u0026lt;(cat input.txt) This command calculates the total number of lines in the input.txt file. Here, the \u0026lt;(cat input.txt) commad creates an input-type file descriptor containing the output of the cat command and wc -l reads that content from there. The extra \u0026lt; redirects the file-like object as an input stream again. This is a roundabout way of doing the following:\ncat input.txt | wc -l Processing the content of a file line by line while read line; do echo $line; done \u0026lt; \u0026lt;(cat input.txt) This command reads each line from the file input.txt and echoes it. It uses a while loop with the read command to iterate over the lines, assigning each line to the variable line and the echo $line command displays the line. Process substitution \u0026lt;() is used to treat the output of cat input.txt as a temporary file, providing the input to the loop.\nComparing directory sizes diff -r \u0026lt;(du -sh dir1) \u0026lt;(du -sh dir2) The command compares the disk usage of two directories, dir1 and dir2, using the diff command. The process substitution \u0026lt;() is employed to capture the output of the du -sh command, which calculates the disk usage of each directory and provides a summary in a human-readable format. The output of each du -sh command, representing the disk usage of dir1 and dir2, is treated as temporary files and passed as arguments to the diff command. This enables the comparison of the disk usage between the two directories, highlighting any discrepancies in file sizes or subdirectories.\nPicking or rejecting lines common between two sorted files comm \u0026lt;(echo \u0026#39;hello world\\nhello mars\u0026#39; | sort) \\ \u0026lt;(echo \u0026#39;hello world\\nhello venus\u0026#39; | sort) This returns:\nhello mars hello venus hello world This performs a comparison between the sorted outputs of two separate commands using comm. The comm command expects two files but we\u0026rsquo;re using process substitution to make two file-like objects from stdout.\nWithin the first process substitution \u0026lt;(), echo is used to generate a string containing two lines: hello world and hello mars. This string is then piped to the sort command, which sorts the lines alphabetically.\nSimilarly, the second part of the command \u0026lt;() uses process substitution as well. It follows the same pattern as the first process substitution, but this time the string contains hello world and hello venus.\nThe file-like objects containing the sorted output from the two process substitutions are then passed as arguments to the comm command. Then comm compares the input files line by line and generates three columns of output: lines unique to the first input, lines unique to the second input, and lines common to both inputs.\nFurther reading Process substitution in Bash ","permalink":"https://rednafi.com/misc/process-substitution-in-bash/","summary":"\u003cp\u003eI needed to compare two large directories with thousands of similarly named PDF files and\nfind the differing filenames between them. In the first pass, this is what I did:\u003c/p\u003e\n\u003cp\u003eListed out the content of the first directory and saved it in a file:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003els dir1 \u0026gt; dir1.txt\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eDid the same for the second directory:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003els dir2 \u0026gt; dir2.txt\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCompared the difference between the two outputs:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ediff dir1.txt dir2.txt\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis returned the name of the differing files likes this:\u003c/p\u003e","title":"Process substitution in Bash"},{"content":"Whenever I need to whip up a quick command line tool, my go-to is usually Python. Python\u0026rsquo;s CLI solutions tend to be more robust than their Shell counterparts. However, dealing with its portability can sometimes be a hassle, especially when all you want is to distribute a simple script. That\u0026rsquo;s why while toying around with argparse to create a dynamic menu, I decided to ask ChatGPT if there\u0026rsquo;s a way to achieve the same using native shell scripting. Delightfully, it introduced me to the dead-simple select command that I probably should\u0026rsquo;ve known about years ago. But I guess better late than never! Here\u0026rsquo;s what I was trying to accomplish:\nPrint a menu that allows a user to choose an option and then trigger a specific function associated with the chosen option. When you run the script, it should present you with something similar to this:\n1) Option 1 2) Option 2 3) Option 3 4) Quit Please enter your choice: 1 You selected Option 1. Please enter your choice: 2 You selected Option 2. Please enter your choice: 3 You selected Option 3. Please enter your choice: 4 Whenever the user selects an option, the script dispatches an associated function with the option. Currently, the associated function just prints You selected option x but it has the freedom to do whatever it wants. The following native Shell script uses select to produce the output above:\n#!/usr/bin/env bash # script.sh # Define an array of menu options options=(\u0026#34;Option 1\u0026#34; \u0026#34;Option 2\u0026#34; \u0026#34;Option 3\u0026#34; \u0026#34;Quit\u0026#34;) # Function to handle Option 1 function option1 { echo \u0026#34;You selected Option 1.\u0026#34; # Add your Option 1 code here } # Function to handle Option 2 function option2 { echo \u0026#34;You selected Option 2.\u0026#34; # Add your Option 2 code here } # Function to handle Option 3 function option3 { echo \u0026#34;You selected Option 3.\u0026#34; # Add your Option 3 code here } # Display the menu and process user selection PS3=\u0026#34;Please enter your choice: \u0026#34; select option in \u0026#34;${options[@]}\u0026#34;; do case $option in \u0026#34;Option 1\u0026#34;) option1 ;; \u0026#34;Option 2\u0026#34;) option2 ;; \u0026#34;Option 3\u0026#34;) option3 ;; \u0026#34;Quit\u0026#34;) break ;; *) echo \u0026#34;Invalid option. Try again.\u0026#34; ;; esac done The snippet allows users to make selections from a list of options. It starts by defining an array called options which holds the available possibilities. Each option corresponds to a specific function, such as option1, option2, and option3 which can be customized to perform specific actions or tasks.\nThe script then prompts the user to enter their choice using the select statement. The user\u0026rsquo;s selection is stored in the variable option. Then it uses a case statement to match the selected option and execute the corresponding function. For example, if the user chooses Option 1 by typing 1 into the console, the script calls the option1 function and displays a message confirming the selection. The same applies to Option 2 and Option 3. If the user selects Quit by typing 4, the script breaks out of the loop and terminates. Moreover, if the user enters an invalid option, the script displays an error message indicating that the option is not recognized and prompts the user to try again.\nHere\u0026rsquo;s a little more useful script to run some common Docker commands based on the user\u0026rsquo;s selection. The script assumes that Docker engine is installed on the targeted system:\n#!/usr/bin/env bash # Define an array of menu options options=( \u0026#34;Show Docker Images\u0026#34; \u0026#34;Remove Docker Image\u0026#34; \u0026#34;Show Docker Containers\u0026#34; \u0026#34;Remove Docker Container\u0026#34; \u0026#34;Stop All Containers\u0026#34; \u0026#34;Reprint Options\u0026#34; \u0026#34;Quit\u0026#34; ) # Function to show all Docker images function show_docker_images { echo \u0026#34;Listing Docker images:\u0026#34; docker images } # Function to remove a Docker image function remove_docker_image { read -p \u0026#34;Enter the image ID or name to remove: \u0026#34; image_id echo \u0026#34;Removing Docker image: $image_id\u0026#34; docker rmi \u0026#34;$image_id\u0026#34; } # Function to show all Docker containers function show_docker_containers { echo \u0026#34;Listing Docker containers:\u0026#34; docker ps -a } # Function to remove a Docker container function remove_docker_container { read -p \u0026#34;Enter the container ID or name to remove: \u0026#34; container_id echo \u0026#34;Removing Docker container: $container_id\u0026#34; docker rm \u0026#34;$container_id\u0026#34; } # Function to stop all Docker containers function stop_all_containers { echo \u0026#34;Stopping all Docker containers:\u0026#34; docker stop $(docker ps -aq) } # Function to reprint the options function reprint_options { echo \u0026#34;Available Options:\u0026#34; for index in \u0026#34;${!options[@]}\u0026#34;; do echo \u0026#34;$((index+1))) ${options[index]}\u0026#34; done } # Display the menu and process user selection PS3=\u0026#34;Please enter your choice: \u0026#34; select option in \u0026#34;${options[@]}\u0026#34;; do case $option in \u0026#34;Show Docker Images\u0026#34;) show_docker_images ;; \u0026#34;Remove Docker Image\u0026#34;) remove_docker_image ;; \u0026#34;Show Docker Containers\u0026#34;) show_docker_containers ;; \u0026#34;Remove Docker Container\u0026#34;) remove_docker_container ;; \u0026#34;Stop All Containers\u0026#34;) stop_all_containers ;; \u0026#34;Reprint Options\u0026#34;) reprint_options ;; \u0026#34;Quit\u0026#34;) break ;; *) echo \u0026#34;Invalid option. Try again.\u0026#34; ;; esac done ","permalink":"https://rednafi.com/misc/dynamic-menu-with-select-in-bash/","summary":"\u003cp\u003eWhenever I need to whip up a quick command line tool, my go-to is usually Python. Python\u0026rsquo;s\nCLI solutions tend to be more robust than their Shell counterparts. However, dealing with\nits portability can sometimes be a hassle, especially when all you want is to distribute a\nsimple script. That\u0026rsquo;s why while toying around with \u003ccode\u003eargparse\u003c/code\u003e to create a dynamic menu, I\ndecided to ask ChatGPT if there\u0026rsquo;s a way to achieve the same using native shell scripting.\nDelightfully, it introduced me to the dead-simple \u003ccode\u003eselect\u003c/code\u003e command that I probably should\u0026rsquo;ve\nknown about years ago. But I guess better late than never! Here\u0026rsquo;s what I was trying to\naccomplish:\u003c/p\u003e","title":"Dynamic menu with select statement in Bash"},{"content":"When writing shell scripts, I\u0026rsquo;d often resort to using hardcoded ANSI escape codes to format text, such as:\n#!/usr/bin/env bash BOLD=\u0026#34;\\033[1m\u0026#34; UNBOLD=\u0026#34;\\033[22m\u0026#34; FG_RED=\u0026#34;\\033[31m\u0026#34; BG_YELLOW=\u0026#34;\\033[43m\u0026#34; BG_BLUE=\u0026#34;\\033[44m\u0026#34; RESET=\u0026#34;\\033[0m\u0026#34; # Print a message in bold red text on a yellow background. echo -e \u0026#34;${BOLD}${FG_RED}${BG_YELLOW}This is a warning message${RESET}\u0026#34; # Print a message in white text on a blue background. echo -e \u0026#34;${BG_BLUE}This is a debug message${RESET}\u0026#34; This shell snippet above shows how to add text formatting and color to shell script output via ANSI escape codes. It defines a few variables that contain different escape codes for bold, unbold, foreground, and background colors. Then, we echo two log messages with different colors and formatting options.\nThe first message is printed in bold red text on a yellow background, while the second message is printed in white text on a blue background. To ensure that subsequent output is not affected by the previous formatting, the RESET variable is used to reset all color and formatting options back to their defaults after each message is printed. The -e option is used with echo to enable the interpretation of backslash escapes, which includes the ANSI escape codes.\nWhile this works fairly well, every time I have to write a fancy shell script, I have to either look up the ANSI color codes, copy-paste from an existing script, or explain to an LLM what I need. Then chatGPT serendipitously recommended a shell tool called tput that makes this workflow quite a bit better. Underneath tput also uses ANSI escape codes to control various text formatting options but it doesn\u0026rsquo;t require you to hardcode these ugly escape codes.\nBasic usage The basic syntax of the tput command goes as follows:\ntput \u0026lt;formatting_option\u0026gt; Formatting options Here are some commonly used tput formatting options:\nsetaf \u0026lt;color\u0026gt;: set the foreground (text) color to a specific color. For example, setaf 1 sets the color to red, while setaf 2 sets the color to green. setab \u0026lt;color\u0026gt;: set the background color to a specific color. bold: set the text to bold. sgr0: reset all formatting options to their defaults. smul: underline the text. Example usage #!/usr/bin/env bash # Print text in red on a yellow background tput setaf 1 tput setab 3 echo \u0026#34;Error: some error occurred\u0026#34; tput sgr0 # Print bold text tput bold echo \u0026#34;This text is bold\u0026#34; tput sgr0 # Print underlined text in blue tput setaf 4 tput smul echo \u0026#34;This text is underlined and blue\u0026#34; tput sgr0 # Print text with a custom RGB color tput setaf 38 # specify an RGB color using 8-bit mode tput setaf 5 # specify a color index in 256-color mode echo \u0026#34;This text is in a custom color\u0026#34; # Print text with a background color gradient tput setaf 0 for i in {0..7}; do tput setab $i echo \u0026#34;Background color $i\u0026#34; done tput sgr0 # Print blinking text tput blink echo \u0026#34;This text is blinking\u0026#34; tput sgr0 Running the script will give you the following output:\nThis also hardcodes the color and formatting codes but it\u0026rsquo;s much easier than having to remember or search for the ANSI escape codes. Currently, I\u0026rsquo;m using a 256-bit macOS terminal and it supports fairly large sets of formatting options. You can run man tput to find out other features that are supported by your terminal. The following loop will print all the supported colors:\nfor i in {0..255}; do tput setab $i printf \u0026#34; \u0026#34; tput sgr0 done On my terminal, it prints this nice color palette:\n","permalink":"https://rednafi.com/misc/terminal-text-formatting-with-tput/","summary":"\u003cp\u003eWhen writing shell scripts, I\u0026rsquo;d often resort to using hardcoded \u003ca href=\"https://gist.github.com/fnky/458719343aabd01cfb17a3a4f7296797\"\u003eANSI escape codes\u003c/a\u003e to\nformat text, such as:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cp\"\u003e#!/usr/bin/env bash\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003eBOLD\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\\033[1m\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003eUNBOLD\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\\033[22m\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003eFG_RED\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\\033[31m\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003eBG_YELLOW\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\\033[43m\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003eBG_BLUE\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\\033[44m\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003eRESET\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\\033[0m\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Print a message in bold red text on a yellow background.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eecho\u003c/span\u003e -e \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"si\"\u003e${\u003c/span\u003e\u003cspan class=\"nv\"\u003eBOLD\u003c/span\u003e\u003cspan class=\"si\"\u003e}${\u003c/span\u003e\u003cspan class=\"nv\"\u003eFG_RED\u003c/span\u003e\u003cspan class=\"si\"\u003e}${\u003c/span\u003e\u003cspan class=\"nv\"\u003eBG_YELLOW\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003eThis is a warning message\u003c/span\u003e\u003cspan class=\"si\"\u003e${\u003c/span\u003e\u003cspan class=\"nv\"\u003eRESET\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Print a message in white text on a blue background.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eecho\u003c/span\u003e -e \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"si\"\u003e${\u003c/span\u003e\u003cspan class=\"nv\"\u003eBG_BLUE\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003eThis is a debug message\u003c/span\u003e\u003cspan class=\"si\"\u003e${\u003c/span\u003e\u003cspan class=\"nv\"\u003eRESET\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis shell snippet above shows how to add text formatting and color to shell script output\nvia ANSI escape codes. It defines a few variables that contain different escape codes for\nbold, unbold, foreground, and background colors. Then, we \u003ccode\u003eecho\u003c/code\u003e two log messages with\ndifferent colors and formatting options.\u003c/p\u003e","title":"Simple terminal text formatting with tput"},{"content":"Whenever I plan to build something, I spend 90% of my time researching and figuring out the idiosyncrasies of the tools that I decide to use for the project. LLM tools like ChatGPT has helped me immensely in that regard. I\u0026rsquo;m taking on more tangential side projects because they\u0026rsquo;re no longer as time-consuming as they used to be and provide me with an immense amount of joy and learning opportunities. While LLM interfaces like ChatGPT may hallucinate, confabulate, and confidently give you misleading information, they also allow you to avoid starting from scratch when you decide to work on something. Personally, this benefits me enough to keep language models in my tool belt and use them to churn out more exploratory work at a much faster pace.\nFor some strange reason, I never took the time to explore ObservableHQ, despite knowing what it does and how it can help me quickly build nifty client-side tools without going through the hassle of containerizing and deploying them as dedicated applications. So, I asked ChatGPT to build me a tool that would allow me to:\nUpload two CSV files Calculate the row and column counts from the files Show the number of rows and columns in a table and include the headers of the columns and their corresponding index numbers, so that you can compare them easily. Here\u0026rsquo;s the initial prompt that I used:\nGive me the JavaScript code for an Observable notebook that\u0026rsquo;ll allow me to upload a CSV file, calculate the row and column counts from it, and then display the stats with column headers and their corresponding index starting from 0. Display the info in an HTML table.\nThen I refactored the JavaScript it returned so that it\u0026rsquo;ll allow me to upload two CSV files and compare their stats. I made ChatGPT do it for me with this follow-up prompt:\nCan you change the code so that it allows uploading two CSV files and displays the stats of both of them in two HTML tables? Don\u0026rsquo;t blindly repeat the logic from the previous section twice.\nFinally, I asyncified the code and changed some HTML parsing to make the table look a bit better. Here\u0026rsquo;s the complete 85-line code snippet:\n{ // create file input elements for the two files const fileInput1 = html`\u0026lt;input type=\u0026#34;file\u0026#34; /\u0026gt;`; const fileInput2 = html`\u0026lt;input type=\u0026#34;file\u0026#34; /\u0026gt;`; // create empty HTML tables for the two files const table1 = html` \u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;\u0026lt;/tbody\u0026gt; \u0026lt;/table\u0026gt;`; const table2 = table1.cloneNode(true); // function to handle file load event and display stats in table const handleFileLoad = (table) =\u0026gt; async (event) =\u0026gt; { const file = event.target.files[0]; const reader = new FileReader(); // read the file contents as text reader.readAsText(file); // create a promise to wait for the file to load and parse const fileLoaded = new Promise((resolve, reject) =\u0026gt; { reader.onload = () =\u0026gt; { const contents = reader.result; const lines = contents.trim().split(\u0026#34;\\n\u0026#34;); const headers = lines[0].split(\u0026#34;,\u0026#34;); const numColumns = headers.length; const numRows = lines.length - 1; // create a row for the number of rows const numRowsRow = html`\u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;Number of rows:\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;${numRows}\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt;`; // create a row for the number of columns const numColsRow = html`\u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;Number of columns:\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;${numColumns}\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt;`; // create a row for the column names const headerRow = html`\u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;Column names:\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;${headers.map((h, i) =\u0026gt; `${i}: ${h}`).join(\u0026#34;, \u0026#34;)}\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt;`; // add the rows to the table body const tableBody = html`\u0026lt;tbody\u0026gt; ${numRowsRow}${numColsRow}${headerRow} \u0026lt;/tbody\u0026gt;`; table.replaceChild(tableBody, table.lastChild); // resolve the promise with the parsed data resolve({ numRows, numColumns, headers }); }; reader.onerror = () =\u0026gt; { reject(reader.error); }; }); // Wait for promise before displaying results in table try { const { numRows, numColumns, headers } = await fileLoaded; console.log( `File loaded: ${file.name}, Rows: ${numRows}, Columns: ${numColumns}, Headers: ${headers}` ); } catch (err) { console.error(err); } }; // add event listeners to the file input elements fileInput1.addEventListener(\u0026#34;change\u0026#34;, handleFileLoad(table1)); fileInput2.addEventListener(\u0026#34;change\u0026#34;, handleFileLoad(table2)); // display the file input and table elements in the notebook return html`${fileInput1} ${table1} ${fileInput2} ${table2}`; } The snippet above starts by creating two file input elements using HTML input tags. These are used to allow the user to select and upload CSV files. Two empty HTML tables are also created to hold the extracted statistics for each CSV file.\nNext, it defines a function called handleFileLoad which takes a table element as its argument. This function is called when the user uploads a file, and it reads the contents of the file and extracts some basic statistics from it. These statistics are then used to populate the HTML table with the extracted information.\nInside the handleFileLoad function, the FileReader API is used to read the contents of the uploaded file. The file contents are then parsed as text and split into lines. The first line contains the column headers, which are extracted by splitting the line by commas. The number of columns is then determined by the number of headers, and the number of rows is determined by counting the number of lines in the file (excluding the header).\nIt then creates three rows for the extracted statistics: one row for the number of rows, one row for the number of columns, and one row for the column headers with their corresponding indexes starting from zero. The rows are then added to the HTML table.\nFinally, the code adds event listeners to the file input elements to trigger the handleFileLoad function when the user uploads a file. The file input elements and HTML tables are then returned as an HTML fragment using the HTML template literal, and displayed in the notebook.\nYou can find the working application embedded in the following section. Try uploading two CSV files by clicking on the Choose File button and see how the app displays the stats in separate HTML tables.\nHere\u0026rsquo;s a gif of it in action:\nClick on the following thumbnail to take the notebook for a spin:\n","permalink":"https://rednafi.com/javascript/exploring-observable-notebooks/","summary":"\u003cp\u003eWhenever I plan to build something, I spend 90% of my time researching and figuring out the\nidiosyncrasies of the tools that I decide to use for the project. LLM tools like ChatGPT has\nhelped me immensely in that regard. I\u0026rsquo;m taking on more tangential side projects because\nthey\u0026rsquo;re no longer as time-consuming as they used to be and provide me with an immense amount\nof joy and learning opportunities. While LLM interfaces like ChatGPT may hallucinate,\nconfabulate, and confidently give you misleading information, they also allow you to avoid\nstarting from scratch when you decide to work on something. Personally, this benefits me\nenough to keep language models in my tool belt and use them to churn out more exploratory\nwork at a much faster pace.\u003c/p\u003e","title":"Building a web app to display CSV file stats with ChatGPT \u0026 Observable"},{"content":"In multi-page web applications, a common workflow is where a user:\nLoads a specific page or clicks on some button that triggers a long-running task. On the server side, a background worker picks up the task and starts processing it asynchronously. The page shouldn\u0026rsquo;t reload while the task is running. The backend then communicates the status of the long-running task in real-time. Once the task is finished, the client needs to display a success or an error message depending on the final status of the finished task. The de facto tool for handling situations where real-time bidirectional communication is necessary is WebSocket. However, in the case above, you can see that the communication is mostly unidirectional where the client initiates some action in the server and then the server continuously pushes data to the client during the lifespan of the background job.\nIn Django, I usually go for the channels library whenever I need to do any real-time communication over WebSockets. It\u0026rsquo;s a fantastic tool if you need real-time full duplex communication between the client and the server. But it can be quite cumbersome to set up, especially if you\u0026rsquo;re not taking full advantage of it or not working with Django. Moreover, WebSockets can be quite flaky and usually have quite a bit of overhead. So, I was looking for a simpler alternative and found out that Server-Sent Events (SSEs) work quite nicely when all I needed was to stream some data from the server to the client in a unidirectional manner.\nServer-Sent Events (SSEs) Server-Sent Events (SSE) is a way for a web server to send real-time updates to a web page without the need for the page to repeatedly ask for updates. Instead of the page asking the server for new data every few seconds, the server can just send updates as they happen, like a live stream. This is useful for things like live chat, news feeds, and stock tickers but won\u0026rsquo;t work in situations where you also need to send real-time updates from the client to the server. In the latter scenarios, WebSockets are kind of your only option.\nSSEs are sent over traditional HTTP. That means they don\u0026rsquo;t need any special protocol or server implementation to get working. WebSockets on the other hand, need full-duplex connections and new WebSocket servers like Daphne to handle the protocol. In addition, SSEs have a variety of features that WebSockets lack by design such as automatic reconnection, event IDs, and the ability to send arbitrary events. This is quite nice since on the browser, you won\u0026rsquo;t have to write additional logic to handle reconnections and stuff.\nThe biggest reason why I wanted to explore SSE is because of its simplicity and the fact that it plays in the HTTP realm. If you want to learn more about how SSEs stack up against WebSockets, I recommend this SSE vs WebSockets post by Germano Gabbianelli.\nThe wire protocol The wire protocol works on top of HTTP and is quite simple. The server needs to send the data maintaining the following structure:\nHTTP/1.1 200 OK date: Sun, 02 Apr 2023 20:17:53 GMT server: uvicorn content-type: text/event-stream access-control-allow-origin: * cache-control: no-cache Transfer-Encoding: chunked event: start data: streaming started id: 0 data: message 1 : this is a comment data: message 2 retry: 5000 Here, the server header needs to set the MIME type to text/event-stream and ask the client not to cache the response by setting the cache-control header to no-cache. Next, in the message payload, only the data field is required, everything else is optional. Let\u0026rsquo;s break down the message structure:\nevent: This is an optional field that specifies the name of the event. If present, it must be preceded by the string \u0026rsquo;event:\u0026rsquo;. If not present, the event is considered to have the default name \u0026lsquo;message\u0026rsquo;.\nid: This is an optional field that assigns an ID to the event. If present, it must be preceded by the string \u0026lsquo;id:\u0026rsquo;. Clients can use this ID to resume an interrupted connection and receive only events that they have not yet seen.\ndata: This field is required and contains the actual message data that the server wants to send to the client. It must be preceded by the string \u0026lsquo;data:\u0026rsquo; and can contain any string of characters.\nretry: This is an optional field that specifies the number of milliseconds that the client should wait before attempting to reconnect to the server in case the connection is lost. If present, it must be preceded by the string \u0026lsquo;retry:\u0026rsquo;.\nEach message must end with double newline characters (\u0026quot;\\n\\n\u0026quot;). Yep, this is part of the protocol. The server can send multiple messages in a single HTTP response, and each message will be treated as a separate event by the client.\nA simple example In this section, I\u0026rsquo;ll prop up a simple HTTP streaming server with Starlette and collect the events from the browser. Here\u0026rsquo;s the complete server implementation:\n# server.py from __future__ import annotations import asyncio import logging from typing import AsyncGenerator from starlette.applications import Starlette from starlette.requests import Request from starlette.responses import Response, StreamingResponse from starlette.routing import Route logging.basicConfig(level=logging.INFO) # This is just so that you can head over to an index page # and run the client side code. async def index(request: Request) -\u0026gt; Response: return Response(\u0026#34;SSE demo\u0026#34;, media_type=\u0026#34;text/plain\u0026#34;) async def stream(request: Request) -\u0026gt; StreamingResponse: async def _stream() -\u0026gt; AsyncGenerator[str, None]: attempt = 0 # Give up after 3 attempts. while True: # Start sending messages. # Sets the type of the next message to \u0026#39;start\u0026#39;. yield \u0026#34;event: start\\n\u0026#34; yield \u0026#34;data: streaming started\\n\\n\u0026#34; # A \u0026#39;start\u0026#39; event message. yield f\u0026#34;id: {attempt}\\n\\n\u0026#34; # Sends the id field. yield \u0026#34;data: message 1\\n\\n\u0026#34; # A default event message. yield \u0026#34;: this is a comment\\n\\n\u0026#34; # Keep-alive comment. yield \u0026#34;data: message 2\\n\\n\u0026#34; # Another default event message. # Controls autoretry from the client side (ms). yield \u0026#34;retry: 5000\\n\\n\u0026#34; # Don\u0026#39;t flood the client with messages. await asyncio.sleep(1) attempt += 1 # Give up after 3 attempts to avoid dangling connections. if attempt == 3: # Close the connection yield \u0026#34;data: closing connection\\n\\n\u0026#34; break response = StreamingResponse( _stream(), headers={ \u0026#34;Content-Type\u0026#34;: \u0026#34;text/event-stream\u0026#34;, \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Cache-Control\u0026#34;: \u0026#34;no-cache\u0026#34;, }, ) return response routes = [ Route(\u0026#34;/\u0026#34;, endpoint=index), Route(\u0026#34;/stream\u0026#34;, endpoint=stream), ] app = Starlette(debug=True, routes=routes) The server exposes a /stream endpoint that will just continuously send data to any connected client. The stream function returns a StreamingResponse object that the framework uses to send SSE messages to the client. Internally, it defines an asynchronous generator function _stream which produces a sequence of messages that follows the SSE wire protocol and yields them line by line.\nThe index / page is there so that you can head over to it in your browser and paste the client-side code.\nYou can run this server with uvicorn via the following command:\nuvicorn server:app --port 5000 --reload This will expose the server to the localhost\u0026rsquo;s port 5000. Now you can head over to your browser, go to the localhost:5000 URL and paste this following snippet to the dev console to catch the streamed data from the client side:\n// client.js // Connect to the event stream server. const eventSource = new EventSource(\u0026#34;http://localhost:5000/stream\u0026#34;); // Log something when the client connects to the server. eventSource.onconnect = (event) =\u0026gt; console.log(\u0026#34;connected to the server\u0026#34;); // Log a message while closing the connection. eventSource.onclose = (event) =\u0026gt; console.log(\u0026#34;closing connection\u0026#34;); // Log an error message on account of an error. eventSource.onerror = (event) =\u0026gt; console.log(\u0026#34;an error occured\u0026#34;); // This is how you can attach an event listener to a custom event. eventSource.addEventListener(\u0026#34;start\u0026#34;, (event) =\u0026gt; { console.log(`start event: ${event.data}`); }); // Log the default message. eventSource.onmessage = (event) =\u0026gt; { console.log(`Default event: ${event.data}`); // Don\u0026#39;t reconnect when the server closes the connection. if (event.data === \u0026#34;closing connection\u0026#34;) eventSource.close(); }; Notice, how the client API is quite similar to the WebSocket API but simpler. Once you\u0026rsquo;ve pasted the code snippet to the browser console, you\u0026rsquo;ll be able to see the streamed data from the server that looks like this:\nstart event: streaming started Default event: message 1 Default event: message 2 start event: streaming started Default event: message 1 Default event: message 2 start event: streaming started Default event: message 1 Default event: message 2 Default event: closing connection A more practical example This section will demonstrate the scenario that was mentioned at the beginning of this post where loading a particular page in your browser will trigger a long-running asynchronous Celery task in the background. While the task runs, the server will communicate the progress with the client.\nOnce the task is finished, the server will send a specific message to the client and it\u0026rsquo;ll update the DOM to let the user know that the task has been finished. The workflow only requires unidirectional communication and SSE is a perfect candidate for this situation.\nTo test it out, you\u0026rsquo;ll need to install a few dependencies. You can pip install them as such:\npip install \u0026#39;celery[redis]\u0026#39; jinja2 starlette uvicorn You\u0026rsquo;ll also need to set up a Redis server that Celery will use for broker communication. If you have Docker installed in your system, you can run the following command to start a Redis server:\ndocker run --name dev-redis -d -h localhost -p 6379:6379 redis:alpine The application will live in a directory called sse with the following structure:\nsse ├── __init__.py ├── index.html # Client side SSE code. └── views.py # Server side SSE code. The view.py contains the server implementation that looks like this:\nfrom __future__ import annotations import json import logging import time from typing import TYPE_CHECKING, AsyncGenerator from celery import Celery from celery.result import AsyncResult from starlette.applications import Starlette from starlette.responses import StreamingResponse from starlette.routing import Route from starlette.templating import Jinja2Templates if TYPE_CHECKING: from starlette.requests import Request from starlette.responses import Response logging.basicConfig(level=logging.INFO) templates = Jinja2Templates(directory=\u0026#34;./\u0026#34;) celery_app = Celery(\u0026#34;tasks\u0026#34;, backend=\u0026#34;redis://\u0026#34;, broker=\u0026#34;redis://\u0026#34;) @celery_app.task() def background() -\u0026gt; str: time.sleep(5) return \u0026#34;Hello from background task...\u0026#34; async def index(request: Request) -\u0026gt; Response: task_id = background.apply_async(queue=\u0026#34;default\u0026#34;) logging.info(\u0026#34;Task id: %s\u0026#34;, task_id) response = templates.TemplateResponse( \u0026#34;index.html\u0026#34;, {\u0026#34;request\u0026#34;: request} ) response.set_cookie(\u0026#34;task_id\u0026#34;, task_id) return response async def task_status(request: Request) -\u0026gt; StreamingResponse: task_id = request.path_params[\u0026#34;task_id\u0026#34;] async def stream() -\u0026gt; AsyncGenerator[str, None]: task = AsyncResult(task_id, app=celery_app) logging.info(\u0026#34;Task state: %s\u0026#34;, task.state) attempt = 0 # Give up and close the connection after 10 attempts. while True: data = { \u0026#34;state\u0026#34;: task.state, \u0026#34;result\u0026#34;: task.result, } logging.info(\u0026#34;Server sending data: %s\u0026#34;, data) # Send a stringified JSON SSE message. yield f\u0026#34;data: {json.dumps(data)}\\n\\n\u0026#34; attempt += 1 # Close the connection when the task has successfully finished. if data.get(\u0026#34;state\u0026#34;) == \u0026#34;SUCCESS\u0026#34;: break # Give up after 10 attempts to avoid dangling connections. if attempt \u0026gt; 10: data[\u0026#34;state\u0026#34;] = \u0026#34;UNFINISHED\u0026#34; data[\u0026#34;result\u0026#34;] = \u0026#34;Task is taking too long to complete.\u0026#34; yield f\u0026#34;data: {json.dumps(data)}\\n\\n\u0026#34; break # Don\u0026#39;t flood the client with messages. time.sleep(1) response = StreamingResponse( stream(), headers={ \u0026#34;Content-Type\u0026#34;: \u0026#34;text/event-stream\u0026#34;, \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Cache-Control\u0026#34;: \u0026#34;no-cache\u0026#34;, }, ) return response routes = [ Route(\u0026#34;/index\u0026#34;, endpoint=index), Route(\u0026#34;/task_status/{task_id}\u0026#34;, endpoint=task_status), ] # Add session middleware app = Starlette(debug=True, routes=routes) Here, first, we\u0026rsquo;re setting up celery and connecting it to the local Redis instance. Next up, the background function simulates some async work where it just waits for a while and returns a message. The index view calls the asynchronous background task and sets the id of the task as a session cookie with response.set_cookie(\u0026quot;task_id\u0026quot;, task_id). The frontend JavaScript will look for this task_id cookie to identify a running background task.\nThen we expose a task_status endpoint that takes in the value of a task_id and streams the status of the running task to the frontend as SSE messages. To avoid dangling connections, we stream the task status for 10 seconds before giving up.\nNow on the client side, the index.html looks like this:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;style\u0026gt; html, body { height: 100%; margin: 0; } .centered { height: 100%; display: flex; justify-content: center; align-items: center; } \u0026lt;/style\u0026gt; \u0026lt;link rel=\u0026#34;shortcut icon\u0026#34; href=\u0026#34;#\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body class=\u0026#34;centered\u0026#34;\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h1\u0026gt;SSE Demo\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;Message\u0026lt;/h2\u0026gt; \u0026lt;p id=\u0026#34;message\u0026#34;\u0026gt;Waiting for server-sent message...\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;script\u0026gt; // Get result from server sent events. async function waitForResult() { console.log(\u0026#34;Waiting for task result...\u0026#34;); // Collect the task_id from the session cookie. const taskId = await waitForTaskIdCookie(); // Connect to the task_status streaming endpoing. const eventSource = new EventSource(`/task_status/${taskId}/`); // This will get triggered when the server sends an update on // the task status eventSource.onmessage = function(event) { console.log(\u0026#34;Task result:\u0026#34;, event.data); // Parser the JSONified event message. const data = JSON.parse(event.data); // Log the message to the console. const message = `Server sent: ${data.result}`; if(data.state === \u0026#34;SUCCESS\u0026#34;) { document.getElementById(\u0026#34;message\u0026#34;).innerHTML = message; eventSource.close(); // Close the connection. } else if(data.state === \u0026#34;UNFINISHED\u0026#34;) { document.getElementById(\u0026#34;message\u0026#34;).innerHTML = message; eventSource.close(); // Close the connection. } }; eventSource.onerror = function(event) { console.log(\u0026#34;Error:\u0026#34;, event); }; eventSource.onopen = function(event) { console.log(\u0026#34;Connection opened:\u0026#34;, event); }; eventSource.onclose = function(event) { console.log(\u0026#34;Connection closed:\u0026#34;, event); }; } // Wait for the task_id cookie to be set from the server. async function waitForTaskIdCookie() { while(true) { const taskId = getCookie(\u0026#34;task_id\u0026#34;); if(taskId) { console.log(\u0026#34;Found task_id cookie:\u0026#34;, taskId); return taskId; } // Wait for 300ms between each iteration so that we don\u0026#39;t overwhelm // the client. console.log(\u0026#34;Waiting for task_id cookie...\u0026#34;); await sleep(300); } } // Get cookie value by name. function getCookie(cookieName) { const cookieString = document.cookie; if(!cookieString) { return null; } const cookies = cookieString.split(\u0026#34;; \u0026#34;); for(const cookie of cookies) { if(cookie.startsWith(`${cookieName}=`)) { return cookie.split(\u0026#34;=\u0026#34;)[1]; } } return null; } // Sleep for given milliseconds. function sleep(ms) { return new Promise((resolve) =\u0026gt; setTimeout(resolve, ms)); } // Call the function when the page has finished loading. window.onload = function() { waitForResult(); }; \u0026lt;/script\u0026gt; \u0026lt;/html\u0026gt; When the index page is loaded, the server starts a background task and sets the task_id=\u0026lt;task_id\u0026gt; session cookie. The HTML above then defines a paragraph element to show the message streamed from the server:\n\u0026lt;body class=\u0026#34;centered\u0026#34;\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h1\u0026gt;SSE Demo\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;Message\u0026lt;/h2\u0026gt; \u0026lt;p id=\u0026#34;message\u0026#34;\u0026gt;Waiting for server-sent message...\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; The JavaScript code defines a function named waitForResult() that listens for updates on the status of a long-running task that is being executed on the server. The function first waits for the task_id to be set in a cookie by calling waitForTaskIdCookie(). Once the task_id is obtained, the function creates a new EventSource object that connects to the streaming endpoint on the server using the ID to get updates on the status of the task.\nThe EventSource object is set up with four event listeners: onmessage, onerror, onopen, and onclose. The onmessage listener is triggered when the server sends an update on the task status. The listener first logs the updated task status and then checks if the state of the task is SUCCESS or UNFINISHED. In either case, the client fetches the message element on the DOM and updates it with the result of the background task streamed by the server.\nThe client-side SSE API will automatically keep reconnecting if the connection fails for some reason. This is handy since you don\u0026rsquo;t have to write any additional logic to make the connection more robust. However, you do need to be mindful about closing the connection from the client side once you\u0026rsquo;ve received the final task status. The onmessage event listener explicitly closes the connection with eventSource.close() once the final message about a specific task has reached the client from the server.\nThe onerror listener handles errors that occur with the connection. The onopen callback is called when the connection is successfully opened, and onclose gets called when the connection is closed.\nThe waitForTaskIdCookie() function that is called by the entrypoint waits for the task_id to be set in a cookie by repeatedly calling getCookie() until the ID is obtained. The function waits for 300ms between each iteration so that it doesn\u0026rsquo;t overwhelm the client.\nThe getCookie() function is a utility function that returns the value of a cookie given its name.\nFinally, the code sets the window.onload event listener to call the waitForResult() function when the page has finished loading.\nNow, go to the sse directory and start the server with the following command:\nuvicorn views:app --port 5000 --reload On another terminal, start the celery workers:\ncelery -A views.celery_app worker -l info -Q default -c 1 Finally, head over to your browser and go to http://localhost:5000/index page and see that the server has triggered a background job. Once the job finishes after 5 seconds, the client shows a message:\nNotice, how the server pushes the result of the task automatically once it finishes.\nLimitations While SSE-driven pages are much easier to bootstrap than their WebSocket counterparts — apart from only supporting unidirectional communication, they suffer from a few other limitations:\nSSE is limited to sending text data only. If an application needs to send binary data, it must encode the data as text before sending it over SSE. SSE connections are subject to the same connection limitations as HTTP connections. In some cases, a large number of SSE connections can overload the server, leading to performance issues. However, this can be mitigated by taking advantage of connection multiplexing in HTTP/2. Further reading Using server-sent events ","permalink":"https://rednafi.com/python/server-sent-events/","summary":"\u003cp\u003eIn multi-page web applications, a common workflow is where a user:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLoads a specific page or clicks on some button that triggers a long-running task.\u003c/li\u003e\n\u003cli\u003eOn the server side, a background worker picks up the task and starts processing it\nasynchronously.\u003c/li\u003e\n\u003cli\u003eThe page shouldn\u0026rsquo;t reload while the task is running.\u003c/li\u003e\n\u003cli\u003eThe backend then communicates the status of the long-running task in real-time.\u003c/li\u003e\n\u003cli\u003eOnce the task is finished, the client needs to display a success or an error message\ndepending on the final status of the finished task.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe de facto tool for handling situations where real-time bidirectional communication is\nnecessary is \u003ca href=\"https://en.wikipedia.org/wiki/WebSocket\"\u003eWebSocket\u003c/a\u003e. However, in the case above, you can see that the communication is\nmostly unidirectional where the client initiates some action in the server and then the\nserver continuously pushes data to the client during the lifespan of the background job.\u003c/p\u003e","title":"Pushing real-time updates to clients with Server-Sent Events (SSEs)"},{"content":"I\u0026rsquo;ve always had a vague idea about what Unix domain sockets are from my experience working with Docker for the past couple of years. However, lately, I\u0026rsquo;m spending more time in embedded edge environments and had to explore Unix domain sockets in a bit more detail. This is a rough documentation of what I\u0026rsquo;ve explored to gain some insights.\nThe dry definition Unix domain sockets (UDS) are similar to TCP sockets in a way that they allow two processes to communicate with each other, but there are some core differences. While TCP sockets are used for communication over a network, Unix domain sockets are used for communication between processes running on the same computer.\nA Unix domain socket is a way for programs to exchange data in a fast and efficient way without having to worry about the overhead of network protocols like TCP/IP or UDP. It works by creating a special file on the file system called a socket, which acts as a bidirectional data channel between the processes. The processes can send and receive data through the socket just like they would with a network socket. Also, just like TCP/UDP sockets, Unix domain sockets can also be either stream-based (TCP equivalent) or datagram-based (UDP equivalent).\nUnix domain sockets are commonly used in server-client applications, such as web servers, databases, and email servers, where they provide a secure and efficient way for processes on the same machine to communicate with each other. They\u0026rsquo;re also used in many other types of programs where different parts of the program need to work together or share data. Another cool thing about them is that you can control access to your server just by tuning the permission of the socket file on the system.\nPrerequisites I\u0026rsquo;m running these experiments on an M-series Macbook pro. However, any Unix-y environment will work as long as you can run the following tools:\nsocat: To create the socket servers and clients. curl: To make HTTP requests to a supported socket server. jq: To pretty print JSON payloads. lsof: To display currently listening socket server processes. Inspecting Unix domain sockets in your system Most likely, there are currently multiple processes listening on different sockets in your system. You can explore them using lsof with the following command:\nsudo lsof -U This will return a list of all Unix domain socket files and the server process PIDs that are currently listening on them:\nCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME launchd 1 root 3u unix 0x25269ff9edd05165 0t0 /private//var/run/syslog launchd 1 root 4u unix 0x25269ff9edd0522d 0t0 -\u0026gt;0x25269ff9edd05165 launchd 1 root 6u unix 0x25269ff9edd052f5 0t0 /private/var/run/cupsd launchd 1 root 7u unix 0x25269ff9edd053bd 0t0 /var/rpc/ncalrpc/NETLOGON launchd 1 root 8u unix 0x25269ff9edd05485 0t0 /var/run/vpncontrol.sock launchd 1 root 9u unix 0x25269ff9edd0554d 0t0 /var/run/portmap.socket ... You can also filter out the socket files by their process names. Docker processes listen on a few socket files:\nsudo lsof -U -a -c \u0026#39;com.docker\u0026#39; This will return:\nCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME com.docke 15451 rednafi 10u unix 0x25269ff9edcfd55d 0t0 vpnkit-bridge-fd.sock com.docke 15451 rednafi 11u unix 0x25269ff9edcfd625 0t0 vpnkit-bridge.sock com.docke 15451 rednafi 12u unix 0x25269ff9edcfd6ed 0t0 vpnkit.port.sock com.docke 15451 rednafi 13u unix 0x25269ff9edcfd7b5 0t0 vpnkit.data.sock com.docke 15451 rednafi 14u unix 0x25269ff9edcfd87d 0t0 httpproxy.sock com.docke 15451 rednafi 15u unix 0x25269ff9edcfd3cd 0t0 backend.sock ... Creating a Unix domain socket Running the following command on your terminal will create a stream-based Unix domain socket:\nsocat unix-listen:/tmp/stream.sock,fork STDOUT This process listens on the /tmp/stream.sock and prints the incoming data to the stdout. The fork portion on the command ensures that multiple clients can be connected to the server process and they\u0026rsquo;ll be served by forking child processes.\nFrom another console, you can try to send data to the socket file as a client:\necho \u0026#34;hello world\u0026#34; | socat - unix-connect:/tmp/stream.sock Running this command will send the hello world string to the /tmp/stream.sock file and the server process will print it on the standard output stream.\nSimilarly, you can also create a datagram-based socket server with socat like this:\nsocat unix-recvfrom:/tmp/datagram.sock,fork STDOUT Now send data to the server with this:\necho \u0026#34;hello world\u0026#34; | socat - unix-sendto:/tmp/datagram.sock Connecting to Docker engine via a Unix domain socket By default, Docker runs through a non-networked UNIX socket. It can also optionally communicate using SSH or a TLS (HTTPS) socket. On MacOS, the socket file can be found in ~/.docker/run/docker.sock. We can make HTTP requests against the listening socket server and use Docker engine\u0026rsquo;s RESTful API suite.\nChecking the engine\u0026rsquo;s version number: The following command uses curl to spawn a client process and send a request against the Docker engine running in my local system.\ncurl --unix-socket ~/.docker/run/docker.sock http://localhost/version | jq This returns (truncated output for readability):\n{ \u0026#34;Platform\u0026#34;:{ \u0026#34;Name\u0026#34;:\u0026#34;Docker Desktop 4.17.0 (99724)\u0026#34; }, \u0026#34;Components\u0026#34;:[ \u0026#34;...\u0026#34; ], \u0026#34;Version\u0026#34;:\u0026#34;20.10.23\u0026#34;, \u0026#34;ApiVersion\u0026#34;:\u0026#34;1.41\u0026#34;, \u0026#34;MinAPIVersion\u0026#34;:\u0026#34;1.12\u0026#34;, \u0026#34;GitCommit\u0026#34;:\u0026#34;6051f14\u0026#34;, \u0026#34;GoVersion\u0026#34;:\u0026#34;go1.18.10\u0026#34;, \u0026#34;Os\u0026#34;:\u0026#34;linux\u0026#34;, \u0026#34;Arch\u0026#34;:\u0026#34;arm64\u0026#34;, \u0026#34;KernelVersion\u0026#34;:\u0026#34;5.15.49-linuxkit\u0026#34;, \u0026#34;BuildTime\u0026#34;:\u0026#34;2023-01-19T17:31:28.000000000+00:00\u0026#34; } Listing the containers: This command lists all the running containers on my machine.\ncurl --unix-socket \\ ~/.docker/run/docker.sock http://localhost/containers/json | jq Listing the images:\ncurl --unix-socket \\ ~/.docker/run/docker.sock http://localhost/images/json | jq Downloading a container: This allows you to programmatically download the hello-world image from Dockerhub:\ncurl -sX POST \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ --unix-socket ~/.docker/run/docker.sock \\ \u0026#39;http://localhost/images/create?fromImage=hello-world:latest\u0026#39; Listening for docker events: This API call lets you listen for all incoming events from the docker engine. You can run the following command on one terminal and send events from another:\ncurl --no-buffer --unix-socket \\ ~/.docker/run/docker.sock http://localhost/events | jq Here, the --no-buffer flag is necessary for instructing curl to send the output events to the input stream of jq without doing any buffering. This allows jq to pretty-print the outputs in real-time. Now from another console if you run the following command, you\u0026rsquo;ll see events pouring into the console that\u0026rsquo;s listening for them:\ndocker run hello-world The complete list of APIs can be found in the Docker engine API documentation.\nWriting a Unix domain socket server in Python You can quickly write a simple server that allows clients to connect to it via Unix domain sockets. If the clients exist on the same machine then, a UDS server has the advantage of having lower overhead than its networked TCP counterpart.\nThe following server uses Python\u0026rsquo;s socketserver module to create a stream-based echo server:\n# server.py from __future__ import annotations import logging import socketserver from pathlib import Path logging.basicConfig(level=logging.INFO) class RequestHandler(socketserver.BaseRequestHandler): def setup(self) -\u0026gt; None: logging.info(\u0026#34;Start request.\u0026#34;) def handle(self) -\u0026gt; None: conn = self.request while True: data = conn.recv(1024) if not data: break logging.info(f\u0026#34;recv: {data!r}\u0026#34;) conn.sendall(data) def finish(self) -\u0026gt; None: logging.info(\u0026#34;Finish request.\u0026#34;) class Server(socketserver.ThreadingUnixStreamServer): def server_activate(self) -\u0026gt; None: logging.info(\u0026#34;Server started on %s\u0026#34;, self.server_address) super().server_activate() if __name__ == \u0026#34;__main__\u0026#34;: # Remove the socket file if it already exists. # UDS doesn\u0026#39;t let you reuse the socket file. socket_path = Path(\u0026#34;/tmp/stream.sock\u0026#34;) if socket_path.exists(): socket_path.unlink() with Server(str(socket_path), RequestHandler) as server: server.serve_forever() Here, socketserver.ThreadingUnixStreamServer enables us to create a server that allows multiple clients to be connected to it via Unix domain sockets. The server spins up a new thread to serve each new client and does bi-directional communication via UDS. The client code is quite similar to a TCP client:\n# client.py import socket import time import logging logging.basicConfig(level=logging.INFO) ADDRESS = \u0026#34;/tmp/stream.sock\u0026#34; with socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) as s: s.connect(ADDRESS) while True: time.sleep(1) s.sendall(b\u0026#34;hello world\u0026#34;) data = s.recv(1024) logging.info(f\u0026#34;Received {data!r}\u0026#34;) The client connects to the server through the /tmp/stream.sock socket and sends a static hello world string to it. The server then sends that data back and the client sends it to the stdout stream.\nRunning the server and client as two separate processes will yield the following output:\nExposing an HTTP application via a Unix domain socket Webservers usually allow you to expose HTTP applications via Unix domain sockets. In Python, the uvicorn ASGI server lets you do this quite easily. This can come as handy whenever you need to spin up a local server and all the clients are running on the same machine or you\u0026rsquo;re running your server behind a proxy. Here\u0026rsquo;s an example of a simple webserver built with starlette and served with uvicorn.\n# server.py (http server) from __future__ import annotations from starlette.applications import Starlette from starlette.requests import Request from starlette.responses import HTMLResponse from starlette.routing import Route async def index(request: Request) -\u0026gt; HTMLResponse: return HTMLResponse( \u0026#34;\u0026#34;\u0026#34;\u0026lt;h1\u0026gt;Hello, world!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;This is the index page.\u0026lt;/p\u0026gt;\u0026#34;\u0026#34;\u0026#34; ) app = Starlette( debug=True, routes=[ Route(\u0026#34;/index\u0026#34;, index), ], ) You can expose this server through a UDS like this:\nuvicorn --uds /tmp/stream.sock server:app Calling this API with curl from another console will return the HTML content in the response:\ncurl --unix-socket /tmp/stream.sock http://localhost/index \u0026lt;h1\u0026gt;Hello, world!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;This is the index page.\u0026lt;/p\u0026gt; If you want to access this server from a browser, you\u0026rsquo;ll need to make sure that your reverse proxy server (Nginx / Apache / Caddy) is configured to relay the incoming request from the network to the UDS server. For a quick and dirty approach, you can use socat to proxy the request from a HOST:PORT pair to the UDS server like this:\nuvicorn --uds /tmp/stream.sock server:app \\ \u0026amp; socat tcp-listen:9999,fork unix-connect:/tmp/stream.sock \u0026amp; The uvicorn command spins up a webserver in the background as before and listens on the socket file /tmp/stream.sock. Then we\u0026rsquo;re using socat to create a forking TCP server that handles the incoming HTTP requests from the network and relays them to the webserver via UDS. It also relays the server\u0026rsquo;s responses back to the client — doing the work of a reverse proxy.\nYou can then head over to your browser and go to http://localhost:9999. This will display the HTML page:\nFurther reading Understanding sockets - Digital Ocean\nFun with Unix domain sockets - Simon Willison\n","permalink":"https://rednafi.com/misc/tinkering-with-unix-domain-socket/","summary":"\u003cp\u003eI\u0026rsquo;ve always had a vague idea about what Unix domain sockets are from my experience working\nwith Docker for the past couple of years. However, lately, I\u0026rsquo;m spending more time in\nembedded edge environments and had to explore Unix domain sockets in a bit more detail. This\nis a rough documentation of what I\u0026rsquo;ve explored to gain some insights.\u003c/p\u003e\n\u003ch2 id=\"the-dry-definition\"\u003eThe dry definition\u003c/h2\u003e\n\u003cp\u003eUnix domain sockets (UDS) are similar to TCP sockets in a way that they allow two processes\nto communicate with each other, but there are some core differences. While TCP sockets are\nused for communication over a network, Unix domain sockets are used for communication\nbetween processes running on the same computer.\u003c/p\u003e","title":"Tinkering with Unix domain sockets"},{"content":"While working on a multithreaded socket server in an embedded environment, I realized that the default behavior of Python\u0026rsquo;s socketserver.ThreadingTCPServer requires some extra work if you want to shut down the server gracefully in the presence of an interruption signal. The intended behavior here is that whenever any of SIGHUP, SIGINT, SIGTERM, or SIGQUIT signals are sent to the server, it should:\nAcknowledge the signal and log a message to the output console of the server. Notify all the connected clients that the server is going offline. Give the clients enough time (specified by a timeout parameter) to close the requests. Close all the client requests and then shut down the server after the timeout exceeds. Here\u0026rsquo;s a quick implementation of a multithreaded echo server and see what happens when you send SIGINT to shut down the server:\n# server.py from __future__ import annotations import logging import socketserver logging.basicConfig(level=logging.INFO) class RequestHandler(socketserver.BaseRequestHandler): \u0026#34;\u0026#34;\u0026#34;Handler that handles an incoming client request.\u0026#34;\u0026#34;\u0026#34; def handle(self) -\u0026gt; None: conn = self.request while True: data = conn.recv(1024) if not data: break logging.info(f\u0026#34;recv: {data!r}\u0026#34;) conn.sendall(data) if __name__ == \u0026#34;__main__\u0026#34;: with socketserver.ThreadingTCPServer( (\u0026#34;localhost\u0026#34;, 9999), RequestHandler ) as server: server.serve_forever() Here\u0026rsquo;s the client code:\n# client.py import logging import socket import time logging.basicConfig(level=logging.INFO) HOST = \u0026#34;localhost\u0026#34; # The server\u0026#39;s hostname or IP address. PORT = 9999 # The port used by the server. with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.connect((HOST, PORT)) while True: time.sleep(1) s.sendall(b\u0026#34;hello world\u0026#34;) data = s.recv(1024) if not data: break logging.info(f\u0026#34;Received {data!r}\u0026#34;) Here, the server logs and echoes back whatever the client sends and the client just sends the string hello world continuously in a while loop. This is pretty much the canonical multithreaded server-client example that\u0026rsquo;s found in the socketserver docs. In the client code, the only thing that\u0026rsquo;s a little different is that within the while loop, a time.sleep(1) function was added to simulate the client performing some processing tasks. Also, without the sleep, the server would\u0026rsquo;ve flooded the stdout with the client message logs and made the demonstration difficult.\nLet\u0026rsquo;s run the server and the client in two separate processes and then send a SIGINT signal to the server by clicking Ctrl + C on the server console:\nAt first, the server just ignores the signal, and clicking Ctrl + C multiple times crashes the server down with this nasty traceback (full traceback trimmed for brevity):\nTraceback (most recent call last): File \u0026#34;.../server.py\u0026#34;, line 137, in \u0026lt;module\u0026gt; server.serve_forever() File \u0026#34;.../socketserver.py\u0026#34;, line 233, in serve_forever ready = selector.select(poll_interval) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \u0026#34;.../selectors.py\u0026#34;, line 415, in select fd_event_list = self._selector.poll(timeout) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ KeyboardInterrupt ... Multithreaded socket server with graceful shutdown What we want here is that whenever the server gets SIGHUP, SIGINT, SIGTERM, or SIGQUIT, it should notify the clients and gracefully shut itself down. I played around with the socketserver.ThreadingTCPServer API for a while to come up with a solution that worked nicely for my use case. Here\u0026rsquo;s the full server implementation:\n# server.py from __future__ import annotations import logging import os import signal import socket import socketserver import threading import time from types import FrameType from typing import Callable logging.basicConfig(level=logging.INFO) class RequestHandler(socketserver.BaseRequestHandler): server: SocketServer def notify_clients_when_server_is_interrupted(self) -\u0026gt; None: logging.info(\u0026#34;Server interrupted, notifying all clients...\u0026#34;) self.request.sendall(b\u0026#34;SHUTDOWN\u0026#34;) self.request.sendall(b\u0026#34;\u0026#34;) def setup(self) -\u0026gt; None: # Prevent new connections from being accepted when the server is # shutting down. if self.server._is_interrupted: self.notify_clients_when_server_is_interrupted() def handle(self) -\u0026gt; None: conn = self.request while True: data = conn.recv(1024) if self.server._is_interrupted: self.notify_clients_when_server_is_interrupted() break if not data: break logging.info(f\u0026#34;recv: {data!r}\u0026#34;) conn.sendall(data) class SocketServer(socketserver.ThreadingTCPServer): reuse_address = True daemon_threads = True block_on_close = False _is_interrupted = False def server_activate(self) -\u0026gt; None: logging.info( \u0026#34;PID:%s. Server started on %s:%s\u0026#34;, os.getpid(), *self.server_address, ) super().server_activate() def get_request(self) -\u0026gt; tuple[socket.socket, str]: conn, addr = super().get_request() logging.info(\u0026#34;Starting connection from %s:%s\u0026#34;, *addr) return conn, addr def shutdown_request( self, request: socket.socket | tuple[bytes, socket.socket] ) -\u0026gt; None: if isinstance(request, socket.socket): logging.info( \u0026#34;Closing connection from %s:%s\u0026#34;, *request.getpeername(), ) super().shutdown_request(request) def shutdown(self) -\u0026gt; None: logging.info(\u0026#34;Server is shutting down...\u0026#34;) super().shutdown() def handle_signal( self, timeout: int ) -\u0026gt; Callable[[int, FrameType | None], None]: \u0026#34;\u0026#34;\u0026#34;A simple signal handler factory that takes in some additional parameters and passes them to the actual signal handler. Defines and returns the final handler. \u0026#34;\u0026#34;\u0026#34; def handler(signum: int, _: FrameType | None) -\u0026gt; None: deadline = time.monotonic() + timeout signame = signal.Signals(signum).name self._is_interrupted = True while (current_time := time.monotonic()) \u0026lt; deadline: delta = int(deadline - current_time) + 1 logging.info( \u0026#34;%s received, closing server in %s seconds...\u0026#34; % (signame, delta) ) time.sleep(1) self.server_close() self.shutdown() return handler if __name__ == \u0026#34;__main__\u0026#34;: with SocketServer((\u0026#34;localhost\u0026#34;, 9999), RequestHandler) as server: for sig in ( signal.SIGHUP, signal.SIGINT, signal.SIGTERM, signal.SIGQUIT, ): signal.signal(sig, server.handle_signal(timeout=5)) t = threading.Thread(target=server.serve_forever) t.start() t.join() Apart from a few extra methods that perform logging and signal handling, the overall structure of this server is similar to the vanilla multithreaded server from the previous section. In the RequestHandler, we have defined a custom notify_clients_when_server_is_interrupted method that notifies all clients whenever the server receives an interruption signal. This is a custom method that\u0026rsquo;s not defined in the BaseRequestHandler class. The notify method logs the status of the interruption signal and then sends a SHUTDOWN message to the clients. Afterward, it closes the client connection.\nThe setup method extends the eponymous method from the BaseRequestHandler class and calls the notify_clients_when_server_is_interrupted method. This ensures that whenever the server is shutting down, it refuses any new client connections. Within the handle method, in the data processing while loop, we check the value of the _is_interrupted flag on the server instance. If the value is True, we call the notify method. The value of this flag is managed by the SocketServer class. Calling the notify method from within the data processing loop will notify all currently connected clients.\nNext, we define a new server class called SocketServer that inherits from the socketserver.ThreadingTCPServer class. The reuse_address, daemon_threads, and block_on_close class variables override the default values inherited from the base ThreadingTCPServer class. Here are the explanations for each:\nreuse_address: This variable determines whether the server can reuse a socket that\u0026rsquo;s still in the TIME_WAIT state after a previous connection has been closed. If this variable is set to True, the server can reuse the socket. Otherwise, the socket will be unavailable for a short period of time after it\u0026rsquo;s closed.\ndaemon_threads: This variable determines whether the server\u0026rsquo;s worker threads should be daemon threads. Daemon threads are threads that run in the background and don\u0026rsquo;t prevent the Python interpreter from exiting when they are still running. If this variable is set to True, the server\u0026rsquo;s worker threads will be daemon threads. I found that daemon threads work better when I need to shut down the server that\u0026rsquo;s connected to multiple long-running clients.\nblock_on_close: This variable determines whether the server should block until all client connections have been closed before shutting down. If this variable is set to True, the server will block until all client connections have been closed. Otherwise, the server will shut down immediately, even if there are still active client connections. We want to set it to False since we\u0026rsquo;ll handle the graceful shutdown in a custom signal handler method on the server class.\nGoing forward, the SocketServer class overrides the server_activate, get_request, shutdown_request, and shutdown methods from the base class. All of them just log a few key pieces of information to the console and calls the methods from the parent class verbatim. The interesting part happens in the custom handle_signal method. When an interruption signal is sent to the server, the handle_signal method is activated. The method takes an integer parameter timeout which specifies how many seconds the server should wait before shutting down after receiving the signal.\nThe method then returns the actual signal handler function that takes two parameters: an integer signum representing the signal number and a FrameType object which represents the current stack frame. The function is responsible for handling the signal by making the server wait for timeout seconds before shutting it down gracefully.\nFirst, the method sets a variable _is_interrupted to True to indicate that the server has received an interruption signal. Then, the method enters a while loop that continues until the current time exceeds the deadline time, which is calculated by adding the timeout to the current monotonic time. During each iteration of the while loop, the method logs a message to the console to indicate that the signal has been received and the server will be closed in a certain number of seconds. The delta variable is calculated as the difference between the deadline and the current monotonic time, plus 1. This ensures that the logging message displays an accurate countdown of the remaining time until the server shuts down.\nOnce the deadline exceeds and the while loop completes, the method calls server_close() and shutdown() methods of the server to close the requests and shut itself down gracefully. The server_close() method closes the listening socket and stops accepting new client connections, while the shutdown() method stops all active client connections and waits for them to finish processing their current requests. However, in this case, since we are giving the clients enough time to close the connections and using daemon threads to process the requests, calling shutdown() will immediately close all the client requests and bring down the server.\nFinally, in the __main__ section, we instantiate the SocketServer class and register the RequestHandler. Then we register the signal handler with a timeout of 5 seconds. This means, upon receiving the interruption signal, the server will wait 5 seconds before shutting itself down. Notice, how we\u0026rsquo;re running the server.serve_forever method in a new thread. That\u0026rsquo;s because our custom signal handler explicitly calls the shutdown of the server instance and the shutdown method can only be called when the serve_forever loop is running in a different thread. From the shutdown documentation:\nTell the serve_forever() loop to stop and wait until it does. shutdown() must be called while serve_forever() is running in a different thread otherwise it will deadlock.\nNow that the server is coded to shut down gracefully, we also expect the client to behave properly. That means, whenever the client receives the SHUTDOWN message, it should immediately close the connection. Here\u0026rsquo;s a slightly modified version of the vanilla socket client code that we\u0026rsquo;ve seen before:\n# client.py import logging import socket import time logging.basicConfig(level=logging.INFO) HOST = \u0026#34;localhost\u0026#34; # The server\u0026#39;s hostname or IP address. PORT = 9999 # The port used by the server. with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.connect((HOST, PORT)) while True: time.sleep(1) s.sendall(b\u0026#34;hello world\u0026#34;) data = s.recv(1024) if data == b\u0026#34;SHUTDOWN\u0026#34;: logging.info(\u0026#34;Closing connection...\u0026#34;) break if not data: break logging.info(f\u0026#34;Received {data!r}\u0026#34;) The only difference between this and the previous client is that this client will break out of the process loop when it encounters the SHUTDOWN message from the server. Now to see the whole thing in action, you can fire up the server and the client from two different terminals. Once both the server and client are running, try sending a SIGINT or any of the three other handled signals. You see that the server acknowledges the interruption signal, gives the clients enough time to disconnect, then shut itself down in a graceful manner:\nFurther reading socketserver ","permalink":"https://rednafi.com/python/multithreaded-socket-server-signal-handling/","summary":"\u003cp\u003eWhile working on a multithreaded socket server in an embedded environment, I realized that\nthe default behavior of Python\u0026rsquo;s \u003ccode\u003esocketserver.ThreadingTCPServer\u003c/code\u003e requires some extra work\nif you want to shut down the server gracefully in the presence of an interruption signal.\nThe intended behavior here is that whenever any of \u003ccode\u003eSIGHUP\u003c/code\u003e, \u003ccode\u003eSIGINT\u003c/code\u003e, \u003ccode\u003eSIGTERM\u003c/code\u003e, or\n\u003ccode\u003eSIGQUIT\u003c/code\u003e signals are sent to the server, it should:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAcknowledge the signal and log a message to the output console of the server.\u003c/li\u003e\n\u003cli\u003eNotify all the connected clients that the server is going offline.\u003c/li\u003e\n\u003cli\u003eGive the clients enough time (specified by a timeout parameter) to close the requests.\u003c/li\u003e\n\u003cli\u003eClose all the client requests and then shut down the server after the timeout exceeds.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHere\u0026rsquo;s a quick implementation of a multithreaded echo server and see what happens when you\nsend \u003ccode\u003eSIGINT\u003c/code\u003e to shut down the server:\u003c/p\u003e","title":"Signal handling in a multithreaded socket server"},{"content":"I was working on a project where I needed to poll multiple data sources and consume the incoming data points in a single thread. In this particular case, the two data streams were coming from two different Redis lists. The correct way to consume them would be to write two separate consumers and spin them up as different processes.\nHowever, in this scenario, I needed a simple way to poll and consume data from one data source, wait for a bit, then poll and consume from another data source, and keep doing this indefinitely. That way I could get away with doing the whole workflow in a single thread without the overhead of managing multiple processes.\nHere\u0026rsquo;s what I\u0026rsquo;m trying to do:\n# pseudocode.py def stream_a(): while True: # Poll the first redis list. def stream_b(): while True: # Poll the second redis list. def consume(): # How do I alternate between two infinite streams and consume them? while True: stream_a() # Somehow break out of the infinite while loop. stream_b() # Somehow run this infinite loop after one iteration of # the first one. One way is to poll the data sources in two generator functions and yield the result. Then in the consumer, we\u0026rsquo;ll have to alternate between the generators to fetch the next result like this:\n# pseudocode.py import redis def stream_a(): while True: # Fetch result from the first redis list. yield redis.rpop(\u0026#34;stream_a\u0026#34;) def stream_b(): while True: # Fetch result from the second redis list. yield redis.rpop(\u0026#34;stream_b\u0026#34;) def consume(): streams = (stream_a(), stream_b()) while True: # Iterate through the stream generators. for stream in streams: # Wait for a second before polling each data source. time.sleep(1) # Get the result. If the result is None then go back to the # beginning of the loop if (result := next(stream, None)) is None: continue print(f\u0026#34;From {stream.__name__}:\u0026#34;, result) Let\u0026rsquo;s make a concrete example out of the pesudocode:\n# src.py from __future__ import annotations import time from itertools import count from typing import Generator def stream_even() -\u0026gt; Generator[int, None, None]: yield from count(start=0, step=2) def stream_odd() -\u0026gt; Generator[int, None, None]: yield from count(start=1, step=2) def consume() -\u0026gt; None: streams = (stream_even(), stream_odd()) while True: for stream in streams: time.sleep(1) if (result := next(stream, None)) is None: continue print(f\u0026#34;From {stream.__name__}:\u0026#34;, result) if __name__ == \u0026#34;__main__\u0026#34;: consume() The code above defines two generator functions, stream_even() and stream_odd(), that use the count() function from the itertools module to generate an infinite sequence of even and odd integers respectively.\nThe consume() function creates a tuple containing the two generator objects, and enters an infinite loop. On each iteration of the loop, it iterates over the tuple using a for loop; effectively alternating between the two streams. In each iteration, it waits for 1 second using the time.sleep() function and then uses the next() function to retrieve the next item from the current stream. If the result is not None, it prints a message to the console indicating which stream it came from and what the value was. Else, it loops back to the beginning of the iteration.\nRunning the snippet will print the folling output to the console:\n$ python src.py From stream_even: 0 From stream_odd: 1 From stream_even: 2 From stream_odd: 3 From stream_even: 4 From stream_odd: 5 From stream_even: 6 From stream_odd: 7 From stream_even: 8 From stream_odd: 9 From stream_even: 10 ^CTraceback (most recent call last): File \u0026#34;/Users/rednafi/Canvas/personal/reflections/src.py\u0026#34;, line 29, in \u0026lt;module\u0026gt; consume() File \u0026#34;/Users/rednafi/Canvas/personal/reflections/src.py\u0026#34;, line 22, in consume time.sleep(1) KeyboardInterrupt The consumer infinite loop can be written in a more concise manner with itertools.cycle. Instead of using the while loop, we can use this function to indefinitely cycle between the elements of an iterable.\n# src.py ... from itertools import cycle def consume() -\u0026gt; None: streams = (stream_even(), stream_odd()) for stream in cycle(streams): # Use itertools.cycle time.sleep(1) if (result := next(stream, None)) is None: break print(f\u0026#34;From {stream.__name__}:\u0026#34;, result) ... Here, the finalized executable script:\n# src.py from __future__ import annotations import time from itertools import count, cycle from typing import Generator def stream_even() -\u0026gt; Generator[int, None, None]: yield from count(start=0, step=2) def stream_odd() -\u0026gt; Generator[int, None, None]: yield from count(start=1, step=2) def consume() -\u0026gt; None: streams = (stream_even(), stream_odd()) for stream in cycle(streams): time.sleep(1) if (result := next(stream, None)) is None: continue print(f\u0026#34;From {stream.__name__}:\u0026#34;, result) if __name__ == \u0026#34;__main__\u0026#34;: consume() $ python src.py From stream_even: 0 From stream_odd: 1 From stream_even: 2 From stream_odd: 3 From stream_even: 4 From stream_odd: 5 From stream_even: 6 ^CTraceback (most recent call last): File \u0026#34;/Users/rednafi/Canvas/personal/reflections/src.py\u0026#34;, line 28, in \u0026lt;module\u0026gt; consume() File \u0026#34;/Users/rednafi/Canvas/personal/reflections/src.py\u0026#34;, line 21, in consume time.sleep(1) KeyboardInterrupt Further reading itertools.cycle ","permalink":"https://rednafi.com/python/switch-between-multiple-datastreams/","summary":"\u003cp\u003eI was working on a project where I needed to poll multiple data sources and consume the\nincoming data points in a single thread. In this particular case, the two data streams were\ncoming from two different Redis lists. The correct way to consume them would be to write two\nseparate consumers and spin them up as different processes.\u003c/p\u003e\n\u003cp\u003eHowever, in this scenario, I needed a simple way to poll and consume data from one data\nsource, wait for a bit, then poll and consume from another data source, and keep doing this\nindefinitely. That way I could get away with doing the whole workflow in a single thread\nwithout the overhead of managing multiple processes.\u003c/p\u003e","title":"Switching between multiple data streams in a single thread"},{"content":"Consider this iterable:\nit = (1, 2, 3, 0, 4, 5, 6, 7) Let\u0026rsquo;s say you want to build another iterable that includes only the numbers that appear starting from the element 0. Usually, I\u0026rsquo;d do this:\n# This returns (0, 4, 5, 6, 7). from_zero = tuple( elem for idx, elem in enumerate(it) if idx \u0026gt;= it.index(0) ) While this is quite terse and does the job, it won\u0026rsquo;t work with a generator. There\u0026rsquo;s an even more generic and terser way to do the same thing with itertools.dropwhile function. Here\u0026rsquo;s how to do it:\nfrom itertools import dropwhile # This returns the same thing as before (0, 4, 5, 6, 7). from_zero = tuple(dropwhile(lambda x: x != 0, it)) Here, itertools.dropwhile is a generator function that returns elements from an iterable starting from the first element for which the predicate returns False. The predicate is a function that takes one argument and returns a boolean value.\nThe dropwhile function takes two arguments:\nA function (the predicate), which takes one argument and returns a boolean value. An iterable, which can be any object that can be iterated over, such as a list, tuple, string, or even another generator. The dropwhile function starts iterating over the elements of the iterable, and drops the elements for which the predicate returns True. It then returns all the remaining elements of the iterable, regardless of whether they satisfy the condition or not.\nApart from being concise, this implementation is more generic and can be used for other purposes like skipping the header lines in a file. For example:\nfrom itertools import dropwhile with open(\u0026#34;/etc/passwd\u0026#34;) as f: for line in dropwhile(lambda x: x.startswith(\u0026#34;#\u0026#34;), f): print(line) This will print all the lines from the /etc/passwd file after the header comments:\nnobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false root:*:0:0:System Administrator:/var/root:/bin/sh daemon:*:1:1:System Services:/var/root:/usr/bin/false ... Finally, let\u0026rsquo;s see how you can skip straight to the data rows in a CSV file that contains arbitrary comments and headers like this:\n# persons.csv This is a comment These are some other comments The fake header starts from the next line id,name,age,height The real header starts from here ID,Name,Age,Height 1,John,20,1.8 2,Jane,21,1.7 3,Jack,22,1.6 import csv from itertools import dropwhile with open(\u0026#34;persons.csv\u0026#34;, \u0026#34;r\u0026#34;) as f: reader = csv.DictReader(f, fieldnames=(\u0026#34;ID\u0026#34;, \u0026#34;Name\u0026#34;, \u0026#34;Age\u0026#34;, \u0026#34;Height\u0026#34;)) # Rows without comments. rows = dropwhile(lambda x: x[\u0026#34;ID\u0026#34;] != \u0026#34;ID\u0026#34;, reader) # Skip the header. next(rows) for row in rows: print(row) Running this will give you the dicts containing the data rows only:\n{\u0026#39;ID\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;Name\u0026#39;: \u0026#39;John\u0026#39;, \u0026#39;Age\u0026#39;: \u0026#39;20\u0026#39;, \u0026#39;Height\u0026#39;: \u0026#39;1.8\u0026#39;} {\u0026#39;ID\u0026#39;: \u0026#39;2\u0026#39;, \u0026#39;Name\u0026#39;: \u0026#39;Jane\u0026#39;, \u0026#39;Age\u0026#39;: \u0026#39;21\u0026#39;, \u0026#39;Height\u0026#39;: \u0026#39;1.7\u0026#39;} {\u0026#39;ID\u0026#39;: \u0026#39;3\u0026#39;, \u0026#39;Name\u0026#39;: \u0026#39;Jack\u0026#39;, \u0026#39;Age\u0026#39;: \u0026#39;22\u0026#39;, \u0026#39;Height\u0026#39;: \u0026#39;1.6\u0026#39;} Further reading Python Cookbook - David Beazley, Ch 4: Iterators and Generators itertools.dropwhile ","permalink":"https://rednafi.com/python/skip-first-part-of-an-iterable/","summary":"\u003cp\u003eConsider this iterable:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eit\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e6\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e7\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eLet\u0026rsquo;s say you want to build another iterable that includes only the numbers that appear\nstarting from the element \u003ccode\u003e0\u003c/code\u003e. Usually, I\u0026rsquo;d do this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# This returns (0, 4, 5, 6, 7).\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003efrom_zero\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003etuple\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eelem\u003c/span\u003e \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003eidx\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eelem\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003eenumerate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eidx\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;=\u003c/span\u003e \u003cspan class=\"n\"\u003eit\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eindex\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWhile this is quite terse and does the job, it won\u0026rsquo;t work with a generator. There\u0026rsquo;s an even\nmore generic and terser way to do the same thing with \u003ccode\u003eitertools.dropwhile\u003c/code\u003e function. Here\u0026rsquo;s\nhow to do it:\u003c/p\u003e","title":"Skipping the first part of an iterable in Python"},{"content":"I needed to write a socket server in Python that would allow me to intermittently pause the server loop for a while, run something else, then get back to the previous request-handling phase; repeating this iteration until the heat death of the universe. Initially, I opted for the low-level socket module to write something quick and dirty. However, the implementation got hairy pretty quickly. While the socket module gives you plenty of control over how you can tune the server\u0026rsquo;s behavior, writing a server with robust signal and error handling can be quite a bit of boilerplate work.\nThankfully, I found out that Python is already shipped with a higher level library named socketserver that uses the socket module underneath but gives you more tractable hooks to latch onto and build fairly robust servers where the low-level details are handled for you. Not only that, socketserver makes it easy to write a sever that can concurrently handle multiple clients either by spinning child threads or forking child processes.\nWhile all this sounds good and dandy, my primary objective was to be able to write a server that can pause serving the clients every now and then, do some work and then come back to the previous work. Here\u0026rsquo;s how I did it with a multi-threaded socket server:\nfrom __future__ import annotations import logging import socket import socketserver import time logging.basicConfig(level=logging.INFO) class RequestHandler(socketserver.BaseRequestHandler): def setup(self) -\u0026gt; None: logging.info(\u0026#34;Start request.\u0026#34;) def handle(self) -\u0026gt; None: conn = self.request while True: data = conn.recv(1024) if not data: break logging.info(f\u0026#34;recv: {data!r}\u0026#34;) conn.sendall(data) def finish(self) -\u0026gt; None: logging.info(\u0026#34;Finish request.\u0026#34;) class ThreadingTCPServer(socketserver.ThreadingTCPServer): _timeout = 5 # seconds _start_time = time.monotonic() def server_activate(self) -\u0026gt; None: logging.info(\u0026#34;Server started on %s:%s\u0026#34;, *self.server_address) super().server_activate() def get_request(self) -\u0026gt; tuple[socket.socket, str]: conn, addr = super().get_request() logging.info(\u0026#34;Connection from %s:%s\u0026#34;, *addr) return conn, addr def service_actions(self) -\u0026gt; None: if time.monotonic() - self._start_time \u0026gt; self._timeout: logging.info(\u0026#34;Server paused, something else is running...\u0026#34;) self._start_time = time.monotonic() if __name__ == \u0026#34;__main__\u0026#34;: with ThreadingTCPServer((\u0026#34;localhost\u0026#34;, 9999), RequestHandler) as server: server.serve_forever() This is a simple echo server that receives client connections and reflects back the data sent by the clients. The server can handle multiple client connections simultaneously using the ThreadingTCPServer class. This class is derived from the socketserver.ThreadingTCPServer class and is responsible for implementing the server\u0026rsquo;s main loop, which listens for incoming client connections and creates a separate thread for each one to handle the incoming request.\nThe RequestHandler class is used to handle each incoming request. This class is derived from the socketserver.BaseRequestHandler class and is responsible for handling the connection between a client and the server. It implements the setup, handle, and finish methods to perform any necessary initialization work, handle the incoming data, and clean up after the request has been processed. In the setup and finish methods, we\u0026rsquo;re only printing some message to indicate that these methods are called before and after the handle method respectively. In the handle method, we\u0026rsquo;re collecting the data sent by the clients and echoing them back. Here, inside the while loop, conn.recv is a blocking method and will keep reading from the clients indefinitely. We need the server to break out from this, do something else, and then get back to it gracefully.\nIn the __main__ section of the code snippet, a ThreadingTCPServer object is created and the server is started using the serve_forever method. This method will continuously run the server loop, listen for incoming connections and create a separate thread for each one to handle the request.\nThe ThreadingTCPServer class implements server_activate and get_request methods. These two methods are already implemented in the base and we\u0026rsquo;re just calling the methods from there with some additonal logging. Here, server_activate prints out the server\u0026rsquo;s IP address and port. Similarly, the get_request method calls the eponymous method from the superclass and logs the IP and the port of the incoming clients.\nThe server also implements a service_actions method that is called by the server loop. This is where we\u0026rsquo;re periodically pausing the server and performing some blocking actions. In this case, the service_actions method checks the current time and compares it to the start time of the server. If the difference is greater than the specified timeout, the server is paused and a message is printed to the console indicating that something else is running. Then after one iteration, the start time is updated so that the server gets paused again after the timeout period.\nTo test the server out, here\u0026rsquo;s a simple client that sends some data to the server:\n# client.py import socket import time import logging logging.basicConfig(level=logging.INFO) HOST = \u0026#34;localhost\u0026#34; # The server\u0026#39;s hostname or IP address. PORT = 9999 # The port used by the server. with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.connect((HOST, PORT)) while True: time.sleep(1) s.sendall(b\u0026#34;hello world\u0026#34;) data = s.recv(1024) logging.info(f\u0026#34;Received {data!r}\u0026#34;) This client connects to the server via port 9999 and sends the b'hello world' byte string. The server will capture and echo it back to the client which the client will print as Received .... You can run the server in one console with python server.py and the client in another one with the python client.py command.\nYou\u0026rsquo;ll see that the server will pause every 5 seconds, do something else in a blocking manner and then come back to handle the client requests. If you attach a second client from another console, you\u0026rsquo;ll see that the server can also handle that while retaining the expected behavior. The server will pause even if there\u0026rsquo;s no client sending requests to the server. You can test that behavior by detaching all the clients from the server.\nNow, we could also make the work in the serving_actions non-blocking by spinning a new thread or process and doing the work there. However, for the task that I was tackling, simply running the function in a blocking manner was enough.\n","permalink":"https://rednafi.com/python/pause-and-resume-a-socket-server/","summary":"\u003cp\u003eI needed to write a socket server in Python that would allow me to intermittently pause the\nserver loop for a while, run something else, then get back to the previous request-handling\nphase; repeating this iteration until the heat death of the universe. Initially, I opted for\nthe low-level \u003ccode\u003esocket\u003c/code\u003e module to write something quick and dirty. However, the\nimplementation got hairy pretty quickly. While the \u003ccode\u003esocket\u003c/code\u003e module gives you plenty of\ncontrol over how you can tune the server\u0026rsquo;s behavior, writing a server with robust signal and\nerror handling can be quite a bit of boilerplate work.\u003c/p\u003e","title":"Pausing and resuming a socket server in Python"},{"content":"Back in the days when I was working as a data analyst, I used to spend hours inside Jupyter notebooks exploring, wrangling, and plotting data to gain insights. However, as I shifted my career gear towards backend software development, my usage of interactive exploratory tools dwindled.\nNowadays, I spend the majority of my time working on a fairly large Django monolith accompanied by a fleet of microservices. Although I love my text editor and terminal emulators, I miss the ability to just start a Jupyter Notebook server and run code snippets interactively. While Django allows you to open up a shell environment and run code snippets interactively, it still isn\u0026rsquo;t as flexible as a notebook.\nSo, I wanted to see if I could connect a Jupyter notebook server to a containerized Django application running on my local machine and interactively start making queries from there. Turns out, you can do that by integrating three tools into your Dockerized environment: ipykernel, jupyter, and django-extensions. Before I start explaining how everything is tied together, here\u0026rsquo;s a fully working example of a containerized Django application where you can log into the Jupyter server and start debugging the app.\nThe app is just a Dockerized version of the famous polls-app from the Django tutorial. The directory structure looks as follows:\n../django-jupyter/ ├── Dockerfile ├── docker-compose.yml ├── mysite │ ├── db.sqlite3 │ ├── manage.py │ ├── mysite │ │ ├── __init__.py │ │ ├── _debug_settings.py │ │ ├── asgi.py │ │ ├── settings.py │ │ ├── urls.py │ │ └── wsgi.py │ ├── polls │ │ ├── __init__.py │ │ ├── admin.py │ │ ├── apps.py │ │ ├── migrations │ │ │ ├── 0001_initial.py │ │ │ └── __init__.py │ │ ├── models.py │ │ ├── tests.py │ │ ├── urls.py │ │ └── views.py │ └── script.ipynb ├── requirements.txt └── requirements-dev.txt We define and pin the dependencies required for the Jupyter integration in the requirements-dev.txt file:\n# These pinned deps will probably get outdated by the time you\u0026#39;re reading it. # Use the latest version but always pin them in applications. ipykernel==6.20.1 jupyter==1.0.0 django-extensions==3.2.1 The application dependencies are defined in the requirements.txt file:\ndjango==4.1.5 In the mysite/mysite/_debug_settings.py file, we import the configs from the primary settings file and add the Jupyter configuration attributes there. Here\u0026rsquo;s the full content of the extended _debug_settings.py file:\nfrom .settings import * # noqa INSTALLED_APPS.append(\u0026#34;django_extensions\u0026#34;) # noqa SHELL_PLUS = \u0026#34;ipython\u0026#34; SHELL_PLUS_PRINT_SQL = True IPYTHON_ARGUMENTS = [ \u0026#34;--ext\u0026#34;, \u0026#34;django_extensions.management.notebook_extension\u0026#34;, \u0026#34;--debug\u0026#34;, ] IPYTHON_KERNEL_DISPLAY_NAME = \u0026#34;Django Shell-Plus\u0026#34; NOTEBOOK_ARGUMENTS = [ \u0026#34;--ip\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8895\u0026#34;, \u0026#34;--allow-root\u0026#34;, \u0026#34;--no-browser\u0026#34;, \u0026#34;--NotebookApp.iopub_data_rate_limit=1e5\u0026#34;, \u0026#34;--NotebookApp.token=\u0026#39;\u0026#39;\u0026#34;, ] DJANGO_ALLOW_ASYNC_UNSAFE = True Notice how we\u0026rsquo;re appending the django_extensions app to the INSTALLED_APPS list defined in the main settings file. Then we\u0026rsquo;re setting the shell to ipython with the SHELL_PLUS attribute. The NOTEBOOK_ARGUMENTS defines the port of the Jupyter server and some auth-specific settings.\nNext, in the Dockerfile, we\u0026rsquo;re defining the application like this:\n# Dockerfile FROM python:3.11-bullseye # Set the working directory inside the container. WORKDIR /code # Don\u0026#39;t write .pyc files and make the output unbuffered. ENV PYTHONDONTWRITEBYTECODE 1 ENV PYTHONUNBUFFERED 1 # Install dependencies. RUN pip install --upgrade pip COPY requirements.txt requirements-dev.txt ./ RUN pip install -r requirements.txt -r requirements-dev.txt # Copy the project code. COPY . /code Finally, we\u0026rsquo;re orchestrating the application and the Jupyter server in the docker-compose.yml file. Here\u0026rsquo;s how it looks:\nversion: \u0026#34;3.9\u0026#34; services: web: build: . working_dir: /code/mysite volumes: - .:/code webserver: extends: service: web command: python manage.py runserver 0.0.0.0:8000 ports: - \u0026#34;8000:8000\u0026#34; jupyter: extends: service: web environment: - DJANGO_SETTINGS_MODULE=mysite._debug_settings - DJANGO_ALLOW_ASYNC_UNSAFE=true command: python manage.py shell_plus --notebook ports: - \u0026#34;8895:8895\u0026#34; debug: extends: service: web working_dir: /code command: sleep infinity We\u0026rsquo;re orchestrating three services here: webserver, jupyter, and debug. All of them extend the base web service that builds the Dockerfile. The webserver service is where the Django app is run and exposed via the 8000 port. The jupyter service runs the Jupyter server and makes it accessible through your browser via the 8895 port. Additionally, note how we are using our extended version of the main settings by overriding the DJANGO_SETTINGS_MODULE environment variable and setting it to mysite._debug_settings. The debug container is spun up to run the migration commands and perform other maintenance tasks within the container network. All the maintenance commands are defined in the Makefile for your convenience. You can run any of these by running make \u0026lt;target\u0026gt; from the root directory.\nAnd that\u0026rsquo;s it!\nIf you have Docker and docker-compose installed on your local system, you can give it a try. Clone the fully working example repo, navigate to the root directory and run:\ndocker compose up -d Then run the migration command:\ndocker compose exec debug python mysite/manage.py makemigrations \\ \u0026amp;\u0026amp; docker compose exec debug python mysite/manage.py migrate Now head over to your browser and go to http://localhost:8000. You should see an empty page with a simple header like this:\nIf you go to http://localhost:8895, you\u0026rsquo;ll be able to open a new notebook that automatically connects to your database and allows you to write interactive code immediately.\nYou can run the following snippet and it\u0026rsquo;ll create two questions and two choices in the database.\nfrom polls import models as polls_models from datetime import datetime, timezone for question_text in (\u0026#34;Are you okay?\u0026#34;, \u0026#34;Do you wanna go there?\u0026#34;): question = polls_models.Question.objects.create( question_text=question_text, pub_date=datetime.now(tz=timezone.utc), ) question.choice_set.set( polls_models.Choice.objects.create(choice_text=ctext) for ctext in (\u0026#34;yes\u0026#34;, \u0026#34;no\u0026#34;) ) If you run this and refresh your application server, you\u0026rsquo;ll that the objects have been created and they appear in the view:\nFurther reading How to access Jupyter notebook in a Docker container ","permalink":"https://rednafi.com/python/django-and-jupyter-notebook/","summary":"\u003cp\u003eBack in the days when I was working as a data analyst, I used to spend hours inside Jupyter\nnotebooks exploring, wrangling, and plotting data to gain insights. However, as I shifted my\ncareer gear towards backend software development, my usage of interactive exploratory tools\ndwindled.\u003c/p\u003e\n\u003cp\u003eNowadays, I spend the majority of my time working on a fairly large Django monolith\naccompanied by a fleet of microservices. Although I love my text editor and terminal\nemulators, I miss the ability to just start a Jupyter Notebook server and run code snippets\ninteractively. While Django allows you to open up a shell environment and run code snippets\ninteractively, it still isn\u0026rsquo;t as flexible as a notebook.\u003c/p\u003e","title":"Debugging a containerized Django application in Jupyter Notebook"},{"content":"I was working with a table that had a similar (simplified) structure like this:\n| uuid | file_path | |----------------------------------|---------------------------| | b8658dfc3e80446c92f7303edf31dcbd | media/private/file_1.pdf | | 3d750874a9df47388569a23c559a4561 | media/private/file_2.csv | | d177b7f7d8b046768ab65857451a0354 | media/private/file_3.txt | | df45742175d7451dad59761f15653d9d | media/private/image_1.png | | a542966fc193470dab84351c15523042 | media/private/image_2.jpg | Let\u0026rsquo;s say the above table is represented by the following Django model:\nfrom django.db import models class FileCabinet(models.Model): uuid = models.UUIDField( primary_key=True, default=uuid.uuid4, editable=False ) file_path = models.FileField(upload_to=\u0026#34;files/\u0026#34;) I needed to extract the file names with their extensions from the file_path column and create new paths by adding the prefix dir/ before each file name. This would involve stripping everything before the file name from a file path and adding the prefix, resulting in a list of new file paths like this: ['dir/file_1.pdf', ..., 'dir/image_2.jpg'].\nUsing Django ORM and some imperative Python code you could do the following:\n... # This will give you a queryset with the file paths. # e.g. \u0026lt;QuerySet [\u0026#39;media/private/file_1.pdf\u0026#39;, ... ]\u0026gt; file_paths = FileCabinet.objects.values_list(\u0026#34;file_path\u0026#34;, flat=True) # Now the file names can be collected in a list via a listcomp. # This will return: [\u0026#34;dir/file_1.pdf\u0026#34;, ..., \u0026#34;dir/image_2.jpg\u0026#34;] file_paths_new = [ f\u0026#34;dir/{file_path.split(\u0026#39;/\u0026#39;)[-1]}\u0026#34; for file_path in file_paths ] ... Here, we use the FileCabinet model to make a query and obtain the file paths. We then use Python to split the file paths and extract the file names, and add the prefix dir/ to create the new paths. While this approach is relatively simple, it can be slow and resource-intensive if the size of the working dataset is large. This is because the entire working dataset is loaded into memory and the text manipulation is performed in Python.\nTo improve performance and efficiency, Django offers a declarative approach using expressions. These expressions allow you to offload operations like this to the database, which can be significantly faster and less resource-intensive than the imperative approach, especially for larger querysets. Here\u0026rsquo;s how you can achieve the same result in a declarative manner:\n... from django.db.models import F, Value from django.db.models.functions import ( Concat, Reverse, Right, StrIndex, ) file_cabinet = polls_models.FileCabinet.objects.annotate( last_occur=StrIndex(Reverse(F(\u0026#34;file_path\u0026#34;)), Value(\u0026#34;/\u0026#34;)), file_name=Right(F(\u0026#34;file_path\u0026#34;), F(\u0026#34;last_occur\u0026#34;) - 1), file_path_new=Concat(Value(\u0026#34;dir/\u0026#34;), F(\u0026#34;file_name\u0026#34;)), ) ... You can see the new file paths by inspecting the file_cabinet queryset as follows:\nfile_paths_new = file_cabinet.values_list(\u0026#34;file_path_new\u0026#34;, flat=True) This will give you the following queryset:\n\u0026lt;QuerySet [\u0026#39;dir/file_1.pdf\u0026#39;, \u0026#39;dir/file_2.csv\u0026#39;, \u0026#39;dir/file_3.txt\u0026#39;, \u0026#39;dir/image_1.png\u0026#39;, \u0026#39;dir/image_2.jpg\u0026#39;] \u0026gt; Now, let\u0026rsquo;s step through the each of the ORM functionality that was levereged here:\nThe annotate function is being used to add additional information to each returned FileCabinet object. This function allows you to specify additional fields that should be calculated and included in the returned queryset. Inside the annotation method, we use F objects to reference a model field within the query. They can be used to refer to a field\u0026rsquo;s value in the context of an update or filter, rather than referring to the actual field itself.\nThree fields are being added to the FileCabinet objects: last_occur, file_name, and file_path_new.\nlast_occur is being calculated by using the StrIndex function. This function takes two arguments: the string to search and the string to search for. In this case, the string being searched is the file_path field, but it has been passed through the Reverse function to reverse the string. This is done so that the StrIndex function starts searching from the end of the string, rather than the beginning. The second argument to StrIndex is the string to search for, which in this case is /. The StrIndex function returns the position of the first occurrence of the search string in the main string.\nfile_name is being calculated by using the Right function. This function takes two arguments: the string to extract from and the number of characters to extract. In this case, the string being extracted from is the file_path field, and the number of characters to extract is specified by the last_occur field. The last_occur field represents the position of the last occurrence of / in the file_path field, so extracting the characters from this position onwards gives us the file name with its extension. The - 1 at the end is used to remove the / character itself from the extracted string.\nFinally, the file_path_new is constructed by using the Concat function. This function takes a variable number of arguments and concatenates them together into a single string. In this case, the dir/ prefix is being concatenated with file_name field.\nPerfection!\nFurther reading Do database work in the database rather than in Python Use StrIndex to get the last instance of a character for an annotation in Django ","permalink":"https://rednafi.com/python/manipulate-text-with-django-query-expression/","summary":"\u003cp\u003eI was working with a table that had a similar (simplified) structure like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e|               uuid               |         file_path         |\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e|----------------------------------|---------------------------|\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e| b8658dfc3e80446c92f7303edf31dcbd | media/private/file_1.pdf  |\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e| 3d750874a9df47388569a23c559a4561 | media/private/file_2.csv  |\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e| d177b7f7d8b046768ab65857451a0354 | media/private/file_3.txt  |\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e| df45742175d7451dad59761f15653d9d | media/private/image_1.png |\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e| a542966fc193470dab84351c15523042 | media/private/image_2.jpg |\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eLet\u0026rsquo;s say the above table is represented by the following Django model:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003edjango.db\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003emodels\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eFileCabinet\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emodels\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eModel\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003euuid\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003emodels\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eUUIDField\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003eprimary_key\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edefault\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003euuid\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003euuid4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eeditable\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003efile_path\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003emodels\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eFileField\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eupload_to\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;files/\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eI needed to extract the file names with their extensions from the \u003ccode\u003efile_path\u003c/code\u003e column and\ncreate new paths by adding the prefix \u003ccode\u003edir/\u003c/code\u003e before each file name. This would involve\nstripping everything before the file name from a file path and adding the prefix, resulting\nin a list of new file paths like this: \u003ccode\u003e['dir/file_1.pdf', ..., 'dir/image_2.jpg']\u003c/code\u003e.\u003c/p\u003e","title":"Manipulating text with query expressions in Django"},{"content":"At my workplace, I was writing a script to download multiple files from different S3 buckets. The script relied on Django ORM, so I couldn\u0026rsquo;t use Python\u0026rsquo;s async paradigm to speed up the process. Instead, I opted for boto3 to download the files and concurrent.futures.ThreadPoolExecutor to spin up multiple threads and make the requests concurrently.\nHowever, since the script was expected to be long-running, I needed to display progress bars to show the state of execution. It\u0026rsquo;s quite easy to do with tqdm when you\u0026rsquo;re just looping over a list of file paths and downloading the contents synchronously:\nfrom tqdm import tqdm for file_path in tqdm(file_paths): download_file(file_path) But you can\u0026rsquo;t do this when multiple threads or processes are doing the work. Here\u0026rsquo;s what I\u0026rsquo;ve found that works quite well:\nfrom __future__ import annotations import time from concurrent.futures import ThreadPoolExecutor, as_completed from typing import Generator import httpx from tqdm import tqdm def make_request(url: str) -\u0026gt; dict: with httpx.Client() as client: response = client.get(url) # Additional delay to simulate a slow request. time.sleep(1) return response.json() def make_requests( urls: list[str], ) -\u0026gt; Generator[list[dict], None, None]: with tqdm(total=len(urls)) as pbar: with ThreadPoolExecutor(max_workers=5) as executor: futures = [executor.submit(make_request, url) for url in urls] for future in as_completed(futures): pbar.update(1) yield future.result() def main() -\u0026gt; None: urls = [ \u0026#34;https://httpbin.org/get\u0026#34;, \u0026#34;https://httpbin.org/get?foo=bar\u0026#34;, \u0026#34;https://httpbin.org/get?foo=baz\u0026#34;, \u0026#34;https://httpbin.org/get?foo=qux\u0026#34;, \u0026#34;https://httpbin.org/get?foo=quux\u0026#34;, ] results = [] for result in make_requests(urls): results.append(result) print(results) if __name__ == \u0026#34;__main__\u0026#34;: main() Running this will print:\n100%|█████████████████████████████████████████████████████| 5/5 [00:01\u0026lt;00:00, 3.51it/s] ... This script makes 5 concurrent requests by leveraging ThreadPoolExecutor from the concurrent.futures module. The make_request function just sends one request to a URL and sleeps for a second to simulate a long-running task. Then the make_requests function spins up 5 threads and calls the make_request function in each one with a different URL.\nHere, we\u0026rsquo;re instantiating tqdm as a context manager and passing the total length of the urls. This allows tqdm to calculate the progress bar. Then in a nested context manager, we spin up the threads and pass the make_request to the executor.submit method. We collect the future objects returned by the executor.submit methods in a list and update the progress bar with pbar.update(1) while iterating through the futures. And that\u0026rsquo;s it, mission successful.\nI usually use contextlib.ExitStack to avoid nested context managers like this:\n... from contextlib import ExitStack def make_requests( urls: list[str], ) -\u0026gt; Generator[list[dict], None, None]: with ExitStack() as stack: executor = stack.enter_context(ThreadPoolExecutor(max_workers=5)) pbar = stack.enter_context(tqdm(total=len(urls))) futures = [executor.submit(make_request, url) for url in urls] for future in as_completed(futures): pbar.update(1) yield future.result() ... Running this script will yield the same result as before.\nFurther reading How to use tqdm with multithreading? ","permalink":"https://rednafi.com/python/tqdm-progressbar-with-concurrent-futures/","summary":"\u003cp\u003eAt my workplace, I was writing a script to download multiple files from different S3\nbuckets. The script relied on Django ORM, so I couldn\u0026rsquo;t use Python\u0026rsquo;s async paradigm to speed\nup the process. Instead, I opted for \u003ccode\u003eboto3\u003c/code\u003e to download the files and\n\u003ccode\u003econcurrent.futures.ThreadPoolExecutor\u003c/code\u003e to spin up multiple threads and make the requests\nconcurrently.\u003c/p\u003e\n\u003cp\u003eHowever, since the script was expected to be long-running, I needed to display progress bars\nto show the state of execution. It\u0026rsquo;s quite easy to do with \u003ccode\u003etqdm\u003c/code\u003e when you\u0026rsquo;re just looping\nover a list of file paths and downloading the contents synchronously:\u003c/p\u003e","title":"Using tqdm with concurrent.fututes in Python"},{"content":"The colon : command is a shell utility that represents a truthy value. It can be thought of as an alias for the built-in true command. You can test it by opening a shell script and typing a colon on the command line, like this:\n: If you then inspect the exit code by typing $? on the command line, you\u0026rsquo;ll see a 0 there, which is exactly what you\u0026rsquo;d see if you had used the true command.\n: ; echo $? The output will be:\n0 I find the colon command useful when running a shell script with the -x flag, which prints out the commands being executed by the interpreter. For example, consider the following script:\n#!/bin/bash # script.sh echo \u0026#34;section 1: print the first 2 lines of the current directory\u0026#34; ls -lah | head -n 2 echo \u0026#34;section 2: print the size of the /usr/bin directory\u0026#34; du -sh /usr/bin Running this script with bash -x script.sh will print the following lines:\n+ echo \u0026#39;section 1: print the first 2 lines of the current directory\u0026#39; section 1: print the first 2 lines of the current directory + ls -lah + head -n 2 total 120 drwxr-xr-x 26 rednafi staff 832B Dec 23 13:35 . + echo \u0026#39;section 2: print the size of the /usr/bin directory\u0026#39; section 2: print the size of the /usr/bin directory + du -sh /usr/bin 76M /usr/bin Notice that the above script prints out each command first (denoted by a preceding + sign) and then its respective output. However, the echo \u0026quot;section...\u0026quot; commands in this script are only used for debugging purposes, to enhance the readability of the output by providing separation between different sections. Therefore, repeating these commands and their outputs can be a little redundant. You can use the colon command to eliminate this repetition, as follows:\n#!/bin/bash : \u0026#34;section 1: print the first 2 lines of the current directory\u0026#34; ls -lah | head -n 2 : \u0026#34;section 2: print the size of the /usr/bin directory\u0026#34; du -sh /usr/bin Running this script with the -x flag will produce the following output:\n+ : \u0026#39;section 1: print the first 2 lines of the current directory\u0026#39; + ls -lah + head -n 2 total 120 drwxr-xr-x 26 rednafi staff 832B Dec 23 13:35 . + : \u0026#39;section 2: print the size of the /usr/bin directory\u0026#39; + du -sh /usr/bin 76M /usr/bin If you look closely, you\u0026rsquo;ll see that the debug commands and their outputs are no longer getting repeated.\nFurther reading Why I use the colon command by @anthonywritescode ","permalink":"https://rednafi.com/misc/colon-command-in-shell-scripts/","summary":"\u003cp\u003eThe colon \u003ccode\u003e:\u003c/code\u003e command is a shell utility that represents a truthy value. It can be thought\nof as an alias for the built-in \u003ccode\u003etrue\u003c/code\u003e command. You can test it by opening a shell script\nand typing a colon on the command line, like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIf you then inspect the exit code by typing \u003ccode\u003e$?\u003c/code\u003e on the command line, you\u0026rsquo;ll see a \u003ccode\u003e0\u003c/code\u003e\nthere, which is exactly what you\u0026rsquo;d see if you had used the true command.\u003c/p\u003e","title":"Colon command in shell scripts"},{"content":"Django has a Model.objects.bulk_update method that allows you to update multiple objects in a single pass. While this method is a great way to speed up the update process, oftentimes it\u0026rsquo;s not fast enough. Recently, at my workplace, I found myself writing a script to update half a million user records and it was taking quite a bit of time to mutate them even after leveraging bulk update. So I wanted to see if I could use multiprocessing with .bulk_update to quicken the process even more. Turns out, yep I can!\nHere\u0026rsquo;s a script that creates 100k users in a PostgreSQL database and updates their usernames via vanilla .bulk_update. Notice how we\u0026rsquo;re timing the update duration:\n# app_name/vanilla_bulk_update.py import os import django # This allows us to run this module as a script inside a Django app. os.environ.setdefault(\u0026#34;DJANGO_SETTINGS_MODULE\u0026#34;, \u0026#34;mysite.settings\u0026#34;) django.setup() import time from django.contrib.auth.models import User # Delete the previous users, if there\u0026#39;s any. User.objects.all().delete() # Create 100k users. users = User.objects.bulk_create( (User(username=f\u0026#34;user_{i}\u0026#34;) for i in range(100_000)), ) # Start time. s1 = time.perf_counter() # Update all the users\u0026#39; usernames to use upper case. for user in users: user.username = user.username.upper() # Save all the users. The batch_size determines how many records will # be saved at once. User.objects.bulk_update(users, [\u0026#34;username\u0026#34;], batch_size=1_000) # End time. e1 = time.perf_counter() # Print the time taken. print(f\u0026#34;Time taken to update 100k users: {e1 - s1} seconds.\u0026#34;) # Print a few usernames to verify the script changed them. print(\u0026#34;Updated usernames:\u0026#34;) print(\u0026#34;===================\u0026#34;) for username in User.objects.values_list(\u0026#34;username\u0026#34;, flat=True)[:5]: print(username) This can be executed as a script like this:\npython -m app_name.vanilla_bulk_update It\u0026rsquo;ll return:\nTime taken to update 100k users: 9.220380916005524 seconds. Updated usernames: =================== USER_99840 USER_99841 USER_99842 USER_99843 USER_99844 A little over 9 seconds isn\u0026rsquo;t too bad for 100k users but we can do better. Here\u0026rsquo;s how I\u0026rsquo;ve updated the above script to make it 4x faster:\n# app_name/multiprocessing_bulk_update.py import os import django # This allows us to run this module as a script inside a Django app. os.environ.setdefault(\u0026#34;DJANGO_SETTINGS_MODULE\u0026#34;, \u0026#34;mysite.settings\u0026#34;) django.setup() import multiprocessing as mp import time from django.contrib.auth.models import User MAX_WORKERS = 2 * mp.cpu_count() - 1 CHUNK_SIZE = 1_000 def main(): # Delete the previous users, if there\u0026#39;s any. User.objects.all().delete() # Create 100k users. users = User.objects.bulk_create( (User(username=f\u0026#34;user_{i}\u0026#34;) for i in range(100_000)) ) # Start time. s1 = time.perf_counter() # Mutate the usernames to use upper case. for user in users: user.username = user.username.upper() # Split the users into chunks for each process to work on. This returns # [[USER_0, USER_1, USER_2, ...], [USER_3, USER_4, USER_5, ...], ...] user_chunks = ( users[i : i + CHUNK_SIZE] for i in range(0, len(users), CHUNK_SIZE) ) # Close the connection before forking. django.db.connections.close_all() # Create a pool of processes and run the update_users function on # each chunk. with mp.Pool(MAX_WORKERS) as pool: pool.map(update_users, user_chunks, chunksize=10) # End time. e1 = time.perf_counter() # Print the time taken. print( \u0026#34;Time taken to update 100k users with multiprocessing: \u0026#34; f\u0026#34;{e1 - s1} seconds.\u0026#34; ) # Print a few usernames to see that the script has changed them # as expected. print(\u0026#34;Updated usernames:\u0026#34;) print(\u0026#34;===================\u0026#34;) for username in User.objects.values_list(\u0026#34;username\u0026#34;, flat=True)[:5]: print(username) def update_users(user_chunk): # batch_size determines how many records are saved at once. User.objects.bulk_update( user_chunk, [\u0026#34;username\u0026#34;], batch_size=CHUNK_SIZE ) if __name__ == \u0026#34;__main__\u0026#34;: main() This script divides the updated user list into a list of multiple user chunks and assigns that to the user_chunks variable. The update_users function takes a single user chunk and runs .bulk_update on that. Then we fork a bunch of processes and run the update_users function over the user_chunks via multiprocessing.Pool.map. Each process consumes 10 chunks of users in a single go — determined by the chunksize parameter of the pool.map function. Running the updated script will give you similar output as before but with a much smaller runtime:\npython -m app_name.multiprocessing_bulk_update This will print the following:\nTime taken to update 100k users with multiprocessing: 2.27 seconds. Updated usernames: =================== USER_960 USER_961 USER_962 USER_963 USER_964 Whoa! This updated the records in under 2.5 seconds. Quite a bit of performance gain there.\nThis won\u0026rsquo;t work if you\u0026rsquo;re using SQLite database as your backend since SQLite doesn\u0026rsquo;t support concurrent writes from multiple processes. Trying to run the second script with SQLite backend will incur a database error.\nFurther reading Django bulk_update Using a pool of forked workers ","permalink":"https://rednafi.com/python/faster-bulk-update-in-django/","summary":"\u003cp\u003eDjango has a \u003ccode\u003eModel.objects.bulk_update\u003c/code\u003e method that allows you to update multiple objects\nin a single pass. While this method is a great way to speed up the update process,\noftentimes it\u0026rsquo;s not fast enough. Recently, at my workplace, I found myself writing a script\nto update half a million user records and it was taking quite a bit of time to mutate them\neven after leveraging bulk update. So I wanted to see if I could use \u003ccode\u003emultiprocessing\u003c/code\u003e with\n\u003ccode\u003e.bulk_update\u003c/code\u003e to quicken the process even more. Turns out, yep I can!\u003c/p\u003e","title":"Faster bulk_update in Django"},{"content":"I\u0026rsquo;ve just migrated from Ubuntu to macOS for work and am still in the process of setting up the machine. I\u0026rsquo;ve been a lifelong Linux user and this is the first time I\u0026rsquo;ve picked up an OS that\u0026rsquo;s not just another flavor of Debian. Primarily, I work with Python, NodeJS, and a tiny bit of Go. Previously, any time I had to install these language runtimes, I\u0026rsquo;d execute a bespoke script that\u0026rsquo;d install:\nPython via deadsnake ppa. NodeJS via nvm. Go from the official Go binary source. Along with the hassle of having to manage three version managers, setting up multiple versions of Python almost always felt like a chore. I\u0026rsquo;ve used pyenv before which kind of feels like nvm and works quite well in practice. However, on Twitter, I came across this reply by Adam Johnson which mentions that asdf can manage multiple runtimes of different languages — one version manager to rule them all. Also, it\u0026rsquo;s written in pure bash so there\u0026rsquo;s no external dependency required for the tool to work. Since I\u0026rsquo;m starting from scratch on a new OS, I wanted to give this a tool to try. Spoiler alert, it works with zero drama. Here, I\u0026rsquo;ll quickly explain how to get up and running with multiple versions of Python and make them work seamlessly.\nPrerequisites For this to work, I\u0026rsquo;m assuming that you\u0026rsquo;ve got homebrew installed on your system. Install asdf with the following command:\nbrew install asdf Once asdf is installed, you\u0026rsquo;ll need to install the asdf Python plugin. Run this:\nasdf plugin-add python Also, you\u0026rsquo;ll need to make sure that your system has the asdf plugin dependencies in place.\nBootstrapping Python Once the prerequisites are fulfilled, you\u0026rsquo;re ready to install the Python versions from the source. Let\u0026rsquo;s say you want to install Python 3.11. To do so, run:\nasdf install python 3.11.0 This will install Python in the /Users/$USER/.asdf/shims/python3.11 location. Just concat the command to install multiple versions of Python:\nasdf install 3.10.15 \u0026amp;\u0026amp; asdf install 3.9.9 Selecting a specific Python version Once you\u0026rsquo;ve installed your desired Python versions with asdf, if you try to invoke global Python with python or python3 command, you\u0026rsquo;ll encounter the following error:\nNo version is set for command python3 Consider adding one of the following versions in your config file at python 3.8.15 python 3.11.0 python 3.10.8 To address this, you can run the next command to select the latest available version of Python (here it\u0026rsquo;s 3.11.0) as the global default runtime:\nasdf global python latest Running this will add a $HOME/.tool-versions file with the following content:\npython 3.11.0 You can also select other Python versions as the global runtime like this:\nasdf global python \u0026lt;python-version\u0026gt; In a project, if you want to use a specific Python version other than the global one, you can run:\nasdf local python \u0026lt;python-version\u0026gt; This will add a $PATH/.tool-versions similar to the global file. Now you can just go ahead and start using that specific version of Python. Running this command will create a virtual environment using the locally specified Python runtime and start the interpreter inside that:\npython -m venv .venv \u0026amp;\u0026amp; source .venv/bin/activate \u0026amp;\u0026amp; python Removing a runtime Running asdf uninstall python \u0026lt;python-version\u0026gt; will do the trick.\n","permalink":"https://rednafi.com/python/install-python-with-asdf/","summary":"\u003cp\u003eI\u0026rsquo;ve just migrated from Ubuntu to macOS for work and am still in the process of setting up\nthe machine. I\u0026rsquo;ve been a lifelong Linux user and this is the first time I\u0026rsquo;ve picked up an OS\nthat\u0026rsquo;s not just another flavor of Debian. Primarily, I work with Python, NodeJS, and a tiny\nbit of Go. Previously, any time I had to install these language runtimes, I\u0026rsquo;d execute a\nbespoke script that\u0026rsquo;d install:\u003c/p\u003e","title":"Installing Python on macOS with asdf"},{"content":"TIL that you can specify update_fields while saving a Django model to generate a leaner underlying SQL query. This yields better performance while updating multiple objects in a tight loop. To test that, I\u0026rsquo;m opening an IPython shell with python manage.py shell -i ipython command and creating a few user objects with the following lines:\nIn [1]: from django.contrib.auth import User In [2]: for i in range(1000): ...: fname, lname = f\u0026#39;foo_{i}\u0026#39;, f\u0026#39;bar_{i}\u0026#39; ...: User.objects.create( ...: first_name=fname, last_name=lname, username=fname) ...: Here\u0026rsquo;s the underlying query Django generates when you\u0026rsquo;re trying to save a single object:\nIn [3]: from django.db import reset_queries, connections In [4]: reset_queries() In [5]: user_0 = User.objects.first() In [6]: user_0.first_name = \u0026#39;foo_updated\u0026#39; In [7]: user_0.save() In [8]: connection.queries This will print:\n[ ..., { \u0026#34;sql\u0026#34;: \u0026#39;UPDATE \u0026#34;auth_user\u0026#34; SET \u0026#34;password\u0026#34; = \\\u0026#39;\\\u0026#39;, \u0026#34;last_login\u0026#34; = NULL, \u0026#34;is_superuser\u0026#34; = 0, \u0026#34;username\u0026#34; = \\\u0026#39;foo_0-bar_0\\\u0026#39;, \u0026#34;first_name\u0026#34; = \\\u0026#39;foo_updated\\\u0026#39;, \u0026#34;last_name\u0026#34; = \\\u0026#39;bar_0\\\u0026#39;, \u0026#34;email\u0026#34; = \\\u0026#39;\\\u0026#39;, \u0026#34;is_staff\u0026#34; = 0, \u0026#34;is_active\u0026#34; = 1, \u0026#34;date_joined\u0026#34; = \\\u0026#39;2022-11-09 22:27:39.291676\\\u0026#39; WHERE \u0026#34;auth_user\u0026#34;.\u0026#34;id\u0026#34; = 1002\u0026#39;, \u0026#34;time\u0026#34;: \u0026#34;0.009\u0026#34;, }, ] If you inspect the query, you\u0026rsquo;ll see that although we\u0026rsquo;re only updating the first_name field on the user_0 object, Django is generating a query that updates all the underlying fields on the object. The SQL query always passes the pre-existing values of the fields that weren\u0026rsquo;t touched. This might seem trivial, but what if the model consisted of 20 fields and you need to call save() on it frequently? At a certain scale the database query that updates all of your columns every time you call save() can start becoming expensive.\nSpecifying update_fields inside the save() method can make the query leaner. Consider this:\nIn[9]: reset_queries() In[10]: user_0.first_name = \u0026#34;foo_updated_again\u0026#34; In[11]: user_0.save(update_fields=[\u0026#34;first_name\u0026#34;]) In[12]: connection.queries This prints:\n[ {\u0026#39;sql\u0026#39;: \u0026#39;UPDATE \u0026#34;auth_user\u0026#34; SET \u0026#34;first_name\u0026#34; = \\\u0026#39;changed_again\\\u0026#39; WHERE \u0026#34;auth_user\u0026#34;.\u0026#34;id\u0026#34; = 1002\u0026#39;, \u0026#39;time\u0026#39;: \u0026#39;0.008\u0026#39; } ] You can see this time, Django generates a SQL that only updates the specific field we want and doesn\u0026rsquo;t send any redundant data over the wire. The following snippet quantifies the performance gain while updating 1000 objects in a tight loop:\n# src.py import os import time import django os.environ.setdefault(\u0026#34;DJANGO_SETTINGS_MODULE\u0026#34;, \u0026#34;mysite.settings\u0026#34;) django.setup() from django.contrib.auth.models import User # Create 1000 users. for i in range(1000): User.objects.create_user( first_name=f\u0026#34;foo_{i}\u0026#34;, last_name=f\u0026#34;bar_{i}\u0026#34;, username=f\u0026#34;foo_{i}-bar_{i}\u0026#34;, ) ############### Update all users with \u0026#39;.save()\u0026#39; ############### s1 = time.perf_counter() for i, user in zip(range(1000), User.objects.all()): user.first_name = f\u0026#34;foo_updated_{i}\u0026#34; user.save() e1 = time.perf_counter() t1 = e1 - s1 print(f\u0026#34;User.save(): {t1:.2f}s\u0026#34;) ############################################################### ###### Update all users with \u0026#39;.save(update_fields=[...])\u0026#39;###### s2 = time.perf_counter() for i, user in zip(range(1000), User.objects.all()): user.first_name = f\u0026#34;foo_updated_again_{i}\u0026#34; user.save(update_fields=[\u0026#34;first_name\u0026#34;]) e2 = time.perf_counter() t2 = e2 - s2 print(f\u0026#34;User.save(update_fields=[...]): {t2:.2f}s\u0026#34;) ############################################################### print( f\u0026#34;save(update_fields=[...]) is {t1 / t2:.2f}x faster than save()\u0026#34; ) Running this script will print the following:\nUser.save(): 1.86s User.save(update_fields=[...]): 1.77s User.save(update_fields=[...] is 1.05x faster than User.save() You can see that User.save(updated_fields=[...]) is a tad bit faster than plain User.save.\nShould you always use it? Probably not. While the performance gain is measurable when you\u0026rsquo;re updating multiple objects in a loop, it\u0026rsquo;s quite negligible if the object count is low. Also, this adds maintenance overhead as any time you change the model, you\u0026rsquo;ll have to remember to keep the Model.save(update_fields=[...]) in sync. If you forget to add a field to the update_fields, Django will silently ignore the incoming data against that field and data will be lost.\nFurther reading Specifying which fields to save - Django docs Save your Django models using update_fields for better performance - Reddit ","permalink":"https://rednafi.com/python/save-with-update-fields-in-django/","summary":"\u003cp\u003eTIL that you can specify \u003ccode\u003eupdate_fields\u003c/code\u003e while saving a Django model to generate a leaner\nunderlying SQL query. This yields better performance while updating multiple objects in a\ntight loop. To test that, I\u0026rsquo;m opening an IPython shell with\n\u003ccode\u003epython manage.py shell -i ipython\u003c/code\u003e command and creating a few user objects with the\nfollowing lines:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eIn\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e \u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003edjango.contrib.auth\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eUser\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eIn\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ei\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1000\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e   \u003cspan class=\"o\"\u003e...\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e     \u003cspan class=\"n\"\u003efname\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003elname\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;foo_\u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;bar_\u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e   \u003cspan class=\"o\"\u003e...\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e     \u003cspan class=\"n\"\u003eUser\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eobjects\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecreate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e   \u003cspan class=\"o\"\u003e...\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e         \u003cspan class=\"n\"\u003efirst_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003efname\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003elast_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003elname\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eusername\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003efname\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e   \u003cspan class=\"o\"\u003e...\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHere\u0026rsquo;s the underlying query Django generates when you\u0026rsquo;re trying to save a single object:\u003c/p\u003e","title":"Save models with update_fields for better performance in Django"},{"content":"At my workplace, while working on a Lambda function, I noticed that my Python logs weren\u0026rsquo;t appearing on the corresponding Cloudwatch log dashboard. At first, I thought that the function wasn\u0026rsquo;t picking up the correct log level from the environment variables. We were using Serverless framework and GitLab CI to deploy the function, so my first line of investigation involved checking for missing environment variables in those config files.\nHowever, I quickly realized that the environment variables were being propagated to the Lambda function as expected. So, the issue had to be coming from somewhere else. After perusing through some docs, I discovered from the source code of Lambda Python Runtime Interface Client that AWS Lambda Python runtime pre-configures a logging handler that modifies the format of the log message, and also adds some metadata to the record if available. What\u0026rsquo;s not pre-configured though is the log level. This means that no matter the type of log message you try to send, it won\u0026rsquo;t print anything.\nAccording to the Lambda function logging docs, to make your logging work in the Lambda environment, you\u0026rsquo;ll only need to set the log level for the root logger like this:\n# src.py import logging # Get the root logger. logger = logging.getLogger() # Set the log level to the logger object. logger.setLevel(logging.INFO) # Use the logger. This will print the message only in the Lambda runtime. logger.info(\u0026#34;Hello from Lambda!\u0026#34;) While this does make the log messages appear on the Cloudwatch dashboard, it doesn\u0026rsquo;t work whenever you\u0026rsquo;ll need to introspect the logs in your local Python interpreter. If you execute the above snippet locally, you won\u0026rsquo;t see any log message on your console. That\u0026rsquo;s because here we\u0026rsquo;re only setting the log level for the root logger and we haven\u0026rsquo;t defined any handler. To fix the local logging, you\u0026rsquo;ll need to add a handler to the logger as and set the log level on it as follows:\n# src.py import logging # Get the root logger. logger = logging.getLogger() # Create a handler. s_handler = logging.StreamHandler() # Link the handler to the logger. logger.addHandler(s_handler) # Set the log level to the logger. logger.setLevel(logging.INFO) # Use the logger. This will print the message in the local environment. logger.info(\u0026#34;This is an info message.\u0026#34;) In the Lambda Python runtime, the root logger is already pre-configured to have modified handlers. The snippet above first adds another handler to the logger and sets the log level. So technically, the root logger will contain two handlers in the Lambda environment and print every log message twice with different handlers. However, you won\u0026rsquo;t see the duplicate messages in your local environment since the local logger will have only the one handler that we\u0026rsquo;ve defined. So, the logger will behave differently in the two environments; not good.\nHaving multiple stream handlers on the root logger that send the message to the stdout will print every log message twice.\nSo, this still doesn\u0026rsquo;t do what we want. Besides, sometimes in the local environment, I just want to use logging.basicConfig and start logging with minimal configuration. The goal here is to configure the root logger in a way that doesn\u0026rsquo;t conflict with Lambda\u0026rsquo;s pre-configured handlers and also works locally without any side effects. Here\u0026rsquo;s what I\u0026rsquo;ve found that works:\n# src.py import logging # If the logger has pre-configured handlers, set the log level to the # root logger only. This branch will get executed in the Lambda runtime. if logging.getLogger().hasHandlers(): logging.getLogger().setLevel(logging.INFO) else: # Just configure with basicConfig for local usage. This branch will # get executed in the local environment. logging.basicConfig(level=logging.INFO) # Use the logger. logging.info(\u0026#34;This is an info message.\u0026#34;) The above snippet first inspects whether the root logger contains any handlers and if it does then sets the log level for the root logger. Otherwise, it just configures the logger with basicConfig for local development. This will print out the log messages both in the local and Lambda environment and won\u0026rsquo;t suffer from any side effects like message duplication. It\u0026rsquo;ll also make sure that the pre-configured formatting of the log message is kept intact.\nFurther reading Using Python logging with AWS Lambda ","permalink":"https://rednafi.com/python/logging-quirks-in-lambda-environment/","summary":"\u003cp\u003eAt my workplace, while working on a \u003ca href=\"https://aws.amazon.com/lambda/\"\u003eLambda\u003c/a\u003e function, I noticed that my Python logs weren\u0026rsquo;t\nappearing on the corresponding \u003ca href=\"https://aws.amazon.com/cloudwatch/\"\u003eCloudwatch\u003c/a\u003e log dashboard. At first, I thought that the\nfunction wasn\u0026rsquo;t picking up the correct log level from the environment variables. We were\nusing \u003ca href=\"https://www.serverless.com/\"\u003eServerless framework\u003c/a\u003e and GitLab CI to deploy the function, so my first line of\ninvestigation involved checking for missing environment variables in those config files.\u003c/p\u003e\n\u003cp\u003eHowever, I quickly realized that the environment variables were being propagated to the\nLambda function as expected. So, the issue had to be coming from somewhere else. After\nperusing through some docs, I discovered from the source code of \u003ca href=\"https://github.com/aws/aws-lambda-python-runtime-interface-client\"\u003eLambda Python Runtime\nInterface Client\u003c/a\u003e that AWS Lambda Python runtime \u003ca href=\"https://github.com/aws/aws-lambda-python-runtime-interface-client/blob/970e9c1d2613e0ce9c388547c76ac30992ad0e96/awslambdaric/bootstrap.py#L376-L385\"\u003epre-configures\u003c/a\u003e a logging handler that\nmodifies the format of the log message, and also adds some metadata to the record if\navailable. What\u0026rsquo;s not pre-configured though is the log level. This means that no matter the\ntype of log message you try to send, it won\u0026rsquo;t print anything.\u003c/p\u003e","title":"Python logging quirks in AWS Lambda environment"},{"content":"Python makes it freakishly easy to load the whole content of any file into memory and process it afterward. This is one of the first things that\u0026rsquo;s taught to people who are new to the language. While the following snippet might be frowned upon by many, it\u0026rsquo;s definitely not uncommon:\n# src.py with open(\u0026#34;foo.csv\u0026#34;, \u0026#34;r\u0026#34;) as f: # Load whole content of the file as a string in memory. f_content = f.read() # ...do your processing here. ... Adopting this pattern as the default way of handling files isn\u0026rsquo;t the most terrible thing in the world for sure. Also, this is often the preferred way of dealing with image files or blobs. However, overzealously loading file content is only okay as long as the file size is smaller than the volatile memory of the working system.\nMoreover, you\u0026rsquo;ll need to be extra careful if you\u0026rsquo;re accepting files from users and running further procedures on the content of those files. Indiscriminantly loading up the full content into memory can be dangerous as it can cause OOM errors and crash the working process if the system runs out of memory while processing a large file. This simple overlook was the root cause of a major production incident at my workplace today.\nThe affected part of our primary Django monolith asks the users to upload a CSV file to a panel, runs some procedures on the content of the file, and displays the transformed rows in a paginated HTML table. Since the application is primarily used by authenticated users and we knew the expected file size, there wasn\u0026rsquo;t any guardrail that\u0026rsquo;d prevent someone from uploading a humongous file and crashing down the whole system. To make things worse, the associated background function in the Django view was buffering the entire file into memory before starting to process the rows. Buffering the entire file surely makes the process a little faster but at the cost of higher memory usage.\nAlthough we were using background processes to avoid chugging files in the main server process, that didn\u0026rsquo;t help when the users suddendly started to send large CSV files in parallel. The workers were hitting OOM errors and getting restarted by the process manager. In our particular case, we didn\u0026rsquo;t have much reason to buffer the whole file before processing. Apparently, the naive way scaled up pretty gracefully and we didn\u0026rsquo;t pay much attention since no one was uploading file that our server instances couln\u0026rsquo;t handle. We were storing the incoming file in a models.FileField type attribute of a Django model. When a user uploads a CSV file, we\u0026rsquo;d:\nOpen the file in binary mode via the open(filepath, \u0026quot;rb\u0026quot;) callable. Buffer the whole file in memory and transform the binary content into a unicode string. Pass the stringified file-like object to csv.DictReader to load that as a CSV file. Apply transformation on the rows line by line and render the HTML table. This is how the code looks:\n# src.py import csv import io # Django mandates us to open the file in binary mode. with model_instance.file.open(mode=\u0026#34;rb\u0026#34;) as f: reader = csv.DictReader( io.StringIO(f.read().decode(errors=\u0026#34;ignore\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;)), ) with row in reader: # ... data processing goes here. The csv.DictReader callable only accepts a file-like object that\u0026rsquo;s been opened in text mode. However, Django\u0026rsquo;s FileField type doesn\u0026rsquo;t make any assumptions about the file content. It mandates us to open the file in binary mode and then decode it if necessary. So, we open the file in binary mode with model_instance.file.open(mode=\u0026quot;rb\u0026quot;) which returns an io.BufferedReader type file object. This file-like object can\u0026rsquo;t be passed directly to the csv.DictReader because a byte stream doesn\u0026rsquo;t have the concept of EOL and the CSV reader need that to know where a row ends. As a consequence, the csv.DictReader expects a file-like object opened in text mode where the rows are explicitly delineated by platform-specific EOLs like \\n or \\n\\r.\nTo solve this, we load the content of the file in memory with f.read() and decode it by calling .decode() on the result of the preceding operation. Then we create an in-memory text file-like buffer by passing the decoded string to io.StringIO. Now the CSV reader can consume this transformed file-like object and build dictionaries of rows off of that. Unfortunately, this stringified file buffer stays alive in the memory throughout the entire lifetime of the processor function. Imagine 100s of large CSV files getting thrown at the workers that execute the above code snippet. You see, at this point, overwhelming the background workers doesn\u0026rsquo;t seem too difficult.\nWhen our workers started to degrade in production and the alerts went bonkers, we began investigating the problem. After pinpointing the issue, we immediately responded to it by vertically scaling up the machines. The surface area of this issue was quite large and we didn\u0026rsquo;t want to hotfix it in fear of triggering inadvertent regressions. Once we were out of the woods, we started patching the culprit.\nThe solution to this is quite simple — convert the binary file-like object into a text file-like object without buffering everything in memory and then pass the file to the CSV reader. We were already processing the CSV rows in a lazy manner and just removing f.read() fixed the overzealous buffering issue. The corrected code snippet looks like this:\n# src.py import csv import io # Django mandates us to open the file in binary mode. with model_instance.file.open(mode=\u0026#34;rb\u0026#34;) as f: reader = csv.DictReader( io.TextIOWrapper(f, errors=\u0026#34;ignore\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;), ) with row in reader: # ... data processing goes here. Here, io.TextIOWrapper wraps the binary file-like object in a way that makes it behave as if it were opened in text mode. In fact when you open a file in text mode, the native implementation of open returns a file-like object wrapped in io.TextIOWrapper. You can find more details about the implementation of open in PEP-3116.\nThe csv.DictReader callable can consume this transformed file-like object without any further modifications. Since we aren\u0026rsquo;t calling f.read() anymore, no overzealous content buffering is going on here and we can lazily ask for new rows from the reader object as we sequentially process them.\nFurther reading How to use python csv.DictReader with a binary file? ","permalink":"https://rednafi.com/python/outage-caused-by-eager-loading-file/","summary":"\u003cp\u003ePython makes it freakishly easy to load the whole content of any file into memory and\nprocess it afterward. This is one of the first things that\u0026rsquo;s taught to people who are new to\nthe language. While the following snippet might be frowned upon by many, it\u0026rsquo;s definitely not\nuncommon:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"nb\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;foo.csv\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;r\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Load whole content of the file as a string in memory.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ef_content\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eread\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# ...do your processing here.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"o\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAdopting this pattern as the default way of handling files isn\u0026rsquo;t the most terrible thing in\nthe world for sure. Also, this is often the preferred way of dealing with image files or\nblobs. However, overzealously loading file content is only okay as long as the file size is\nsmaller than the volatile memory of the working system.\u003c/p\u003e","title":"Dissecting an outage caused by eager-loading file content"},{"content":"After reading Simon Willison\u0026rsquo;s amazing piece on how he builds a feature, I wanted to adopt some of the good practices and incorporate them into my own workflow. One of the highlights of that post was how to kick off a feature work. The process roughly goes like this:\nOpening a new GitHub issue for the feature in the corresponding repository. Adding a rough description of the feature to the issue. Creating a feature branch off of main/master/trunk. If the feature is trivial or just a doc update, this step can be skipped. Referring to the issue in every commit message as you start working on the feature: Appending #refs \u0026lt;issue-number\u0026gt; to every commit message. This will attach the commit to the concerning issue on the GitHub UI. Appending #closes \u0026lt;issue-number\u0026gt; to the final commit message when the feature is complete. If you need to refer to an issue after it\u0026rsquo;s closed, you can still do that by appending #refs \u0026lt;issue-number\u0026gt; to the commit message. So a commit message should look similar to Feature foo, refs #120 or Update foo, closes #115. The comma (,) before refs/closes is essential here. I like to enforce it. This pattern also works for bugfixes without any changes. Here\u0026rsquo;s an example issue that shows the workflow in action. Plus, I follow a similar pattern to write the blogs on this site as well. This is what a feature issue might look like on GitHub:\nWhile I\u0026rsquo;m quite happy with how the process is working for me, often time, I get careless and push commits without a reference to any issue. This pollutes the Git history and breaks my streak of maintaining good hygiene. So, I was looking for a way to make sure that the CI fails and reprimands me whenever I\u0026rsquo;m not following the process correctly. It\u0026rsquo;s just one less thing to worry about.\nI\u0026rsquo;ve decided to use GitHub Actions to audit the conformity of the commit messages. The CI pipeline is orchestrated as follows:\nAfter every push and pull-request, the audit-commits job in an audit.yml workflow file will verify the conformity of the commit messages. This job runs a regex pattern against every commit message and fails with exit code 1 if the message doesn\u0026rsquo;t respect the expected format. If the audit-commits job passes successfully, only then the primary jobs in the ci.yml workflow will execute. The entire pipeline will fail and the primary CI workflow won\u0026rsquo;t be triggered at all if the audit-commit job fails at any point. On GitHub, you\u0026rsquo;re expected to place your workflow files in the .github/workflows directory. If you inspect this blog\u0026rsquo;s workflows folder, you\u0026rsquo;ll see this pattern in action. Here, the directory has three workflow files:\n.github/workflows ├── audit.yml ├── automerge.yml └── ci.yml The automerge.yml file automatically merges a pull-request when the primary CI jobs pass. I wrote about it in more detail in another post about automerging Dependabot PRs. We\u0026rsquo;ll ignore the automerge.yml file for now. Here, the audit file runs after every push and pull-request and verifies the structure of the commit message. I picked a generic name like audit.yml instead of a more specific one like audit-commit.yml because in the future if I want to add another check, I can easily extend this file without renaming it. Here\u0026rsquo;s the unabridged content of the audit.yml file:\n# .github/workflows/audit.yml # Auditing commit structure. name: Audit on: workflow_call: jobs: audit-commits: runs-on: ubuntu-latest if: ${{ github.actor != \u0026#39;dependabot[bot]\u0026#39; }} steps: - name: \u0026#34;Check commit message format\u0026#34; shell: bash run: | set -euo pipefail # Get the commit payload from GH Actions push event. # See GitHub\u0026#39;s webhook-events docs for details commits=\u0026#39;${{ toJSON(github.event.commits) }}\u0026#39; # Exit with 0 if no new commit is found. if [[ $commits =~ \u0026#34;null\u0026#34; ]]; then echo \u0026#34;No commit found. Exiting...\u0026#34; exit 0 fi # Get the unique messages from the commits event. parsed=$(echo -n \u0026#34;$commits\u0026#34; | jq -r \u0026#34;.[].message\u0026#34; | sort -u) mtch=\u0026#39;(, refs|, closes) #[0-9]+\u0026#39; echo \u0026#34;$parsed\u0026#34; | while IFS= read -r raw_line; do line=$(echo \u0026#34;$raw_line\u0026#34; | tr -d \u0026#34;\\r\\n\u0026#34;) # Ignore empty lines. if [[ -z \u0026#34;$line\u0026#34; ]]; then continue # Check for \u0026#39;refs #N\u0026#39; or \u0026#39;closes #N\u0026#39;. Else error. elif [[ \u0026#34;$line\u0026#34; =~ $mtch ]]; then echo \u0026#34;Commit message: $line ✅\u0026#34; else echo \u0026#34;Commit message: $line ❌\u0026#34; echo -n \u0026#34;Commit message must contain \u0026#34; echo -n \u0026#34;\u0026#39;refs #issue_number\u0026#39; or \u0026#39;closes #issue_number\u0026#39;.\u0026#34; exit 1 fi done I\u0026rsquo;ve defined this workflow as a reusable one. A reusable workflow can be called like a function with parameters from another workflow. The workflow_call node the audit.yml file makes it a reusable one and you can define additional parameters in this section if you need to do so. However, in this particular case, I don\u0026rsquo;t need to pass any parameters while calling the audit.yml workflow from the ci.yml workflow. You can find more details on how to define reusable workflows in the docs.\nIn the jobs section of the audit.yml file, we define a single audit-commits job that runs a bash script against every incoming commit message and verifies its structure. The commit messages can be accessed from the '${{ toJSON(github.event.commits) }}' context variable. Then the script loops over every commit message and verifies the structure. It\u0026rsquo;ll terminate the job with exit code 1 if the incoming message doesn\u0026rsquo;t match the expected structure. Otherwise, the script will gracefully terminate the job with exit code 0.\nIn the main ci.yml file the audit.yml workflow is called like this:\n... jobs: audit: uses: rednafi/reflections/.github/workflows/audit.yml@master ... The ci.yml file roughly looks like this:\nname: CI on: push: pull_request: # Everyday at 0:37 UTC. schedule: - cron: \u0026#34;37 0 * * *\u0026#34; # Cancel any running workflow if the CI gets triggered again. concurrency: group: ${{ github.head_ref || github.run_id }} cancel-in-progress: true jobs: audit: uses: rednafi/reflections/.github/workflows/audit.yml@master build: needs: [\u0026#34;audit\u0026#34;] runs-on: ubuntu-latest steps: ... test: needs: [\u0026#34;build\u0026#34;] runs-on: ubuntu-latest steps: ... deploy: needs: [\u0026#34;deploy\u0026#34;] runs-on: ubuntu-latest steps: ... Here the needs: [\u0026quot;audit\u0026quot;] node in the build section ensures that the build will only trigger if the audit job passes successfully. Otherwise, none of the build, test, or deploy jobs will run and the CI will fail with a non-zero exit code. Here\u0026rsquo;s the fully working ci.yml file.\nNotes GitHub Actions terminology can be confusing.\nA workflow is a separate file that contains one or more jobs. A job is a set of steps in a workflow that executes on the same runner. A runner is a server that runs your workflows when they\u0026rsquo;re triggered. Each runner can run a single job at a time. A reusable workflow can be called from another workflow file. The docs have more information on understanding GitHub Actions.\n","permalink":"https://rednafi.com/misc/audit-commit-messages-on-github/","summary":"\u003cp\u003eAfter reading Simon Willison\u0026rsquo;s \u003ca href=\"https://simonwillison.net/2022/Jan/12/how-i-build-a-feature/\"\u003eamazing piece on how he builds a feature\u003c/a\u003e, I wanted to adopt\nsome of the good practices and incorporate them into my own workflow. One of the highlights\nof that post was how to kick off a feature work. The process roughly goes like this:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpening a new GitHub issue for the feature in the corresponding repository.\u003c/li\u003e\n\u003cli\u003eAdding a rough description of the feature to the issue.\u003c/li\u003e\n\u003cli\u003eCreating a feature branch off of \u003ccode\u003emain/master/trunk\u003c/code\u003e. If the feature is trivial or just a\ndoc update, this step can be skipped.\u003c/li\u003e\n\u003cli\u003eReferring to the issue in every commit message as you start working on the feature:\n\u003cul\u003e\n\u003cli\u003eAppending \u003ccode\u003e#refs \u0026lt;issue-number\u0026gt;\u003c/code\u003e to every commit message. This will attach the commit\nto the concerning issue on the GitHub UI.\u003c/li\u003e\n\u003cli\u003eAppending \u003ccode\u003e#closes \u0026lt;issue-number\u0026gt;\u003c/code\u003e to the final commit message when the feature is\ncomplete.\u003c/li\u003e\n\u003cli\u003eIf you need to refer to an issue after it\u0026rsquo;s closed, you can still do that by appending\n\u003ccode\u003e#refs \u0026lt;issue-number\u0026gt;\u003c/code\u003e to the commit message. So a commit message should look similar\nto \u003ccode\u003eFeature foo, refs #120\u003c/code\u003e or \u003ccode\u003eUpdate foo, closes #115\u003c/code\u003e. The comma (\u003ccode\u003e,\u003c/code\u003e) before\n\u003ccode\u003erefs/closes\u003c/code\u003e is essential here. I like to enforce it.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis pattern also works for bugfixes without any changes. Here\u0026rsquo;s an \u003ca href=\"https://github.com/rednafi/reflections/issues/170\"\u003eexample issue\u003c/a\u003e that\nshows the workflow in action. Plus, I follow a similar pattern to write the blogs on this\nsite as well. This is what a feature issue might look like on GitHub:\u003c/p\u003e","title":"Auditing commit messages on GitHub"},{"content":"My grug brain can never remember the correct semantics of quoting commands and variables in a UNIX shell environment. Every time I work with a shell script or run some commands in a Docker compose file, I\u0026rsquo;ve to look up how to quote things properly to stop my ivory tower from crashing down. So, I thought I\u0026rsquo;d list out some of the most common rules that I usually look up all the time.\nI mostly work with bash; so that\u0026rsquo;s what I\u0026rsquo;ll focus on. However, the rules should be similar for any POSIX compliant shell.\nSingle quotes vs double quotes vs backticks Use single quotes when you don\u0026rsquo;t want your shell to expand variables. For example:\necho \u0026#39;$HOST\u0026#39; This prints:\n\u0026#39;$HOST\u0026#39; In the previous snippet, the single quotes ensure that the value of the HOST variable doesn\u0026rsquo;t get expanded by the shell and instead the literal name of the variable is used. On the contrary, your shell will evaulate the variable if you use double quotes here:\necho \u0026#34;$HOST\u0026#34; xps In this case, the command prints the name of my host machine. Lastly, a backtick pair is used to open a subshell and run some command. The following command allows you to check out to the HEAD-1th commit in Git:\ngit checkout `git rev-parse --short HEAD~1` In the above command, first, the command within the backtick runs in a subshell and then returns the result to the main shell. The git checkout part of the command in the main shell then uses the output value of the git rev-parse --short HEAD~1 sub-command to carry out the intended action.\nWhile this works, `...` is the legacy syntax for command substitution, required by only the very oldest of non-POSIX-compatible Bourne shells. A better alternative is to use the $(...) syntax.\ngit checkout $(git rev-parse --short HEAD~1) When to quote variables Quote if the variable can either be empty or contain any whitespace or special characters like spaces, backslashs or wildcards. Not quoting strings with spaces often leads to the shell breaking apart a single argument into many. Consider this command:\nexport x=some filename echo $x This will print:\nsome Ideally, this should\u0026rsquo;ve returned some filename. You can fix this by quoting the value:\nexport x=\u0026#34;some filename\u0026#34; echo $x some filename In the shell environment, the value of a variable is delimited by space. So if the value of your variable contains a space, it won\u0026rsquo;t work correctly unless you quote it properly. This can also happen while accepting a value from a user and assigning it to a variable. For example:\nread -p \u0026#34;Enter the name of a file: \u0026#34; file; cat $file If the user provides a file name that contains a space or any special character like *, ? or /, the command above will behave unexpectedly. To ensure that the cat is applied on a single file, wrap the file variable with double quotes.\nread -p \u0026#34;Enter the name of a file: \u0026#34; file; cat \u0026#34;$file\u0026#34; Instead of double quotes, if you wrap the variable with single quotes, the command will try to apply cat on a file that\u0026rsquo;s literally named $file which is most likely not what you want.\n","permalink":"https://rednafi.com/misc/to-quote-or-not-to-quote/","summary":"\u003cp\u003eMy \u003ca href=\"https://grugbrain.dev/\"\u003egrug brain\u003c/a\u003e can never remember the correct semantics of quoting commands and variables\nin a UNIX shell environment. Every time I work with a shell script or run some commands in a\nDocker compose file, I\u0026rsquo;ve to look up how to quote things properly to stop my ivory tower\nfrom crashing down. So, I thought I\u0026rsquo;d list out some of the most common rules that I usually\nlook up all the time.\u003c/p\u003e","title":"To quote or not to quote"},{"content":"TIL that returning a value from a function in bash doesn\u0026rsquo;t do what I thought it does. Whenever you call a function that\u0026rsquo;s returning some value, instead of giving you the value, Bash sets the return value of the callee as the status code of the calling command. Consider this example:\n#!/usr/bin/bash # script.sh return_42() { return 42 } # Call the function and set the return value to a variable. value=$return_42 # Print the return value. echo $value I was expecting this to print out 42 but instead it doesn\u0026rsquo;t print anything to the console. Turns out, a shell function doesn\u0026rsquo;t return the value when it encounters the return keyword. Rather, it stops the execution of the function and sets the status code of the last command in the function as the value that the function returns.\nTo test it out, you can print out the status code of the last command when a script exits with echo $?. Here\u0026rsquo;s the same snippet from the previous section where the last line is the command that calls the return_42 function:\n#!/usr/bin/bash # script.sh return_42() { return 42 } # Call the function. return_42 Run the snippet and print the exit code of the last line of the script with the following command:\n./script.sh; echo $? This prints out:\n42 Status code evaluation pattern Here\u0026rsquo;s one pattern that you can use whenever you need to return a value from a shell function. In the following snippet, I\u0026rsquo;m evaluating whether a number provided by the user is a prime or not and printing out a message accordingly:\n#!/usr/bin/bash # script.sh # Check whether a number is prime or not. is_prime(){ factor_count=$(factor $1 | wc -w) if [[ $factor_count -eq 2 ]]; then return 0 # Sets the status code to 0. else return 1 # Any non-zero value will work here. fi } # Call the function. is_prime $1 # Inspect the status code. status=$? # Print message according to the status code. if [[ $status -eq 0 ]]; then echo \u0026#34;$1 is prime.\u0026#34; else echo \u0026#34;$1 is not prime.\u0026#34; fi Since the returned values are treated as status codes where 0 is used to denote no error and a non-zero value represents an error, you\u0026rsquo;ll need to return 0 as a truthy value and 1 as a falsy value. While this works, returning 0 to denote a truthy value is the opposite of what you\u0026rsquo;d usually do in other programming languages and can confuse someone who might not be familiar with shell quirks. If you only need to return a boolean value from a function, here\u0026rsquo;s a better pattern:\n#!/usr/bin/bash # script.sh # Check whether a number is prime or not. is_prime(){ factor_count=$(factor $1 | wc -w) if [[ $factor_count -eq 2 ]]; then true else false fi } # Call the function. is_prime $1 # Inspect the status code. status=$? # Print message according to the status code. if [[ $status -eq 0 ]]; then echo \u0026#34;$1 is prime.\u0026#34; else echo \u0026#34;$1 is not prime.\u0026#34; fi In this snippet, notice how the is_prime function doesn\u0026rsquo;t explicitly return anything. Instead, it just adds the true or false expression to the end of the return path accordingly. This implicitly sets the status code to 0 when the input number is a prime and to 1 when it\u0026rsquo;s not. The rest of the status checking works the same as in the previous script.\nThe second pattern won\u0026rsquo;t work if you need to set the status code to something other than 0 or 1. In that case you can resort the first pattern without confusing anyone.\nFurther reading Returning a boolean from a Bash function ","permalink":"https://rednafi.com/misc/return-values-from-a-shell-function/","summary":"\u003cp\u003eTIL that returning a value from a function in bash doesn\u0026rsquo;t do what I thought it does.\nWhenever you call a function that\u0026rsquo;s returning some value, instead of giving you the value,\nBash sets the return value of the callee as the status code of the calling command. Consider\nthis example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cp\"\u003e#!/usr/bin/bash\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# script.sh\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ereturn_42\u003cspan class=\"o\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"m\"\u003e42\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Call the function and set the return value to a variable.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nv\"\u003evalue\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"nv\"\u003e$return_42\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Print the return value.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eecho\u003c/span\u003e \u003cspan class=\"nv\"\u003e$value\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eI was expecting this to print out \u003ccode\u003e42\u003c/code\u003e but instead it doesn\u0026rsquo;t print anything to the console.\nTurns out, a shell function doesn\u0026rsquo;t return the value when it encounters the \u003ccode\u003ereturn\u003c/code\u003e\nkeyword. Rather, it stops the execution of the function and sets the status code of the last\ncommand in the function as the value that the function returns.\u003c/p\u003e","title":"Returning values from a shell function"},{"content":"While working with GitHub webhooks, I discovered a common webhook security pattern a receiver can adopt to verify that the incoming webhooks are indeed arriving from GitHub; not from some miscreant trying to carry out a man-in-the-middle attack. After some amount of digging, I found that it\u0026rsquo;s quite a common practice that many other webhook services employ as well. Also, check out how Sentry handles webhook verification.\nMoreover, GitHub\u0026rsquo;s documentation demonstrates the pattern in Ruby. So I thought it\u0026rsquo;d be a good idea to translate that into Python in a more platform-agnostic manner. The core idea of the pattern goes as follows:\nThe webhook sender will hash the JSONified webhook payload with a well-known hashing algorithm like MD5, SHA-1, or SHA-256. A secret token known to the receiver will be used to sign the calculated hash of the payload.\nThe sender will include the payload hash digest prefixed by the name of the hash algorithm to the header of the webhook request. For example, the GitHub webhook\u0026rsquo;s request header has a key like the following. Notice how the digest is prefixed with the name of the algorithm sha256:\nX-Hub-Signature-256=\\ sha-256=e863e1f6370b60981bbbcbc2da3313321e65eaaac36f9d1262af415965df9320 The webhook receiver is then expected to hash the received JSON payload with the same algorithm found in the prefix of the header and sign with the common secret token known to both the sender and the receiver. Afterward, the receiver compares the calculated hash with the incoming hash in the request header. If the two digests match, that ensures that the payload hasn\u0026rsquo;t been tampered with. Otherwise, the receiver should reject the incoming payload. This provides a second layer of protection over the usual authentication that the receiver might have in place. To demonstrate the workflow, here\u0026rsquo;s an example of how the webhook sender might be implemented:\n# sender.py from __future__ import annotations import hashlib import json from http import HTTPStatus import httpx from starlette.applications import Starlette from starlette.requests import Request from starlette.responses import JSONResponse from starlette.routing import Route async def send_webhook(request: Request) -\u0026gt; JSONResponse: # Get the request body as bytes. raw_body = await request.body() # Disallow empty body. if not raw_body: return JSONResponse( {\u0026#34;error\u0026#34;: \u0026#34;Empty body\u0026#34;}, status_code=HTTPStatus.BAD_REQUEST, ) # Check that the request body is a valid JSON payload. try: body = json.loads(raw_body) except json.JSONDecodeError: return JSONResponse( {\u0026#34;error\u0026#34;: \u0026#34;Invalid JSON body\u0026#34;}, status_code=HTTPStatus.BAD_REQUEST, ) # Hash the body and sign it with a secret. x_payload_signature = hashlib.sha256(raw_body) x_payload_signature.update(b\u0026#34;some-secret\u0026#34;) x_payload_signature = x_payload_signature.hexdigest() # Send the webhook. async with httpx.AsyncClient() as client: response = await client.post( \u0026#34;http://localhost:6000/receive-webhook\u0026#34;, json=body, headers={ \u0026#34;X-Payload-Signature-256\u0026#34;: f\u0026#34;sha256={x_payload_signature}\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, }, ) if response.status_code != HTTPStatus.ACCEPTED: return JSONResponse( {\u0026#34;error\u0026#34;: \u0026#34;Could not sent webhook\u0026#34;}, status_code=HTTPStatus.BAD_REQUEST, ) return JSONResponse( { \u0026#34;message\u0026#34;: \u0026#34;Webhook sent\u0026#34;, \u0026#34;response_payload\u0026#34;: response.json(), }, status_code=HTTPStatus.OK, ) app = Starlette( debug=True, routes=[ Route(\u0026#34;/send-webhook\u0026#34;, send_webhook, methods=[\u0026#34;POST\u0026#34;]), ], ) Here, I\u0026rsquo;ve implemented a simple POST API that:\nAccepts a payload from the user. Hashes the payload with sha-256 algorithm and signs it with a some-secret token. Adds the digest to the request header to the receiver. The header has a key called X-Payload-Signature-256 that contains the prefixed payload digest: X-Payload-Signature-256: \\ sha-256=e863e1f6370b60981bbbcbc2da3313321e65eaaac36f9d1262af415965df9320 After hashing, the sender sends the payload to the receiver via HTTP POST request. Here, I\u0026rsquo;m using HTTPx to send the request to the receiver. For demonstration purposes, I\u0026rsquo;m assuming that the receiver endpoint is localhost:6000/receive-webhook. The receiver will:\nAccept the incoming request from the sender. Parse the header and store the value of X-Payload-Signature-256. Calculate the hash value of the incoming payload in the same manner as the sender. Sign the payload with the common secret that\u0026rsquo;s known to both parties. Compare the newly calculated signed-hash with the digest value of the X-Payload-Signature-256 attribute. Only accept and process the payload if the incoming and the computed hashes match. Here\u0026rsquo;s how you can implement the receiver:\n# receiver.py from __future__ import annotations import hashlib import json import secrets from http import HTTPStatus from starlette.applications import Starlette from starlette.requests import Request from starlette.responses import JSONResponse from starlette.routing import Route async def receive_webhook(request: Request) -\u0026gt; JSONResponse: # Get the payload signature from the request headers. x_payload_signature_256 = request.headers.get( \u0026#34;X-Payload-Signature-256\u0026#34; ) # Disallow empty signature. if x_payload_signature_256 is None: return JSONResponse( {\u0026#34;error\u0026#34;: \u0026#34;Missing X-Payload-Signature header\u0026#34;}, status_code=HTTPStatus.BAD_REQUEST, ) # Check that the signature is valid. if not x_payload_signature_256.startswith(\u0026#34;sha256=\u0026#34;): return JSONResponse( {\u0026#34;error\u0026#34;: \u0026#34;Invalid X-Payload-Signature header\u0026#34;}, status_code=HTTPStatus.BAD_REQUEST, ) # Get x_payload_signature_256 without the \u0026#34;sha256=\u0026#34; prefix. x_payload_signature = x_payload_signature_256.removeprefix(\u0026#34;sha256=\u0026#34;) raw_body = await request.body() # Disallow empty body. if not raw_body: return JSONResponse( {\u0026#34;error\u0026#34;: \u0026#34;Empty body\u0026#34;}, status_code=HTTPStatus.BAD_REQUEST, ) # Check that the request body is a valid JSON payload. try: body = json.loads(raw_body) except json.JSONDecodeError: return JSONResponse( {\u0026#34;error\u0026#34;: \u0026#34;Invalid JSON body\u0026#34;}, status_code=HTTPStatus.BAD_REQUEST, ) # Hash the incoming body with the secret. expected_signature = hashlib.sha256(raw_body) expected_signature.update(b\u0026#34;some-secret\u0026#34;) expected_signature = expected_signature.hexdigest() # Compare the expected signature with the incoming signature. if ( secrets.compare_digest(x_payload_signature, expected_signature) is False ): return JSONResponse( {\u0026#34;error\u0026#34;: \u0026#34;Invalid signature\u0026#34;}, status_code=HTTPStatus.UNAUTHORIZED, ) return JSONResponse( {\u0026#34;message\u0026#34;: \u0026#34;Webhook accepted\u0026#34;}, status_code=HTTPStatus.ACCEPTED, ) app = Starlette( debug=True, routes=[ Route(\u0026#34;/receive-webhook\u0026#34;, receive_webhook, methods=[\u0026#34;POST\u0026#34;]), ], ) In the receiver, instead of using plain string comparison to compare the payload hashes, leverage secrets.compare_digest to mitigate the possibility of timing attacks.\nTo test the end-to-end workflow, you\u0026rsquo;ll need to pip install httpx and uvicorn. Then on your console, you can run the two scripts in the background with the following command:\nnohup uvicorn sender:app --reload --port 5000 \u0026gt; /dev/null \\ \u0026amp; nohup uvicorn receiver:app --reload --port 6000 \u0026gt; /dev/null \u0026amp; This will spin up two uvicorn servers in the background where the sender and the receiver can be accessed via ports 5000 and 6000 respectively. Now if you make a request to the sender service, you\u0026rsquo;ll see that the sender sends the webhook payload to the receiver service and returns an HTTP 200 code only if the receiver has been able to verify the signed-hash of the payload:\ncurl -si POST http://localhost:5000/send-webhook -d \u0026#39;{\u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34;}\u0026#39; This will return:\nHTTP/1.1 200 OK date: Tue, 20 Sep 2022 06:31:07 GMT server: uvicorn content-length: 76 content-type: application/json {\u0026#34;message\u0026#34;:\u0026#34;Webhook sent\u0026#34;,\u0026#34;response_payload\u0026#34;:{\u0026#34;message\u0026#34;:\u0026#34;Webhook accepted\u0026#34;}} The reciver will return a HTTP 400 error code if it can\u0026rsquo;t verify the payload. Once you\u0026rsquo;re done, kill the running servers with sudo pkill uvicorn command.\n","permalink":"https://rednafi.com/python/verify-webhook-origin/","summary":"\u003cp\u003eWhile working with GitHub webhooks, I discovered a common \u003ca href=\"https://docs.github.com/en/developers/webhooks-and-events/webhooks/securing-your-webhooks\"\u003ewebhook security pattern\u003c/a\u003e a\nreceiver can adopt to verify that the incoming webhooks are indeed arriving from GitHub; not\nfrom some miscreant trying to carry out a man-in-the-middle attack. After some amount of\ndigging, I found that it\u0026rsquo;s quite a common practice that many other webhook services employ\nas well. Also, check out how \u003ca href=\"https://docs.sentry.io/product/integrations/integration-platform/webhooks/#sentry-hook-resource\"\u003eSentry handles webhook verification\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eMoreover, GitHub\u0026rsquo;s documentation demonstrates the pattern in Ruby. So I thought it\u0026rsquo;d be a\ngood idea to translate that into Python in a more platform-agnostic manner. The core idea of\nthe pattern goes as follows:\u003c/p\u003e","title":"Verifying webhook origin via payload hash signing"},{"content":"While going through the documentation of Python\u0026rsquo;s sqlite3 module, I noticed that it\u0026rsquo;s quite API-driven, where different parts of the module are explained in a prescriptive manner. I, however, learn better from examples, recipes, and narratives. Although a few good recipes already exist in the docs, I thought I\u0026rsquo;d also enlist some of the examples I tried out while grokking them.\nExecuting individual statements To execute individual statements, you\u0026rsquo;ll need to use the cursor_obj.execute(statement) primitive.\n# src.py import sqlite3 conn = sqlite3.connect(\u0026#34;:memory:\u0026#34;) c = conn.cursor() with conn: c.execute( \u0026#34;\u0026#34;\u0026#34; create table if not exists stat (id integer primary key, cat text, score real); \u0026#34;\u0026#34;\u0026#34; ) c.execute(\u0026#34;\u0026#34;\u0026#34;insert into stat (cat, score) values (\u0026#39;a\u0026#39;, 1.0);\u0026#34;\u0026#34;\u0026#34;) c.execute(\u0026#34;\u0026#34;\u0026#34;insert into stat (cat, score) values (\u0026#39;b\u0026#39;, 2.0);\u0026#34;\u0026#34;\u0026#34;) result = c.execute(\u0026#34;\u0026#34;\u0026#34;select * from stat;\u0026#34;\u0026#34;\u0026#34;).fetchall() print(result) [(1, \u0026#39;a\u0026#39;, 1.0), (2, \u0026#39;b\u0026#39;, 2.0)] Executing batch statements You can bundle up multiple statements and execute them in a single go with the cursor_obj.executemany(template_statement, (data, ...)) API.\n# src.py import sqlite3 conn = sqlite3.connect(\u0026#34;:memory:\u0026#34;) c = conn.cursor() with conn: c.execute( \u0026#34;\u0026#34;\u0026#34; create table if not exists stat (id integer primary key, cat text, score real); \u0026#34;\u0026#34;\u0026#34; ) # Data needs to be passed as an iterable of tuples. data = ( (\u0026#34;a\u0026#34;, 1.0), (\u0026#34;b\u0026#34;, 2.0), (\u0026#34;c\u0026#34;, 3.0), ) c.executemany(\u0026#34;insert into stat (cat, score) values (?, ?);\u0026#34;, data) result = c.execute(\u0026#34;\u0026#34;\u0026#34;select * from stat;\u0026#34;\u0026#34;\u0026#34;).fetchall() print(result) [(1, \u0026#39;a\u0026#39;, 1.0), (2, \u0026#39;b\u0026#39;, 2.0), (3, \u0026#39;c\u0026#39;, 3.0)] Applying user-defined callbacks You can define and apply arbitrary Python callbacks to different data points in an SQLite table. There are two types of callbacks that you can apply:\nScalar function: A scalar function returns one value per invocation; in most cases, you can think of this as returning one value per row.\nAggregate function: In contrast, an aggregate function returns one value per group of rows.\nApplying user-defined scalar functions In the following example, I\u0026rsquo;ve created a table called users with two text type columns — username and password. Here, we define a transformation scalar function named sha256 that applies sha256 hashing to all the elements of the password column. The function is then registered via the connection_obj.create_function(func_name, narg, func) API.\n# src.py import sqlite3 import hashlib conn = sqlite3.connect(\u0026#34;:memory:\u0026#34;) c = conn.cursor() def sha256(t: str) -\u0026gt; str: return hashlib.sha256( t.encode(\u0026#34;utf-8\u0026#34;), usedforsecurity=True, ).hexdigest() # Register the scalar function. conn.create_function(\u0026#34;sha256\u0026#34;, 1, sha256) with conn: c.execute( \u0026#34;\u0026#34;\u0026#34; create table if not exists users ( username text, password text ); \u0026#34;\u0026#34;\u0026#34; ) c.execute( \u0026#34;insert into users values (?, sha256(?));\u0026#34;, (\u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;), ) c.execute( \u0026#34;insert into users values (?, sha256(?));\u0026#34;, (\u0026#34;user\u0026#34;, \u0026#34;otherpass\u0026#34;), ) result = c.execute(\u0026#34;select * from users;\u0026#34;).fetchall() print(result) [ ( \u0026#39;admin\u0026#39;, \u0026#39;5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8\u0026#39;), ( \u0026#39;user\u0026#39;, \u0026#39;0da86a02c6944c679c5a7f06418bfde6bddb445de708639a3131af3682b34108\u0026#39; ) ] Applying user-defined aggregate functions Aggregate functions are defined as classes and then registered with the connection_obj.create_aggregate(func_name, narg, aggregate_class) API. In the example below, I\u0026rsquo;ve created a table called series with a single integer type column val. To define an aggregate function, we\u0026rsquo;ll need to write a class with two methods — step and finalize where step will return the value of an intermediary progression step and finalize will return the final result. Below, you can see that the aggregate function returns a single value in the output.\n# src.py import sqlite3 import hashlib conn = sqlite3.connect(\u0026#34;:memory:\u0026#34;) c = conn.cursor() class Mult: def __init__(self): self._result = 1 def step(self, value): self._result *= value def finalize(self): return self._result # Register the aggregate class. conn.create_aggregate(\u0026#34;mult\u0026#34;, 1, Mult) with conn: c.execute( \u0026#34;\u0026#34;\u0026#34; create table if not exists series ( val integer ); \u0026#34;\u0026#34;\u0026#34; ) c.execute(\u0026#34;insert into series (val) values (?);\u0026#34;, (2,)) c.execute(\u0026#34;insert into series (val) values (?);\u0026#34;, (3,)) result = c.execute(\u0026#34;select mult(val) from series;\u0026#34;).fetchall() print(result) [(6,)] Printing traceback when a user-defined callback raises an error By default, sqlite3 will suppress the traceback of any error raised from an user-defined function. However, you can turn on the traceback option as follows:\n# src import sqlite3 sqlite3.enable_callback_tracebacks(True) ... Transforming types Conventionally, Python sqlite3 documentation uses the term adaptation to refer to the transformation that changes Python types to SQLite types and conversion to refer to the change in the reverse direction.\nAdapting Python types to SQLite types To transform Python types to native SQLite types, you\u0026rsquo;ll need to define a transformation callback that\u0026rsquo;ll carry out the task. Then the callback will need to be registered with the sqlite3.register_adapter(type, adapter_callback) API.\nHere, I\u0026rsquo;ve created an in-memory table called colors with a single text type column name that refers to the name of the color. Then I register the lambda color: color.value anonymous function that serializes an enum value to a text value. This allows me to pass an enum member directly into the cursor_obj.execute method.\n# src.py import enum import sqlite3 conn = sqlite3.connect(\u0026#34;:memory:\u0026#34;) c = conn.cursor() class Color(enum.Enum): RED = \u0026#34;red\u0026#34; GREEN = \u0026#34;green\u0026#34; BLUE = \u0026#34;blue\u0026#34; # Register an adapter to transform a Python type to an SQLite type. sqlite3.register_adapter(Color, lambda color: color.value) with conn: c.execute( \u0026#34;\u0026#34;\u0026#34; create table if not exists colors ( name integer ); \u0026#34;\u0026#34;\u0026#34; ) c.execute(\u0026#34;insert into colors (name) values (?);\u0026#34;, (Color.RED,)) c.execute(\u0026#34;insert into colors (name) values (?);\u0026#34;, (Color.GREEN,)) result = c.execute(\u0026#34;select name from colors;\u0026#34;).fetchall() print(result) [(\u0026#39;red\u0026#39;,), (\u0026#39;green\u0026#39;,)] Converting SQLite types to Python types Converting SQLite types to Python types works similarly to the previous section. Here, as well, I\u0026rsquo;ve created the same colors table with a single name column as before. But this time, I want to insert string values into the name column and get back native enum objects from that field while performing a get query.\nTo do so, I\u0026rsquo;ve registered a converter function with the sqlite3.register_converter(\u0026quot;sqlite_type_as_a_string\u0026quot;, converter_callback) API. Another point to keep in mind is that you\u0026rsquo;ll have to set detect_type=sqlite3.PARSE_DECLTYPES in the sqlite3.connection method for the adaptation to work. Notice the output of the last select ... statement and you\u0026rsquo;ll see that we\u0026rsquo;re getting enum objects in the returned list.\n# src.py import enum import sqlite3 class Color(enum.Enum): RED = \u0026#34;red\u0026#34; GREEN = \u0026#34;green\u0026#34; BLUE = \u0026#34;blue\u0026#34; color_map = {v.value: v for v in Color.__members__.values()} # Register a convert to convert text to Color enum sqlite3.register_converter( \u0026#34;text\u0026#34;, lambda v: color_map[v.decode(\u0026#34;utf-8\u0026#34;)], ) conn = sqlite3.connect( \u0026#34;:memory:\u0026#34;, detect_types=sqlite3.PARSE_DECLTYPES, # Parse declaration types. ) c = conn.cursor() with conn: c.execute( \u0026#34;\u0026#34;\u0026#34; create table if not exists colors ( name text ); \u0026#34;\u0026#34;\u0026#34; ) c.execute(\u0026#34;insert into colors (name) values (?);\u0026#34;, (\u0026#34;red\u0026#34;,)) c.execute(\u0026#34;insert into colors (name) values (?);\u0026#34;, (\u0026#34;green\u0026#34;,)) c.execute(\u0026#34;insert into colors (name) values (?);\u0026#34;, (\u0026#34;blue\u0026#34;,)) result = c.execute(\u0026#34;select name from colors;\u0026#34;).fetchall() print(result) [ (\u0026lt;Color.RED: \u0026#39;red\u0026#39;\u0026gt;,), (\u0026lt;Color.GREEN: \u0026#39;green\u0026#39;\u0026gt;,), (\u0026lt;Color.BLUE: \u0026#39;blue\u0026#39;\u0026gt;,) ] Using the default adapters and converters The sqlite3 module also employs some default adapters and converters that you can take advantage of without defining and registering custom transformers. For example, SQLite doesn\u0026rsquo;t have any special types to represent a date or timestamp. However, Python sqlite3 allows you to annotate a column with a special type and it\u0026rsquo;ll automatically convert the values of the column to a compatible type of Python object while returning the result of a get query.\nHere, I\u0026rsquo;ve created a table called timekeeper with two columns — d and dt where d expects a date and dt expects a timestamp. So, in the table creation DDL statement, we annotate the columns with date and timestamp types respectively. We\u0026rsquo;ve also turned on column type parsing by setting detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES in the sqlite3.connect method.\nFinally, notice how we\u0026rsquo;re inserting datetime.date and datetime.datetime objects directly into the table. Also, this time, the final select ... statement looks a bit different. We\u0026rsquo;re specifying the expected type in the select ... statement and it\u0026rsquo;s returning native Python objects in the returned list.\n# src.py import datetime import sqlite3 import zoneinfo conn = sqlite3.connect( \u0026#34;:memory:\u0026#34;, detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES, ) with conn: c = conn.cursor() c.execute( \u0026#34;\u0026#34;\u0026#34; create table if not exists timekeeper (id integer primary key, d date, dt timestamp);\u0026#34;\u0026#34;\u0026#34; ) tz = zoneinfo.ZoneInfo(\u0026#34;America/New_York\u0026#34;) dt = datetime.datetime.now(tz) d = dt.date() c.execute( \u0026#34;insert into timekeeper (d, dt) values (?, ?);\u0026#34;, ((d, dt)), ) result = c.execute( \u0026#34;\u0026#34;\u0026#34; select d as \u0026#34;d [date]\u0026#34;, dt as \u0026#34;dt [timestamp]\u0026#34; from timekeeper;\u0026#34;\u0026#34;\u0026#34; ).fetchall() print(result) [ ( datetime.date(2022, 9, 5), datetime.datetime(2022, 9, 5, 14, 59, 52, 867917) ) ] Implementing authorization control Sometimes you need control over what operations are allowed to be run on an SQLite database and what aren\u0026rsquo;t. The connection_obj.set_authorizer(auth_callback) allows you to implement authorization control. Here, auth_callback takes in 5 arguments. From the docs:\nThe 1st argument to the callback signifies what kind of operation is to be authorized. The 2nd and 3rd arguments will be arguments or None depending on the 1st argument. The 4th argument is the name of the database (\u0026ldquo;main\u0026rdquo;, \u0026ldquo;temp\u0026rdquo;, etc.) if applicable. The 5th argument is the name of the inner-most trigger or view that is responsible for the access attempt or None if this access attempt is directly from input SQL code. Please consult the SQLite documentation about the possible values for the first argument and the meaning of the second and third arguments depending on the first one.\nYou can find the list of all the supported SQLite actions. In the following example, I\u0026rsquo;m disallowing create table, create index, drop table, and drop index actions. To deny an action, the auth_callback will have to return sqlite3.SQLITE_DENY and that\u0026rsquo;ll raise an sqlite3.DatabaseError exception whenever a user tries to execute any of the restricted actions. Returning sqlite3.SQLITE_OK from the callback ensures that unfiltered actions can still pass through the guardrail without incurring any errors.\n# src.py import sqlite3 conn = sqlite3.connect(\u0026#34;:memory:\u0026#34;) def authorizer(action, arg1, arg2, dbname, trigger): # Print the params. print(action, arg1, arg2, dbname, trigger) # Disallow these actions. if action in ( sqlite3.SQLITE_CREATE_TABLE, sqlite3.SQLITE_CREATE_INDEX, sqlite3.SQLITE_DROP_TABLE, sqlite3.SQLITE_DROP_INDEX, ): return sqlite3.SQLITE_DENY # Let everything else pass through. return sqlite3.SQLITE_OK c = conn.cursor() with conn: c.execute( \u0026#34;\u0026#34;\u0026#34; create table if not exists colors ( name text ); \u0026#34;\u0026#34;\u0026#34; ) # After creating the table, let\u0026#39;s make sure users can\u0026#39;t perform # certain DDL operations. conn.set_authorizer(authorizer) c.execute(\u0026#34;insert into colors (name) values (?);\u0026#34;, (\u0026#34;red\u0026#34;,)) # This will fail because the authorizer will deny the operation. c.execute(\u0026#34;drop table colors;\u0026#34;) 18 colors None main None 22 BEGIN None None None 9 sqlite_master None main None 11 colors None main None 22 ROLLBACK None None None Traceback (most recent call last): File \u0026#34;/home/rednafi/canvas/personal/reflections/src.py\u0026#34;, line 45, in \u0026lt;module\u0026gt; c.execute(\u0026#34;drop table colors;\u0026#34;) sqlite3.DatabaseError: not authorized Changing the representation of a row The sqlite3 module allows you to change the representation of a database row to your liking. By default, the result of a query comes out as a list of tuples where each tuple represents a single row. However, you can change the representation of database rows in such a way that the result might come out as a list of dictionaries or a list of custom objects.\nVia an arbitrary container object as the row factory You can attach a callback to the connection_obj.row_factory attribute to change how you want to display the rows in a result list. The factory callback takes in two arguments — cursor and row where cursor is a tuple containing some metadata related to a single table record and row is the default representation of a single database row as a tuple.\nIn the following snippet, just like before, I\u0026rsquo;m creating the same colors table with two columns — name and hex. Here, the row_factory function is the factory callback that converts the default row representation from a tuple to a dictionary. We\u0026rsquo;re then registering the row_factory function with the connection_obj.row_factory = row_factory assignment statement. Afterward, the sqlite3 module calls this statement on each record and transforms the representation of the rows. When you run the snippet, you\u0026rsquo;ll see that the result comes out as a list of dictionaries instead of a list of tuples.\n# src.py import sqlite3 conn = sqlite3.connect(\u0026#34;:memory:\u0026#34;) # Using a dictionary to represent a row. def row_factory(cursor, row): # cursor.description: # (name, type_code, display_size, # internal_size, precision, scale, null_ok) # row: (value, value, ...) return { col[0]: row[idx] for idx, col in enumerate( cursor.description, ) } conn.row_factory = row_factory c = conn.cursor() with conn: c.execute( \u0026#34;\u0026#34;\u0026#34; create table if not exists colors ( name text, hex text ); \u0026#34;\u0026#34;\u0026#34; ) c.execute( \u0026#34;insert into colors (name, hex) values (?, ?);\u0026#34;, (\u0026#34;red\u0026#34;, \u0026#34;#ff0000\u0026#34;), ) c.execute( \u0026#34;insert into colors (name, hex) values (?, ?);\u0026#34;, (\u0026#34;green\u0026#34;, \u0026#34;#00ff00\u0026#34;), ) c.execute( \u0026#34;insert into colors (name, hex) values (?, ?);\u0026#34;, (\u0026#34;blue\u0026#34;, \u0026#34;#0000ff\u0026#34;), ) result = c.execute(\u0026#34;select * from colors;\u0026#34;).fetchall() print(result) [ {\u0026#39;name\u0026#39;: \u0026#39;red\u0026#39;, \u0026#39;hex\u0026#39;: \u0026#39;#ff0000\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;green\u0026#39;, \u0026#39;hex\u0026#39;: \u0026#39;#00ff00\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;blue\u0026#39;, \u0026#39;hex\u0026#39;: \u0026#39;#0000ff\u0026#39;} ] Via a specialized Row object as the row factory Instead of rolling with your own custom row factory, you can also take advantage of the highly optimized sqlite3.Row object. From the docs:\nA Row instance serves as a highly optimized row_factory for Connection objects. It supports iteration, equality testing, len(), and mapping access by column name and index. Two row objects compare equal if have equal columns and equal members.\nIn the following example, I\u0026rsquo;ve reused the script from the previous section and just replaced the custom row factory callback with sqlite3.Row. In the output, you\u0026rsquo;ll see that the Row object not only allows us to access the value of a column by row[column_name] syntax but also let us convert the representation of the final result.\n# src.py import sqlite3 conn = sqlite3.connect(\u0026#34;:memory:\u0026#34;) # Registering a highly optimized \u0026#39;Row\u0026#39; object as the # default row_factory. Row is a map-like object that # allows you to access column values by name. conn.row_factory = sqlite3.Row c = conn.cursor() with conn: c.execute( \u0026#34;\u0026#34;\u0026#34; create table if not exists colors ( name text, hex text ); \u0026#34;\u0026#34;\u0026#34; ) c.executemany( \u0026#34;insert into colors (name, hex) values (?, ?);\u0026#34;, ( (\u0026#34;red\u0026#34;, \u0026#34;#ff0000\u0026#34;), (\u0026#34;green\u0026#34;, \u0026#34;#00ff00\u0026#34;), (\u0026#34;blue\u0026#34;, \u0026#34;#0000ff\u0026#34;), ), ) result = c.execute(\u0026#34;select * from colors;\u0026#34;).fetchall() # Access the values of a row by column name. for row in result: print(row[\u0026#34;name\u0026#34;], row[\u0026#34;hex\u0026#34;]) # Convert the result to a list of dicts. result_dict = [dict(row) for row in result] print(result_dict) red #ff0000 green #00ff00 blue #0000ff [ {\u0026#39;name\u0026#39;: \u0026#39;red\u0026#39;, \u0026#39;hex\u0026#39;: \u0026#39;#ff0000\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;green\u0026#39;, \u0026#39;hex\u0026#39;: \u0026#39;#00ff00\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;blue\u0026#39;, \u0026#39;hex\u0026#39;: \u0026#39;#0000ff\u0026#39;} ] Via text factory If you need to apply a common transformation callback to multiple text columns, the sqlite3 module has a shortcut to do so. You can certainly write an ordinary row factory that\u0026rsquo;ll only transform the text columns but the connection_obj.text_factory attribute enables you to do that in a more elegant fashion. You can set connection_obj.text_factory = row_factory and that\u0026rsquo;ll selectively apply the row_factory callback only to the text columns. In the following example, I\u0026rsquo;m applying an anonymous function to the text columns to translate the color names to English.\n# src.py import sqlite3 conn = sqlite3.connect(\u0026#34;:memory:\u0026#34;) c = conn.cursor() # Apply factory only to text fields. color_map = {\u0026#34;το κόκκινο\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;সবুজ\u0026#34;: \u0026#34;green\u0026#34;} # Translate all the text fields. conn.text_factory = lambda x: color_map.get(x.decode(\u0026#34;utf-8\u0026#34;), x) with conn: c.execute(\u0026#34;create table if not exists colors (name text);\u0026#34;) c.execute(\u0026#34;insert into colors (name) values (?);\u0026#34;, (\u0026#34;κόκκινο\u0026#34;,)) c.execute(\u0026#34;insert into colors (name) values (?);\u0026#34;, (\u0026#34;সবুজ\u0026#34;,)) result = c.execute(\u0026#34;select * from colors;\u0026#34;).fetchall() print(result) [(\u0026#39;red\u0026#39;,), (\u0026#39;green\u0026#39;,)] Creating custom collation Collation defines how the string values in a text column are compared. It also dictates how the data in the column will be ordered when you perform any kind of sort operation. A collation callback can be registered with the connection_obj.create_collation(name, collation_callback) syntax where the name denotes the name of the collation rule and the collation_callback determines how the string comparison should be done. The callback accepts two string values as arguments and returns:\n1 if the first is ordered higher than the second -1 if the first is ordered lower than the second 0 if they are ordered equal Then you can use the collation rules with an order by clause as follows:\nselect * from table_name order by column_name collate collation_name Here\u0026rsquo;s a full example of a collation callback in action:\n# src.py import sqlite3 conn = sqlite3.connect(\u0026#34;:memory:\u0026#34;) c = conn.cursor() def reverse_collate(a, b): return 1 if a \u0026lt; b else -1 if a \u0026gt; b else 0 # Register the collation function. conn.create_collation(\u0026#34;reverse\u0026#34;, reverse_collate) with conn: c.execute(\u0026#34;create table if not exists colors (name text);\u0026#34;) c.executemany( \u0026#34;insert into colors (name) values (?);\u0026#34;, ((\u0026#34;το κόκκινο\u0026#34;,), (\u0026#34;সবুজ\u0026#34;,)), ) result = c.execute( \u0026#34;\u0026#34;\u0026#34;select * from colors order by name collate reverse;\u0026#34;\u0026#34;\u0026#34; ).fetchall() print(result) [(\u0026#39;সবুজ\u0026#39;,), (\u0026#39;το κόκκινο\u0026#39;,)] Registering trace callbacks to introspect running SQL statements During debugging, I often find it helpful to be able to trace all the SQL statements running under a certain connection. This becomes even more useful in a multiprocessing environment where each process opens a new connection to the DB and runs its own sets of SQL queries. We can leverage the connection_obj.set_trace_callback method to trace the statements. The set_trace_callback method accepts a callable that takes a single argument and sqlite3 module passes the currently running statement to the callback every time it invokes it. Notice how the output prints all the statements executed by SQLite behind the scene. This also reveals that cursor_obj.executemany wraps up multiple statements in a transaction and runs them in an atomic manner.\n# src.py import sqlite3 conn = sqlite3.connect(\u0026#34;:memory:\u0026#34;) c = conn.cursor() # Print all the statements executed. def introspect(s): print(s) # Register the trace function. conn.set_trace_callback(introspect) with conn: c.execute(\u0026#34;create table if not exists colors (name text);\u0026#34;) c.executemany( \u0026#34;insert into colors (name) values (?);\u0026#34;, ((\u0026#34;red\u0026#34;,), (\u0026#34;green\u0026#34;,), (\u0026#34;blue\u0026#34;,)), ) result = c.execute(\u0026#34;\u0026#34;\u0026#34;select * from colors\u0026#34;\u0026#34;\u0026#34;).fetchall() print(result) create table if not exists colors (name text); BEGIN insert into colors (name) values (?); insert into colors (name) values (?); insert into colors (name) values (?); select * from colors [(\u0026#39;red\u0026#39;,), (\u0026#39;green\u0026#39;,), (\u0026#39;blue\u0026#39;,)] COMMIT Backing up a database There are a few ways you can back up your database file via Python sqlite3.\nDumping the database iteratively The following snippet creates a table, inserts some data into it, and then, iteratively fetches the database content via the connection_obj.iterdump() API. Afterward, the returned content is written to another database file using the file.write primitive.\nFor demonstration purposes, I\u0026rsquo;m using an in-memory DB and backing that up in another NamedTemporaryFile. This will work the same way with an on-disk DB and on-disk backup file as well. One advantage of this approach is that your data is not loaded into memory at once, rather it\u0026rsquo;s streamed iteratively from the main DB to the backup DB.\n# src.py import sqlite3 import tempfile from contextlib import ExitStack conn = sqlite3.connect(\u0026#34;:memory:\u0026#34;) with ExitStack() as stack: conn = stack.enter_context(conn) dst_file = stack.enter_context(tempfile.NamedTemporaryFile()) c = conn.cursor() c.execute(\u0026#34;create table if not exists colors (name text);\u0026#34;) c.executemany( \u0026#34;insert into colors (name) values (?);\u0026#34;, ((\u0026#34;red\u0026#34;,), (\u0026#34;green\u0026#34;,), (\u0026#34;blue\u0026#34;,)), ) for line in conn.iterdump(): dst_file.write(line.encode(\u0026#34;utf-8\u0026#34;) + b\u0026#34;\\n\u0026#34;) dst_file.seek(0) print(dst_file.read().decode(\u0026#34;utf-8\u0026#34;)) BEGIN TRANSACTION; CREATE TABLE colors (name text); INSERT INTO \u0026#34;colors\u0026#34; VALUES(\u0026#39;red\u0026#39;); INSERT INTO \u0026#34;colors\u0026#34; VALUES(\u0026#39;green\u0026#39;); INSERT INTO \u0026#34;colors\u0026#34; VALUES(\u0026#39;blue\u0026#39;); COMMIT; Copying an on-disk database to another This example shows another approach that you can adopt to create a second copy of your on-disk DB. First, it connects to the source DB and then creates another connection to an empty backup DB. Afterward, the source data is backed up to the destination DB with the connection_obj_source.backup(connection_obj_destination) API.\nThe .backup method takes in three values — a connection object that points to the destination DB, the number of pages to copy in a single pass, and a callback to introspect the progress. You can set the value of the progress parameter to -1 if you want to load the entire source database into memory and copy everything to the destination in a single pass. Also, in this example, the progress hook just prints the progress of the copied pages.\n# src.py import sqlite3 from contextlib import ExitStack conn_src = sqlite3.connect(\u0026#34;src.db\u0026#34;) conn_dst = sqlite3.connect(\u0026#34;dst.db\u0026#34;) # Hook that indicates backup progress. def progress(status, remaining, total): print(f\u0026#34;Copied {total-remaining} of {total} pages...\u0026#34;) with ExitStack() as stack: conn_src = stack.enter_context(conn_src) conn_dst = stack.enter_context(conn_dst) cursor_src = conn_src.cursor() cursor_src.execute( \u0026#34;\u0026#34;\u0026#34; create table if not exists colors ( name text, hex text ); \u0026#34;\u0026#34;\u0026#34; ) cursor_src.executemany( \u0026#34;insert into colors (name, hex) values (?, ?);\u0026#34;, ( (\u0026#34;red\u0026#34;, \u0026#34;#ff0000\u0026#34;), (\u0026#34;green\u0026#34;, \u0026#34;#00ff00\u0026#34;), (\u0026#34;blue\u0026#34;, \u0026#34;#0000ff\u0026#34;), ), ) # Must commit before backup. conn_src.commit() # Copy a to b. The \u0026#39;pages\u0026#39; parameter determines how many DB pages # to copy in a single iteration. Set to -1 to load everything into # memory at once and do it in a single iteration. conn_src.backup(conn_dst, pages=1, progress=progress) # Ensure that the backup is complete. result = conn_dst.execute(\u0026#34;select count(*) from colors;\u0026#34;).fetchone() print(f\u0026#34;Number of rows in dst: {result[0]}\u0026#34;) Copied 1 of 2 pages... Copied 2 of 2 pages... Number of rows in dst: 3 Loading an on-disk database into the memory The connection_obj.backup API also lets you load your existing database into memory. This is helpful when the DB you\u0026rsquo;re working with is small and you want to leverage the extra performance benefits that come with an in-memory DB. The workflow is almost exactly the same as before and the only difference is that the destination connection object points to an in-memory DB instead of an on-disk one.\n# src.py import sqlite3 conn_src = sqlite3.connect(\u0026#34;src.db\u0026#34;) conn_dst = sqlite3.connect(\u0026#34;:memory:\u0026#34;) with conn_src: cursor_src = conn_src.cursor() cursor_src.execute( \u0026#34;\u0026#34;\u0026#34; create table if not exists colors ( name text, hex text ); \u0026#34;\u0026#34;\u0026#34; ) cursor_src.executemany( \u0026#34;insert into colors (name, hex) values (?, ?);\u0026#34;, ( (\u0026#34;red\u0026#34;, \u0026#34;#ff0000\u0026#34;), (\u0026#34;green\u0026#34;, \u0026#34;#00ff00\u0026#34;), (\u0026#34;blue\u0026#34;, \u0026#34;#0000ff\u0026#34;), ), ) # Must commit before backup. conn_src.commit() # Copy a to memory. conn_src.backup(conn_dst) # Ensure that the backup is complete. result = conn_dst.execute(\u0026#34;select count(*) from colors;\u0026#34;).fetchone() print(f\u0026#34;Number of rows in dst: {result[0]}\u0026#34;) Number of rows in dst: 3 Copying an in-memory database to an on-disk file You can also dump your in-memory DB into the disk. Just point the source connection object to the in-memory DB and the destination connection to the on-disk DB file.\n# src.py import sqlite3 conn_src = sqlite3.connect(\u0026#34;:memory:\u0026#34;) conn_dst = sqlite3.connect(\u0026#34;dst.db\u0026#34;) with conn_src: cursor_src = conn_src.cursor() cursor_src.execute( \u0026#34;\u0026#34;\u0026#34; create table if not exists colors ( name text, hex text ); \u0026#34;\u0026#34;\u0026#34; ) cursor_src.executemany( \u0026#34;insert into colors (name, hex) values (?, ?);\u0026#34;, ( (\u0026#34;red\u0026#34;, \u0026#34;#ff0000\u0026#34;), (\u0026#34;green\u0026#34;, \u0026#34;#00ff00\u0026#34;), (\u0026#34;blue\u0026#34;, \u0026#34;#0000ff\u0026#34;), ), ) # Must commit before backup. conn_src.commit() # Copy a to an on-disk file. conn_src.backup(conn_dst) # Ensure that the backup is complete. result = conn_dst.execute(\u0026#34;select count(*) from colors;\u0026#34;).fetchone() print(f\u0026#34;Number of rows in dst: {result[0]}\u0026#34;) Number of rows in dst: 3 Implementing a full text search engine This is not exactly a feature that\u0026rsquo;s specific to the sqlite3 API. However, I wanted to showcase how effortless it is to leverage SQLite\u0026rsquo;s native features via the Python API. The following example creates a virtual table and implements a full-text search engine that allows us to fuzzy search the colors in the colors table by their names or hex values.\n# src.py import sqlite3 import uuid conn = sqlite3.connect(\u0026#34;:memory:\u0026#34;) # Get the search result as a Python dict. conn.row_factory = lambda cursor, row: { col[0]: row[idx] for idx, col in enumerate(cursor.description) } with conn: c = conn.cursor() # Unindexed ensures that uuid field doesn\u0026#39;t appear in the ft5 index. c.execute( \u0026#34;\u0026#34;\u0026#34; create virtual table if not exists colors using fts5(uuid unindexed, name, hex);\u0026#34;\u0026#34;\u0026#34; ) get_uuid = lambda: str(uuid.uuid4()) # noqa: E731 color_data = ( (get_uuid(), \u0026#34;red\u0026#34;, \u0026#34;#ff0000\u0026#34;), (get_uuid(), \u0026#34;green\u0026#34;, \u0026#34;#00ff00\u0026#34;), (get_uuid(), \u0026#34;blue\u0026#34;, \u0026#34;#0000ff\u0026#34;), (get_uuid(), \u0026#34;yellow\u0026#34;, \u0026#34;#ffff00\u0026#34;), (get_uuid(), \u0026#34;cyan\u0026#34;, \u0026#34;#00ffff\u0026#34;), (get_uuid(), \u0026#34;magenta\u0026#34;, \u0026#34;#ff00ff\u0026#34;), (get_uuid(), \u0026#34;black\u0026#34;, \u0026#34;#000000\u0026#34;), (get_uuid(), \u0026#34;white\u0026#34;, \u0026#34;#ffffff\u0026#34;), ) c.executemany(\u0026#34;insert into colors values (?, ?, ?);\u0026#34;, color_data) result = c.execute( \u0026#34;\u0026#34;\u0026#34;select * from colors where name match \u0026#39;cyan OR red NOT magenta\u0026#39;;\u0026#34;\u0026#34;\u0026#34; ).fetchall() print(result) [ { \u0026#39;uuid\u0026#39;: \u0026#39;c5f6e5ea-124b-44fe-afad-69aad565541e\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;red\u0026#39;, \u0026#39;hex\u0026#39;: \u0026#39;#ff0000\u0026#39; }, { \u0026#39;uuid\u0026#39;: \u0026#39;0dedbb75-6e1d-4a7f-85f7-f0ed0ae4d162\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;cyan\u0026#39;, \u0026#39;hex\u0026#39;: \u0026#39;#00ffff\u0026#39; } ] ","permalink":"https://rednafi.com/python/recipes-from-python-sqlite-docs/","summary":"\u003cp\u003eWhile going through the documentation of Python\u0026rsquo;s \u003ca href=\"https://docs.python.org/3/library/sqlite3.html\"\u003esqlite3\u003c/a\u003e module, I noticed that it\u0026rsquo;s\nquite API-driven, where different parts of the module are explained in a prescriptive\nmanner. I, however, learn better from examples, recipes, and narratives. Although a few good\nrecipes already exist in the docs, I thought I\u0026rsquo;d also enlist some of the examples I tried\nout while grokking them.\u003c/p\u003e\n\u003ch2 id=\"executing-individual-statements\"\u003eExecuting individual statements\u003c/h2\u003e\n\u003cp\u003eTo execute individual statements, you\u0026rsquo;ll need to use the \u003ccode\u003ecursor_obj.execute(statement)\u003c/code\u003e\nprimitive.\u003c/p\u003e","title":"Recipes from Python SQLite docs"},{"content":"TIL from this video by Anthony Sottile that Python\u0026rsquo;s urlparse is quite slow at parsing URLs. I\u0026rsquo;ve always used urlparse to destructure URLs and didn\u0026rsquo;t know that there\u0026rsquo;s a faster alternative to this in the standard library. The official documentation also recommends the alternative function.\nThe urlparse function splits a supplied URL into multiple seperate components and returns a ParseResult object. Consider this example:\nIn [1]: from urllib.parse import urlparse In [2]: url = \u0026#34;https://httpbin.org/get?q=hello\u0026amp;r=22\u0026#34; In [3]: urlparse(url) Out[3]: ParseResult( scheme=\u0026#39;https\u0026#39;, netloc=\u0026#39;httpbin.org\u0026#39;, path=\u0026#39;/get\u0026#39;, params=\u0026#39;\u0026#39;, query=\u0026#39;q=hello\u0026amp;r=22\u0026#39;, fragment=\u0026#39;\u0026#39; ) You can see how the function disassembles the URL and builds a ParseResult object with the URL components. Along with this, the urlparse function can also parse an obscure type of URL that you\u0026rsquo;ll most likely never need. If you notice closely in the previous example, you\u0026rsquo;ll see that there\u0026rsquo;s a params argument in the ParseResult object. This params argument gets parsed whether you need it or not and that adds some overhead. The params field will be populated if you have a URL like this:\nIn [1]: from urllib.parse import urlparse In [2]: url = \u0026#34;https://httpbin.org/get;a=mars\u0026amp;b=42?q=hello\u0026amp;r=22\u0026#34; In [3]: urlparse(url) Out[4]: ParseResult( scheme=\u0026#39;https\u0026#39;, netloc=\u0026#39;httpbin.org\u0026#39;, path=\u0026#39;/get\u0026#39;, params=\u0026#39;a=mars\u0026amp;b=42\u0026#39;, query=\u0026#39;q=hello\u0026amp;r=22\u0026#39;, fragment=\u0026#39;\u0026#39; ) Notice the parts in the URL that appears after https://httpbin.org/get. There\u0026rsquo;s a semicolon and a few more parameters succeeding that — ;a=mars\u0026amp;b=42. The resulting ParseResult now has the params field populated with the parsed param value a=mars\u0026amp;b=42. Unless you need this param support, there\u0026rsquo;s a better and faster alternative to this in the standard library. The urlsplit function does the same thing as urlparse minus the param parsing and is twice as fast. Here\u0026rsquo;s how you\u0026rsquo;d use urlsplit:\nIn [1]: from urllib.parse import urlsplit In [2]: url = \u0026#34;https://httpbin.org/get?q=hello\u0026amp;r=22\u0026#34; In [3]: urlsplit(url) Out[3]: SplitResult( scheme=\u0026#39;https\u0026#39;, netloc=\u0026#39;httpbin.org\u0026#39;, path=\u0026#39;/get\u0026#39;, query=\u0026#39;q=hello\u0026amp;r=22\u0026#39;, fragment=\u0026#39;\u0026#39; ) The urlsplit function returns a SplitResult object similar to the ParseResult object you\u0026rsquo;ve seen before. Notice there\u0026rsquo;s no param argument in the output here. I measured the speed difference like this:\nIn [1]: from urllib.parse import urlparse, urlsplit In [2]: url = \u0026#34;https://httpbin.org/get?q=hello\u0026amp;r=22\u0026#34; In [3]: %timeit urlparse(url) 1.7 µs ± 2.91 ns per loop ( mean ± std. dev. of 7 runs, 1,000,000 loops each) In [4]: %timeit urlsplit(url) 885 ns ± 10.9 ns per loop ( mean ± std. dev. of 7 runs, 1,000,000 loops each) Wow, that\u0026rsquo;s almost 2x speed improvement. Although this shouldn\u0026rsquo;t be much of an issue in a real codebase but it can matter if you are parsing URLs in a critical hot path.\n","permalink":"https://rednafi.com/python/use-urlsplit-over-urlparse/","summary":"\u003cp\u003eTIL from this \u003ca href=\"https://www.youtube.com/watch?v=ABJvdsIANds\"\u003evideo by Anthony Sottile\u003c/a\u003e that Python\u0026rsquo;s \u003ca href=\"https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urlparse\"\u003eurlparse\u003c/a\u003e is quite slow at parsing\nURLs. I\u0026rsquo;ve always used \u003ccode\u003eurlparse\u003c/code\u003e to destructure URLs and didn\u0026rsquo;t know that there\u0026rsquo;s a faster\nalternative to this in the standard library. The official documentation also recommends the\nalternative function.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003eurlparse\u003c/code\u003e function splits a supplied URL into multiple seperate components and returns\na \u003ccode\u003eParseResult\u003c/code\u003e object. Consider this example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eIn\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e \u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003eurllib.parse\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eurlparse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eIn\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e \u003cspan class=\"n\"\u003eurl\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;https://httpbin.org/get?q=hello\u0026amp;r=22\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eIn\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e \u003cspan class=\"n\"\u003eurlparse\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eurl\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eOut\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e \u003cspan class=\"n\"\u003eParseResult\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003escheme\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;https\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003enetloc\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;httpbin.org\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;/get\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eparams\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003equery\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;q=hello\u0026amp;r=22\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003efragment\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eYou can see how the function disassembles the URL and builds a \u003ccode\u003eParseResult\u003c/code\u003e object with the\nURL components. Along with this, the \u003ccode\u003eurlparse\u003c/code\u003e function can also parse an obscure type of\nURL that you\u0026rsquo;ll most likely never need. If you notice closely in the previous example,\nyou\u0026rsquo;ll see that there\u0026rsquo;s a \u003ccode\u003eparams\u003c/code\u003e argument in the \u003ccode\u003eParseResult\u003c/code\u003e object. This \u003ccode\u003eparams\u003c/code\u003e\nargument gets parsed whether you need it or not and that adds some overhead. The \u003ccode\u003eparams\u003c/code\u003e\nfield will be populated if you have a URL like this:\u003c/p\u003e","title":"Prefer urlsplit over urlparse to destructure URLs"},{"content":"Python has a random.choice routine in the standard library that allows you to pick a random value from an iterable. It works like this:\n# src.py import random # The seed ensures that you\u0026#39;ll get the same random choice # every time you run the script. random.seed(90) # This builds a list: [\u0026#34;choice_0\u0026#34;, \u0026#34;choice_1\u0026#34;, ..., \u0026#34;choice_9\u0026#34;] lst = [f\u0026#34;choice_{i}\u0026#34; for i in range(10)] print(random.choice(lst)) print(random.choice(lst)) This will print:\nchoice_3 choice_1 I was looking for a way to quickly hydrate a table with random data in an SQLite database. To be able to do so, I needed to extract unpremeditated values from an array of predefined elements. The issue is, that SQLite doesn\u0026rsquo;t support array types or have a built-in function to pick random values from an array. However, recently I came across this trick from Ricardo Ander-Egg\u0026rsquo;s tweet, where he exploits SQLite\u0026rsquo;s JSON support to parse an array. This idea can be further extended to pluck random values from an array.\nTo extract values from any JSON object in SQLite, you can use the json_extract function. Start a SQLite CLI session and run the following query:\nselect json_extract( \u0026#39;{\u0026#34;greetings\u0026#34;: [\u0026#34;Hello\u0026#34;, \u0026#34;Hola\u0026#34;, \u0026#34;Ohe\u0026#34;]}\u0026#39;, \u0026#39;$.greetings[2]\u0026#39; ) This will give you an output as follows:\nOhe The above query parses the JSON object inside the json_extract function and extracts the last element from the greetings array. If you want to know more details about how you can extract specific elements from JSON objects, head over to the SQLite [docs on this topic].\nYou can pick any value from a JSON array by its index:\nselect json_extract( \u0026#39;[\u0026#34;Columbus\u0026#34;, \u0026#34;Cincinnati\u0026#34;, \u0026#34;Dayton\u0026#34;, \u0026#34;Toledo\u0026#34;]\u0026#39;, \u0026#39;$[2]\u0026#39; ) Dayton Now, how do we extract random elements from the above array? If we can generate a set of random indices, those can be used to access values arbitrarily from the JSON array. These random indices can be generated using SQLite\u0026rsquo;s built-in random() function. The function doesn\u0026rsquo;t take any arguments and generates a large positive or negative arbitrary integer. From this integer, a random index can be found by computing abs(random()) modulo n where abs(random()) denotes the absolute result of the random function and n represents the length of the target array.\nFor example, if the length of the array is 4, and random() produces the integer -123456789, then the index will be 123456789 % 4 = 1 :\nselect abs(random()) % 4; If you run this query multiple times, you\u0026rsquo;ll see that it prints a value between 0 and 3 in random order.\nsqlite\u0026gt; select abs(random()) % 4; 0 sqlite\u0026gt; select abs(random()) % 4; 3 sqlite\u0026gt; select abs(random()) % 4; 2 sqlite\u0026gt; select abs(random()) % 4; 1 Similarly, if you compute abs(random()) % 5, it\u0026rsquo;ll print a value between 0 to 4 and so on. Armed with this knowledge, we can extract a random value from a JSON array like this:\nselect json_extract( \u0026#39;[\u0026#34;Columbus\u0026#34;, \u0026#34;Cincinnati\u0026#34;, \u0026#34;Dayton\u0026#34;, \u0026#34;Toledo\u0026#34;]\u0026#39;, \u0026#39;$[\u0026#39; || cast(abs(random()) % 4 as text) || \u0026#39;]\u0026#39; ); Running the above query will give you a single value from the JSON array in random order. Execute the query multiple times to see it in action.\nsqlite\u0026gt; select json_extract(\u0026#39;[\u0026#34;Columbus\u0026#34;, ... Toledo sqlite\u0026gt; select json_extract(\u0026#39;[\u0026#34;Columbus\u0026#34;, ... Cincinnati sqlite\u0026gt; select json_extract(\u0026#39;[\u0026#34;Columbus\u0026#34;, ... Columbus Voila, we\u0026rsquo;ve successfully emulated Python\u0026rsquo;s random.choice in SQL.\nPopulating a table with random data Populating a table with randomly distributed data is useful, especially when you need to demonstrate a feature or flex your SQL fu. We can leverage the above pattern to populate a simple table with 100 data points like this:\n-- Create the \u0026#39;stat\u0026#39; table with \u0026#39;id\u0026#39;, \u0026#39;cat\u0026#39;, and \u0026#39;score\u0026#39; columns. create table if not exists stat ( id integer primary key, cat text, score real ); -- Populate the \u0026#39;stat\u0026#39; table with random data. with recursive cte (x, y) as ( select \u0026#39;a\u0026#39;, random() % 1000 union all select json_extract (\u0026#39;[\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;]\u0026#39;, \u0026#39;$[\u0026#39; || cast(abs(random()) % 3 as text) || \u0026#39;]\u0026#39;), random() % 1000 from cte limit 100) insert into stat (cat, score) select * from cte where not exists ( -- This block ensures that the query select * -- can be run multiple times without from stat -- any side effects. If you run this where stat.cat = cte.x -- multiple times, it\u0026#39;ll or stat.score = cte.x); -- only insert the values once. -- Inspect the populated table. select * from stat; If you run the above queries via the SQLite CLI, the final statement will reveal the stat table with the randomly filled in data:\n| id | cat | score | |-----|-----|--------| | 1 | a | 390.0 | | 2 | a | 864.0 | | 3 | b | -856.0 | | 4 | b | -307.0 | | 5 | c | -405.0 | | 6 | a | -61.0 | | 7 | a | 794.0 | | 8 | b | -560.0 | | 9 | a | -355.0 | | 10 | c | 10.0 | ... | 100 | c | 420.0 | ","permalink":"https://rednafi.com/system/random-choice-in-sqlite/","summary":"\u003cp\u003ePython has a \u003ccode\u003erandom.choice\u003c/code\u003e routine in the standard library that allows you to pick a\nrandom value from an iterable. It works like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003erandom\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# The seed ensures that you\u0026#39;ll get the same random choice\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# every time you run the script.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003erandom\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eseed\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e90\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# This builds a list: [\u0026#34;choice_0\u0026#34;, \u0026#34;choice_1\u0026#34;, ..., \u0026#34;choice_9\u0026#34;]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003elst\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;choice_\u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ei\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e)]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erandom\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoice\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elst\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erandom\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoice\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elst\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis will print:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003echoice_3\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003echoice_1\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eI was looking for a way to quickly hydrate a table with random data in an SQLite database.\nTo be able to do so, I needed to extract unpremeditated values from an array of predefined\nelements. The issue is, that SQLite doesn\u0026rsquo;t support array types or have a built-in function\nto pick random values from an array. However, recently I came across this \u003ca href=\"https://twitter.com/ricardoanderegg/status/1564723221173342220?s=20\u0026amp;t=V4TtJsxqyH0IuheqhEvb4w\"\u003etrick from\nRicardo Ander-Egg\u0026rsquo;s tweet\u003c/a\u003e, where he exploits SQLite\u0026rsquo;s JSON support to parse an array. This\nidea can be further extended to pluck random values from an array.\u003c/p\u003e","title":"Pick random values from an array in SQL(ite)"},{"content":"Over the years, I\u0026rsquo;ve used Python\u0026rsquo;s contextlib.ExitStack in a few interesting ways. The official ExitStack documentation advertises it as a way to manage multiple context managers and has a couple of examples of how to leverage it. However, neither in the docs nor in GitHub code search could I find examples of some of the maybe unusual ways I\u0026rsquo;ve used it in the past. So, I thought I\u0026rsquo;d document them here.\nEnforcing request level transaction While consuming APIs, it\u0026rsquo;s important to handle errors in a way that prevents database state corruption. In the following example, I\u0026rsquo;m making two POST requests to an API and rolling back to the original state if any one of them fails:\n# src.py from __future__ import annotations import logging import uuid from contextlib import ExitStack from http import HTTPStatus import httpx logging.basicConfig(level=logging.INFO) def group_create(uuid_a: str, uuid_b: str) -\u0026gt; tuple[httpx.Response, ...]: with httpx.Client() as client: url = \u0026#34;https://httpbin.org/post\u0026#34; response_a = client.post( url, json={\u0026#34;uuid\u0026#34;: uuid_a, \u0026#34;foo\u0026#34;: \u0026#34;bar\u0026#34;}, ) response_b = client.post( url, json={\u0026#34;uuid\u0026#34;: uuid_b, \u0026#34;fizz\u0026#34;: \u0026#34;bazz\u0026#34;}, ) return response_a, response_b def maybe_rollback( uuid: str, incoming_status_code: int, expected_status_code: int = HTTPStatus.OK, ) -\u0026gt; None: if incoming_status_code != expected_status_code: logging.info(f\u0026#34;Rolling back request: {uuid}\u0026#34;) url = f\u0026#34;https://httpbin.org/delete?uuid={uuid}\u0026#34; response = httpx.delete(url) assert response.status_code == HTTPStatus.OK else: logging.info(f\u0026#34;Request {uuid} completed successfully.\u0026#34;) def main() -\u0026gt; None: with ExitStack() as stack: uuid_a = str(uuid.uuid4()) uuid_b = str(uuid.uuid4()) response_a, response_b = group_create(uuid_a, uuid_b) stack.callback( maybe_rollback, uuid=uuid_a, incoming_status_code=response_a.status_code, ) stack.callback( maybe_rollback, uuid=uuid_b, incoming_status_code=response_b.status_code, ) if __name__ == \u0026#34;__main__\u0026#34;: main() Running this will print the following output:\nINFO:root:Request fec8fc9f-7762-4d53-b8f9-3dc7802108a4 completed successfully. INFO:root:Request 4b6ed0ed-b7cf-46f0-9374-85627be4c26c completed successfully. Here, the group_create function makes two calls to POST httpbin.org/post endpoint and the maybe_rollback function deletes the created record if any one of the two requests fails. In the main function, I\u0026rsquo;ve used the ExitStack.callback method to register the maybe_rollback callback. If you change the expected_status_code in the maybe_rollback function to something like HTTPStatus.FORBIDDEN, you\u0026rsquo;ll be able to see the cleanup callbacks in action:\nINFO:root:Rolling back request: 50eb2734-f84c-4013-b5f6-0ccf1aa5d79a INFO:root:Rolling back request: b326e567-a006-4648-bf04-202397f44e31 Invoking conditional event hooks The same strategy used in the previous section can be applied to invoke event hooks conditionally. For example, let\u0026rsquo;s say you want to run a callback function when some event function executes. However, you want only a particular type of callback function to be executed depending on the state of your conditionals or code path. I\u0026rsquo;ve found the following pattern useful in this case:\n# src.py from __future__ import annotations from contextlib import ExitStack from typing import Any class EventHook: def __init__(self, event_name: str) -\u0026gt; None: self.event_name = event_name self.dispatch_config = { \u0026#34;success\u0026#34;: self.on_success, \u0026#34;failure\u0026#34;: self.on_failure, } def on_success(self) -\u0026gt; None: print(f\u0026#34;\u0026#39;{self.event_name}\u0026#39; hook called\u0026#34;) def on_failure(self) -\u0026gt; None: print(f\u0026#34;\u0026#39;{self.event_name}\u0026#39; hook called\u0026#34;) def __call__(self) -\u0026gt; Any: return self.dispatch_config[self.event_name]() def successful_event() -\u0026gt; None: print(\u0026#34;\u0026#39;successful_event\u0026#39; executed\u0026#34;) def failed_event() -\u0026gt; None: print(\u0026#34;\u0026#39;failed_event\u0026#39; executed\u0026#34;) 1 / 0 def main() -\u0026gt; None: success_hook = EventHook(\u0026#34;success\u0026#34;) failure_hook = EventHook(\u0026#34;failure\u0026#34;) with ExitStack() as stack: try: # Run successful event and attach success hook. successful_event() stack.callback(success_hook) failed_event() except ZeroDivisionError: # When the failed even raises an error, attach failure hook. stack.callback(failure_hook) if __name__ == \u0026#34;__main__\u0026#34;: main() \u0026#39;successful_event\u0026#39; executed \u0026#39;failed_event\u0026#39; executed \u0026#39;failure\u0026#39; hook called \u0026#39;success\u0026#39; hook called Here the .on_failure hook will only be called if there\u0026rsquo;s an error in your execution path raises an exception.\nAvoiding nested context structure It can get ugly pretty quickly when you start using multiple nested context managers. For example, if you need to open two files and copy content from one file to the other, you\u0026rsquo;d typically start two nested context managers and transfer the content like this:\n# src.py with open(\u0026#34;file1.md\u0026#34;) as f1: with open(\u0026#34;file2.md\u0026#34;) as f2: # Copy content from f1 to f2 and save it. ExitStack can help you get away with only one level of nesting here. Here\u0026rsquo;s a complete example:\n# src.py import io import shutil import tempfile from contextlib import ExitStack def copy_over( fsrc: io.IOBase, fdst: io.IOBase, skip_line: int = 0, ) -\u0026gt; None: if skip_line \u0026gt; 0: for _ in range(skip_line): fsrc.readline() shutil.copyfileobj(fsrc, fdst) def main() -\u0026gt; None: with ExitStack() as stack: # Enter into the respective context managers without explicit # \u0026#39;with\u0026#39; blocks. fsrc = stack.enter_context( tempfile.SpooledTemporaryFile(mode=\u0026#34;rb\u0026#34;), ) fdst = stack.enter_context( tempfile.SpooledTemporaryFile(mode=\u0026#34;rb+\u0026#34;), ) # Write some data to the source file. fsrc.write(b\u0026#34;hello world\\nhello mars\u0026#34;) # Rewind the source file and copy it to the destination file. fsrc.seek(0) copy_over(fsrc, fdst, skip_line=1) # Rewind the destination file and assert the data. fdst.seek(0) assert fdst.read() == b\u0026#34;hello mars\u0026#34; # Rewind the dst file and print out the shape of the fdst # content. fdst.seek(0) print(fdst.read()) if __name__ == \u0026#34;__main__\u0026#34;: main() This example creates two in-memory temporary file instances with tempfile.SpooledTemporaryFile. The SpooledTemporaryFile can be used as a context manager. However, instead of nesting the two instances, I\u0026rsquo;m using ExitStack.enter_context to enter into the context manager without explicitly using the with statement. This .enter_context method ensures that the __exit__ method of the respective context managers will be called properly at the end of the main() function run.\nThen in the body of the ExitStack, we\u0026rsquo;re writing some content to the first in-memory file and then copying the content to the other in-memory file. If we had to open and manage even more context managers, in this way, we\u0026rsquo;d be able to that without crating any additional nestings.\nApplying multiple patches as context managers Python\u0026rsquo;s unittest.mock.patch can be used as both decorators and context managers. For granular patching and unpatching during tests, the context manager approach gives you more control than its decorator counterpart. In this case, ExitStack can help you avoid multiple nestings just like in the previous section:\n# src.py from __future__ import annotations from contextlib import ExitStack from http import HTTPStatus from typing import Any from unittest.mock import patch import httpx def get(url: str) -\u0026gt; dict[str, Any]: return httpx.get(url).json() def post(url: str, data: dict[str, Any]) -\u0026gt; dict[str, Any]: return httpx.post(url, json=data).json() def main() -\u0026gt; dict[str, Any]: res_get = get(\u0026#34;https://httpbin.org/get\u0026#34;) res_post = post(\u0026#34;https://httpbin.org/post\u0026#34;, {\u0026#34;foo\u0026#34;: \u0026#34;bar\u0026#34;}) return {\u0026#34;get\u0026#34;: res_get, \u0026#34;post\u0026#34;: res_post} def test_main() -\u0026gt; None: with ExitStack() as stack: # Arrange mock_httpx_get = stack.enter_context( patch( \u0026#34;httpx.get\u0026#34;, autospec=True, return_value=httpx.Response( json={\u0026#34;fizz\u0026#34;: \u0026#34;bazz\u0026#34;}, status_code=HTTPStatus.OK ), ), ) mock_httpx_post = stack.enter_context( patch( \u0026#34;httpx.post\u0026#34;, autospec=True, return_value=httpx.Response( json={\u0026#34;foo\u0026#34;: \u0026#34;bar\u0026#34;}, status_code=HTTPStatus.CREATED ), ) ) # Act res = main() # Assert assert res[\u0026#34;get\u0026#34;][\u0026#34;fizz\u0026#34;] == \u0026#34;bazz\u0026#34; assert res[\u0026#34;post\u0026#34;][\u0026#34;foo\u0026#34;] == \u0026#34;bar\u0026#34; assert mock_httpx_get.call_count == 1 assert mock_httpx_post.call_count == 1 Running the above snippet with pytest will reveal that the test passes without any error:\nsrc.py::test_main PASSED ======================= 1 passed in 0.11s ======================= Here, I\u0026rsquo;m making GET and POST requests with the httpx library and in the test_main function, the httpx.get and httpx.post callable are patched with the patch context manager. However, ExitStack allows me here to do it without creating additional nested with blocks.\n","permalink":"https://rednafi.com/python/exitstack/","summary":"\u003cp\u003eOver the years, I\u0026rsquo;ve used Python\u0026rsquo;s \u003ccode\u003econtextlib.ExitStack\u003c/code\u003e in a few interesting ways. The\n\u003ca href=\"https://docs.python.org/3/library/contextlib.html#contextlib.ExitStack\"\u003eofficial ExitStack documentation\u003c/a\u003e advertises it as a way to manage multiple context\nmanagers and has a couple of examples of how to leverage it. However, neither in the docs\nnor in \u003ca href=\"https://github.com/search?l=Python\u0026amp;q=ExitStack\u0026amp;type=Code\"\u003eGitHub code search\u003c/a\u003e could I find examples of some of the maybe unusual ways I\u0026rsquo;ve\nused it in the past. So, I thought I\u0026rsquo;d document them here.\u003c/p\u003e\n\u003ch2 id=\"enforcing-request-level-transaction\"\u003eEnforcing request level transaction\u003c/h2\u003e\n\u003cp\u003eWhile consuming APIs, it\u0026rsquo;s important to handle errors in a way that prevents database state\ncorruption. In the following example, I\u0026rsquo;m making two \u003ccode\u003ePOST\u003c/code\u003e requests to an API and rolling\nback to the original state if any one of them fails:\u003c/p\u003e","title":"ExitStack in Python"},{"content":"While reading the second version of Brian Okken\u0026rsquo;s pytest book, I came across this neat trick to compose multiple levels of fixtures. Suppose, you want to create a fixture that returns some canned data from a database. Now, let\u0026rsquo;s say that invoking the fixture multiple times is expensive, and to avoid that you want to run it only once per test session. However, you still want to clear all the database states after each test function runs. Otherwise, a test might inadvertently get coupled with another test that runs before it via the fixture\u0026rsquo;s shared state. Let\u0026rsquo;s demonstrate this:\n# test_src.py import pytest @pytest.fixture(scope=\u0026#34;session\u0026#34;) def create_files(tmp_path_factory): \u0026#34;\u0026#34;\u0026#34;Fixture that creates files in the tmp_path/tmp directory, writes something stuff, then and returns the directory.\u0026#34;\u0026#34;\u0026#34; directory = tmp_path_factory.mktemp(\u0026#34;tmp\u0026#34;) for filename in (\u0026#34;foo.txt\u0026#34;, \u0026#34;bar.txt\u0026#34;, \u0026#34;baz.txt\u0026#34;): file = directory / filename file.write_text(\u0026#34;Hello, World!\u0026#34;) yield directory def test_read_default_content(create_files): directory = create_files for filename in (\u0026#34;foo.txt\u0026#34;, \u0026#34;bar.txt\u0026#34;, \u0026#34;baz.txt\u0026#34;): file = directory / filename assert file.read_text() == \u0026#34;Hello, World!\u0026#34; def test_read_custom_content(create_files): directory = create_files for filename in (\u0026#34;foo.txt\u0026#34;, \u0026#34;bar.txt\u0026#34;, \u0026#34;baz.txt\u0026#34;): file = directory / filename file.write_text(\u0026#34;Hello, Mars!\u0026#34;) assert file.read_text() == \u0026#34;Hello, Mars!\u0026#34; In the above snippet, we\u0026rsquo;ve created a session-scoped fixture called create_files that creates three files in a temporary directory, writes some content to them, and then yields the directory. Afterward, we write two tests where the first one tests the files\u0026rsquo; default content and the second one writes some stuff to each of the file and then test their content.\nIf we run this with pytest, both of the tests pass. However, if we change the order of the tests where the test_read_custom_content runs before test_read_default_content, pytest will raise an error:\ntest_src.py .F [100%] ==================== FAILURES ==================== ____________________ test_read_default_content ____________________ create_files = PosixPath(\u0026#39;/tmp/pytest-of-rednafi/pytest-33/tmp0\u0026#39;) def test_read_default_content(create_files): directory = create_files for filename in (\u0026#34;foo.txt\u0026#34;, \u0026#34;bar.txt\u0026#34;, \u0026#34;baz.txt\u0026#34;): file = directory / filename \u0026gt; assert file.read_text() == \u0026#34;Hello, World!\u0026#34; E AssertionError: assert \u0026#39;Hello, Mars!\u0026#39; == \u0026#39;Hello, World!\u0026#39; E - Hello, World! E + Hello, Mars! test_src.py:32: AssertionError ==================== short test summary info ==================== FAILED test_src.py::test_read_default_content - AssertionError: assert \u0026#39;Hello, Mars!\u0026#39; == \u0026#39;Hello, World!\u0026#39; Our tests behave differently when the order of their execution changes. This is bad. You should always make sure that running your tests randomly or reversely doesn\u0026rsquo;t change the outcome of the test run. You can use a plugin like pytest-reverse to change your test execution order.\nThis happens because the data of the fixture create_files persists across multiple tests since it\u0026rsquo;s defined as a session-scoped fixture. Here, test_read_custom_content overwrites the default contents of the files and when the other test runs after this one, it can\u0026rsquo;t find the default content and hence raises an AssertionError. To fix this, we\u0026rsquo;ll need to make sure that the fixture\u0026rsquo;s state gets cleaned up after each test function executes.\nOne way to achieve this is by making the create_files fixture function-scoped; instead of session-scoped. If you decorate create_files with @pytest.fixture(scope=\u0026quot;function\u0026quot;) and then run the above snippet in a reverse manner, you\u0026rsquo;ll see that the error doesn\u0026rsquo;t occur this time. However, making the fixture function-scoped means, the fixture will be executed once before running each test function. This can be a deal breaker if the fixture has to perform some time-consuming setups.\nTo solve this, we can keep the create_files fixture session-scoped and use another function-scoped fixture to clean up its state. This way, before running each test function, the function-scoped fixture will clean up the state of the session-scoped fixture. We can write the previous example as follows:\n# test_src.py import pytest @pytest.fixture(scope=\u0026#34;session\u0026#34;) def create_files(tmp_path_factory): \u0026#34;\u0026#34;\u0026#34;Fixture that creates files in the tmp_path/tmp directory, writes something stuff, then and returns the directory.\u0026#34;\u0026#34;\u0026#34; directory = tmp_path_factory.mktemp(\u0026#34;tmp\u0026#34;) for filename in (\u0026#34;foo.txt\u0026#34;, \u0026#34;bar.txt\u0026#34;, \u0026#34;baz.txt\u0026#34;): file = directory / filename file.write_text(\u0026#34;Hello, World!\u0026#34;) yield directory @pytest.fixture(scope=\u0026#34;function\u0026#34;) def get_files(create_files): yield create_files # Clean up the files after each test function runs. for filename in (\u0026#34;foo.txt\u0026#34;, \u0026#34;bar.txt\u0026#34;, \u0026#34;baz.txt\u0026#34;): file = create_files / filename file.write_text(\u0026#34;Hello, World!\u0026#34;) def test_read_custom_content(get_files): directory = get_files for filename in (\u0026#34;foo.txt\u0026#34;, \u0026#34;bar.txt\u0026#34;, \u0026#34;baz.txt\u0026#34;): file = directory / filename file.write_text(\u0026#34;Hello, Mars!\u0026#34;) assert file.read_text() == \u0026#34;Hello, Mars!\u0026#34; def test_read_default_content(get_files): directory = get_files for filename in (\u0026#34;foo.txt\u0026#34;, \u0026#34;bar.txt\u0026#34;, \u0026#34;baz.txt\u0026#34;): file = directory / filename assert file.read_text() == \u0026#34;Hello, World!\u0026#34; Notice that I\u0026rsquo;ve swapped the order of the tests just for demonstration purposes. Here, we\u0026rsquo;ve defined another fixture called get_files which is function-scoped. Underneath, get_files uses create_files to create the file contents and then cleans up the state after the yield statement. We could refactor some of the clean-up code to make it DRY but I intentionally kept it verbose for simplicity\u0026rsquo;s sake.\nIn this case, the lighter get_files fixture gets executed before every test function runs and keeps the state of the create_files clean. On the other hand, the create_files fixture gets executed only once per test session. This time, if you run the tests, all the tests should pass successfully. We have successfully composed two different levels of fixture functions!\n","permalink":"https://rednafi.com/python/compose-multiple-levels-of-pytest_fixtures/","summary":"\u003cp\u003eWhile reading the second version of \u003ca href=\"https://pragprog.com/titles/bopytest2/python-testing-with-pytest-second-edition/\"\u003eBrian Okken\u0026rsquo;s pytest book\u003c/a\u003e, I came across this neat\ntrick to compose multiple levels of fixtures. Suppose, you want to create a fixture that\nreturns some canned data from a database. Now, let\u0026rsquo;s say that invoking the fixture multiple\ntimes is expensive, and to avoid that you want to run it only once per test session.\nHowever, you still want to clear all the database states after each test function runs.\nOtherwise, a test might inadvertently get coupled with another test that runs before it via\nthe fixture\u0026rsquo;s shared state. Let\u0026rsquo;s demonstrate this:\u003c/p\u003e","title":"Compose multiple levels of fixtures in pytest"},{"content":"I was reading Ned Bachelder\u0026rsquo;s blog Why your mock doesn\u0026rsquo;t work and it triggered an epiphany in me about a testing pattern that I\u0026rsquo;ve been using for a while without being aware that there might be an aphorism on the practice.\nPatch where the object is used; not where it\u0026rsquo;s defined.\nTo understand it, consider the example below. Here, you have a module containing a function that fetches data from some fictitious database.\n# db.py from __future__ import annotations import random def get_data() -\u0026gt; list[int]: # ...run some side effects and return data # from a fictitous database. return [random.randint(100, 200) for _ in range(4)] Let\u0026rsquo;s say another module named service.py imports the get_data function and calls that inside of a function named process_data:\n# service.py from __future__ import annotations from db import get_data def process_data() -\u0026gt; list[int]: data = get_data() # ... do some processing. return data Now, let\u0026rsquo;s say we want to write a test for the service.process_data function. Since the function depends on db.get_data, we\u0026rsquo;ll patch the get_data function and replace it with a mock object that returns a canned response. This will make sure that calling process doesn\u0026rsquo;t invoke the real get_data which might have side effects that we don\u0026rsquo;t want to trigger during test runs. Also, in this case, instead of returning a list of pseudo-random integers, the replaced get_data function will deterministically return a list of known integers.\nYou could patch get_data in multiple ways. Here\u0026rsquo;s the first attempt:\n# test_service.py from unittest.mock import patch from service import process # Patching happens here! @patch(\u0026#34;db.get_data\u0026#34;, return_value=[1, 2, 3, 4], autospec=True) def test_process(mock_get_data): # Call the target function. result = process() # Check the result. assert result == [1, 2, 3, 4] # Check that get_data was called. mock_get_data.assert_called_once() Since get_data is defined in the db.py module, we pass db.get_data to the patch decorator. Unfortunately, if you run the above test with pytest, you\u0026rsquo;ll see that the test fails with the following error:\ntest_service.py F [100%] ========== FAILURES ========== __________ test_process __________ mock_get_data = \u0026lt;function get_data at 0x7fc8b04d6440\u0026gt; @patch( \u0026#34;db.get_data\u0026#34;, return_values=[1, 2, 3, 4], autospec=True ) def test_process(mock_get_data): # Call the target function. result = process() # Check the result. \u0026gt; assert result == [1, 2, 3, 4] E assert [184, 112, 189, 135] == [1, 2, 3, 4] E At index 0 diff: 184 != 1 E Use -v to get more diff test_src.py:13: AssertionError ========== short test summary info ========== FAILED test_src.py::test_process - assert [184, 112, 189, 135] == [1, 2, 3, 4] ========== 1 failed in 0.14s ========== The original implementation of get_data returns a list of 4 pseudo-random integers where the values lie between 100 and 200 whereas our patched version of get_data always returns [1, 2, 3, 4]. So, the test is failing because the get_data function didn\u0026rsquo;t get patched properly and it\u0026rsquo;s calling the original get_data function during the test run.\nWhile the function get_data is defined in the db.py module, it\u0026rsquo;s actally used in the service.py module. So, we can avoid this missing target issue by patching get_data in the location where it\u0026rsquo;s used; not where it\u0026rsquo;s defined. Here\u0026rsquo;s how to do it:\n# test_service.py # Notice how we\u0026#39;re patching \u0026#39;get_data\u0026#39; in the \u0026#39;service.py\u0026#39; module. @patch(\u0026#34;service.get_data\u0026#34;, return_value=[1, 2, 3, 4], autospec=True) def test_process(mock_get_data): # ...rest of the test implementation is the same as before. This time, when you run the tests, pytest doesn\u0026rsquo;t complain.\n","permalink":"https://rednafi.com/python/patch-where-the-object-is-used/","summary":"\u003cp\u003eI was reading Ned Bachelder\u0026rsquo;s blog \u003ca href=\"https://nedbatchelder.com/blog/201908/why_your_mock_doesnt_work.html\"\u003eWhy your mock doesn\u0026rsquo;t work\u003c/a\u003e and it triggered an epiphany\nin me about a testing pattern that I\u0026rsquo;ve been using for a while without being aware that\nthere might be an aphorism on the practice.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePatch where the object is used; not where it\u0026rsquo;s defined.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eTo understand it, consider the example below. Here, you have a module containing a function\nthat fetches data from some fictitious database.\u003c/p\u003e","title":"Patch where the object is used"},{"content":"I just found out that you can use Python\u0026rsquo;s unittest.mock.ANY to make assertions about certain arguments in a mock call, without caring about the other arguments. This can be handy if you want to test how a callable is called but only want to make assertions about some arguments. Consider the following example:\n# test_src.py import random import time def fetch() -\u0026gt; list[float]: # Simulate fetching data from a database. time.sleep(2) return [random.random() for _ in range(4)] def add(w: float, x: float, y: float, z: float) -\u0026gt; float: return w + x + y + z def procss() -\u0026gt; float: return add(*fetch()) Let\u0026rsquo;s say we only want to test the process function. But process ultimately depends on the fetch function, which has multiple side effects — it returns pseudo-random values and waits for 2 seconds on a fictitious network call. Since we only care about process, we\u0026rsquo;ll mock the other two functions. Here\u0026rsquo;s how unittest.mock.ANY can make life easier:\n# test_src.py from unittest.mock import patch, ANY @patch(\u0026#34;test_src.fetch\u0026#34;, return_value=[1, 2, 3, 4]) @patch(\u0026#34;test_src.add\u0026#34;, return_value=42) def test_process(mock_add, mock_fetch): result = procss() assert result == 42 mock_fetch.assert_called_once() # Assert that the \u0026#39;add\u0026#39; function was called with the correct # arguments. Notice we only care about the first two arguments, # so we\u0026#39;ve set the remaining ones to ANY. mock_add.assert_called_once_with(1, 2, ANY, ANY) While this is a simple example, I found ANY to be quite useful while making assertions about callables that accept multiple complex objects as parameters. Being able to ignore some aruments while calling mock_callable.assert_called_with() can make the tests more tractable.\nUnder the hood, the implementation of ANY is quite simple. It\u0026rsquo;s an instance of a class that defines __eq__ and __ne__ in a way that comparing any value with ANY will return True. Here\u0026rsquo;s the full implementation:\nfrom __future__ import annotations from typing import Any, Literal class _ANY: \u0026#34;A helper object that compares equal to everything.\u0026#34; def __eq__(self, other: Any) -\u0026gt; Literal[True]: return True def __ne__(self, other: Any) -\u0026gt; Literal[False]: return False def __repr__(self) -\u0026gt; str: return \u0026#34;\u0026lt;ANY\u0026gt;\u0026#34; ANY = _ANY() It always returns True whenever compared with some value:\nIn [1]: from unittest.mock import ANY In [2]: ANY == 1 Out[2]: True In [3]: ANY == \u0026#34;anything\u0026#34; Out[3]: True In [4]: ANY == True Out[4]: True In [5]: ANY == False Out[5]: True In [6]: ANY == None Out[6]: True Further reading unittest.mock.ANY ANY in the wild ","permalink":"https://rednafi.com/python/partially-assert-callable-arguments/","summary":"\u003cp\u003eI just found out that you can use Python\u0026rsquo;s \u003ccode\u003eunittest.mock.ANY\u003c/code\u003e to make assertions about\ncertain arguments in a mock call, without caring about the other arguments. This can be\nhandy if you want to test how a callable is called but only want to make assertions about\nsome arguments. Consider the following example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# test_src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003erandom\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003etime\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003efetch\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003elist\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Simulate fetching data from a database.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003etime\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esleep\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003erandom\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erandom\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003e_\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e)]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eadd\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ew\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ez\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ew\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003ez\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eprocss\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eadd\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003efetch\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eLet\u0026rsquo;s say we only want to test the \u003ccode\u003eprocess\u003c/code\u003e function. But \u003ccode\u003eprocess\u003c/code\u003e ultimately depends on\nthe \u003ccode\u003efetch\u003c/code\u003e function, which has multiple side effects — it returns pseudo-random values and\nwaits for 2 seconds on a fictitious network call. Since we only care about \u003ccode\u003eprocess\u003c/code\u003e, we\u0026rsquo;ll\nmock the other two functions. Here\u0026rsquo;s how \u003ccode\u003eunittest.mock.ANY\u003c/code\u003e can make life easier:\u003c/p\u003e","title":"Partially assert callable arguments with 'unittest.mock.ANY'"},{"content":"Whenever your local branch diverges from the remote branch, you can\u0026rsquo;t directly pull from the remote branch and merge it into the local branch. This can happen when, for example:\nYou checkout from the main branch to work on a feature in a branch named alice. When you\u0026rsquo;re done, you merge alice into main. After that, if you try to pull the main branch from remote again and the content of the main branch changes by this time, you\u0026rsquo;ll encounter a merge error. Reproduce the issue Create a new branch named alice from main. Run:\ngit checkout -b alice From alice branch, add a line to a newly created file foo.txt:\necho \u0026#34;from branch alice\u0026#34; \u0026gt;\u0026gt; foo.txt Add, commit, and push the branch:\ngit commit -am \u0026#34;From branch alice\u0026#34; \u0026amp;\u0026amp; git push From the GitHub UI, send a pull request against the main branch and merge it:\nIn your local machine, switch to main and try to pull the latest content merged from the alice branch. You\u0026rsquo;ll encounter the following error:\nhint: You have divergent branches and need to specify how to reconcile them. hint: You can do so by running one of the following commands sometime before hint: your next pull: hint: hint: git config pull.rebase false # merge (the default strategy) hint: git config pull.rebase true # rebase hint: git config pull.ff only # fast-forward only hint: hint: You can replace \u0026#34;git config\u0026#34; with \u0026#34;git config --global\u0026#34; to set a default hint: preference for all repositories. You can also pass --rebase, --no-rebase, hint: or --ff-only on the command line to override the configured default per hint: invocation. fatal: Need to specify how to reconcile divergent branches. This means that the history of your local main branch and the remote main branch have diverged and they aren\u0026rsquo;t reconciliable.\nSolution From the main branch, you can run:\ngit pull --rebase This will rebase your local main by adding your local commits on top of the remote commits.\nFurther reading When should I use git pull \u0026ndash;rebase An example repo that reproduces the issue ","permalink":"https://rednafi.com/misc/when-to-use-git-pull-rebase/","summary":"\u003cp\u003eWhenever your local branch diverges from the remote branch, you can\u0026rsquo;t directly pull from the\nremote branch and merge it into the local branch. This can happen when, for example:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eYou checkout from the \u003ccode\u003emain\u003c/code\u003e branch to work on a feature in a branch named \u003ccode\u003ealice\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eWhen you\u0026rsquo;re done, you merge \u003ccode\u003ealice\u003c/code\u003e into \u003ccode\u003emain\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eAfter that, if you try to pull the \u003ccode\u003emain\u003c/code\u003e branch from remote again and the content of the\n\u003ccode\u003emain\u003c/code\u003e branch changes by this time, you\u0026rsquo;ll encounter a merge error.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"reproduce-the-issue\"\u003eReproduce the issue\u003c/h2\u003e\n\u003cp\u003eCreate a new branch named \u003ccode\u003ealice\u003c/code\u003e from \u003ccode\u003emain\u003c/code\u003e. Run:\u003c/p\u003e","title":"When to use 'git pull --rebase'"},{"content":"Whenever I need to apply some runtime constraints on a value while building an API, I usually compare the value to an expected range and raise a ValueError if it\u0026rsquo;s not within the range. For example, let\u0026rsquo;s define a function that throttles some fictitious operation. The throttle function limits the number of times an operation can be performed by specifying the throttle_after parameter. This parameter defines the number of iterations after which the operation will be halted. The current_iter parameter tracks the current number of times the operation has been performed. Here\u0026rsquo;s the implementation:\n# src.py def throttle(current_iter: int, throttle_after: int = -1) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; The value of \u0026#39;throttle_after\u0026#39; must be -1 or an integer greater than 0. Here, -1 means no throttling, and \u0026#39;n\u0026#39; means that the function will throttle some operation after \u0026#39;n\u0026#39; iterations. The `current_iter` parameter denotes the current iteration of some operation. When \u0026#39;current_iter \u0026gt; throttle_after\u0026#39; this function will throttle the operation. \u0026#34;\u0026#34;\u0026#34; # Return early if \u0026#39;throttle_after=-1\u0026#39;. if throttle_after == -1: print(\u0026#34;No throttling.\u0026#34;) return # Ensure \u0026#39;current_iter\u0026#39; is a positive integer. if not (isinstance(current_iter, int) and current_iter \u0026gt;= 0): raise ValueError( \u0026#34;Value of \u0026#39;current_iter\u0026#39; must be a\u0026#34; \u0026#34; positive integer.\u0026#34; ) # Ensure \u0026#39;throttle_after\u0026#39; is a non-zero positive integer. if not (isinstance(throttle_after, int) and throttle_after \u0026gt; 0): raise ValueError( \u0026#34;Value of \u0026#39;throttle_after\u0026#39; must be either -1 or an\u0026#34; \u0026#34; integer greater than 0.\u0026#34; ) # Do the throttling. if current_iter \u0026gt; throttle_after: print(f\u0026#34;Thottling after {throttle_after} iteration(s).\u0026#34;) return if __name__ == \u0026#34;__main__\u0026#34;: # Prints \u0026#39;Throttling after 1 iteration(s).\u0026#39; throttle(current_iter=2, throttle_after=1) We return early if the value of throttle_after is -1. Otherwise, we check to see if current_iter is a positive integer and throttle_after is a non-zero positive integer. If not, we raise a ValueError. When the parameters pass these checks then we compare current_iter with throttle_after. If the value of current_iter exceeds that of the throttle_after parameter, we throttle the operation.\nWhile this works fine, recently, I\u0026rsquo;ve started to use assert to replace the conditionals with ValueError pattern. It works as follows:\n# src.py def throttle(current_iter: int, throttle_after: int = -1) -\u0026gt; None: # Return early if \u0026#39;throttle_after=-1\u0026#39;. if throttle_after == -1: print(\u0026#34;No throttling.\u0026#34;) return # Ensure \u0026#39;current_iter\u0026#39; is a positive integer. assert ( isinstance(current_iter, int) and current_iter \u0026gt;= 0 ), \u0026#34;Value of \u0026#39;current_iter\u0026#39; must be a positive integer.\u0026#34; # Ensure \u0026#39;throttle_after\u0026#39; is a non-zero positive integer. assert isinstance(throttle_after, int) and throttle_after \u0026gt; 0, ( \u0026#34;Value of \u0026#39;throttle_after\u0026#39; must be either -1 or an \u0026#34; \u0026#34; integer greater than 0.\u0026#34; ) # Do the throttling. if current_iter \u0026gt; throttle_after: print(f\u0026#34;Thottling after {throttle_after} iterations.\u0026#34;) return if __name__ == \u0026#34;__main__\u0026#34;: # AssertionError: Value of \u0026#39;current_iter\u0026#39; must be a positive # integer. throttle(current_iter=-2, throttle_after=1) So, instead of using the if not expression ... raise ValueError pattern, we can leverage assert expression, \u0026quot;Error message\u0026quot; pattern. In the latter case, assert will raise AssertionError with the \u0026ldquo;Error message\u0026rdquo; if the expression evaluates to a falsy value. Otherwise, the statement will remain silent and allow the execution to move forward.\nThis is more succinct and makes the code flatter. I\u0026rsquo;ve no idea why I haven\u0026rsquo;t started using it earlier and this assert usage in Starlette jolted my brain. Eh bien, better late than never, I guess.\nBreadcrumbs After this blog was published, several people mentioned on Twitter that the second approach has a small caveat. Python has a flag that allows you to disable assert statements in a script. You can disable the assertions in the snippet above by running the script with the -OO flag:\npython -00 src.py Removing assert statements will disable the constraints needed for the second throttle function to work, which could lead to unexpected behavior or even subtle bugs. However, I see this being used frequently in frameworks like Starlette and FastAPI. Also, from my experience, using assertions is much more common than running production code with the optimization flag.\n","permalink":"https://rednafi.com/python/apply-constraint-with-assert/","summary":"\u003cp\u003eWhenever I need to apply some runtime constraints on a value while building an API, I\nusually compare the value to an expected range and raise a \u003ccode\u003eValueError\u003c/code\u003e if it\u0026rsquo;s not within\nthe range. For example, let\u0026rsquo;s define a function that throttles some fictitious operation.\nThe \u003ccode\u003ethrottle\u003c/code\u003e function limits the number of times an operation can be performed by\nspecifying the \u003ccode\u003ethrottle_after\u003c/code\u003e parameter. This parameter defines the number of iterations\nafter which the operation will be halted. The \u003ccode\u003ecurrent_iter\u003c/code\u003e parameter tracks the current\nnumber of times the operation has been performed. Here\u0026rsquo;s the implementation:\u003c/p\u003e","title":"Apply constraints with 'assert' in Python"},{"content":"Whether I\u0026rsquo;m trying out a new tool or just prototyping with a familiar stack, I usually create a new project on GitHub and run all the experiments there. Some examples of these are:\nrubric: linter config initializer for Python exert: declaratively apply converter functions to class attributes hook-slinger: generic service to send, retry, and manage webhooks think-async: exploring cooperative concurrency primitives in Python epilog: container log aggregation with Elasticsearch, Kibana \u0026amp; Filebeat While many of these prototypes become full-fledged projects, most end up being just one-time journies. One common theme among all of these endeavors is that I always include instructions in the readme.md on how to get the project up and running — no matter how small it is. Also, I tend to configure a rudimentary CI pipeline that runs the linters and tests. GitHub Actions and Dependabot make it simple to configure a basic CI workflow. Dependabot keeps the dependencies fresh and makes pull requests automatically when there\u0026rsquo;s a new version of a dependency used in a project.\nThings can get quickly out of hand if you\u0026rsquo;ve got a large collection of repos where the automated CI runs periodically. Every now and then, I get a sizable volume of PRs in these fairly stale repos that I still want to keep updated. Merging these manually is a chore. Luckily, there are multiple ways to automatically merge PRs that GitHub offers. The workflow that is documented here is the one I happen to like the most. I also think that this process leads to the path of the least surprise. Instead of depending on a bunch of GitHub settings, we\u0026rsquo;ll write a GitHub Actions workflow to enable auto-merge to automate the process.\nFirst, you\u0026rsquo;ll need to turn on the auto-merge option from the repository settings. To do so, go to the repo\u0026rsquo;s settings tab and turn on the Allow auto-merge option from the Pull Requests section:\nNow, you probably don\u0026rsquo;t want to mindlessly merge every pull request Dependabot throws at you. You most likely want to make sure that a pull request triggers certain tests and it\u0026rsquo;ll be merged only if all of those checks pass. To do so, you can turn on branch protection. From the settings panel, select Branches on the left panel:\nOnce you\u0026rsquo;ve selected the tab, add a branch protection rule to the target branch against which Dependabot will send the pull requests:\nIn this case, I\u0026rsquo;m adding the protection layer to the main branch. I\u0026rsquo;ve turned on the Require status checks to pass before merging toggle and added the build step to the list of status checks that are required. Here, you can select any job from your CI files in the .github/workflows directory:\nOnce this is done, you can drop the following CI file in the .github/workflows directory of your repo. It\u0026rsquo;s the same automerge workflow file that\u0026rsquo;s currently living inside this site\u0026rsquo;s CI folder.\n# .github/workflows/automerge.yml name: Dependabot auto-merge on: pull_request permissions: contents: write pull-requests: write # Needed if in a private repository jobs: dependabot: runs-on: ubuntu-latest if: ${{ github.actor == \u0026#39;dependabot[bot]\u0026#39; }} steps: - name: Enable auto-merge for Dependabot PRs run: gh pr merge --auto --merge \u0026#34;$PR_URL\u0026#34; env: PR_URL: ${{github.event.pull_request.html_url}} # GitHub provides this variable in the CI env. You don\u0026#39;t # need to add anything to the secrets vault. GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} From now on, every time Dependabot sends a merge request, the checks will be triggered and if all the mandatory checks pass, the automerge.yml workflow will merge it into the target branch.\n","permalink":"https://rednafi.com/misc/automerge-dependabot-prs-on-github/","summary":"\u003cp\u003eWhether I\u0026rsquo;m trying out a new tool or just prototyping with a familiar stack, I usually\ncreate a new project on GitHub and run all the experiments there. Some examples of these\nare:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/rednafi/rubric\"\u003erubric\u003c/a\u003e: linter config initializer for Python\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/rednafi/exert\"\u003eexert\u003c/a\u003e: declaratively apply converter functions to class attributes\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/rednafi/hook-slinger\"\u003ehook-slinger\u003c/a\u003e: generic service to send, retry, and manage webhooks\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/rednafi/think-async\"\u003ethink-async\u003c/a\u003e: exploring cooperative concurrency primitives in Python\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/rednafi/epilog\"\u003eepilog\u003c/a\u003e: container log aggregation with Elasticsearch, Kibana \u0026amp; Filebeat\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhile many of these prototypes become full-fledged projects, most end up being just one-time\njournies. One common theme among all of these endeavors is that I always include\ninstructions in the \u003ccode\u003ereadme.md\u003c/code\u003e on how to get the project up and running — no matter how\nsmall it is. Also, I tend to configure a rudimentary CI pipeline that runs the linters and\ntests. GitHub Actions and \u003ca href=\"https://docs.github.com/en/code-security/dependabot/dependabot-security-updates/configuring-dependabot-security-updates\"\u003eDependabot\u003c/a\u003e make it simple to configure a basic CI workflow.\nDependabot keeps the dependencies fresh and makes pull requests automatically when there\u0026rsquo;s a\nnew version of a dependency used in a project.\u003c/p\u003e","title":"Automerge Dependabot PRs on GitHub"},{"content":"A common bottleneck for processing large data files is — memory. Downloading the file and loading the entire content is surely the easiest way to go. However, it\u0026rsquo;s likely that you\u0026rsquo;ll quickly hit OOM errors. Often time, whenever I have to deal with large data files that need to be downloaded and processed, I prefer to stream the content line by line and use multiple processes to consume them concurrently.\nFor example, say, you have a CSV file containing millions of rows with the following structure:\n--------------------------------------- | a | b | --------------------------------------- 0.902210680227088 | 0.236522024407207 | 0.424413804319515 | 0.400788559643378 | 0.601611774624256 | 0.4992389256938 | 0.332269908707654 | 0.72328094652184 | --------------------------------------- Here, let\u0026rsquo;s say you need to download the file from some source and run some other heavy tasks that depends on the data from the file. To avoid downloading the file to the disk, you can stream and read the content line by line directly from the network. While doing so, you may want to trigger multiple other tasks that can run independent of the primary process.\nAt my workplace, I often have to create objects in a relational database using the information in a CSV file. The idea here is to consume the information in the CSV file directly from the network and create the objects in the database. This database object creation task can be offloaded to a separate process outside of the main process that\u0026rsquo;s streaming the file contents.\nSince we\u0026rsquo;re streaming the content from the network line by line, there should be zero disk usage and minimal memory footprint. Also, to speed up the consumption, we\u0026rsquo;ll fork multiple OS processes. To put in concisely, we\u0026rsquo;ll need to perform the following steps:\nStream a single row from the target CSV file. Write the content of the row in an in-memory string buffer. Parse the file buffer with csv.DictReader. Collect the dict the contains the information of the parsed row. Yield the dict. Flush the buffer. Another process will collect the yielded dict and consume that outside of the main process. And continue the loop for the next row. The following snippet implements the workflow mentioned above:\n# src.py from __future__ import annotations import csv import io import multiprocessing as mp import time from operator import itemgetter from typing import Iterator, Mapping import httpx def stream_csv(url: str) -\u0026gt; Iterator[Mapping[str, str | int]]: \u0026#34;\u0026#34;\u0026#34;Return an iterator that yields a dict representing a single row of a CSV file. Args: url (str): URL that holds the CSV file Yields: Iterator[dict[str, str]]: Returns a generator that yields a dict. \u0026#34;\u0026#34;\u0026#34; with httpx.Client() as client: # Make a streaming HTTP request. with client.stream(\u0026#34;GET\u0026#34;, url, follow_redirects=True) as r: # Create instance of an in-memory file. We save the row # of the incoming CSV file here. f = io.StringIO() # The content of the source CSV file is iterated # line by line. lines = r.iter_lines() # Ignore the header row. This is the first row. next(lines) # Enumerate allows us to attach a line number to each row. # We start from two since header is the first line. for lineno, line in enumerate(lines, 2): # Write one line to the in-memory file. f.write(line) # Seek sends the file handle to the top of the file. f.seek(0) # We initiate a CSV reader to read and parse each line # of the CSV file reader = csv.DictReader(f, fieldnames=(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;)) # Since we know that there\u0026#39;s only one row in the reader # we just call \u0026#39;next\u0026#39; on it to get the parsed dict. # The row dict looks like this: # {\u0026#39;a\u0026#39;: \u0026#39;0.902210680227088\u0026#39;, \u0026#39;b\u0026#39;: \u0026#39;0.236522024407207\u0026#39;} row = next(reader) # Add a line number to the dict. It makes the dict looks # like this: # { # \u0026#39;a\u0026#39;: \u0026#39;0.902210680227088\u0026#39;, # \u0026#39;b\u0026#39;: \u0026#39;0.236522024407207\u0026#39;, # \u0026#39;lineno\u0026#39;: 2 # } row[\u0026#34;lineno\u0026#34;] = lineno # type: ignore # Yield the row. This allows us to call the function # in a lazy manner. yield row # The file handle needs to be set to the top before # cleaning up the buffer. f.seek(0) # Clean up the buffer. f.flush() def process_row(row: Mapping[str, str | int]) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Consume a single row and do some work. Args: row (dict[str, str]): Represents a single parsed row of a CSV file. \u0026#34;\u0026#34;\u0026#34; a, b = itemgetter(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;)(row) float_a, float_b = float(a), float(b) # Do some processing. print( f\u0026#34;Processed row {row[\u0026#39;lineno\u0026#39;]}:\u0026#34; f\u0026#34;a={float_a:.15f}, b={float_b:.15f}\u0026#34;, ) # Mimick some other heavy processing. time.sleep(2) if __name__ == \u0026#34;__main__\u0026#34;: # fmt: off csv_url = ( \u0026#34;https://github.com/rednafi/reflections/files\u0026#34; \\ \u0026#34;/9006167/foo.csv\u0026#34;, ) with mp.Pool(4) as pool: for res in pool.imap(process_row, stream_csv(csv_url)): pass The first function stream_csv accepts a URL that points to a CSV file. In this case, the URL used here points to a real CSV file hosted on GitHub. HTTPx allows you to make a streaming GET request and iterate through the contents of the file without fully downloading it to the disk.\nInside the client.stream block, we\u0026rsquo;ve created an in-memory file instance with io.StringIO. This allows us to write the streamed content of the source CSV file to the in-memory file. Then we pull one row from the source file, write it to the in-memory buffer, and pass the in-memory file buffer over to the csv.DictReader class.\nThe DictReader class will parse the content of the row and emit a reader object. Running next on the reader iterator returns a dictionary with the parsed content of the row. The parsed content for the first row of the example CSV looks like this:\n{ \u0026#34;a\u0026#34;: \u0026#34;0.902210680227088\u0026#34;, \u0026#34;b\u0026#34;: \u0026#34;0.236522024407207\u0026#34;, \u0026#34;lineno\u0026#34;: 1, } Next, the process_row function takes in the data of a single row as a dict like the one above and does some processing on that. For demonstration, currently, it just prints the values of the rows and then sleeps for two seconds.\nFinally, in the __main__ block, we fire up four processes to apply the process_row function to the output of the stream_csv function. Running the script will print the following output:\nProcessed row 2:a=0.902210680227088, b=0.236522024407207 Processed row 3:a=0.424413804319515, b=0.400788559643378 Processed row 4:a=0.601611774624256, b=0.499238925693800 Processed row 5:a=0.332269908707654, b=0.723280946521840 # Sleep 2 sec Processed row 6:a=0.024648655864128, b=0.585924680177486 Processed row 7:a=0.116178678991780, b=0.027524894156040 Processed row 8:a=0.313182023389972, b=0.373896338507016 Processed row 9:a=0.252893754537173, b=0.809821115129037 # Sleep 2 sec Processed row 10:a=0.770407022765901, b=0.021249180774146 ... ... ... Since we\u0026rsquo;re forking 4 processes, the script will print four items, and then it\u0026rsquo;ll pause roughly for 2 seconds before moving on. If we were using a single process, the script would wait for 2 seconds after printing every row. By increasing the number of processes, you can speed up the consumption rate. Also, if the consumer tasks are lightweight, you can open multiple threads to consume them.\n","permalink":"https://rednafi.com/python/stream-process-a-csv-file/","summary":"\u003cp\u003eA common bottleneck for processing large data files is — memory. Downloading the file and\nloading the entire content is surely the easiest way to go. However, it\u0026rsquo;s likely that you\u0026rsquo;ll\nquickly hit OOM errors. Often time, whenever I have to deal with large data files that need\nto be downloaded and processed, I prefer to stream the content line by line and use multiple\nprocesses to consume them concurrently.\u003c/p\u003e","title":"Stream process a CSV file in Python"},{"content":"I\u0026rsquo;ve rarely been able to take advantage of Django\u0026rsquo;s bulk_create / bulk_update APIs in production applications; especially in the cases where I need to create or update multiple complex objects with a script. Often time, these complex objects trigger a chain of signals or need non-trivial setups before any operations can be performed on each of them.\nThe issue is, bulk_create / bulk_update doesn\u0026rsquo;t trigger these signals or expose any hooks to run any setup code. The Django doc mentions these bulk_create caveats in detail. Here are a few of them:\nThe model\u0026rsquo;s save() method will not be called, and the pre_save and post_save signals will not be sent. It does not work with child models in a multi-table inheritance scenario. If the model\u0026rsquo;s primary key is an AutoField, the primary key attribute can only be retrieved on certain databases (currently PostgreSQL, MariaDB 10.5+, and SQLite 3.35+). On other databases, it will not be set. It does not work with many-to-many relationships. It casts objs to a list, which fully evaluates objs if it\u0026rsquo;s a generator. Here, obj is the iterable that passes the information necessary to create the database objects in a single go. To solve this, I wanted to take advantage of Python\u0026rsquo;s concurrent.futures module. It exposes a similar API for both thread-based and process-based concurrency. The snippet below creates ten thousand user objects in the database and runs some setup code before creating each object.\n# script.py from __future__ import annotations import os from typing import Iterable import django os.environ[\u0026#34;DJANGO_SETTINGS_MODULE\u0026#34;] = \u0026#34;mysite.settings\u0026#34; django.setup() from concurrent.futures import ProcessPoolExecutor from django.contrib.auth.models import User from tqdm import tqdm MAX_WORKERS = 4 def create_user_setup() -\u0026gt; None: # ... Run some heavy weight setup code here. pass def create_user(username: str, email: str) -\u0026gt; None: # ... Call complex setup code here. This allows the # setup code to run concurrently. create_user_setup() User.objects.create(username=username, email=email) def bulk_create_users(users: Iterable[dict[str, str]]) -\u0026gt; None: # A container for the pending future objects. futures = [] # With PostgreSQL, Psycopg2 often complains about closed cursors # and this fixes that. django.db.connections.close_all() with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor: for user in users: future = executor.submit(create_user, **user) futures.append(future) # Wait for all the futures to complete and give the # user a visual feedback with tqdm progressbar. for future in tqdm(futures): future.result() print(\u0026#34;done!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: users = ( { \u0026#34;username\u0026#34;: f\u0026#34;{i}\u0026#34;, \u0026#34;email\u0026#34;: f\u0026#34;{i}@{i}.com\u0026#34;, } for i in range(10_000) ) bulk_create_users(users=users) Here, the create_user_setup function runs some complex setup code before the creation of each user object. We wrap the user creation process in a function named create_user and call the setup code in that. This allows us to run the complex setup code concurrently. The magic happens in the bulk_create_users function. It takes in an iterable containing the information to create the users and runs the create_user functions concurrently.\nThe ProcessPoolExecutor forks 4 processes and starts consuming the iterable. We use the executor.submit method for maximum flexibility. This allows us to further process the returned value from the create_user function (in this case it\u0026rsquo;s None). Running this snippet will also show a progress bar as the processes start chewing through the work.\nYou can also try experimenting with ThreadPoolExecutor, executor.map, and chunksize. I didn\u0026rsquo;t choose executor.map because it\u0026rsquo;s tricky to show the progress bar with map. Also, I encountered some psycopg2 errors in a PostgreSQL database whenever I switched to the ThreadPoolExecutor. Another gotcha is that psycopg can complain about closed cursors and closing the database connection before running each process is a way to avoid that. Notice that the script above runs django.db.connections.close_all() before entering into the ProcessPoolExecutor context manager.\nThis appoach will run the pre_save and post_save signals which allows me to take advantage of these hooks without losing the ability of being able to perform concurrent row operations.\nBreadcrumbs Example shown here performs a trivial task of creating 10k user objects. In cases like this, you might find that a simple for-loop might be faster. Always run at least a rudimentary benchmark before adding concurrency to your workflow.\nAlso, this approach primarily targets ad-hoc scripts and tasks. I don\u0026rsquo;t recommend forking multiple processes in your views or forms since Python processes aren\u0026rsquo;t cheap.\nFurther reading concurrent.futures documentation ","permalink":"https://rednafi.com/python/django-bulk-operation-with-process-pool/","summary":"\u003cp\u003eI\u0026rsquo;ve rarely been able to take advantage of Django\u0026rsquo;s \u003ccode\u003ebulk_create / bulk_update\u003c/code\u003e APIs in\nproduction applications; especially in the cases where I need to create or update multiple\ncomplex objects with a script. Often time, these complex objects trigger a chain of signals\nor need non-trivial setups before any operations can be performed on each of them.\u003c/p\u003e\n\u003cp\u003eThe issue is, \u003ccode\u003ebulk_create / bulk_update\u003c/code\u003e doesn\u0026rsquo;t trigger these signals or expose any hooks\nto run any setup code. The Django doc mentions these \u003ca href=\"https://docs.djangoproject.com/en/dev/ref/models/querysets/#bulk-create\"\u003ebulk_create caveats\u003c/a\u003e in detail. Here\nare a few of them:\u003c/p\u003e","title":"Bulk operations in Django with process pool"},{"content":"I frequently have to write ad-hoc scripts that download a CSV file from AWS S3, do some processing on it, and then create or update objects in the production database using the parsed information from the file. In Python, it\u0026rsquo;s trivial to download any file from s3 via boto3, and then the file can be read with the csv module from the standard library. However, these scripts are usually run from a separate script server and I prefer not to clutter the server\u0026rsquo;s disk with random CSV files. Loading the s3 file directly into memory and reading its contents isn\u0026rsquo;t difficult but the process has some subtleties. I do this often enough to justify documenting the workflow here.\nAlong with boto3, we can leverage Python\u0026rsquo;s tempfile.NamedTemporaryFile to directly download the contents of the file to a temporary in-memory file. Afterward, we can do the processing, create the objects in the DB, and delete the file once we\u0026rsquo;re done. The NamedTemporaryFile class can be used as a context manager and it\u0026rsquo;ll delete the file automatically when the with block ends.\nThis is quite straightforward with a simple gotcha. Here\u0026rsquo;s how you\u0026rsquo;d usually download a file from s3 and save that to a file-like object:\n# src.py import boto3 s3 = boto3.client(\u0026#34;s3\u0026#34;) with open(\u0026#34;FILE_NAME\u0026#34;, \u0026#34;wb\u0026#34;) as f: s3.download_fileobj(\u0026#34;BUCKET_NAME\u0026#34;, \u0026#34;OBJECT_NAME\u0026#34;, f) Okay but the doc reminds us about this:\nThe download_fileobj method accepts a writeable file-like object. The file object must be opened in binary mode, not text mode.\nOpening the file in binary mode is an issue. The CSV reader needs the file to be opened in text mode. This is not an issue when you download the file to disk since you can open the file again in text mode to feed it to the CSV reader. However, we\u0026rsquo;re trying to avoid saving the file to disk and opening that again in text mode. So, you can\u0026rsquo;t do this:\n# src.py import boto3 import tempfile import csv s3 = boto3.client(\u0026#34;s3\u0026#34;) with tempfile.NamedTemporaryFile(\u0026#34;wb\u0026#34;) as f: s3.download_fileobj(\u0026#34;BUCKET_NAME\u0026#34;, \u0026#34;OBJECT_NAME\u0026#34;, f) # The csv file. This will raise an error since csv.DictReader # expects a file opened in text mode; not binary mode. csv_reader = csv.DictReader(f) for row in csv_reader: # ... do processing ... The above snippet won\u0026rsquo;t work because:\nThe file-like object is opened in binary mode but the csv.DictReader expects the file pointer to be opened in text mode. So, it\u0026rsquo;ll raise an error.\nEven if you fixed that, the CSV reader wouldn\u0026rsquo;t be able to read anything since the file currently only allows writing in binary mode, not reading.\nEven if you fixed the second issue, the content of the CSV file would be empty. That\u0026rsquo;s because after boto3 downloads and saves the file to the file object, it sets the file handle to the end of the file. So loading the content from there would result in an empty file. Here\u0026rsquo;s how I fixed all three of these problems:\n# src.py import boto3 import tempfile import csv import io BUCKET_NAME = \u0026#34;foo-bucket\u0026#34; OBJECT_NAME = \u0026#34;foo-file.csv\u0026#34; s3 = boto3.client(\u0026#34;s3\u0026#34;) # \u0026#39;w+b\u0026#39; allows both reading from and writing to the file. with tempfile.NamedTemporaryFile(\u0026#34;w+b\u0026#34;) as f: s3.download_fileobj(BUCKET_NAME, OBJECT_NAME, f) # This sets the file handle back to the beginning of the file. # Without this, the loaded file will show no content. f.seek(0) # Here, \u0026#39;io.TextIOWrapper\u0026#39; is converting the binary content of # the file to be compatible with text content. csv_reader = csv.DictReader( io.TextIOWrapper(f, encoding=\u0026#34;utf-8\u0026#34;), ) # Now you\u0026#39;re good to go. for row in csv_reader: # ... do processing ... You can see that the snippet first opens a temporary file in w+b mode which allows both binary read and write operations. Then it downloads the file from s3 and saves it to the file-like object.\nOnce the download is finished, the file handle is placed at the bottom of the file. So, we\u0026rsquo;ll need to call f.seek(0) to place the handle at the beginning of the file; otherwise, our read operation will yield no content. Also, since the currently opened file object only allows binary read and write operations, we\u0026rsquo;ll need to convert it to a text file object before passing it to the CSV reader. The io.TextIOWrapper class does exactly that. Once the file object is in text mode, we pass it to the CSV reader and do further processing.\nFurther reading How to use Python csv.DictReader with a binary file? ","permalink":"https://rednafi.com/python/read-s3-file-in-memory/","summary":"\u003cp\u003eI frequently have to write ad-hoc scripts that download a CSV file from \u003ca href=\"https://aws.amazon.com/s3/\"\u003eAWS S3\u003c/a\u003e, do some\nprocessing on it, and then create or update objects in the production database using the\nparsed information from the file. In Python, it\u0026rsquo;s trivial to download any file from s3 via\n\u003ca href=\"https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\"\u003eboto3\u003c/a\u003e, and then the file can be read with the \u003ccode\u003ecsv\u003c/code\u003e module from the standard library.\nHowever, these scripts are usually run from a separate script server and I prefer not to\nclutter the server\u0026rsquo;s disk with random CSV files. Loading the s3 file directly into memory\nand reading its contents isn\u0026rsquo;t difficult but the process has some subtleties. I do this\noften enough to justify documenting the workflow here.\u003c/p\u003e","title":"Read a CSV file from s3 without saving it to the disk"},{"content":"I run git log --oneline to list out the commit logs all the time. It prints out a compact view of the git history. Running the command in this repo gives me this:\nd9fad76 Publish blog on safer operator.itemgetter, closes #130 0570997 Merge pull request #129 from rednafi/dependabot/... 6967f73 Bump actions/setup-python from 3 to 4 48c8634 Merge pull request #128 from rednafi/dependabot/pip/mypy-0.961 5b7a7b0 Bump mypy from 0.960 to 0.961 However, there are times when I need to list out the commit logs that only represent the changes made to a particular file. Here\u0026rsquo;s the command that does exactly that.\ngit logs --oneline --follow \u0026lt;file_path\u0026gt; Running the command on the Markdown file that you\u0026rsquo;re currently reading prints out the following:\ngit log --oneline \\ --follow content/shell/distil_git_logs_attached_to_a_file.md 7a21b3d (HEAD -\u0026gt; master, origin/master, origin/HEAD) Nit, refs #132 6c08934 Publish distil git logs blog, refs #132 f5d2d4a Git log follow post, closes #132 Unfortunately, this command doesn\u0026rsquo;t support flag chaining. So, you can\u0026rsquo;t use the --follow flag multiple times to concatenate the logs for multiple files. But there\u0026rsquo;s a way to do it via shell command. Here\u0026rsquo;s how:\necho \u0026#34;\u0026lt;file_path_1\u0026gt; \u0026lt;file_path_2\u0026gt;\u0026#34; \\ | xargs -n1 \\ | xargs -I{} sh -c \u0026#34;git log --oneline --follow {}; echo ====\u0026#34; Running the command on two random files in this repo yields the following output:\necho \u0026#34;pelicanconf.py src.py\u0026#34; \\ | xargs -n1 \\ | xargs -I{} sh -c \u0026#34;git log --oneline --follow {}; echo ====\u0026#34; 96c0e8c Aesthetics, refs #131 e6d5409 Add default link-sharing image, closes #83 9ed958c SEO fba05d8 Add footer 8dec778 Transformation 4a402b3 Basic customizations 1c93c23 Add pelican conf ==== b89791f Fix bug in operator itemgetter implementation c75e2ab Push draft of post on typeguard, refs #87 0c6fc7b Add blacken docs to tool stack 20ac41d Publish amphibian decorators blog, closes #54 ==== Here, the first xargs is used to split the line and extract the two filenames. The second xargs applies the git log --oneline --follow command to the two files and concatenates the output with a ==== separator. The separator helps you figure out which output came from which file.\n","permalink":"https://rednafi.com/misc/distil-git-logs-attached-to-a-file/","summary":"\u003cp\u003eI run \u003ccode\u003egit log --oneline\u003c/code\u003e to list out the commit logs all the time. It prints out a compact\nview of the git history. Running the command in this repo gives me this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ed9fad76 Publish blog on safer operator.itemgetter, closes #130\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e0570997 Merge pull request #129 from rednafi/dependabot/...\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e6967f73 Bump actions/setup-python from 3 to 4\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e48c8634 Merge pull request #128 from rednafi/dependabot/pip/mypy-0.961\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e5b7a7b0 Bump mypy from 0.960 to 0.961\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHowever, there are times when I need to list out the commit logs that only represent the\nchanges made to a particular file. Here\u0026rsquo;s the command that does exactly that.\u003c/p\u003e","title":"Distil git logs attached to a single file"},{"content":"Python\u0026rsquo;s operator.itemgetter is quite versatile. It works on pretty much any iterables and map-like objects and allows you to fetch elements from them. The following snippet shows how you can use it to sort a list of tuples by the first element of the tuple:\nIn [2]: from operator import itemgetter ...: ...: l = [(10, 9), (1, 3), (4, 8), (0, 55), (6, 7)] ...: l_sorted = sorted(l, key=itemgetter(0)) In [3]: l_sorted Out[3]: [(0, 55), (1, 3), (4, 8), (6, 7), (10, 9)] Here, the itemgetter callable is doing the work of selecting the first element of every tuple inside the list and then the sorted function is using those values to sort the elements. Also, this is faster than using a lambda function and passing that to the key parameter to do the sorting:\nIn [6]: from operator import itemgetter In [7]: l = [(10, 9), (1, 3), (4, 8), (0, 55), (6, 7)] In [8]: %timeit sorted(l, key=itemgetter(0)) 386 ns ± 4.2 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each) In [9]: %timeit sorted(l, key=lambda x: x[0]) 498 ns ± 0.444 ns per loop (mean ± std. dev. of 7 runs) You can also use itemgetter to extract multiple values from a dictionary in a single pass. Consider this example:\nIn [13]: from operator import itemgetter In [14]: d = {\u0026#39;foo\u0026#39;: 31, \u0026#39;bar\u0026#39;: 12, \u0026#39;baz\u0026#39;: 42, \u0026#39;chez\u0026#39;: 83, \u0026#39;moi\u0026#39;: 24} In [15]: foo, bar, bazz = itemgetter(\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;, \u0026#39;baz\u0026#39;)(d) In [16]: foo, bar, bazz Out[16]: (31, 12, 42) So, instead of extracting the key-value pairs with d['foo'], d['bar'], ..., itemgetter allows us to make it DRY. The source code of the callable is freakishly simple. Here\u0026rsquo;s the entire thing:\n# operator.py class itemgetter: \u0026#34;\u0026#34;\u0026#34; Return a callable object that fetches the given item(s) from its operand. After f = itemgetter(2), the call f(r) returns r[2]. After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3]) \u0026#34;\u0026#34;\u0026#34; __slots__ = (\u0026#34;_items\u0026#34;, \u0026#34;_call\u0026#34;) def __init__(self, item, *items): if not items: self._items = (item,) def func(obj): return obj[item] self._call = func else: self._items = items = (item,) + items def func(obj): return tuple(obj[i] for i in items) self._call = func def __call__(self, obj): return self._call(obj) def __repr__(self): return \u0026#34;%s.%s(%s)\u0026#34; % ( self.__class__.__module__, self.__class__.__name__, \u0026#34;, \u0026#34;.join(map(repr, self._items)), ) def __reduce__(self): return self.__class__, self._items While this is all good and dandy, itemgetter will raise a KeyError if it can\u0026rsquo;t find the corresponding value against a key in a map or an IndexError if the provided index is outside of the range of the sequence. This is how it looks in a dict:\nIn [1]: from operator import itemgetter In [2]: d = {\u0026#39;foo\u0026#39;: 31, \u0026#39;bar\u0026#39;: 12, \u0026#39;baz\u0026#39;: 42, \u0026#39;chez\u0026#39;: 83, \u0026#39;moi\u0026#39;: 24} In [3]: # These keys don\u0026#39;t exist. In [4]: fizz, up = itemgetter(\u0026#39;fiz\u0026#39;, \u0026#39;up\u0026#39;)(d) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) Input In [4], in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 fizz, up = itemgetter(\u0026#39;fiz\u0026#39;, \u0026#39;up\u0026#39;)(d) KeyError: \u0026#39;fiz\u0026#39; In the above snippet, itemgetter can\u0026rsquo;t find the key fiz in the dict d and it complains when we try to fetch the value against it. In a sequence, the error looks like this:\nIn [5]: from operator import itemgetter In [6]: l = [(10, 9), (1, 3), (4, 8), (0, 55), (6, 7)] In [7]: # These indices don\u0026#39;t exist. In [8]: item_42, item_50 = itemgetter(42, 50)(l) --------------------------------------------------------------------------- IndexError Traceback (most recent call last) Input In [8], in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 item_42, item_50 = itemgetter(42, 50)(l) IndexError: list index out of range A more tolerant version of \u0026lsquo;operator.itemgetter\u0026rsquo; I wanted something that works similar to itemgetter but doesn\u0026rsquo;t raise these exceptions when it can\u0026rsquo;t find the key in a dict or the index of a sequence is out of range. Instead, it\u0026rsquo;d return a default value when these exceptions occur. So, to avoid KeyError in a map, it\u0026rsquo;d use d.get(key, default) instead of d[key] to fetch the value. Similarly, in a sequence, it\u0026rsquo;d first compare the length of the sequence with the index and return a default value if the index is out of range.\nSince operator.itemgetter is a class, we could inherit it and overwrite the __init__ method. However, your type-checker will complain if you do so. That\u0026rsquo;s because, in the stub file, the itemgetter class is decorated with the typing.final decorator and isn\u0026rsquo;t meant to be subclassed. So, our only option is to rewrite it. The good news is that this implementation is quite terse just like the original. Here it goes:\n# src.py from collections.abc import Mapping class _Nothing: \u0026#34;\u0026#34;\u0026#34;Works as a sentinel value.\u0026#34;\u0026#34;\u0026#34; def __repr__(self): return \u0026#34;\u0026lt;NOTHING\u0026gt;\u0026#34; _NOTHING = _Nothing() class safe_itemgetter: \u0026#34;\u0026#34;\u0026#34; Return a callable object that fetches the given item(s) from its operand. \u0026#34;\u0026#34;\u0026#34; __slots__ = (\u0026#34;_items\u0026#34;, \u0026#34;_call\u0026#34;) def __init__(self, item, *items, default=_NOTHING): if not items: self._items = (item,) def func(obj): if isinstance(obj, Mapping): return obj.get(item, default) if (item \u0026gt; 0 and len(obj) \u0026lt;= item) or ( item \u0026lt; 0 and len(obj) \u0026lt; abs(item) ): return default return obj[item] self._call = func else: self._items = items = (item,) + items def func(obj): if isinstance(obj, Mapping): get = obj.get # Reduce attibute search call. return tuple(get(i, default) for i in items) return tuple( default if (i \u0026gt; 0 and len(obj) \u0026lt;= i) or (i \u0026lt; 0 and len(obj) \u0026lt; abs(i)) else obj[i] for i in items ) self._call = func # ----------------- same as operator.itemgetter --------------# def __call__(self, obj): return self._call(obj) def __repr__(self): return \u0026#34;%s.%s(%s)\u0026#34; % ( self.__class__.__module__, self.__class__.__name__, \u0026#34;, \u0026#34;.join(map(repr, self._items)), ) def __reduce__(self): return self.__class__, self._items This class behaves almost the same way as the original itemgetter function. The only difference is that you can pass a default value to return instead of raising KeyError/IndexError depending on the type of the container. Let\u0026rsquo;s try it out with a dict:\nIn [12]: d = {\u0026#39;foo\u0026#39;: 31, \u0026#39;bar\u0026#39;: 12, \u0026#39;baz\u0026#39;: 42, \u0026#39;chez\u0026#39;: 83, \u0026#39;moi\u0026#39;: 24} In [13]: safe_itemgetter(-5, -3, -33, \u0026#39;baz\u0026#39;, 1)(d) Out[13]: (\u0026lt;NOTHING\u0026gt;, \u0026lt;NOTHING\u0026gt;, \u0026lt;NOTHING\u0026gt;, 42, \u0026lt;NOTHING\u0026gt;) Here, we\u0026rsquo;re trying to access a bunch of keys that don\u0026rsquo;t exist in the dict d and we want to do this without raising any exceptions. You can see that instead of raising an exception, safe_itemgetter returns a tuple containing the value(s) that it can find and the rest of the positions are filled with the default value; in this case, the \u0026lt;NOTHING\u0026gt; sentinel. We can pass any default value there:\nIn[14]: safe_itemgetter(-5, -3, -33, \u0026#34;baz\u0026#34;, 1, default=\u0026#34;default\u0026#34;)(d) Out[14]: (\u0026#34;default\u0026#34;, \u0026#34;default\u0026#34;, \u0026#34;default\u0026#34;, 42, \u0026#34;default\u0026#34;) This works similarly when a sequence is passed:\nIn[18]: l = [(10, 9), (1, 3), (4, 8), (0, 55), (6, 7)] In[19]: safe_itemgetter(-11, default=())(l) Out[19]: () This returns an empty tuple when the sequence index is out of range. It works with multiple indices as well:\nIn [28]: l = [(10, 9), (1, 3), (4, 8), (0, 55), (6, 7)] In [29]: safe_itemgetter(-1, -3, -7, 1)(l) Out[29]: ((6, 7), (4, 8), \u0026lt;NOTHING\u0026gt;, (1, 3)) Further reading operator.itemgetter - Python docs ","permalink":"https://rednafi.com/python/operators-itemgetter/","summary":"\u003cp\u003ePython\u0026rsquo;s \u003ccode\u003eoperator.itemgetter\u003c/code\u003e is quite versatile. It works on pretty much any iterables and\nmap-like objects and allows you to fetch elements from them. The following snippet shows how\nyou can use it to sort a list of tuples by the first element of the tuple:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eIn\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e \u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003eoperator\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eitemgetter\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e   \u003cspan class=\"o\"\u003e...\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e   \u003cspan class=\"o\"\u003e...\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003el\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[(\u003c/span\u003e\u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e9\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e55\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e6\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e7\u003c/span\u003e\u003cspan class=\"p\"\u003e)]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e   \u003cspan class=\"o\"\u003e...\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003el_sorted\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003esorted\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003el\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eitemgetter\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eIn\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e \u003cspan class=\"n\"\u003el_sorted\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eOut\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e \u003cspan class=\"p\"\u003e[(\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e55\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e6\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e7\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e9\u003c/span\u003e\u003cspan class=\"p\"\u003e)]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHere, the \u003ccode\u003eitemgetter\u003c/code\u003e callable is doing the work of selecting the first element of every\ntuple inside the list and then the \u003ccode\u003esorted\u003c/code\u003e function is using those values to sort the\nelements. Also, this is faster than using a lambda function and passing that to the \u003ccode\u003ekey\u003c/code\u003e\nparameter to do the sorting:\u003c/p\u003e","title":"Safer 'operator.itemgetter' in Python"},{"content":"Nested conditionals suck. They\u0026rsquo;re hard to write and even harder to read. I\u0026rsquo;ve rarely regretted the time I\u0026rsquo;ve spent optimizing for the flattest conditional structure in my code. The following piece mimics the actions of a traffic signal:\n// src.ts enum Signal { YELLOW = \u0026#34;Yellow\u0026#34;, RED = \u0026#34;Red\u0026#34;, GREEN = \u0026#34;Green\u0026#34;, } function processSignal(signal: Signal) :void { if (signal === Signal.YELLOW) { console.log(\u0026#34;Slow down!\u0026#34;); } else { if (signal === Signal.RED) { console.log(\u0026#34;Stop!\u0026#34;); } else { if (signal === Signal.GREEN) { console.log(\u0026#34;Go!\u0026#34;); } } } } // Log processSignal(Signal.YELLOW) // prints \u0026#39;Slow down!\u0026#39; processSignal(Signal.RED) // prints \u0026#39;Stop!\u0026#39; The snippet above suffers from two major issues:\nIt contains three contiguous levels of nested conditionals. The conditionals don\u0026rsquo;t cover the case where the return value is undefined. If you add a fourth member to the Signal enum, now the processing function doesn\u0026rsquo;t exhaustively cover all the cases and it won\u0026rsquo;t communicate that fact with you. We can leverage guard clauses to fix the first two issues.\nThe guard (clause) provides an early exit from a subroutine, and is a commonly used deviation from structured programming, removing one level of nesting and resulting in flatter code: replacing if guard { ... } with if not guard: return; \u0026hellip;\nWe can rewrite the earlier snippet as follows:\n// ...snip... function processSignal(signal: Signal) { if (signal === Signal.YELLOW) { return \u0026#34;Slow down!\u0026#34;; } if (signal === Signal.RED) { return \u0026#34;Stop!\u0026#34;; } if (signal === Signal.GREEN) { return \u0026#34;Go!\u0026#34;; } else { return \u0026#34;Not a valid input!\u0026#34;; } } This model has a flatter structure and now it\u0026rsquo;s gracefully handling the undefined return path. However, the third issue still persists. In an alien world, if someone added a fourth member to the Signal enum, that\u0026rsquo;d make the conditional flow in the processSignal function incomplete since it wouldn\u0026rsquo;t be covering that newly added fourth enum member. In that case, the above snippet will execute the final catch-all conditional statement; not something that we\u0026rsquo;d want.\nTypeScript provides a never type to throw a compilation error if a new member isn\u0026rsquo;t covered by the conditional flow. Here\u0026rsquo;s how you\u0026rsquo;d leverage it:\n// src.ts enum Signal { YELLOW = \u0026#34;Yellow\u0026#34;, RED = \u0026#34;Red\u0026#34;, GREEN = \u0026#34;Green\u0026#34;, PURPLE = \u0026#34;Purple\u0026#34;, // Newly added member. } function assertNever(value: never) { throw Error(`Invalid value: ${value}`); } function processSignal(signal: Signal) { if (signal === Signal.YELLOW) { return \u0026#34;Slow down!\u0026#34;; } if (signal === Signal.RED) { return \u0026#34;Stop!\u0026#34;; } if (signal === Signal.GREEN) { return \u0026#34;Go!\u0026#34;; } // Try commenting out this line and typescript compiler // will throw an error. if (signal === Signal.PURPLE) { return \u0026#34;Go faster!\u0026#34;; } assertNever(signal); } processSignal(Signal.PURPLE); Ideally, the assertNever should never be called. Try removing a conditional and see how TypeScript starts screaming at you regarding the unhandled case. The assertNever function will also raise a runtime error if any case remains unhandled.\nExample in Python The same idea can be demonstrated in Python using Python3.10\u0026rsquo;s match statement and typing.NoReturn type.\n# src.py (Python 3.10+) from __future__ import annotations from enum import Enum from typing import NoReturn class Signal(str, Enum): YELLOW = \u0026#34;Yellow\u0026#34; RED = \u0026#34;Red\u0026#34; GREEN = \u0026#34;Green\u0026#34; PURPLE = \u0026#34;Purple\u0026#34; def assert_never(value: NoReturn) -\u0026gt; NoReturn: raise AssertionError(f\u0026#34;Invalid value: {value!r}\u0026#34;) def process_signal(signal: Signal) -\u0026gt; str: match signal: case Signal.YELLOW: return \u0026#34;Slow down!\u0026#34; case Signal.RED: return \u0026#34;Stop!\u0026#34; case Signal.GREEN: return \u0026#34;Go!\u0026#34; # Try commenting out this line and mypy will throw # an error. case Signal.PURPLE: return \u0026#34;Go faster!\u0026#34; case _: assert_never(signal) if __name__ == \u0026#34;__main__\u0026#34;: print(process_signal(Signal.PURPLE)) Similar to TypeScript, mypy will complain if you add a new member to the enum but forget to handle that in the processor function. Python 3.11 added the Never type and assert_never function to the typing module. Underneath, Never is an alias to the NoReturn type; so you can use them interchangeably. However, in this case, Never seems to communicate the intent better. You may also choose to use the backported versions of the type and function from the typing_extensions module. Here\u0026rsquo;s how:\n# src.py from __future__ import annotations import sys from enum import Enum if sys.version_info \u0026lt; (3, 11): from typing_extensions import assert_never else: from typing import assert_never ... Further reading Guard clause, guard code, or guard statement Never type in TypeScript Unreachable code and exhaustiveness checking in Python ","permalink":"https://rednafi.com/typescript/guard-clauses-and-never-type/","summary":"\u003cp\u003eNested conditionals suck. They\u0026rsquo;re hard to write and even harder to read. I\u0026rsquo;ve rarely\nregretted the time I\u0026rsquo;ve spent optimizing for the flattest conditional structure in my code.\nThe following piece mimics the actions of a traffic signal:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-ts\" data-lang=\"ts\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// src.ts\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kr\"\u003eenum\u003c/span\u003e \u003cspan class=\"nx\"\u003eSignal\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nx\"\u003eYELLOW\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Yellow\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nx\"\u003eRED\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Red\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nx\"\u003eGREEN\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Green\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunction\u003c/span\u003e \u003cspan class=\"nx\"\u003eprocessSignal\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003esignal\u003c/span\u003e: \u003cspan class=\"kt\"\u003eSignal\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e:\u003c/span\u003e\u003cspan class=\"k\"\u003evoid\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003esignal\u003c/span\u003e \u003cspan class=\"o\"\u003e===\u003c/span\u003e \u003cspan class=\"nx\"\u003eSignal\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eYELLOW\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nx\"\u003econsole\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003elog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Slow down!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"p\"\u003e}\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003esignal\u003c/span\u003e \u003cspan class=\"o\"\u003e===\u003c/span\u003e \u003cspan class=\"nx\"\u003eSignal\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eRED\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"nx\"\u003econsole\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003elog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Stop!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e}\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003esignal\u003c/span\u003e \u003cspan class=\"o\"\u003e===\u003c/span\u003e \u003cspan class=\"nx\"\u003eSignal\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eGREEN\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"nx\"\u003econsole\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003elog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Go!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Log\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eprocessSignal\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eSignal\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eYELLOW\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"c1\"\u003e// prints \u0026#39;Slow down!\u0026#39;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nx\"\u003eprocessSignal\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eSignal\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eRED\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"c1\"\u003e// prints \u0026#39;Stop!\u0026#39;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe snippet above suffers from two major issues:\u003c/p\u003e","title":"Guard clause and exhaustiveness checking"},{"content":"While working on a project with EdgeDB and FastAPI, I wanted to perform health checks against the FastAPI server in the GitHub CI. This would notify me about the working state of the application. The idea is to:\nRun the server in the background. Run the commands against the server that\u0026rsquo;ll denote that the app is in a working state. Perform cleanup. Exit with code 0 if the check is successful, else exit with code 1. The following shell script demonstrates a similar workflow with a Python HTTP server. This script:\nRuns a Python web server in the background. Makes an HTTP request against the server and checks if it returns HTTP 200 (OK). If the request fails or the server isn\u0026rsquo;t ready then waits for a second and makes the request again, and keeps retrying for the next 20 times before giving up. Performs cleanups and kills the Python processes. Exit with code 0 if the request is successful, else exit with code 1. #!/bin/bash set -euo pipefail # Run the Python server in the background. nohup python3 -m http.server 5000 \u0026gt;\u0026gt; /dev/null \u0026amp; # Give the server enough time to be ready before accepting requests. c=20 while [[ $c != 0 ]] do # Run the healthcheck. if [[ $(curl -I http://localhost:5000/ 2\u0026gt;\u0026amp;1) =~ \u0026#34;200 OK\u0026#34; ]]; then echo \u0026#34;Health check passed!\u0026#34; # ...do additional cleanups if required. pkill -9 -i python exit 0 fi ((c--)) echo \u0026#34;Server isn\u0026#39;t ready. Retrying...\u0026#34; $c sleep 1 done echo \u0026#34;Health check failed!\u0026#34; # ...do additional cleanups if required. pkill -9 -i python exit 1 The nohup before the python3 -m http.server 5000 makes sure that the SIGHUP signal can\u0026rsquo;t reach the server and shut down the process. The ampersand \u0026amp; after the command runs the process in the background. Afterward, the script starts making requests to the http://localhost:5000/ URL in a loop. If the server returns HTTP 200, the health check is considered successful. This will break the loop and the script will be terminated with exit 0 status. If the server doesn\u0026rsquo;t return HTTP 200 or isn\u0026rsquo;t ready yet, the script will keep retrying 20 times with a 1 second interval between each subsequent request before giving up. A failed health check will cause the script to terminate with exit 1 status.\nFurther reading Difference between nohup and ampersand ","permalink":"https://rednafi.com/misc/health-check-a-server-with-nohup/","summary":"\u003cp\u003eWhile working on a project with \u003ca href=\"https://www.edgedb.com/\"\u003eEdgeDB\u003c/a\u003e and \u003ca href=\"https://fastapi.tiangolo.com/\"\u003eFastAPI\u003c/a\u003e, I wanted to perform health checks\nagainst the FastAPI server in the GitHub CI. This would notify me about the working state of\nthe application. The idea is to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRun the server in the background.\u003c/li\u003e\n\u003cli\u003eRun the commands against the server that\u0026rsquo;ll denote that the app is in a working state.\u003c/li\u003e\n\u003cli\u003ePerform cleanup.\u003c/li\u003e\n\u003cli\u003eExit with code 0 if the check is successful, else exit with code 1.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe following shell script demonstrates a similar workflow with a Python HTTP server. This\nscript:\u003c/p\u003e","title":"Health check a server with 'nohup $(cmd) \u0026'"},{"content":"At my workplace, we have a large Django monolith that powers the main website and works as the primary REST API server at the same time. We use Django Rest Framework (DRF) to build and serve the API endpoints. This means, whenever there\u0026rsquo;s an error, based on the incoming request header — we\u0026rsquo;ve to return different formats of error responses to the website and API users.\nThe default DRF configuration returns a JSON response when the system experiences an HTTP 400 (bad request) error. However, the server returns an HTML error page to the API users whenever HTTP 403 (forbidden), HTTP 404 (not found), or HTTP 500 (internal server error) occurs. This is suboptimal; JSON APIs should never return HTML text whenever something goes wrong. On the other hand, the website needs those error text to appear accordingly.\nThis happens because 403, 404, and 500 are handled by Django\u0026rsquo;s default handlers for those errors and not by DRF\u0026rsquo;s exception handlers. As the DRF doc suggests on generic error views, overriding the error handlers is one way of solving it. But this will only work if the application is an API-only backend or if you haven\u0026rsquo;t already overridden the error handlers for custom error pages.\nIn our case, we already had to override the default error handlers to display custom error pages on the website. These custom pages would bleed into the API endpoints occasionally when errors occur. So, I thought, if I could handle this in the middleware layer, that\u0026rsquo;d be cleaner than most of the solutions that I\u0026rsquo;d seen at that point.\nSolution To fix the dilemma, I wrote a middleware called JSONErrorMiddleware that returns the expected response based on the content type in the request header. If the header has Content-Type: html/text and it experiences an error, the server returns an appropriate HTML page. On the contrary, if the incoming request header has Content-Type: application/json and the server sees an error, it responds with a JSON error payload instead. Here\u0026rsquo;s how the middleware looks:\n# \u0026lt;app\u0026gt;/middleware.py from http import HTTPStatus class JSONErrorMiddleware: \u0026#34;\u0026#34;\u0026#34;Without this middleware, APIs would respond with html/text whenever there\u0026#39;s an error.\u0026#34;\u0026#34;\u0026#34; def __init__(self, get_response): self.get_response = get_response self.status_code_description = { v.value: v.description for v in HTTPStatus } def __call__(self, request): response = self.get_response(request) # If the content_type isn\u0026#39;t \u0026#39;application/json\u0026#39;, do nothing. if not request.content_type == \u0026#34;application/json\u0026#34;: return response # If there\u0026#39;s no error, let Django and DRF\u0026#39;s default views deal # with it. status_code = response.status_code if ( not HTTPStatus.BAD_REQUEST \u0026lt; status_code \u0026lt;= HTTPStatus.INTERNAL_SERVER_ERROR ): return response # Return a JSON error response if any of 403, 404, or 500 occurs. r = JsonResponse( { \u0026#34;error\u0026#34;: { \u0026#34;status_code\u0026#34;: status_code, \u0026#34;message\u0026#34;: self.status_code_description[status_code], \u0026#34;detail\u0026#34;: {\u0026#34;url\u0026#34;: request.get_full_path()}, } }, ) r.status_code = response.status_code return r You\u0026rsquo;ll have to add this middleware to the list of middlewares in the settings.py file:\nMIDDLEWARE = [..., \u0026#34;\u0026lt;app\u0026gt;.middleware.JSONErrorMiddleware\u0026#34;] And voila, now the API and non-API errors will be handled differently as expected!\nTest Here\u0026rsquo;s how you can unit test the behavior of the middleware:\nimport json from unittest.mock import MagicMock from django.http import JsonResponse from django.test import RequestFactory, TestCase, override_settings from main.middleware import JSONErrorMiddleware @override_settings( MIDDLEWARE_CLASSES=(\u0026#34;main.middleware.JSONErrorMiddleware\u0026#34;,), ) class TestJSONErrorMiddleware(TestCase): def setUp(self): super().setUp() self.factory = RequestFactory() def get_response(request): response = MagicMock() response.status_code = HTTPStatus.FORBIDDEN return response self.middleware = JSONErrorMiddleware(get_response) def test_json_error_middleware(self): # Arrange corrupted_url = \u0026#34;/account\u0026#34; # Act request = self.factory.get( path=corrupted_url, ) request.content_type = \u0026#34;application/json\u0026#34; response = self.middleware.__call__(request) # Assert # Assert 404 no longer returns html/text. self.assertTrue(isinstance(response, JsonResponse)) # Assert json format. json_data = json.loads(response.content) expected_json_data = { \u0026#34;error\u0026#34;: { \u0026#34;status_code\u0026#34;: HTTPStatus.FORBIDDEN, \u0026#34;message\u0026#34;: HTTPStatus.FORBIDDEN.description, \u0026#34;detail\u0026#34;: {\u0026#34;url\u0026#34;: \u0026#34;/account\u0026#34;}, } } for k, v in json_data[\u0026#34;error\u0026#34;].items(): self.assertEqual(v, expected_json_data[\u0026#34;error\u0026#34;][k]) Breadcrumbs This workflow has been tested on Django 3.2, 4.0, and DRF 3.13.\nFurther reading HTML sometimes returned when Accept: application/json is provided #3362 Added generic 500 and 400 JSON error handlers #5904 ","permalink":"https://rednafi.com/python/return-json-error-payload-in-drf/","summary":"\u003cp\u003eAt my workplace, we have a large Django monolith that powers the main website and works as\nthe primary REST API server at the same time. We use Django Rest Framework (DRF) to build\nand serve the API endpoints. This means, whenever there\u0026rsquo;s an error, based on the incoming\nrequest header — we\u0026rsquo;ve to return different formats of error responses to the website and API\nusers.\u003c/p\u003e\n\u003cp\u003eThe default DRF configuration returns a JSON response when the system experiences an HTTP\n400 (bad request) error. However, the server returns an HTML error page to the API users\nwhenever HTTP 403 (forbidden), HTTP 404 (not found), or HTTP 500 (internal server error)\noccurs. This is suboptimal; JSON APIs should never return HTML text whenever something goes\nwrong. On the other hand, the website needs those error text to appear accordingly.\u003c/p\u003e","title":"Return JSON error payload instead of HTML text in DRF"},{"content":"Generators can help you decouple the production and consumption of iterables — making your code more readable and maintainable. I learned this trick a few years back from David Beazley\u0026rsquo;s Generator tricks for systems programmers slides. Consider this example:\n# src.py from __future__ import annotations import time from typing import NoReturn def infinite_counter(start: int, step: int) -\u0026gt; NoReturn: i = start while True: time.sleep(1) # Not to flood stdout print(i) i += step infinite_counter(1, 2) # Prints # 1 # 3 # 5 # ... Now, how\u0026rsquo;d you decouple the print statement from the infinite_counter? Since the function never returns, you can\u0026rsquo;t collect the outputs in an iterable, return the container, and print the elements of the iterable in another function. You might be wondering why would you even need to do it. I can think of two reasons:\nThe infinite_counter function is the producer of the numbers and the print function is consuming them. These are two separate responsibilities tangled in the same function which violates the Single Responsibility Principle.\nWhat\u0026rsquo;d you do if you needed a version of the infinite counter where the consumer had different behavior?\nOne way the second point can be addressed is — by accepting the consumer function as a parameter and applying that to the produced value.\n# src.py from __future__ import annotations import time # In Python \u0026lt; 3.9, import this from the \u0026#39;typing\u0026#39; module. from collections.abc import Callable from typing import NoReturn def infinite_counter( start: int, step: int, consumer: Callable = print ) -\u0026gt; NoReturn: i = start while True: time.sleep(1) # Not to flood stdout consumer(i) i += step infinite_counter(1, 2) # Prints # 1 # 3 # 5 # ... You can override the value of consumer with any callable and make the function more flexible. However, applying multiple consumers will still be hairy. Doing this with generators is cleaner. Here\u0026rsquo;s how you\u0026rsquo;d transform the above script to take advantage of generators:\n# src.py from __future__ import annotations import time # In Python \u0026lt; 3.9, import this from the \u0026#39;typing\u0026#39; module. from collections.abc import Generator # Producer. def infinite_counter(start: int, step: int) -\u0026gt; Generator[int, None, None]: i = start while True: time.sleep(1) # Not to flood stdout yield i i += step # Consumer. This can be a callable doing anything. def infinite_printer(gen: Generator[int, None, None]) -\u0026gt; None: for i in gen: print(i) gen = infinite_counter(1, 2) infinite_printer(gen) # Prints # 1 # 3 # 5 # ... The infinite_counter returns a generator that can lazily be iterated to produce the numbers and you can call any arbitrary consumer on the generated result without coupling it with the producer.\nWriting a workflow that mimics \u0026rsquo;tail -f' In a UNIX system, you can call tail -f \u0026lt;filename\u0026gt; | grep \u0026lt;pattern\u0026gt; to print the lines of a file in real-time where the lines match a specific pattern. Running the following command on my terminal allows me to tail the syslog file and print out any line that contains the word xps:\ntail -f /var/logs/syslog | grep xps Apr 3 04:42:21 xps slack.desktop[4613]: [04/03/22, 04:42:21:859] ... If you look carefully, the above command has two parts. The tail -f \u0026lt;filename\u0026gt; returns the new lines appended to the file and grep \u0026lt;pattern\u0026gt; consumes the new lines to look for a particular pattern. This behavior can be mimicked via generators as follows:\n# src.py from __future__ import annotations import os import time # In Python \u0026lt; 3.9, import this from the \u0026#39;typing\u0026#39; module. from collections.abc import Generator # Producer. def tail_f(filepath: str) -\u0026gt; Generator[str, None, None]: file = open(filepath) file.seek(0, os.SEEK_END) # End-of-file while True: line = file.readline() if not line: time.sleep(0.001) # Sleep briefly continue yield line # Consumer. def grep( lines: Generator[str, None, None], pattern: str | None = None ) -\u0026gt; None: for line in gen: if not pattern: print(line) else: if not pattern in line: continue print(line, flush=True) lines = tail_f(filepath=\u0026#34;/var/log/syslog\u0026#34;) grep(lines, \u0026#34;xps\u0026#34;) # Prints # Apr 3 04:42:21 xps slack.desktop[4613]: [04/03/22, 04:42:21:859] # info: Store: SET_SYSTEM_IDLE idle Here, the tail_f continuously yields the logs, and the grep function looks for the pattern xps in the logs. Replacing grep with any other processing function is trivial as long as it accepts a generator. The tail_f function doesn\u0026rsquo;t know anything about the existence of grep or any other consumer function.\nContinuously polling a database and consuming the results This concept of polling a log file for new lines can be extended to databases and caches as well. I was working on a microservice that polls a Redis queue at a steady interval and processes the elements one by one. I took advantage of generators to decouple the function that collects the data and the one that processes the data. Here\u0026rsquo;s how it works:\n# src.py from __future__ import annotations # In Python \u0026lt; 3.9, import this from the \u0026#39;typing\u0026#39; module. from collections.abc import Generator import redis # Requires pip install Data = Generator[tuple[bytes, bytes], None, None] def collect(queue_name: str) -\u0026gt; Data: r = redis.Redis() while True: yield r.brpop(queue_name) def process(data: Data) -\u0026gt; None: for datum in data: queue_name, content = datum[0].decode(), datum[1].decode() print(f\u0026#34;{queue_name=}, {content=}\u0026#34;) data = collect(\u0026#34;default\u0026#34;) process(data) You\u0026rsquo;ll need to run an instance of Redis server and Redis CLI to test this out. If you\u0026rsquo;ve got Docker installed in your system, then you can run docker run -it redis to quickly spin up a Redis instance. Afterward, run the above script and start the CLI. Print the following command on the CLI prompt:\n127.0.0.1:6379\u0026gt; lpush default hello world The above script should print the following:\nqueue_name=\u0026#39;default\u0026#39;, content=\u0026#39;hello\u0026#39; queue_name=\u0026#39;default\u0026#39;, content=\u0026#39;world\u0026#39; This allows you to define multiple consumers and run them in separate threads/processes without the producer ever knowing about their existence at all.\n","permalink":"https://rednafi.com/python/decouple-with-generators/","summary":"\u003cp\u003eGenerators can help you decouple the production and consumption of iterables — making your\ncode more readable and maintainable. I learned this trick a few years back from David\nBeazley\u0026rsquo;s \u003ca href=\"https://www.dabeaz.com/generators/Generators.pdf\"\u003eGenerator tricks for systems programmers\u003c/a\u003e slides. Consider this example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003e__future__\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eannotations\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003etime\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etyping\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eNoReturn\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003einfinite_counter\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003estart\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003estep\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003eNoReturn\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ei\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003estart\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ewhile\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003etime\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esleep\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# Not to flood stdout\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003ei\u003c/span\u003e \u003cspan class=\"o\"\u003e+=\u003c/span\u003e \u003cspan class=\"n\"\u003estep\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003einfinite_counter\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Prints\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# ...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNow, how\u0026rsquo;d you decouple the print statement from the \u003ccode\u003einfinite_counter\u003c/code\u003e? Since the function\nnever returns, you can\u0026rsquo;t collect the outputs in an iterable, return the container, and print\nthe elements of the iterable in another function. You might be wondering why would you even\nneed to do it. I can think of two reasons:\u003c/p\u003e","title":"Decoupling producers and consumers of iterables with generators in Python"},{"content":"In CPython, elements of a list are stored as pointers to the elements rather than the values of the elements themselves. This is evident from the list struct in CPython that represents a list in C:\n// Fetched from CPython main branch. Removed comments for brevity. typedef struct { PyObject_VAR_HEAD PyObject **ob_item; /* Pointer reference to the element. */ Py_ssize_t allocated; }PyListObject; An empty list builds a PyObject and occupies some memory:\nfrom sys import getsizeof l = [] print(getsizeof(l)) This returns:\n56 The exact size of an empty list can vary across different Python versions and implementations.\nA single pointer to an element requires 8 bytes of space in a list. Whenever additional elements are added to the list, Python dynamically allocates extra memory to accommodate future elements without resizing the container. This implies, adding a single element to an empty list will incite Python to allocate more memory than 8 bytes.\nLet\u0026rsquo;s put this to test and append some elements to the list:\n# src.py from sys import getsizeof l = [] l.append(0) print(getsizeof(l)) This returns:\n88 Wait, the size of l should have been 64 bytes (56+8) but instead, it increased to 88 bytes. This happens because in this case, Python over-allocated 32 extra bytes to accommodate future incoming elements. Now, if you append 3 more elements to the list, you\u0026rsquo;ll see that it doesn\u0026rsquo;t increase the size because no re-allocation is happening here:\n# src.py from sys import getsizeof l = [] l.append(0) l.append(1) l.append(2) l.append(3) print(getsizeof(l)) This prints:\n88 Adding a fifth element to the above list will increase the size of the list by 32 bytes (can be different in other implementations) again:\n# src.py from sys import getsizeof l = [] for i in range(6): l.append(i) print(getsizeof(l)) 120 This dynamic memory allocation makes lists so flexible, and since a list only holds references to the elements, it can house heterogenous objects without any issue. But this flexibility of being able to append any number of elements — without ever caring about memory allocation — comes at the cost of slower execution time.\nAlthough usually, you don\u0026rsquo;t need to think about optimizing this at all, there\u0026rsquo;s a way that allows you to perform static pre-allocation of memory in a list instead of letting Python perform dynamic allocation for you. This way, you can make sure that Python won\u0026rsquo;t have to perform dynamic memory allocation multiple times as your list grows.\nStatic pre-allocation will make your code go slightly faster. I had to do this once in a tightly nested loop and the 10% performance improvement was significant for the service that I was working on.\nPre-allocating memory in a list Let\u0026rsquo;s measure the performance of appending elements to an empty list. I\u0026rsquo;m using IPython\u0026rsquo;s built-in %%timeit command to do it:\nIn [1]: %%timeit ...: ...: l=[] ...: for i in range(10_000): ...: l.append(i) ...: 499 µs ± 1.23 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) Now, if you know the final size of the list beforehand, then you don\u0026rsquo;t need to create an empty list and append elements to it via a loop. You can initialize the list with None and then fill in the elements like this:\n# src.py size = 10_000 l = [None] * size for i in range(size): l[i] = i This is quite a bit faster than the previous snippet:\nIn [2]: %%timeit ...: ...: l=[None]*10_000 ...: for i in range(10_000): ...: l[i] = i ...: 321 µs ± 71.1 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each) Breadcrumbs For simple cases demonstrated above, list comprehension is going to be quite a bit quicker than the static pre-allocation technique. See for yourself:\nIn [3]: %%timeit ...: ...: [i for i in range(10_000)] 225 µs ± 711 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each) So, I don\u0026rsquo;t recommend performing micro-optimization without instrumenting your code first. However, list pre-allocation can still come in handy in more complex cases where you already know the size of the final list, and shaving off a few micro-seconds makes a considerable difference.\nFurther reading Create a list with initial capacity in Python ","permalink":"https://rednafi.com/python/preallocated-list/","summary":"\u003cp\u003eIn CPython, elements of a list are stored as pointers to the elements rather than the values\nof the elements themselves. This is evident from the \u003ca href=\"https://github.com/python/cpython/blob/c19c3a09618ac400538ee412f84be4c1196c7bab/Include/cpython/listobject.h#L5\"\u003elist struct in CPython\u003c/a\u003e that\nrepresents a list in C:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-c\" data-lang=\"c\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Fetched from CPython main branch. Removed comments for brevity.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003etypedef\u003c/span\u003e \u003cspan class=\"k\"\u003estruct\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ePyObject_VAR_HEAD\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ePyObject\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"n\"\u003eob_item\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"cm\"\u003e/* Pointer reference to the element. */\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ePy_ssize_t\u003c/span\u003e \u003cspan class=\"n\"\u003eallocated\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"n\"\u003ePyListObject\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAn empty list builds a \u003ccode\u003ePyObject\u003c/code\u003e and occupies some memory:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003esys\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003egetsizeof\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003el\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003egetsizeof\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003el\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis returns:\u003c/p\u003e","title":"Pre-allocated lists in Python"},{"content":"Up until now, I\u0026rsquo;ve always preferred Title Case to demarcate titles and section headers in my writings. However, lately I\u0026rsquo;ve realized that each time I start writing a sentence, I waste a few seconds deciding on the appropriate case of the special words like — technical terms, trademark names, proper nouns, etc — and how they\u0026rsquo;ll blend in with the multiple flavors of title capitalization rules. Plus, often time, special casing of selected words makes title-cased sentences look strange.\nThis led me to asking a question on Twitter to find out if anyone had the same thought as mine or if it\u0026rsquo;d be ignored as trifling pedantry. A few people responded and they had a few excellent points on what they liked and disliked about title case. What surprised me that majority of them preferred Sentence case where you\u0026rsquo;d only capitalize the first letter of the first word of a sentence and then the capitilization of words would conform to the usual grammatical rules. So instead of this:\nDataclasses in Python Eliminates Class Initialization Boilerplates.\nyou\u0026rsquo;d do this:\nDataclasses in Python eliminates class initialization boilerplates.\nThis allows me not to waste those few precious seconds thinking about the rules of upper casing and whether or not sentences with technical words in them will look awkward. Also, it saves me from creating horrors like this where the semantics of a sentence get massacred by the eccentricity of title casing:\nSentence case everywhere is the general trend I\u0026rsquo;m seeing on the technical blogs of some of the big players on the internet:\nDjango docs Dropbox blog GitHub blog Twitter blog GitLab blog One thing that I\u0026rsquo;ll admit is, at first it just doesn\u0026rsquo;t look right to me for a title to have sentence casing. Also, I\u0026rsquo;m not too conviced to adopt sentence case in technical papers. However, Robert Smallshire argues for sentence case, noting that if it works for the Economist, it should work for most of us.\nSo, I\u0026rsquo;m kind of leaning towards sentence casing and have slowly started adopting it in my writings everywhere. This blog used to conform to a mangled version of title casing to delineate the titles and sub headers but I\u0026rsquo;ve converted all the posts to adopt sentence case from now on. I\u0026rsquo;m quite happy with the results so far and it\u0026rsquo;s definitely one less thing to worry about.\n","permalink":"https://rednafi.com/zephyr/in-favor-of-sentence-case/","summary":"\u003cp\u003eUp until now, I\u0026rsquo;ve always preferred \u003cstrong\u003eTitle Case\u003c/strong\u003e to demarcate titles and section headers\nin my writings. However, lately I\u0026rsquo;ve realized that each time I start writing a sentence, I\nwaste a few seconds deciding on the appropriate case of the special words like — technical\nterms, trademark names, proper nouns, etc — and how they\u0026rsquo;ll blend in with the \u003ca href=\"https://capitalizemytitle.com/\"\u003emultiple\nflavors of title capitalization\u003c/a\u003e rules. Plus, often time, special casing of selected words\nmakes title-cased sentences look strange.\u003c/p\u003e","title":"In favor of sentence case"},{"content":"I was working on a DRF POST API endpoint where the consumer is expected to add a URL containing a PDF file and the system would then download the file and save it to an S3 bucket. While this sounds quite straightforward, there\u0026rsquo;s one big issue. Before I started working on it, the core logic looked like this:\n# src.py from __future__ import annoatations from urllib.request import urlopen import tempfile from shutil import copyfileobj def save_to_s3(src_url: str, dest_url: str) -\u0026gt; None: with tempfile.NamedTemporaryFile() as file: with urlopen(src_url) as response: # This stdlib function saves the content of the file # in \u0026#39;file\u0026#39;. copyfileobj(response, file) # Logic to save file in s3. _save_to_s3(des_url) if __name__ == \u0026#34;__main__\u0026#34;: save_to_s3( \u0026#34;https://citeseerx.ist.psu.edu/viewdoc/download?\u0026#34; \u0026#34;doi=10.1.1.92.4846\u0026amp;rep=rep1\u0026amp;type=pdf\u0026#34;, \u0026#34;https://s3-url.com\u0026#34;, ) In the above snippet, there\u0026rsquo;s no guardrail against how large the target file can be. You could bring the entire server down to its knees by posting a link to a ginormous file. The server would be busy downloading the file and keep consuming resources.\nI didn\u0026rsquo;t want to use urllib at all for this purpose and went for HTTPx. It exposes a neat API to perform streaming file download. Also, I didn\u0026rsquo;t want to peek into the Content-Length header to assess the file size since the file server can choose not to include that header key. I was looking for something more dependable than that. Here\u0026rsquo;s how I solved it:\n# src from __future__ import annotations import httpx import tempfile def save_to_s3( src_url: str, dest_url: str, chunk_size: int = 1024 * 1024, # 1 MB buffer. max_size: int = 10 * 1024 * 1024, # 10 MB ) -\u0026gt; None: # Keep track of the already downloaded byte length. downloaded_content_length = 0 # bytes with tempfile.NamedTemporaryFile() as file: with httpx.stream(\u0026#34;GET\u0026#34;, src_url) as response: for chunk in response.iter_bytes(chunk_size): downloaded_content_length += len(chunk) if downloaded_content_length \u0026gt; max_size: raise ValueError( f\u0026#34;File size too large. Make sure your linked \u0026#34; \u0026#34;file is not larger than 10 MB.\u0026#34; ) file.write(chunk) # logic to save file in s3. _save_to_s3(dest_url) if __name__ == \u0026#34;__main__\u0026#34;: save_to_s3( \u0026#34;https://citeseerx.ist.psu.edu/viewdoc/download?\u0026#34; \u0026#34;doi=10.1.1.92.4846\u0026amp;rep=rep1\u0026amp;type=pdf\u0026#34;, \u0026#34;\u0026#34;, ) The chunk_size parameter explicitly dictates the buffer size of the file being downloaded. This means the entire file won\u0026rsquo;t be loaded into memory while being downloaded. The max_size parameter defines the maximum file size that\u0026rsquo;ll be allowed. In this example, we\u0026rsquo;re keeping track of the size of the already downloaded bytes in the downloaded_content_length variable and raising an error if the size exceeds 10MB. Sweet!\nFurther reading Streaming download with HTTPx ","permalink":"https://rednafi.com/python/disallow-large-file-download/","summary":"\u003cp\u003eI was working on a DRF POST API endpoint where the consumer is expected to add a URL\ncontaining a PDF file and the system would then download the file and save it to an S3\nbucket. While this sounds quite straightforward, there\u0026rsquo;s one big issue. Before I started\nworking on it, the core logic looked like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003e__future__\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eannoatations\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003eurllib.request\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eurlopen\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003etempfile\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003eshutil\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003ecopyfileobj\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003esave_to_s3\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esrc_url\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edest_url\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"n\"\u003etempfile\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eNamedTemporaryFile\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003efile\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"n\"\u003eurlopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esrc_url\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"c1\"\u003e# This stdlib function saves the content of the file\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"c1\"\u003e# in \u0026#39;file\u0026#39;.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003ecopyfileobj\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003efile\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"c1\"\u003e# Logic to save file in s3.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003e_save_to_s3\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edes_url\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"vm\"\u003e__name__\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003esave_to_s3\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"s2\"\u003e\u0026#34;https://citeseerx.ist.psu.edu/viewdoc/download?\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"s2\"\u003e\u0026#34;doi=10.1.1.92.4846\u0026amp;rep=rep1\u0026amp;type=pdf\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"s2\"\u003e\u0026#34;https://s3-url.com\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIn the above snippet, there\u0026rsquo;s no guardrail against how large the target file can be. You\ncould bring the entire server down to its knees by posting a link to a ginormous file. The\nserver would be busy downloading the file and keep consuming resources.\u003c/p\u003e","title":"Disallow large file download from URLs in Python"},{"content":"While writing microservices in Python, I like to declaratively define the shape of the data coming in and out of JSON APIs or NoSQL databases in a separate module. Both TypedDict and dataclass are fantastic tools to communicate the shape of the data with the next person working on the codebase.\nWhenever I need to do some processing on the data before starting to work on that, I prefer to transform the data via dataclasses. Consider this example:\n# src.py from __future__ import annotations from dataclasses import dataclass from typing import Any @dataclass class WebhookPayload: \u0026#34;\u0026#34;\u0026#34;Save data to DynamoDB.\u0026#34;\u0026#34;\u0026#34; url: str request_payload: dict response_payload: dict status_code: int def to_dynamodb_item(self) -\u0026gt; None: ... The above class defines the structure of a payload that\u0026rsquo;ll be saved in a DynamoDB table. To make things simpler, I want to serialize the request_payload, response_payload, and status_code fields to JSON string before saving them to the DB. Usually, I\u0026rsquo;d do it in the to_dynamodb_item like this:\n# src.py from __future__ import annotations from dataclasses import dataclass import json from typing import Any @dataclass class WebhookPayload: \u0026#34;\u0026#34;\u0026#34;Save data to DynamoDB.\u0026#34;\u0026#34;\u0026#34; # Snip... def to_dynamodb_item(self) -\u0026gt; dict[str, Any]: request_payload = json.dumps(self.request_payload) response_payload = json.dumps(self.response_payload) status_code = json.dumps(self.response_payload) # ... dyanmodb_item = ... return dynamodb_item However, keeping track of this json.dumps transformation that\u0026rsquo;s buried in a method can be difficult. Also, it can be hard to track the fields that need to be deserialized whenever you want the rich data structures back. Another disadvantage is that you\u0026rsquo;ll have to perform the same transformation again if you need serialized fields in another method. A better way is to take advantage of the __post_init__ hook exposed by dataclasses. Here\u0026rsquo;s how you can do it:\n# src.py from __future__ import annotations import json from dataclasses import dataclass, field from typing import Any @dataclass class WebhookPayload: \u0026#34;\u0026#34;\u0026#34;Save data to DynamoDB.\u0026#34;\u0026#34;\u0026#34; url: str request_payload: dict | None response_payload: dict | None status_code: int | None _json_transform: bool = field(default=True, repr=False) _json_fields: tuple[str, ...] = field( default=( \u0026#34;request_payload\u0026#34;, \u0026#34;response_payload\u0026#34;, \u0026#34;status_code\u0026#34;, ), repr=False, ) def __post_init__(self) -\u0026gt; None: if not self._json_transform: return for field in self._json_fields: # Here\u0026#39;s where the magic happens! setattr(self, field, json.dumps(getattr(self, field))) def to_dynamodb_item(self) -\u0026gt; dict[str, Any]: ... if __name__ == \u0026#34;__main__\u0026#34;: wh = WebhookPayload( url=\u0026#34;https//:httpbin.org/post\u0026#34;, request_payload={\u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34;}, response_payload=None, status_code=None, ) print(wh) Running the script will print the following:\nWebhookPayload( url=\u0026#39;https//:httpbin.org/post\u0026#39;, request_payload=\u0026#39;{\u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34;}\u0026#39;, response_payload=\u0026#39;null\u0026#39;, status_code=\u0026#39;null\u0026#39; ) Notice, how the intended fields are now JSON encoded. Python calls the __post_init__ hook of a dataclass after calling the __init__ method. If you don\u0026rsquo;t generate any init by decorating the target class with @dataclass(init=False), in that case, the __post_init__ hook won\u0026rsquo;t be executed.\nThe field function with repr=False allows us to exclude the configuration fields like _json_transform and _json_fields from the final __repr__ of the class. Notice that these two fields are absent in the final representation of the dataclass instance.\nYou can turn off the JSON conversion by setting the _json_transform to False:\n# src.py ... WebhookPayload( url=\u0026#34;https//:httpbin.org/post\u0026#34;, request_payload={\u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34;}, response_payload=None, status_code=None, _json_transform=False, ) You can also add or remove fields to be transformed by changing the value of the _json_fields iterable of the class:\n# src.py ... WebhookPayload( url=\u0026#34;https//:httpbin.org/post\u0026#34;, request_payload={\u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34;}, response_payload=None, status_code=None, _json_fields=(\u0026#34;status_code\u0026#34;,), ) This will only serialize the status_code field. Neat!\nFurther reading Post-init processing — Python docs ","permalink":"https://rednafi.com/python/declaratively-transform-dataclass-fields/","summary":"\u003cp\u003eWhile writing microservices in Python, I like to declaratively define the shape of the data\ncoming in and out of JSON APIs or NoSQL databases in a separate module. Both \u003ccode\u003eTypedDict\u003c/code\u003e and\n\u003ccode\u003edataclass\u003c/code\u003e are fantastic tools to communicate the shape of the data with the next person\nworking on the codebase.\u003c/p\u003e\n\u003cp\u003eWhenever I need to do some processing on the data before starting to work on that, I prefer\nto transform the data via dataclasses. Consider this example:\u003c/p\u003e","title":"Declaratively transform data class fields in Python"},{"content":"To avoid instantiating multiple DB connections in Python apps, a common approach is to initialize the connection objects in a module once and then import them everywhere. So, you\u0026rsquo;d do this:\n# src.py import boto3 # Pip install boto3 import redis # Pip install redis dynamo_client = boto3.client(\u0026#34;dynamodb\u0026#34;) redis_client = redis.Redis() However, this adds import time side effects to your module and can turn out to be expensive. In search of a better solution, my first instinct was to go for functools.lru_cache(None) to immortalize the connection objects in memory. It works like this:\nfrom __future__ import annotations # In Py \u0026lt; 3.9, use \u0026#39;functools.lru_cache(None)\u0026#39;. from functools import cache import boto3 import redis @cache def get_dynamo_client() -\u0026gt; boto3.session.Session.client: return boto3.client(\u0026#34;dynamodb\u0026#34;) @cache def get_redis_client() -\u0026gt; redis.Redis: return redis.Redis() This way, the connection objects returned by the functions are cached and any subsequent calls to the functions will provide the same connection objects from the cache without reinitializing them.\nOne problem with the above approach is — how complex the implementation of the cache decorator is. Underneath, the functools.cache decorator is an alias for functools.lru_cache(None) and it employs a Least Recently Used cache eviction policy. While this policy is quite useful when you need it but to cache simple connection objects, arguably, the complexity and the overhead of the cache decorator offset its benefits. There\u0026rsquo;s a simpler way to do it and James Powell on Twitter pointed me to it. This works as follows:\n# src.py from __future__ import annotations import boto3 import redis _cache = {} def get_dynamo_client( service_name: str = \u0026#34;dynamodb\u0026#34;, ) -\u0026gt; boto3.session.Session.client: \u0026#34;\u0026#34;\u0026#34;Immortalize the Dynamo client object so that this function always returns the same connection object .\u0026#34;\u0026#34;\u0026#34; if service_name not in _cache: _cache[service_name] = boto3.client(service_name) return _cache[service_name] def get_redis_client(service_name: str = \u0026#34;redis\u0026#34;) -\u0026gt; redis.Redis: \u0026#34;\u0026#34;\u0026#34;Immortalize Redis connection object.\u0026#34;\u0026#34;\u0026#34; if service_name not in _cache: _cache[service_name] = redis.Redis() return _cache[service_name] Is this singleton pattern? Probably so.\n","permalink":"https://rednafi.com/python/caching-connection-objects/","summary":"\u003cp\u003eTo avoid instantiating multiple DB connections in Python apps, a common approach is to\ninitialize the connection objects in a module once and then import them everywhere. So,\nyou\u0026rsquo;d do this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eboto3\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# Pip install boto3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eredis\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# Pip install redis\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edynamo_client\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eboto3\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eclient\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;dynamodb\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eredis_client\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eredis\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eRedis\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHowever, this adds import time side effects to your module and can turn out to be expensive.\nIn search of a better solution, my first instinct was to go for \u003ccode\u003efunctools.lru_cache(None)\u003c/code\u003e\nto immortalize the connection objects in memory. It works like this:\u003c/p\u003e","title":"Caching connection objects in Python"},{"content":"When I first started working with Python, nothing stumped me more than how bizarre Python\u0026rsquo;s import system seemed to be. Often time, I wanted to run a module inside of a package with the python src/sub/module.py command, and it\u0026rsquo;d throw an ImportError that didn\u0026rsquo;t make any sense. Consider this package structure:\nsrc ├── __init__.py ├── a.py └── sub ├── __init__.py └── b.py Let\u0026rsquo;s say you\u0026rsquo;re importing module a in module b:\n# b.py from src import a ... Now, if you try to run module b.py with the following command, it\u0026rsquo;d throw an import error:\npython src/sub/b.py Traceback (most recent call last): File \u0026#34;.../src/sub/b.py\u0026#34;, line 2, in \u0026lt;module\u0026gt; from src import a ModuleNotFoundError: No module named \u0026#39;src\u0026#39; What! But you can see the src/a.py module right there. Why can\u0026rsquo;t Python access the module here? Turns out Python puts the path of the module that you\u0026rsquo;re trying to access to the top of the sys.path stack. Let\u0026rsquo;s print the sys.path before importing module a in the src/sub/b.py file:\n# b.py import sys print(sys.path) from src import a Now running this module with python src/sub/b.py will print the following:\n[\u0026#39;.../src/sub\u0026#39;, \u0026#39;/usr/lib/python310.zip\u0026#39;, \u0026#39;/usr/lib/python3.10\u0026#39;, \u0026#39;/usr/lib/python3.10/lib-dynload\u0026#39;, \u0026#39;.../.venv/lib/python3.10/site-packages\u0026#39;] Traceback (most recent call last): File \u0026#34;.../src/sub/b.py\u0026#34;, line 5, in \u0026lt;module\u0026gt; from src import a ModuleNotFoundError: No module named \u0026#39;src\u0026#39; From the first section of the above output, it\u0026rsquo;s evident that Python looks for the imported module in the src/sub/ directory, not in the root directory from where the command is being executed. That\u0026rsquo;s why it can\u0026rsquo;t find the a.py module because it exists in a directory above the sys.path\u0026rsquo;s first entry. To solve this, you should run the module with the -m switch as follows:\npython -m src.sub.b This will not raise the import error and return the following output:\n[\u0026#39;/home/rednafi/canvas/personal/reflections\u0026#39;, \u0026#39;/usr/lib/python310.zip\u0026#39;, \u0026#39;/usr/lib/python3.10\u0026#39;, \u0026#39;/usr/lib/python3.10/lib-dynload\u0026#39;, \u0026#39;/home/rednafi/canvas/personal/reflections/.venv/lib/python3.10/site-packages\u0026#39;] Here, the first entry denotes the root directory from where the script is being run from. Voila, problem solved!\nFurther reading Don\u0026rsquo;t run python my/script.py ","permalink":"https://rednafi.com/python/how-not-to-run-a-script/","summary":"\u003cp\u003eWhen I first started working with Python, nothing stumped me more than how bizarre Python\u0026rsquo;s\nimport system seemed to be. Often time, I wanted to run a module inside of a package with\nthe \u003ccode\u003epython src/sub/module.py\u003c/code\u003e command, and it\u0026rsquo;d throw an \u003ccode\u003eImportError\u003c/code\u003e that didn\u0026rsquo;t make any\nsense. Consider this package structure:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esrc\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e├── __init__.py\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e├── a.py\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e└── sub\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    ├── __init__.py\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    └── b.py\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eLet\u0026rsquo;s say you\u0026rsquo;re importing module \u003ccode\u003ea\u003c/code\u003e in module \u003ccode\u003eb\u003c/code\u003e:\u003c/p\u003e","title":"How not to run a script in Python"},{"content":"This is the 4th time in a row that I\u0026rsquo;ve wasted time figuring out how to mock out a function during testing that calls the chained methods of a datetime.datetime object in the function body. So I thought I\u0026rsquo;d document it here. Consider this function:\n# src.py from __future__ import annotations import datetime def get_utcnow_isoformat() -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Get UTCnow as an isoformat compliant string.\u0026#34;\u0026#34;\u0026#34; return datetime.datetime.utcnow().isoformat() How\u0026rsquo;d you test it? Mocking out datetime.datetime is tricky because of its immutable nature. Third-party libraries like freezegun make it easier to mock and test functions like the one above. However, it\u0026rsquo;s not too difficult to cover this simple case without any additional dependencies. Here\u0026rsquo;s one way to achieve the goal:\n# src.py from __future__ import annotations import datetime from unittest.mock import patch import pytest def get_utcnow_isoformat() -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Get UTCnow as an isoformat compliant string.\u0026#34;\u0026#34;\u0026#34; return datetime.datetime.utcnow().isoformat() @pytest.fixture def mock_datetime(): with patch(\u0026#34;datetime.datetime\u0026#34;) as m: # This is where the magic happens! m.utcnow.return_value.isoformat.return_value = ( \u0026#34;2022-03-15T23:11:12.432048\u0026#34; ) yield m def test_get_utcnow_isoformat(mock_datetime): frozen_date = \u0026#34;2022-03-15T23:11:12.432048\u0026#34; assert get_utcnow_isoformat() == frozen_date Here, the mock_datetime fixture function makes the output of the chained calls on the datetime object deterministic. Then I used it in the test_get_utcnow_isoformat function to get a frozen output every time the function get_utcnow_isoformat gets called. If you run the above snippet with Python, it\u0026rsquo;ll pass.\n======test session starts ====== platform linux -- Python 3.10.2, pytest-7.0.1, pluggy-1.0.0 rootdir: /home/rednafi/canvas/personal/reflections plugins: anyio-3.5.0 collected 1 item src.py . [100%] ====== 1 passed in 0.01s ====== Further reading Python test using mock with datetime.utcnow — Stackoverflow ","permalink":"https://rednafi.com/python/mocking-datetime-objects/","summary":"\u003cp\u003eThis is the 4th time in a row that I\u0026rsquo;ve wasted time figuring out how to mock out a function\nduring testing that calls the chained methods of a \u003ccode\u003edatetime.datetime\u003c/code\u003e object in the\nfunction body. So I thought I\u0026rsquo;d document it here. Consider this function:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003e__future__\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eannotations\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003edatetime\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eget_utcnow_isoformat\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;Get UTCnow as an isoformat compliant string.\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003edatetime\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edatetime\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eutcnow\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eisoformat\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHow\u0026rsquo;d you test it? Mocking out \u003ccode\u003edatetime.datetime\u003c/code\u003e is tricky because of its immutable\nnature. Third-party libraries like \u003ca href=\"https://github.com/spulec/freezegun\"\u003efreezegun\u003c/a\u003e make it easier to mock and test functions\nlike the one above. However, it\u0026rsquo;s not too difficult to cover this simple case without any\nadditional dependencies. Here\u0026rsquo;s one way to achieve the goal:\u003c/p\u003e","title":"Mocking chained methods of datetime objects in Python"},{"content":"While working with microservices in Python, a common pattern that I see is — the usage of dynamically filled dictionaries as payloads of REST APIs or message queues. To understand what I mean by this, consider the following example:\n# src.py from __future__ import annotations import json from typing import Any import redis # Do a pip install. def get_payload() -\u0026gt; dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Get the \u0026#39;zoo\u0026#39; payload containing animal names and attributes.\u0026#34;\u0026#34;\u0026#34; payload = {\u0026#34;name\u0026#34;: \u0026#34;awesome_zoo\u0026#34;, \u0026#34;animals\u0026#34;: []} names = (\u0026#34;wolf\u0026#34;, \u0026#34;snake\u0026#34;, \u0026#34;ostrich\u0026#34;) attributes = ( {\u0026#34;family\u0026#34;: \u0026#34;Canidae\u0026#34;, \u0026#34;genus\u0026#34;: \u0026#34;Canis\u0026#34;, \u0026#34;is_mammal\u0026#34;: True}, {\u0026#34;family\u0026#34;: \u0026#34;Viperidae\u0026#34;, \u0026#34;genus\u0026#34;: \u0026#34;Boas\u0026#34;, \u0026#34;is_mammal\u0026#34;: False}, ) for name, attr in zip(names, attributes): payload[\u0026#34;animals\u0026#34;].append( # type: ignore {\u0026#34;name\u0026#34;: name, \u0026#34;attribute\u0026#34;: attr}, ) return payload def save_to_cache(payload: dict[str, Any]) -\u0026gt; None: # You\u0026#39;ll need to spin up a Redis db before instantiating # a connection here. r = redis.Redis() print(\u0026#34;Saving to cache...\u0026#34;) r.set(f\u0026#34;zoo:{payload[\u0026#39;name\u0026#39;]}\u0026#34;, json.dumps(payload)) if __name__ == \u0026#34;__main__\u0026#34;: payload = get_payload() save_to_cache(payload) Here, the get_payload function constructs a payload that gets stored in a Redis DB in the save_to_cache function. The get_payload function returns a dict that denotes a contrived payload containing the data of an imaginary zoo. To execute the above snippet, you\u0026rsquo;ll need to spin up a Redis database first. You can use Docker to do so. Install and configure Docker on your system and run:\ndocker run -d -p 6379:6379 redis:alpine If you run the above snippet after instantiating the Redis server, it\u0026rsquo;ll run without raising any error. You can inspect the content saved in Redis with the following command (assuming you\u0026rsquo;ve got redis-cli and jq installed in your system):\necho \u0026#34;get zoo:awesome_zoo\u0026#34; | redis-cli | jq This will return the following payload to your console:\n{ \u0026#34;name\u0026#34;: \u0026#34;awesome_zoo\u0026#34;, \u0026#34;animals\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;wolf\u0026#34;, \u0026#34;attribute\u0026#34;: { \u0026#34;family\u0026#34;: \u0026#34;Canidae\u0026#34;, \u0026#34;genus\u0026#34;: \u0026#34;Canis\u0026#34;, \u0026#34;is_mammal\u0026#34;: true } }, { \u0026#34;name\u0026#34;: \u0026#34;snake\u0026#34;, \u0026#34;attribute\u0026#34;: { \u0026#34;family\u0026#34;: \u0026#34;Viperidae\u0026#34;, \u0026#34;genus\u0026#34;: \u0026#34;Boas\u0026#34;, \u0026#34;is_mammal\u0026#34;: false } } ] } Although this workflow is functional in runtime, there\u0026rsquo;s a big gotcha here! It\u0026rsquo;s really difficult to picture the shape of the payload from the output of the get_payload function; as it dynamically builds the dictionary. First, it declares a dictionary with two fields — name and animals. Here, name is a string value that denotes the name of the zoo. The other field animals is a list containing the names and attributes of the animals in the zoo. Later on, the for-loop fills up the dictionary with nested data structures. This charade of operations makes it difficult to reify the final shape of the resulting payload in your mind.\nIn this case, you\u0026rsquo;ll have to inspect the content of the Redis cache to fully understand the shape of the data. Writing code in the above manner is effortless but it makes it really hard for the next person working on the codebase to understand how the payload looks without tapping into the data storage. There\u0026rsquo;s a better way to declaratively communicate the shape of the payload that doesn\u0026rsquo;t involve writing unmaintainably large docstrings. Here\u0026rsquo;s how you can leverage TypedDict and Annotated to achieve the goals:\n# src.py from __future__ import annotations import json # In \u0026lt; Python 3.8, import \u0026#39;TypedDict\u0026#39; from \u0026#39;typing_extensions\u0026#39;. # In \u0026lt; Python 3.9, import \u0026#39;Annotated\u0026#39; from \u0026#39;typing_extensions\u0026#39;. from typing import Annotated, Any, TypedDict import redis # Do a pip install. class Attribute(TypedDict): family: str genus: str is_mammal: bool class Animal(TypedDict): name: str attribute: Attribute class Zoo(TypedDict): name: str animals: list[Animal] def get_payload() -\u0026gt; Zoo: \u0026#34;\u0026#34;\u0026#34;Get the \u0026#39;zoo\u0026#39; payload containing animal names and attributes.\u0026#34;\u0026#34;\u0026#34; payload: Zoo = {\u0026#34;name\u0026#34;: \u0026#34;awesome_zoo\u0026#34;, \u0026#34;animals\u0026#34;: []} names = (\u0026#34;wolf\u0026#34;, \u0026#34;snake\u0026#34;, \u0026#34;ostrich\u0026#34;) attributes: tuple[Attribute, ...] = ( {\u0026#34;family\u0026#34;: \u0026#34;Canidae\u0026#34;, \u0026#34;genus\u0026#34;: \u0026#34;Canis\u0026#34;, \u0026#34;is_mammal\u0026#34;: True}, {\u0026#34;family\u0026#34;: \u0026#34;Viperidae\u0026#34;, \u0026#34;genus\u0026#34;: \u0026#34;Boas\u0026#34;, \u0026#34;is_mammal\u0026#34;: False}, ) for name, attr in zip(names, attributes): payload[\u0026#34;animals\u0026#34;].append({\u0026#34;name\u0026#34;: name, \u0026#34;attribute\u0026#34;: attr}) return payload def save_to_cache(payload: Annotated[Zoo, dict]) -\u0026gt; None: # You\u0026#39;ll need to spin up a Redis db before instantiating # a connection here. r = redis.Redis() print(\u0026#34;Saving to cache...\u0026#34;) r.set(f\u0026#34;zoo:{payload[\u0026#39;name\u0026#39;]}\u0026#34;, json.dumps(payload)) if __name__ == \u0026#34;__main__\u0026#34;: payload: Zoo = get_payload() save_to_cache(payload) Notice, how I\u0026rsquo;ve used TypedDict to declare the nested structure of the payload Zoo. In runtime, instances of typed-dict classes behave the same way as normal dicts. Here, Zoo contains two fields — name and animals. The animals field is annotated as list[Animal] where Animal is another typed-dict. The Animal typed-dict houses another typed-dict called Attribute that defines various properties of the animal.\nTaking a look at the typed-dict Zoo and following along its nested structure, the final shape of the payload becomes clearer without us having to look for example payloads. Also, Mypy can check whether the payload conforms to the shape of the annotated type. I used Annotated[Zoo, dict] in the input parameter of save_to_cache function to communicate with the reader that an instance of the class Zoo is a dict that conforms to the contract laid out in the type itself. The type Annotated can be used to add any arbitrary metadata to a particular type.\nIn runtime, this snippet will exhibit the same behavior as the previous one. Mypy also approves this.\nHandling missing key-value pairs By default, the type checker will structurally validate the shape of the dict annotated with a TypedDict class and all the key-value pairs expected by the annotation must be present in the dict. It\u0026rsquo;s possible to lax this behavior by specifying totality. This can be helpful to deal with missing fields without letting go of type safety. Consider this:\nfrom __future__ import annotations from typing import TypedDict class Attribute(TypedDict): family: str genus: str is_mammal: bool animal_attribute: Attribute = { \u0026#34;family\u0026#34;: \u0026#34;Hominidae\u0026#34;, \u0026#34;genus\u0026#34;: \u0026#34;Homo\u0026#34;, } # Mypy will complain about the missing \u0026#39;is_mammal\u0026#39; key. Mypy will complain about the missing key:\nsrc.py:12: error: Missing key \u0026#34;is_mammal\u0026#34; for TypedDict \u0026#34;Attribute\u0026#34; animal_attribute: Attribute = { ^ Found 1 error in 1 file (checked 1 source file) You can relax this behavior like this:\n... class Attribute(TypedDict, total=False): family: str genus: str is_mammal: bool ... Now Mypy will no longer complain about the missing field in the annotated dict. However, this will still disallow arbitrary keys that isn\u0026rsquo;t defined in the TypedDict. For example:\n... # Mypy will complain as the key \u0026#39;species\u0026#39; doesn\u0026#39;t exist in the TypedDict. animal_attribute[\u0026#34;species\u0026#34;] = \u0026#34;Sapiens\u0026#34; ... src.py:17: error: TypedDict \u0026#34;Attribute\u0026#34; has no key \u0026#34;species\u0026#34; animal_attribute[\u0026#34;species\u0026#34;] = \u0026#34;Sapiens\u0026#34; ^ Found 1 error in 1 file (checked 3 source files) make: *** [Makefile:134: mypy] Error 1 Sweet type safety without being too strict about missing fields!\nFurther reading PEP 589 – TypedDict: Type hints for dictionaries with a fixed set of keys ","permalink":"https://rednafi.com/python/declarative-payloads-with-typedict/","summary":"\u003cp\u003eWhile working with microservices in Python, a common pattern that I see is — the usage of\ndynamically filled dictionaries as payloads of REST APIs or message queues. To understand\nwhat I mean by this, consider the following example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003e__future__\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eannotations\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ejson\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etyping\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eredis\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# Do a pip install.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eget_payload\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003edict\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;Get the \u0026#39;zoo\u0026#39; payload containing animal names and attributes.\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003epayload\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;name\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;awesome_zoo\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;animals\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[]}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003enames\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;wolf\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;snake\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;ostrich\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eattributes\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;family\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Canidae\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;genus\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Canis\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;is_mammal\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;family\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Viperidae\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;genus\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Boas\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;is_mammal\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eattr\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003ezip\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003enames\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eattributes\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003epayload\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;animals\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eappend\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# type: ignore\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;name\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;attribute\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eattr\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003epayload\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003esave_to_cache\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epayload\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003edict\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# You\u0026#39;ll need to spin up a Redis db before instantiating\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# a connection here.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003er\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eredis\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eRedis\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Saving to cache...\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003er\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eset\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;zoo:\u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003epayload\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;name\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ejson\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edumps\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epayload\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"vm\"\u003e__name__\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003epayload\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eget_payload\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003esave_to_cache\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epayload\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHere, the \u003ccode\u003eget_payload\u003c/code\u003e function constructs a payload that gets stored in a Redis DB in the\n\u003ccode\u003esave_to_cache\u003c/code\u003e function. The \u003ccode\u003eget_payload\u003c/code\u003e function returns a dict that denotes a contrived\npayload containing the data of an imaginary zoo. To execute the above snippet, you\u0026rsquo;ll need\nto spin up a Redis database first. You can use \u003ca href=\"https://www.docker.com/\"\u003eDocker\u003c/a\u003e to do so. Install and configure\nDocker on your system and run:\u003c/p\u003e","title":"Declarative payloads with TypedDict in Python"},{"content":"While most of my pytest fixtures don\u0026rsquo;t react to the dynamically-passed values of function parameters, there have been situations where I\u0026rsquo;ve definitely felt the need for that. Consider this example:\n# test_src.py import pytest @pytest.fixture def create_file(tmp_path): \u0026#34;\u0026#34;\u0026#34;Fixture to create a file in the tmp_path/tmp directory.\u0026#34;\u0026#34;\u0026#34; directory = tmp_path / \u0026#34;tmp\u0026#34; directory.mkdir() file = directory / \u0026#34;foo.md\u0026#34; # The filename is hardcoded here! yield directory, file def test_file_creation(create_file): \u0026#34;\u0026#34;\u0026#34;Check the fixture.\u0026#34;\u0026#34;\u0026#34; directory, file = create_file assert directory.name == \u0026#34;tmp\u0026#34; assert file.name == \u0026#34;foo.md\u0026#34; Here, in the create_file fixture, I\u0026rsquo;ve created a file named foo.md in the tmp folder. Notice that the name of the file foo.md is hardcoded inside the body of the fixture function. The fixture yields the path of the directory and the created file.\nLater on, the test_file_creation function just checks whether the fixture is working as expected. This snippet will pass successfully if you execute it with the pytest command.\nNow, if you needed to create three files — foo.md, bar.md, baz.md — how\u0026rsquo;d you do that in the fixture? You could hardcode the names of the three files in the fixture as follows:\n# test_src.py import pytest @pytest.fixture def create_files(tmp_path): \u0026#34;\u0026#34;\u0026#34; Fixture to create multiple files in the tmp_path/tmp directory. \u0026#34;\u0026#34;\u0026#34; directory = tmp_path / \u0026#34;tmp\u0026#34; directory.mkdir() # Notice the hardcoded file names. The fixture can only # create files with these names. filenames = (\u0026#34;foo.md\u0026#34;, \u0026#34;bar.md\u0026#34;, \u0026#34;baz.md\u0026#34;) files = [directory / filename for filename in filenames] yield directory, files def test_file_creation(create_files): \u0026#34;\u0026#34;\u0026#34;Check the fixture.\u0026#34;\u0026#34;\u0026#34; directory, files = create_files expected_filenames = (\u0026#34;foo.md\u0026#34;, \u0026#34;bar.md\u0026#34;, \u0026#34;baz.md\u0026#34;) assert directory.name == \u0026#34;tmp\u0026#34; assert all(f.name for f in files if f.name in expected_filenames) I had to change the name of the fixture from create_file to create_files because the output signature of the fixture was changed to yield the directory path and a list of the paths of the three newly created files.\nWhile this works, it\u0026rsquo;s cumbersome and inflexible. What if one of your tests needs one file and another one demands two files to be created? How\u0026rsquo;d you tackle that?\nIt\u0026rsquo;d be much better if we could just pass the filename to the fixture as a parameter and the fixture would then create the corresponding file in the temporary folder. Also, if we need n files to be created, then we\u0026rsquo;ll just have to execute the fixture n times. There\u0026rsquo;s a way to do so by leveraging fixture parameters and @pytest.mark.parameterize decorator. This is how you can do it:\n# test_src.py import pytest @pytest.fixture def create_file(tmp_path, filename): \u0026#34;\u0026#34;\u0026#34;Fixture to create a file in the tmp_path/tmp directory.\u0026#34;\u0026#34;\u0026#34; directory = tmp_path / \u0026#34;tmp\u0026#34; directory.mkdir() file = directory / filename # The hardcoded filename is gone! yield directory, file @pytest.mark.parametrize(\u0026#34;filename\u0026#34;, (\u0026#34;foo.md\u0026#34;, \u0026#34;bar.md\u0026#34;, \u0026#34;baz.md\u0026#34;)) def test_file_creation(create_file): \u0026#34;\u0026#34;\u0026#34;Check the fixture.\u0026#34;\u0026#34;\u0026#34; directory, file = create_file expected_filenames = (\u0026#34;foo.md\u0026#34;, \u0026#34;bar.md\u0026#34;, \u0026#34;baz.md\u0026#34;) assert directory.name == \u0026#34;tmp\u0026#34; assert all(f for f in expected_filenames if file.name == f) In this case, the fixture create_file takes an additional parameter called filename and then yields the directory path and the file path; just as the first snippet. Later on, in the test_file_creation function, the desired values of the filename parameter is injected into the fixture via the @pytest.mark.parametrize decorator. In the above snippet, pytest runs the fixture 3 times and creates the desired files in 3 passes — just like how a normal function call would behave.\nFurther reading Pass a parameter to a fixture function - Stackoverflow ","permalink":"https://rednafi.com/python/parametrized-fixtures-in-pytest/","summary":"\u003cp\u003eWhile most of my pytest fixtures don\u0026rsquo;t react to the dynamically-passed values of function\nparameters, there have been situations where I\u0026rsquo;ve definitely felt the need for that.\nConsider this example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# test_src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003epytest\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nd\"\u003e@pytest.fixture\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ecreate_file\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etmp_path\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;Fixture to create a file in the tmp_path/tmp directory.\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003edirectory\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etmp_path\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;tmp\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003edirectory\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emkdir\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003efile\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edirectory\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;foo.md\u0026#34;\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# The filename is hardcoded here!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eyield\u003c/span\u003e \u003cspan class=\"n\"\u003edirectory\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003efile\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003etest_file_creation\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecreate_file\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;Check the fixture.\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003edirectory\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003efile\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ecreate_file\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eassert\u003c/span\u003e \u003cspan class=\"n\"\u003edirectory\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;tmp\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eassert\u003c/span\u003e \u003cspan class=\"n\"\u003efile\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;foo.md\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHere, in the \u003ccode\u003ecreate_file\u003c/code\u003e fixture, I\u0026rsquo;ve created a file named \u003ccode\u003efoo.md\u003c/code\u003e in the \u003ccode\u003etmp\u003c/code\u003e folder.\nNotice that the name of the file \u003ccode\u003efoo.md\u003c/code\u003e is hardcoded inside the body of the fixture\nfunction. The fixture yields the path of the directory and the created file.\u003c/p\u003e","title":"Parametrized fixtures in pytest"},{"content":"If you try to mutate a sequence while traversing through it, Python usually doesn\u0026rsquo;t complain. For example:\n# src.py l = [3, 4, 56, 7, 10, 9, 6, 5] for i in l: if not i % 2 == 0: continue l.remove(i) print(l) The above snippet iterates through a list of numbers and modifies the list l in-place to remove any even number. However, running the script prints out this:\n[3, 56, 7, 9, 5] Wait a minute! The output doesn\u0026rsquo;t look correct. The final list still contains 56 which is an even number. Why did it get skipped? Printing the members of the list while the for-loop advances reveal what\u0026rsquo;s happening inside:\n3 4 7 10 6 [3, 56, 7, 9, 5] From the output, it seems like the for-loop doesn\u0026rsquo;t even visit all the elements of the sequence. However, trying to emulate what happens inside the for-loop with iter and next makes the situation clearer. Notice the following example. I\u0026rsquo;m using ipython shell to explore:\nIn [1]: l = [3, 4, 56, 7, 10, 9, 6, 5] In [2]: # Make the list an iterator. In [3]: it = iter(l) In [4]: # Emulate for-loop by applying \u0026#39;next()\u0026#39; function on \u0026#39;it\u0026#39;. In [5]: next(it) Out[5]: 3 In [6]: next(it) Out[6]: 4 In [7]: # Remove a value that\u0026#39;s already been visited by the iterator. In [8]: l.remove(3) In [9]: next(it) Out[9]: 7 In [10]: # Notice how the iterator skipped 56. Remove another. In [11]: l.remove(4) In [12]: next(it) Out[12]: 9 The REPL experiment reveals that:\nWhenever you remove an element of an iterable that\u0026rsquo;s already been visited by the iterator, in the next iteration, the iterator will jump right by 1 element. This can make the iterator skip a value. The opposite is also true if you prepend some value to a sequence after the iterator has started iterating. In that case, in the next iteration, the iterator will jump left by 1 element and may visit the same value again.\nHere\u0026rsquo;s what happens when you prepend values after the iteration has started:\nIn[1]: l = [3, 4, 56, 7, 10, 9, 6, 5] In[2]: it = iter(l) In[3]: next(it) Out[3]: 3 In[4]: next(it) Out[4]: 4 In[5]: l.insert(0, 44) In[6]: next(it) Out[6]: 4 Notice how the element 4 is being visited twice after prepending a value to the list l.\nSolution To solve this, you\u0026rsquo;ll have to make sure the target elements don\u0026rsquo;t get removed after the iterator has already visited them. You can iterate in the reverse order and remove elements maintaining the original order. The first snippet can be rewritten as follows:\n# src.py l = [3, 4, 56, 7, 10, 9, 6, 5] # Here, \u0026#39;reversed\u0026#39; returns a lazy iterator, so it\u0026#39;s performant! for i in reversed(l): print(i) if not i % 2 == 0: continue l.remove(i) print(l) Running the script prints:\n5 6 9 10 7 56 4 3 [3, 7, 9, 5] Notice, how the iterator now visits all the elements and the final list contains the odd elements as expected.\nAnother way you can solve this is — by copying the list l before iterating. But this can be expensive if l is large:\n# src.py l = [3, 4, 56, 7, 10, 9, 6, 5] # Here \u0026#39;l.copy()\u0026#39; creates a shallow copy of \u0026#39;l\u0026#39;. It\u0026#39;s # less performant than \u0026#39;reversed(l)\u0026#39;. for i in l.copy(): print(i) if not i % 2 == 0: continue l.remove(i) print(l) This time, the order of the iteration and element removal is the same, but that isn\u0026rsquo;t a problem since these two operations occur on two different lists. Running the snippet produces the following output:\n3 4 56 7 10 9 6 5 [3, 7, 9, 5] What about dictionaries Dictionaries don\u0026rsquo;t even allow you to change their sizes while iterating. The following snippet raises a RuntimeError:\n# src.py # {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9} d = {k: k for k in range(10)} for k, v in d.items(): if not v % 2 == 0: continue d.pop(k) Traceback (most recent call last): File \u0026#34;/home/rednafi/canvas/personal/reflections/src.py\u0026#34;, line 4, in \u0026lt;module\u0026gt; for k,v in d.items(): RuntimeError: dictionary changed size during iteration You can solve this by making a copy of the keys of the dictionary and iterating through it while removing the elements from the dictionary. Here\u0026rsquo;s one way to do it:\n# src.py # {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9} d = {k: k for k in range(10)} # This creates a copy of all the keys of \u0026#39;d\u0026#39;. # At least we arent\u0026#39;t creating a new copy of the # entire dict and tuple creation is quite fast. for k in tuple(d.keys()): if not d[k] % 2 == 0: continue d.pop(k) print(d) Running the snippet prints:\n{1: 1, 3: 3, 5: 5, 7: 7, 9: 9} Voila, the key-value pairs of the even numbers have been removed successfully!\nFurther reading How to modify a list while iterating - Anthony Sottile ","permalink":"https://rednafi.com/python/modify-iterables-while-iterating/","summary":"\u003cp\u003eIf you try to mutate a sequence while traversing through it, Python usually doesn\u0026rsquo;t\ncomplain. For example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003el\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e56\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e7\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e9\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e6\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ei\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003el\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"n\"\u003ei\u003c/span\u003e \u003cspan class=\"o\"\u003e%\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003el\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eremove\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003el\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe above snippet iterates through a list of numbers and modifies the list \u003ccode\u003el\u003c/code\u003e in-place to\nremove any even number. However, running the script prints out this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e[3, 56, 7, 9, 5]\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWait a minute! The output doesn\u0026rsquo;t look correct. The final list still contains \u003ccode\u003e56\u003c/code\u003e which is\nan even number. Why did it get skipped? Printing the members of the list while the for-loop\nadvances reveal what\u0026rsquo;s happening inside:\u003c/p\u003e","title":"Modify iterables while iterating in Python"},{"content":"Five traits that almost all the GitHub Action workflows in my Python projects share are:\nIf a new workflow is triggered while the previous one is running, the first one will get canceled. The CI is triggered every day at UTC 1. Tests and the lint-checkers are run on Ubuntu and MacOS against multiple Python versions. Pip dependencies are cached. Dependencies, including the Actions dependencies are automatically updated via Dependabot. I use pip-tools for managing dependencies in applications and setuptools setup.py combo for managing dependencies in libraries. Here\u0026rsquo;s an annotated version of the template action syntax:\n# .github/workflows/ci.yml name: CI on: # Triggers when something is pushed to the \u0026#39;main\u0026#39; branch. push: branches: - master # Triggers when a pull request is sent against the \u0026#39;main\u0026#39; branch. pull_request: branches: - master # Triggers everyday at 1 UTC. schedule: - cron: \u0026#34;0 1 * * *\u0026#34; # Cancel any running workflow if the CI gets triggered again. concurrency: group: ${{ github.head_ref || github.run_id }} cancel-in-progress: true jobs: run-tests: # Tests are run on multiple Python versions. runs-on: ${{ matrix.os }} strategy: matrix: # Multiple OSs. os: [ubuntu-latest, macos-latest] # Multiple Python versions. python-version: [\u0026#34;3.10\u0026#34;, \u0026#34;3.11\u0026#34;, \u0026#34;3.12\u0026#34;] include: - os: ubuntu-latest path: ~/.cache/pip # Cache location on Ubuntu - os: macos-latest path: ~/Library/Caches/pip # Cache location on MacOS steps: # Checkout to the codebase. - uses: actions/checkout@v3 # Sets up Python. - uses: actions/setup-python@v3 with: python-version: ${{ matrix.python-version }} # Cache pip dependencies via \u0026#39;cache\u0026#39; actions. - uses: actions/cache@v2 with: path: ${{ matrix.path }} key: \u0026#34;${{ runner.os }}-pip-${{ hashFiles(\u0026#39;requirements*.txt\u0026#39;) }}\u0026#34; restore-keys: | ${{ runner.os }}-pip- # Dev and app dependencies are kept in separate files. - name: Install the Dependencies run: | pip install --upgrade pip pip install -r requirements.txt pip install -r requirements-dev.txt # Run black, isort, flake8, etc. - name: Check Linter run: | echo \u0026#34;Checking black formatting...\u0026#34; python3 -m black --check . echo \u0026#34;Checking isort formatting...\u0026#34; python3 -m isort --check . echo \u0026#34;Checking flake8 formatting...\u0026#34; python3 -m flake8 . # Run the tests via pytest. - name: Run the tests run: | pytest -v -s The dependabot config looks as follows:\n# .github/dependabot.yml version: 2 updates: - package-ecosystem: \u0026#34;pip\u0026#34; # See documentation for possible values. directory: \u0026#34;/\u0026#34; # Location of package manifests. schedule: interval: \u0026#34;daily\u0026#34; # Maintain dependencies for GitHub Actions. - package-ecosystem: \u0026#34;github-actions\u0026#34; directory: \u0026#34;/\u0026#34; schedule: interval: \u0026#34;daily\u0026#34; Further reading An active version of the above workflow ","permalink":"https://rednafi.com/python/github-action-template-python/","summary":"\u003cp\u003eFive traits that almost all the GitHub Action workflows in my Python projects share are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf a new workflow is triggered while the previous one is running, the first one will get\ncanceled.\u003c/li\u003e\n\u003cli\u003eThe CI is triggered every day at UTC 1.\u003c/li\u003e\n\u003cli\u003eTests and the lint-checkers are run on Ubuntu and MacOS against multiple Python versions.\u003c/li\u003e\n\u003cli\u003ePip dependencies are cached.\u003c/li\u003e\n\u003cli\u003eDependencies, including the Actions dependencies are automatically updated via\n\u003ca href=\"https://github.com/dependabot\"\u003eDependabot\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI use \u003ca href=\"https://github.com/jazzband/pip-tools\"\u003epip-tools\u003c/a\u003e for managing dependencies in applications and \u003ca href=\"https://github.com/pypa/setuptools\"\u003esetuptools\u003c/a\u003e \u003ccode\u003esetup.py\u003c/code\u003e\ncombo for managing dependencies in libraries. Here\u0026rsquo;s an annotated version of the template\naction syntax:\u003c/p\u003e","title":"Github action template for Python based projects"},{"content":"PEP-673 introduces the Self type and it\u0026rsquo;s coming to Python 3.11. However, you can already use that now via the typing_extensions module.\nThe Self type makes annotating methods that return the instances of the corresponding classes trivial. Before this, you\u0026rsquo;d have to do some mental gymnastics to statically type situations as follows:\n# src.py from __future__ import annotations from typing import Any class Animal: def __init__(self, name: str, says: str) -\u0026gt; None: self.name = name self.says = says @classmethod def from_description(cls, description: str = \u0026#34;|\u0026#34;) -\u0026gt; Animal: descr = description.split(\u0026#34;|\u0026#34;) return cls(descr[0], descr[1]) class Dog(Animal): def __init__(self, *args: Any, **kwargs: Any) -\u0026gt; None: super().__init__(*args, **kwargs) @property def legs(self) -\u0026gt; int: return 4 if __name__ == \u0026#34;__main__\u0026#34;: dog = Dog.from_description(\u0026#34;Matt | woof\u0026#34;) print(dog.legs) # Mypy complains here! The class Animal has a from_description class method that acts as an additional constructor. It takes a description string, and then builds and returns an instance of the same class. The return type of the method is annotated as Animal here. However, doing this makes the child class Dog conflate its identity with the Animal class. If you execute the snippet, it won\u0026rsquo;t raise any runtime error. Also, Mypy will complain about the type:\nsrc.py:27: error: \u0026#34;Animal\u0026#34; has no attribute \u0026#34;legs\u0026#34; print(dog.legs) # Mypy complains here! ^ Found 1 error in 1 file (checked 1 source file) To fix this, we\u0026rsquo;ll have to make sure that the return type of the from_description class method doesn\u0026rsquo;t confuse the type checker. This is one way to do this:\nfrom __future__ import annotations from typing import TypeVar T = TypeVar(\u0026#34;T\u0026#34;, bound=\u0026#34;Animal\u0026#34;) class Animal: def __init__(self, name: str, says: str) -\u0026gt; None: self.name = name self.says = says @classmethod # In \u0026lt;Python3.9, Use typing.Type[T]. def from_description(cls: type[T], description: str = \u0026#34;|\u0026#34;) -\u0026gt; T: descr = description.split(\u0026#34;|\u0026#34;) return cls(descr[0], descr[1]) ... In the above snippet, first I had to declare a TypeVar and bind that to the Animal class. Then I had to explicitly type the cls variable in the from_description method. This time, the type checker will be happy. While this isn\u0026rsquo;t a lot of work, it surely goes against the community convention. Usually, we don\u0026rsquo;t explicitly type the self, cls variables and instead, let the type checker figure out their types. Also, subjectively, this sticks out like a sore thumb.\nPEP-673 allows us to solve the issue elegantly:\n# src.py from __future__ import annotations import sys if sys.version_info \u0026gt;= (3, 11): from typing import Self else: from typing_extensions import Self class Animal: def __init__(self, name: str, says: str) -\u0026gt; None: self.name = name self.says = says @classmethod def from_description(cls, description: str = \u0026#34;|\u0026#34;) -\u0026gt; Self: descr = description.split(\u0026#34;|\u0026#34;) return cls(descr[0], descr[1]) ... If you run Mypy against the second snippet, it won\u0026rsquo;t complain.\nTyping instance methods that return self Take a look at this:\n# src.py from __future__ import annotations import sys if sys.version_info \u0026gt;= (3, 11): from typing import Self else: from typing_extensions import Self class Counter: def __init__(self, start: int = 1) -\u0026gt; None: self.val = start def increment(self) -\u0026gt; Self: self.val += 1 return self def decrement(self) -\u0026gt; Self: self.val -= 1 return self The increment and decrement method of the Counter class return the instance of the same class after performing the operations on the start value. This is a perfect case where the Self type can be useful.\nTyping __new__ methods You can also type the __new__ method easily:\nfrom __future__ import annotations import sys if sys.version_info \u0026gt;= (3, 11): from typing import Self else: from typing_extensions import Self from typing import Any class Config: def __new__(cls, var: int, *args: Any, **kwargs: Any) -\u0026gt; Self: \u0026#34;\u0026#34;\u0026#34;Validate the value before constructing the class.\u0026#34;\u0026#34;\u0026#34; if not 0 \u0026lt;= var \u0026lt; 10: raise TypeError( \u0026#34;\u0026#39;var\u0026#39; must be a positive integer between 0 and 9\u0026#34;, ) return super().__new__(cls) def __init__(self, var: int) -\u0026gt; None: self.var = var The __new__ method in the Config class validates the var before constructing an instance of the class. The Self type makes it easy to annotate the method.\nFurther reading Tweet by Raymond Hettinger ","permalink":"https://rednafi.com/python/self-type/","summary":"\u003cp\u003e\u003ca href=\"https://www.python.org/dev/peps/pep-0673/\"\u003ePEP-673\u003c/a\u003e introduces the \u003ccode\u003eSelf\u003c/code\u003e type and it\u0026rsquo;s coming to Python 3.11. However, you can\nalready use that now via the \u003ca href=\"https://typing.readthedocs.io/\"\u003etyping_extensions\u003c/a\u003e module.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003eSelf\u003c/code\u003e type makes annotating methods that return the instances of the corresponding\nclasses trivial. Before this, you\u0026rsquo;d have to do some mental gymnastics to statically type\nsituations as follows:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003e__future__\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eannotations\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etyping\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eAnimal\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003esays\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ename\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esays\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003esays\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nd\"\u003e@classmethod\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003efrom_description\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003ecls\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edescription\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;|\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003eAnimal\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003edescr\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edescription\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esplit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;|\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"bp\"\u003ecls\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edescr\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003edescr\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eDog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eAnimal\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"nb\"\u003esuper\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"fm\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nd\"\u003e@property\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003elegs\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"vm\"\u003e__name__\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003edog\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eDog\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_description\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Matt | woof\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edog\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elegs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# Mypy complains here!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe class \u003ccode\u003eAnimal\u003c/code\u003e has a \u003ccode\u003efrom_description\u003c/code\u003e class method that acts as an additional\nconstructor. It takes a description string, and then builds and returns an instance of the\nsame class. The return type of the method is annotated as \u003ccode\u003eAnimal\u003c/code\u003e here. However, doing this\nmakes the child class \u003ccode\u003eDog\u003c/code\u003e conflate its identity with the \u003ccode\u003eAnimal\u003c/code\u003e class. If you execute\nthe snippet, it won\u0026rsquo;t raise any runtime error. Also, Mypy will complain about the type:\u003c/p\u003e","title":"Self type in Python"},{"content":"In Python, even though I adore writing tests in a functional manner via pytest, I still have a soft corner for the tools provided in the unittest.mock module. I like the fact it\u0026rsquo;s baked into the standard library and is quite flexible. Moreover, I\u0026rsquo;m yet to see another mock library in any other language or in the Python ecosystem that allows you to mock your targets in such a terse, flexible, and maintainable fashion.\nSo, in almost all the tests that I write for both my OSS projects and at my workplace, I use unittest.mock.patch exclusively for performing mock-patch. Consider this example:\n# src.py from __future__ import annotations import random # In \u0026lt;Python3.9, import this from the \u0026#39;typing\u0026#39; module. from collections.abc import Sequence def prepend_random(choices: Sequence[str], target: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Prepend a random prefix from the choices squence to the target string. \u0026#34;\u0026#34;\u0026#34; return f\u0026#34;{random.choice(choices)}_{target}\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: print(prepend_random([\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, \u0026#34;mars\u0026#34;], target=\u0026#34;greet\u0026#34;)) Here, the prepend_random function prepends a random prefix from the choices sequence to a target string. To accomplish this randomness, I used the random.choice function from the standard library. Now, the question is, how\u0026rsquo;d you test this. The function prepend_random has one global dependency; it\u0026rsquo;s the random.choice function and you\u0026rsquo;ll need to mock it out. Otherwise, you won\u0026rsquo;t be able to test the enclosing prepend_random function in a determinstic way. Here\u0026rsquo;s how you might test it with pytest:\n# src.py ... from unittest.mock import patch @patch(\u0026#34;src.random.choice\u0026#34;, return_value=\u0026#34;test_choice\u0026#34;, autospec=True) def test_prepend_random(mock_random_choice): choices = (\u0026#34;some\u0026#34;, \u0026#34;choice\u0026#34;) target = \u0026#34;test_target\u0026#34; expected_value = \u0026#34;test_choice_test_target\u0026#34; assert prepend_random(choices, target) == expected_value mock_random_choice.assert_called_once_with(choices) ... If you run pytest -v -s src.py, the test will pass. The unittest.mock.patch function is used here to mock the random.choice function and guarantee that it returns a controlled value instead of a random one. Shaving off this randomness of the random.choice function helps us test the behaviors of the prepend_random function in a more reproducible fashion. The autospec=True makes sure that the behavior of the mocked object — in this case, the function signature — is the same as the original object.\nAnother thing is, you can also use the patch function as a context manager like this:\n# src.py ... def test_prepend_random(): choices = (\u0026#34;some\u0026#34;, \u0026#34;choice\u0026#34;) target = \u0026#34;test_target\u0026#34; expected_value = \u0026#34;test_choice_test_target\u0026#34; with patch( \u0026#34;src.random.choice\u0026#34;, return_value=\u0026#34;test_choice\u0026#34;, autospec=True ) as mock_random_choice: assert prepend_random(choices, target) == expected_value mock_random_choice.assert_called_once_with(choices) ... While this works, the situation quickly spirals out of control whenever you need to test out multiple behaviors of a function and you want to achieve loose coupling between the tests by disentangling the behaviors in separate test functions. In that case, you\u0026rsquo;ll need to mock out the dependencies in each test function.\nThe situation worsens when each of your target functions has multiple dependencies. To be specific, if your target function has m behaviors and n dependencies that need to be mocked out, then the number of the patch decorators that practically do the same thing will be m x n. Just for a single function, it\u0026rsquo;ll create a monstrosity similar to this:\n# src.py from unittest.mock import patch ... def func() -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Target function that\u0026#39;ll be tested.\u0026#34;\u0026#34;\u0026#34; dep_1() dep_2() return 42 @patch(\u0026#34;src.dep_1\u0026#34;, autospec=True) @patch(\u0026#34;src.dep_2\u0026#34;, autospec=True) def test_func_error(mock_dep_1, mock_dep_2): \u0026#34;\u0026#34;\u0026#34;Behavior one.\u0026#34;\u0026#34;\u0026#34; ... @patch(\u0026#34;src.dep_1\u0026#34;, autospec=True) @patch(\u0026#34;src.dep_2\u0026#34;, autospec=True) def test_func_ok(mock_dep_1, mock_dep_2): \u0026#34;\u0026#34;\u0026#34;Behavior two.\u0026#34;\u0026#34;\u0026#34; ... Now imagine the situation for multiple target functions with multiple behaviors where testing each behavior requires multiple dependencies. The DRY gods will be furious!\nThe situation can be improved by wrapping the tests in a unittest style class and mocking out the common dependencies in the class scope as follows:\n# src.py from unittest.mock import patch ... @patch(\u0026#34;src.dep_1\u0026#34;, return_value=42, autospec=True) @patch(\u0026#34;src.dep_2\u0026#34;, return_value=42, autospec=True) class TestFunc: def test_func_error(self): ... def test_func_ok(self): ... The above solution forces us to write unittest style OOP-driven tests and I\u0026rsquo;d like to avoid that in my test suite. Also, this approach will mock all the dependencies in the class scope and you can\u0026rsquo;t opt-out of mocked dependencies if some of your tests don\u0026rsquo;t need that. We can do better. Let\u0026rsquo;s rewrite a slightly modified version of the above case with pytest.fixture. Here\u0026rsquo;s how to do it:\n# src.py from __future__ import annotations import pytest def dep_1(): pass def dep_2(): pass def func(error: bool = False) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Target function that we\u0026#39;re going to test.\u0026#34;\u0026#34;\u0026#34; dep_1() dep_2() if error: raise TypeError(\u0026#34;Dummy type error.\u0026#34;) return 42 @pytest.fixture def mock_dep_1(): with patch(\u0026#34;src.dep_1\u0026#34;, return_value=0, autospec=True) as m: yield m @pytest.fixture def mock_dep_2(): with patch(\u0026#34;src.dep_2\u0026#34;, return_value=0, autospec=True) as m: yield m def test_func_error(mock_dep_1, mock_dep_2): \u0026#34;\u0026#34;\u0026#34;Test one behavior.\u0026#34;\u0026#34;\u0026#34; with pytest.raises(TypeError, match=\u0026#34;Dummy type error.\u0026#34;): func(error=True) mock_dep_1.assert_called_once() mock_dep_2.assert_called_once() def test_func_ok(mock_dep_1, mock_dep_2): \u0026#34;\u0026#34;\u0026#34;Test another behavior.\u0026#34;\u0026#34;\u0026#34; assert func() == 42 mock_dep_1.assert_called_once() mock_dep_2.assert_called_once() The target function func has two dependencies that need to be mocked-up — dep_1 and dep_2. I mocked out the dependencies using the unittest.mock.patch interjector as context managers while wrapping them in separate functions decorated with the @pytest.fixture decorator.\nPay attention to the yield statement in the fixture functions. Pytest also allows you to use return statement in fixtures. However, in this case, making the fixture functions return generator objects was necessary. This way, the fixture function makes sure that the teardown logic in the with patch()... block gets executed. Had you used return here, the logic in the __exit__ block of the patch context manager wouldn\u0026rsquo;t have the chance to be executed. If you replace the yield statement with return and try to run the above snippet, pytest will throw an error.\nYou can also write your custom teardown logic after the yield statement and pytest will execute the logic each time the fixture gets executed. It\u0026rsquo;s similar to how teardown works in unittest but in a functional and decoupled manner.\nThis approach has the following advantages:\nIt appeases the DRY gods.\nYou won\u0026rsquo;t have to wrap your tests in a class to avoid patching the same objects multiple times.\nThis makes the mocked dependency usage more composable. In a test, you can pick and choose which dependencies you need in their mocked form and which dependencies you don\u0026rsquo;t want to be mocked. If you don\u0026rsquo;t want a particular dependency to be mocked in a test, then don\u0026rsquo;t pass the corresponding fixture as an argument of the test function.\nIf some of your mocked dependencies don\u0026rsquo;t vary much during their test lifetime, you can change the scope of the fixture to speed up the overall execution. By default, fixtures run in function scope; that means, the fixture (mocking) will be executed once per test function. This behavior can be changed via using the scope parameter of the @pytest.fixture(scope=...) decorator. Other allowed scopes are module and session. Module scope means, the fixture will be executed once per test module and session scope means, the fixture will run once per pytest session.\nAnother practical example The following snippet defines the get and post functions that make GET and POST requests to a URL respectively. I used the HTTPx library to make the requests. Here, the functions make external network calls to the https://httpbin.org URL:\n# src.py from __future__ import annotations from typing import Any # You\u0026#39;ll have to pip install this. import httpx client = httpx.Client(headers={\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;}) def get(url: str) -\u0026gt; httpx.Response: return client.get(url) def post(url: str, data: dict[str, Any]) -\u0026gt; httpx.Response: return client.post(url, json=data) if __name__ == \u0026#34;__main__\u0026#34;: r_get = get(\u0026#34;https://httpbin.org/get\u0026#34;) print(r_get.status_code) r_post = post(\u0026#34;https://httpbin.org/post\u0026#34;, data={\u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34;}) print(r_post.status_code) Running the snippet will print the functions\u0026rsquo; respective HTTP response codes. If you look closely, you\u0026rsquo;ll notice that I\u0026rsquo;m instantiating the client object in the global scope. This is because, both the GET and POST API calls share the same header. Let\u0026rsquo;s see how you can test these two functions:\n# test_src.py from http import HTTPStatus from typing import Any from unittest.mock import patch import httpx import pytest import src @pytest.fixture(scope=\u0026#34;module\u0026#34;) def mock_get(): with patch.object(src, \u0026#34;client\u0026#34;, autospec=True) as m: m.get.return_value = httpx.Response( status_code=200, json={\u0026#34;a\u0026#34;: \u0026#34;b\u0026#34;}, ) yield m @pytest.fixture(scope=\u0026#34;module\u0026#34;) def mock_post(): with patch.object(src, \u0026#34;client\u0026#34;, autospec=True) as m: m.post.return_value = httpx.Response( status_code=201, json={\u0026#34;a\u0026#34;: \u0026#34;b\u0026#34;}, ) yield m def test_get(mock_get): assert get(\u0026#34;test_url\u0026#34;).status_code == HTTPStatus.OK mock_get.get.assert_called() def test_get(mock_post): assert post(\u0026#34;test_url\u0026#34;, {}).status_code == HTTPStatus.CREATED mock_post.post.assert_called() The get and post function share a global dependency, the client. Also, these functions have side effects — since they make external network calls. We\u0026rsquo;ll have to mock the functions in a way so that our tests are completely isolated and they don\u0026rsquo;t make any network calls. I mocked out the functions in the mock_get and mock_post fixtures respectively. The functions are mocked in a way that whenever the original functions are called in the mock scope, they\u0026rsquo;ll return consistent values without making any network call.\nSince client was instantiated in the global scope, it had to be mocked using the patch.object(...) interjector. Also, notice how the mocked-up get and post functions' return-values mimic their orginal return-values. In the above case, the fixture runs in the module scope, which implies, they\u0026rsquo;ll only run once in the entire test module. This makes the test session quicker. However, keep in mind that making fixtures run in the module scope has also its demerits. Since the target functions get mocked and stay mocked through the entire module, it can subtly create coupling between your test functions if you aren\u0026rsquo;t careful.\nFurther reading Test async code with pytest-asyncio Unittest mock — mock object library ","permalink":"https://rednafi.com/python/patch-with-pytest-fixture/","summary":"\u003cp\u003eIn Python, even though I adore writing tests in a functional manner via pytest, I still have\na soft corner for the tools provided in the \u003ccode\u003eunittest.mock\u003c/code\u003e module. I like the fact it\u0026rsquo;s\nbaked into the standard library and is quite flexible. Moreover, I\u0026rsquo;m yet to see another\n\u003ccode\u003emock\u003c/code\u003e library in any other language or in the Python ecosystem that allows you to mock your\ntargets in such a terse, flexible, and maintainable fashion.\u003c/p\u003e","title":"Patching test dependencies via pytest fixture \u0026 unittest mock"},{"content":"Static type checkers like Mypy follow your code flow and statically try to figure out the types of the variables without you having to explicitly annotate inline expressions. For example:\n# src.py from __future__ import annotations def check(x: int | float) -\u0026gt; str: if not isinstance(x, int): reveal_type(x) # Type is now \u0026#39;float\u0026#39;. else: reveal_type(x) # Type is now \u0026#39;int\u0026#39;. return str(x) The reveal_type function is provided by Mypy and you don\u0026rsquo;t need to import this. But remember to remove the function before executing the snippet. Otherwise, Python will raise a runtime error as the function is only understood by Mypy. If you run Mypy against this snippet, it\u0026rsquo;ll print the following lines:\nsrc.py:6: note: Revealed type is \u0026#34;builtins.float\u0026#34; src.py:10: note: Revealed type is \u0026#34;builtins.int\u0026#34; Here, I didn\u0026rsquo;t have to explicitly tell the type checker how the conditionals narrow the types.\nStatic type checkers commonly employ a technique called \u0026rsquo;type narrowing\u0026rsquo; to determine a more precise type of an expression within a program\u0026rsquo;s code flow. When type narrowing is applied within a block of code based on a conditional code flow statement (such as if and while statements), the conditional expression is sometimes referred to as a \u0026rsquo;type guard\u0026rsquo;. — PEP-647\nSo, in the above snippet, Mypy performed type narrowing to determine the more precise type of the variable x; and the if ... else conditionals, in this case, is known as type guards.\nHowever, when the type checker encounters a complex expression, often time, it can\u0026rsquo;t figure out the types statically. Mypy will complain when it faces one of these issues:\nfrom __future__ import annotations # In \u0026lt;Python3.9, import this from the \u0026#39;typing\u0026#39; module. from collections.abc import Sequence def check_sequence_str(container: Sequence[object]) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Check all objects in the container is of type str.\u0026#34;\u0026#34;\u0026#34; return all(isinstance(elem, str) for elem in container) def concat( container: Sequence[object], sep: str = \u0026#34;-\u0026#34;, ) -\u0026gt; str | None: \u0026#34;\u0026#34;\u0026#34;Concat a sequence of string with the \u0026#39;sep\u0026#39;.\u0026#34;\u0026#34;\u0026#34; if check_sequence_str(container): return f\u0026#34;{sep}\u0026#34;.join(container) if __name__ == \u0026#34;__main__\u0026#34;: # Mypy complains here, as it can\u0026#39;t figure out the # container type. concat([\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;]) Here, the check_sequence_str checks whether the input argument is a sequence of strings in runtime. Then in the concat function, I used it to check whether the input conforms to the expected type requirement; if it does, the function performs string concatenation on the input and returns the value. Otherwise, it returns None. If you run mypy against this, it\u0026rsquo;ll complain:\nsrc.py:22: error: Argument 1 to \u0026#34;join\u0026#34; of \u0026#34;str\u0026#34; has incompatible type \u0026#34;Sequence[object]\u0026#34;; expected \u0026#34;Iterable[str]\u0026#34; return f\u0026#34;{sep}\u0026#34;.join(container) ^ Found 1 error in 1 file (checked 1 source file) The type checker can\u0026rsquo;t figure out that the container type is list[str].\nFunctions like check_sequence_str that — checks the type of an input object and returns a boolean — are called type guard functions. PEP-647 proposed a TypeGuard class to help the type checkers to narrow down types from more complex expressions. Python 3.10 added the TypeGuard class to the typing module. You can use it like this:\n# src.py ... # In \u0026lt;Python3.10, import this from \u0026#39;typing_extensions\u0026#39; module. from typing import TypeGuard def check_sequence_str( container: Sequence[object], ) -\u0026gt; TypeGuard[Sequence[str]]: \u0026#34;\u0026#34;\u0026#34;Check all objects in the container is of type str.\u0026#34;\u0026#34;\u0026#34; return all(isinstance(elem, str) for elem in container) ... Notice that the return type now has the expected type defined inside the TypeGuard generic. Now, Mypy will be satisfied if you run it against the modified snippet.\nProperties of type guard functions You\u0026rsquo;ve already seen how check_sequence_str narrows down the type of an object in runtime. Functions like this can be loosely called user-defined type guard functions. However, to be considered a proper type guard function by the type checker, the callable needs to pass through a few more checks.\nWhen TypeGuard is used to annotate the return type of a function or method that accepts at least one parameter, that function or method is treated by type checkers as a user-defined type guard. The type argument provided for TypeGuard indicates the type that has been validated by the function. — PEP-647\nUsually, a type guard function only takes a single parameter and returns a boolean value based on the conformity of the type of the incoming object with the expected type. The expected type needs to be wrapped in TypeGuard and added as the return type annotation.\nType checkers will only check if the first positional argument conforms to the expected return type annotation. It\u0026rsquo;ll ignore other parameters if there is more than one.\nIf you define a type guard callable in a class, in that case, the type checker will ignore self/cls argument and check the second positional parameter for type conformity. Additional parameters won\u0026rsquo;t be checked.\nThe input type is usually wider than the output type. In our example case, the input type Sequence[object] is less specific than that of the return type Sequence[str]. However, this is mostly a convention and not enforced by any means.\nThe return type of a user-defined type guard function will normally refer to a type that is strictly \u0026ldquo;narrower\u0026rdquo; than the type of the first argument (that is, it\u0026rsquo;s a more specific type that can be assigned to the more general type). However, it is not required that the return type be strictly narrower. — PEP-647\nGeneric type guard functions User-defined type guards can be generic functions, as shown in this example:\nfrom __future__ import annotations # In \u0026lt;Python3.9, import these from the \u0026#39;typing\u0026#39; module. from collections.abc import Generator, Sequence # In \u0026lt;Python3.10, import TypeGuard from \u0026#39;typing_extensions\u0026#39;. from typing import TypeGuard, TypeVar T = TypeVar(\u0026#34;T\u0026#34;) def list_of_t( container: Sequence[T], types: tuple = (int, str), # Allowed types in the container. ) -\u0026gt; TypeGuard[list[T]]: return all(isinstance(elem, types) for elem in container) def process(container: Sequence[T]) -\u0026gt; Generator[T, None, None]: if list_of_t(container): for elem in container: yield elem if __name__ == \u0026#34;__main__\u0026#34;: container = [\u0026#34;jupiter\u0026#34;, \u0026#34;mars\u0026#34;] for elem in process(container): print(elem) Here, type guard function list_of_t is a generic function since it accepts a generic container Sequence[T]. The first parameter is the input type that the type checker will focus on, and the second parameter denotes the default types that are allowed inside the output list. Running the snippet will print jupiter and mars in the console and mypy will also be happy with the types.\nFurther reading PEP 647 - User-defined type guards Python type hints - how to narrow types with TypeGuard ","permalink":"https://rednafi.com/python/type-guard/","summary":"\u003cp\u003eStatic type checkers like Mypy follow your code flow and statically try to figure out the\ntypes of the variables without you having to explicitly annotate inline expressions. For\nexample:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003e__future__\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eannotations\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003echeck\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e \u003cspan class=\"o\"\u003e|\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"nb\"\u003eisinstance\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003ereveal_type\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"c1\"\u003e# Type is now \u0026#39;float\u0026#39;.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003ereveal_type\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"c1\"\u003e# Type is now \u0026#39;int\u0026#39;.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe \u003ccode\u003ereveal_type\u003c/code\u003e function is provided by Mypy and you don\u0026rsquo;t need to import this. But\nremember to remove the function before executing the snippet. Otherwise, Python will raise a\nruntime error as the function is only understood by Mypy. If you run Mypy against this\nsnippet, it\u0026rsquo;ll print the following lines:\u003c/p\u003e","title":"Narrowing types with TypeGuard in Python"},{"content":"Technically, the type of None in Python is NoneType. However, you\u0026rsquo;ll rarely see types.NoneType being used in the wild as the community has pretty much adopted None to denote the type of the None singleton. This usage is also documented in PEP-484.\nWhenever a callable doesn\u0026rsquo;t return anything, you usually annotate it as follows:\n# src.py from __future__ import annotations def abyss() -\u0026gt; None: return But sometimes a callable raises an exception and never gets the chance to return anything. Consider this example:\n# src.py from __future__ import annotations import logging def raise_verbose_type_error(message: str) -\u0026gt; None: logging.error(\u0026#34;Raising type error\u0026#34;) raise TypeError(message) if __name__ == \u0026#34;__main__\u0026#34;: raise_verbose_type_error(\u0026#34;type error occured\u0026#34;) This semantically makes sense and if you run Mypy against the snippet, it won\u0026rsquo;t complain. However, there\u0026rsquo;s one difference between a callable that returns an implicit None vs one that raises an exception. In the latter case, if you run any code after calling the callable, that code won\u0026rsquo;t be reachable. But Mypy doesn\u0026rsquo;t statically catch that or warn you about the potential dead code. This is apparently fine by the type checker:\n... if __name__ == \u0026#34;__main__\u0026#34;: raise_verbose_type_error(\u0026#34;type error occured\u0026#34;) print( \u0026#34;This part of the code is unreachable due to the exception\u0026#34; \u0026#34;above, but Mypy doesn\u0026#39;t warn us.\u0026#34; ) NoReturn type can be used in cases like this to warn us about potential dead code ahead. To utilize it, you\u0026rsquo;d type the above snippet like this:\n# src.py from __future__ import annotations import logging from typing import NoReturn def raise_verbose_type_error(message: str) -\u0026gt; NoReturn: logging.error(\u0026#34;Raising type error\u0026#34;) raise TypeError(message) if __name__ == \u0026#34;__main__\u0026#34;: raise_verbose_type_error(\u0026#34;type error occured\u0026#34;) print( \u0026#34;This part of the code is unreachable due to the exception\u0026#34; \u0026#34;above, but this time, Mypy will warn us.\u0026#34; ) Notice, that I changed the return type of the raise_verbose_type_error function to typing.NoReturn. Now, if you run Mypy against the snippet with the --warn-unreachable flag, it\u0026rsquo;ll complain:\nmypy --warn-unreachable src.py src.py:14: error: Statement is unreachable print( ^ Found 1 error in 1 file (checked 1 source file) More practical examples Callables containing infinite loops # src.py from __future__ import annotations import itertools from typing import NoReturn def run_indefinitely() -\u0026gt; NoReturn: for i in itertools.cycle(\u0026#34;abc\u0026#34;): print(i) if __name__ == \u0026#34;__main__\u0026#34;: run_indefinitely() print(\u0026#34; Dead code. Mypy will warn us.\u0026#34;) Mypy will warn us about the dead code.\nsrc.py:14: error: Statement is unreachable print( ^ Found 1 error in 1 file (checked 1 source file) Another case where NoReturn can be useful, is to type callables with while True loops. This is common in webservers:\n# src.py from __future__ import annotations from typing import NoReturn def loop_forever() -\u0026gt; NoReturn: while True: do_something() Callables that invoke \u0026lsquo;sys.exit()\u0026rsquo;, \u0026lsquo;os._exit()\u0026rsquo;, \u0026lsquo;os.execvp()\u0026rsquo;, etc Both sys.exit() and os._exit() do similar things. The former function raises the SystemExit() exception and exits the program without printing any stacktrace or whatsoever. On the other hand, the latter function exits the process immediately without letting the interpreter run any cleanup code. Prefer sys.exit() over os._exit().\nThe os.execvp() function execute a new program, replacing the current process. It never returns. Here\u0026rsquo;s how you\u0026rsquo;d type the callables that call these functions:\n# src.py from __future__ import annotations import os import sys from typing import NoReturn def call_sys_exit(code: int) -\u0026gt; NoReturn: sys.exit(code) def call_os_exit(code: int) -\u0026gt; NoReturn: os._exit(code) def call_os_execvp() -\u0026gt; NoReturn: os.execvp(\u0026#34;echo\u0026#34;, (\u0026#34;echo\u0026#34;, \u0026#34;hi\u0026#34;)) Further reading NoReturn vs None - Anthony explains What\u0026rsquo;s the point of NoReturn? - Adam Johnson ","permalink":"https://rednafi.com/python/why-noreturn-type-exists/","summary":"\u003cp\u003eTechnically, the type of \u003ccode\u003eNone\u003c/code\u003e in Python is \u003ccode\u003eNoneType\u003c/code\u003e. However, you\u0026rsquo;ll rarely see\n\u003ccode\u003etypes.NoneType\u003c/code\u003e being used in the wild as the community has pretty much adopted \u003ccode\u003eNone\u003c/code\u003e to\ndenote the type of the \u003ccode\u003eNone\u003c/code\u003e singleton. This usage is also \u003ca href=\"https://www.python.org/dev/peps/pep-0484/#using-none\"\u003edocumented in PEP-484\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWhenever a callable doesn\u0026rsquo;t return anything, you usually annotate it as follows:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003e__future__\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eannotations\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eabyss\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBut sometimes a callable raises an exception and never gets the chance to return anything.\nConsider this example:\u003c/p\u003e","title":"Why 'NoReturn' type exists in Python"},{"content":"While grokking the source code of the http.HTTPStatus module, I came across this technique to add extra attributes to the values of enum members. Now, to understand what do I mean by adding attributes, let\u0026rsquo;s consider the following example:\n# src.py from __future__ import annotations from enum import Enum class Color(str, Enum): RED = \u0026#34;Red\u0026#34; GREEN = \u0026#34;Green\u0026#34; BLUE = \u0026#34;Blue\u0026#34; Here, I\u0026rsquo;ve inherited from str to ensure that the values of the enum members are strings. This class can be used as follows:\n# src.py ... # Print individual members. print(f\u0026#34;{Color.RED=}\u0026#34;) # Print name as a string. print(f\u0026#34;{Color.GREEN.name=}\u0026#34;) # Print value. print(f\u0026#34;{Color.BLUE.value=}\u0026#34;) Running the script will print:\nColor.RED=\u0026lt;Color.RED: \u0026#39;Red\u0026#39;\u0026gt; Color.GREEN.name=\u0026#39;GREEN\u0026#39; Color.BLUE.value=\u0026#39;Blue\u0026#39; While this works but it\u0026rsquo;s evident that you can only assign a single value to an enum member. How\u0026rsquo;d you rewrite this if you needed multiple values attached to a single enum member?\nSuppose, in the above case, along with the color title, you also need to save the hex codes and short descriptions of the colors. One way you can achieve this is via the assignment of an immutable container as the value of an enum member:\n# src.py from __future__ import annotations from enum import Enum class Color(Enum): RED = (\u0026#34;Red\u0026#34;, \u0026#34;#ff0000\u0026#34;, \u0026#34;Ruby Red\u0026#34;) GREEN = (\u0026#34;Green\u0026#34;, \u0026#34;#00ff00\u0026#34;, \u0026#34;Guava Green\u0026#34;) BLUE = (\u0026#34;Blue\u0026#34;, \u0026#34;#0000ff\u0026#34;, \u0026#34;Baby Blue\u0026#34;) Here, I\u0026rsquo;m using a tuple to contain the title, hex code, and description of the Color members. This gets awkward whenever you\u0026rsquo;ll need to access the individual elements inside the tuple. You\u0026rsquo;ll have to use hardcoded indexes to access the elements of the tuple. This is how you\u0026rsquo;ll probably use it:\n... for c in Color: print( f\u0026#34;title={c.value[0]}, hex_code={c.value[1]}, \u0026#34; f\u0026#34;description={c.value[2]}\u0026#34; ) It prints:\ntitle=Red, hex_code=#ff0000, description=Ruby Red title=Green, hex_code=#00ff00, description=Guava Green title=Blue, hex_code=#0000ff, description=Baby Blue Hardcoding indexes in such a manner is fragile and will break if you drop a new value in the middle of the tuple assigned to an enum member. Also, it\u0026rsquo;s hard to reason through logic when you\u0026rsquo;ve to keep the semantic meanings of the index positions in your working memory. A better thing to do is to rewrite the enum in a way that\u0026rsquo;ll allow you to access different elements of the member values by their attribute names. Let\u0026rsquo;s do it:\nfrom __future__ import annotations from enum import Enum class Color(str, Enum): # Declaring the additional attributes here keeps mypy happy. hex_code: str description: str def __new__( cls, title: str, hex_code: str = \u0026#34;\u0026#34;, description: str = \u0026#34;\u0026#34; ) -\u0026gt; Color: obj = str.__new__(cls, title) obj._value_ = title obj.hex_code = hex_code obj.description = description return obj RED = (\u0026#34;Red\u0026#34;, \u0026#34;#ff0000\u0026#34;, \u0026#34;Ruby Red\u0026#34;) GREEN = (\u0026#34;Green\u0026#34;, \u0026#34;#00ff00\u0026#34;, \u0026#34;Guava Green\u0026#34;) BLUE = (\u0026#34;Blue\u0026#34;, \u0026#34;#0000ff\u0026#34;, \u0026#34;Baby Blue\u0026#34;) Here, I overrode the __new__ method of the class Color. Method __new__ is a special class method that you don\u0026rsquo;t need to decorate with the @classmethod decorator. It gets executed during the creation of the Color object; before the __init__ method. Other than the first argument cls, you can define the __new__ method with any number of arbitrarily named arguments.\nIn this case, the value of each member of Color will have three elements — title, hex_code, and description. So, I defined the __new__ method to accept those arguments. In the following line, the str class was initialized via obj = str.__new__(cls, title) and then title was assigned to the newly created string object via obj._value_=title. This line is crucial; without it, the enum won\u0026rsquo;t operate at all. This assignment makes sure that the Enum.member.value will return a string value.\nIn the next two lines, hex_code and description were attached to the member values via the obj.hex_code=hexcode and obj.description=description statements respectively.\nNow, you\u0026rsquo;ll be able to use this enum without any hardcoded shenanigans:\n... # Access the elements of the values of the members by names. print(f\u0026#34;{Color.RED.value=}\u0026#34;) print(f\u0026#34;{Color.BLUE.hex_code=}\u0026#34;) print(f\u0026#34;{Color.GREEN.description=}\u0026#34;) # Iterate through all the memebers. for c in Color: print( f\u0026#34;title={c.value}, hex_code={c.hex_code}, \u0026#34; f\u0026#34;description={c.description}\u0026#34; ) This will print:\nColor.RED.value=\u0026#39;Red\u0026#39; Color.BLUE.hex_code=\u0026#39;#0000ff\u0026#39; Color.GREEN.description=\u0026#39;Guava Green\u0026#39; title=Red, hex_code=#ff0000, description=Ruby Red title=Green, hex_code=#00ff00, description=Guava Green title=Blue, hex_code=#0000ff, description=Baby Blue ","permalink":"https://rednafi.com/python/add-attributes-to-enum-members/","summary":"\u003cp\u003eWhile grokking the source code of the \u003ca href=\"https://github.com/python/cpython/blob/6f1efd19a70839d480e4b1fcd9fecd3a8725824b/Lib/http/__init__.py#L6\"\u003ehttp.HTTPStatus\u003c/a\u003e module, I came across this technique\nto add extra attributes to the values of enum members. Now, to understand what do I mean by\nadding attributes, let\u0026rsquo;s consider the following example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003e__future__\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eannotations\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003eenum\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eEnum\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eColor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eEnum\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eRED\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Red\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eGREEN\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Green\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eBLUE\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Blue\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHere, I\u0026rsquo;ve inherited from \u003ccode\u003estr\u003c/code\u003e to ensure that the values of the enum members are strings.\nThis class can be used as follows:\u003c/p\u003e","title":"Add extra attributes to enum members in Python"},{"content":"The functools.wraps decorator allows you to keep your function\u0026rsquo;s identity intact after it\u0026rsquo;s been wrapped by a decorator. Whenever a function is wrapped by a decorator, identity properties like — function name, docstring, annotations of it get replaced by those of the wrapper function. Consider this example:\nfrom __future__ import annotations # In \u0026lt; Python 3.9, import this from the typing module. from collections.abc import Callable from typing import Any def log(func: Callable) -\u0026gt; Callable: def wrapper(*args: Any, **kwargs: Any) -\u0026gt; Any: \u0026#34;\u0026#34;\u0026#34;Internal wrapper.\u0026#34;\u0026#34;\u0026#34; val = func(*args, **kwargs) return val return wrapper @log def add(x: int, y: int) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Add two numbers. Parameters ---------- x : int First argument. y : int Second argument. Returns ------- int Returns the summation of two integers. \u0026#34;\u0026#34;\u0026#34; return x + y if __name__ == \u0026#34;__main__\u0026#34;: print(add.__doc__) print(add.__name__) Here, I\u0026rsquo;ve defined a simple logging decorator that wraps the add function. The function add has its own type annotations and docstring. So, you\u0026rsquo;d expect the docstring and name of the add function to be printed when the above snippet gets executed. However, running the script prints the following instead:\nInternal wrapper. wrapper This is surprising and probably not something you want. If you pay attention to the function wrapper in the log decorator, you\u0026rsquo;ll see that the identity properties of the wrapper function replace the identity properties of the wrapped function add. This can easily be avoided by decorating the wrapper function inside the log decorator with the functools.wraps decorator:\n# src.py from functools import wraps ... def log(func: Callable) -\u0026gt; Callable: @wraps(func) # Here\u0026#39;s the decorator! def wrapper(*args: Any, **kwargs: Any) -\u0026gt; Any: \u0026#34;\u0026#34;\u0026#34;Internal wrapper.\u0026#34;\u0026#34;\u0026#34; val = func(*args, **kwargs) return val return wrapper ... Now, running the script will return the expected output:\nAdd two numbers. Parameters ---------- x : int First argument. y : int Second argument. Returns ------- int Returns the summation of two integers. add I wanted to take a peek into how the functools.wraps decorator works internally. Turns out that the implementation is quite straightforward. Here\u0026rsquo;s the entire implementation from the functools.py module. For brevity\u0026rsquo;s sake, I\u0026rsquo;ve stripped out the comments and added type annotations:\n# functools.py from __future__ import annotations # In \u0026lt; Python 3.9, import this from the typing module. from collections.abc import Callable WRAPPER_ASSIGNMENTS = ( \u0026#34;__module__\u0026#34;, \u0026#34;__name__\u0026#34;, \u0026#34;__qualname__\u0026#34;, \u0026#34;__doc__\u0026#34;, \u0026#34;__annotations__\u0026#34;, ) WRAPPER_UPDATES = (\u0026#34;__dict__\u0026#34;,) def update_wrapper( wrapper: Callable, wrapped: Callable, assigned: tuple[str, ...] = WRAPPER_ASSIGNMENTS, updated: tuple[str, ...] = WRAPPER_UPDATES, ) -\u0026gt; Callable: for attr in assigned: try: value = getattr(wrapped, attr) except AttributeError: pass else: setattr(wrapper, attr, value) for attr in updated: getattr(wrapper, attr).update(getattr(wrapped, attr, {})) wrapper.__wrapped__ = wrapped return wrapper def wraps( wrapped: Callable, assigned: tuple[str, ...] = WRAPPER_ASSIGNMENTS, updated: tuple[str, ...] = WRAPPER_UPDATES, ) -\u0026gt; Callable: return partial( update_wrapper, wrapped=wrapped, assigned=assigned, updated=updated, ) The bulk of the work is done in the update_wrapper function. It copies the identity properties defined in WRAPPER_ASSIGNMENTS and WRAPPER_UPDATES — from the wrapped function over to the wrapper function. Here, the wrapped function is the decorated one (add function) and the wrapper function is the eponymous function inside the log decorator.\nSince you\u0026rsquo;ve already seen that whenever you try to introspect the identity properties of a wrapped function, the wrapper function obfuscates them and returns its own properties. However, if the identity properties are copied over from the wrapped to the wrapper function, your inspection will return the expected result. The update_wrapper function is doing exactly that.\nThe wraps function just binds the input arguments with the update_wrapper function using the partial function defined in the same module. This allows us to use the wraps function as a decorator.\nYou can also directly use the update_wrapper function to get the same result should you choose to do so. Here\u0026rsquo;s how to do it:\n# src.py from functools import update_wrapper ... def log(func: Callable) -\u0026gt; Callable: def wrapper(*args: Any, **kwargs: Any) -\u0026gt; Any: \u0026#34;\u0026#34;\u0026#34;Internal wrapper.\u0026#34;\u0026#34;\u0026#34; val = func(*args, **kwargs) return val # Only this line is different! return update_wrapper(func, wrapper) ... Further reading functools.update_wrapper source code ","permalink":"https://rednafi.com/python/internals-of-functools-wraps/","summary":"\u003cp\u003eThe \u003ccode\u003efunctools.wraps\u003c/code\u003e decorator allows you to keep your function\u0026rsquo;s identity intact after\nit\u0026rsquo;s been wrapped by a decorator. Whenever a function is wrapped by a decorator, identity\nproperties like — function name, docstring, annotations of it get replaced by those of the\nwrapper function. Consider this example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003e__future__\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eannotations\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# In \u0026lt; Python 3.9, import this from the typing module.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003ecollections.abc\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eCallable\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etyping\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003elog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003efunc\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eCallable\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003eCallable\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ewrapper\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;Internal wrapper.\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003eval\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003efunc\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eval\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ewrapper\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nd\"\u003e@log\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eadd\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;Add two numbers.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    Parameters\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    ----------\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    x : int\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        First argument.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    y : int\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        Second argument.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    Returns\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    -------\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    int\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        Returns the summation of two integers.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"vm\"\u003e__name__\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eadd\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"vm\"\u003e__doc__\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eadd\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"vm\"\u003e__name__\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHere, I\u0026rsquo;ve defined a simple logging decorator that wraps the \u003ccode\u003eadd\u003c/code\u003e function. The function\n\u003ccode\u003eadd\u003c/code\u003e has its own type annotations and docstring. So, you\u0026rsquo;d expect the \u003cstrong\u003edocstring\u003c/strong\u003e and\n\u003cstrong\u003ename\u003c/strong\u003e of the \u003ccode\u003eadd\u003c/code\u003e function to be printed when the above snippet gets executed. However,\nrunning the script prints the following instead:\u003c/p\u003e","title":"Peeking into the internals of Python's 'functools.wraps' decorator"},{"content":"I was working with a rate-limited API endpoint where I continuously needed to send short-polling GET requests without hitting HTTP 429 error. Perusing the API doc, I found out that the API endpoint only allows a maximum of 100 requests per second. So, my goal was to find out a way to send the maximum amount of requests without encountering the too-many-requests error.\nI picked up Python\u0026rsquo;s asyncio and the amazing HTTPx library by Tom Christie to make the requests. This is the naive version that I wrote in the beginning; it quickly hits the HTTP 429 error:\n# src.py from __future__ import annotations import asyncio from http import HTTPStatus from pprint import pprint import httpx # Reusing http client allows us to reuse a pool of TCP connections. client = httpx.AsyncClient() async def make_one_request(url: str, num: int) -\u0026gt; httpx.Response: headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} print(f\u0026#34;Making request {num}\u0026#34;) r = await client.get(url, headers=headers) if r.status_code == HTTPStatus.OK: return r raise ValueError( f\u0026#34;Unexpected Status: Http status code is {r.status_code}.\u0026#34;, ) async def make_many_requests(url: str, count: int) -\u0026gt; list[httpx.Response]: tasks = [] for num in range(count): task = asyncio.create_task(make_one_request(url, num)) tasks.append(task) results = await asyncio.gather(*tasks) # All the results will look the same, so we\u0026#39;re just printing one. print(\u0026#34;\\n\u0026#34;) print(\u0026#34;Final result:\u0026#34;) print(\u0026#34;==============\\n\u0026#34;) pprint(results[0].json()) return results if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(make_many_requests(\u0026#34;https://httpbin.org/get\u0026#34;, count=200)) Here, for this demonstration, I\u0026rsquo;m using the https://httpbin.org/get endpoint that\u0026rsquo;s openly accessible. This particular endpoint doesn\u0026rsquo;t impose any limit on the number of requests per second. However, in the above snippet, if you inspect the for loop in the make_many_requests function, you\u0026rsquo;ll see that it\u0026rsquo;s sending 200 concurrent requests without any restrictions.\nAlso, the snippet will raise a ValueError if it encounters an HTTP-429-too-many-requests error. Running the script produces the following output:\nMaking request 0 Making request 1 Making request 2 Making request 3 Making request 4 Making request 5 Making request 6 ... Making request 199 Final result: ============== {\u0026#39;args\u0026#39;: {}, \u0026#39;headers\u0026#39;: {\u0026#39;Accept\u0026#39;: \u0026#39;*/*\u0026#39;, \u0026#39;Accept-Encoding\u0026#39;: \u0026#39;gzip, deflate\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Host\u0026#39;: \u0026#39;httpbin.org\u0026#39;, \u0026#39;User-Agent\u0026#39;: \u0026#39;python-httpx/0.22.0\u0026#39;, \u0026#39;X-Amzn-Trace-Id\u0026#39;: \u0026#39;Root=1-62042fc6-007ccd7d6b2cf5c15c0963f6\u0026#39;}, \u0026#39;origin\u0026#39;: \u0026#39;103.84.246.3\u0026#39;, \u0026#39;url\u0026#39;: \u0026#39;https://httpbin.org/get\u0026#39;} From the output, it\u0026rsquo;s pretty evident that the script is hammering the server without any delay between the concurrent requests. While 200 requests per second may not be that high but even if there weren\u0026rsquo;t any restrictions, sending so many rogue requests like that isn\u0026rsquo;t desirable. It\u0026rsquo;s easy to overwhelm any service if you\u0026rsquo;re not being careful.\nLuckily, Python exposes a Semaphore construct that allows you to synchronize the concurrent workers (threads, processes, or coroutines) regarding how they should access a shared resource. All concurrency primitives in Python have semaphores to help you control resource access. This means if you\u0026rsquo;re using any of the — multiprocessing, threading, or asyncio module, you can take advantage of it. From the asyncio docs:\nA semaphore manages an internal counter which is decremented by each acquire() call and incremented by each release() call. The counter can never go below zero; when acquire() finds that it is zero, it blocks, waiting until some task calls release().\nYou can use the semaphores in the above script as follows:\n... # Initialize a semaphore object with a limit of 3. limit = asyncio.Semaphore(3) async def make_one_request(url: str, num: int) -\u0026gt; httpx.Response: headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} # No more than 3 concurrent workers will be able to make # get request at the same time. async with limit: print(f\u0026#34;Making request {num}\u0026#34;) r = await client.get(url, headers=headers) # When workers hit the limit, they\u0026#39;ll wait for a second # before making more requests. if limit.locked(): print(\u0026#34;Concurrency limit reached, waiting ...\u0026#34;) await asyncio.sleep(1) if r.status_code == HTTPStatus.OK: return r raise ValueError( f\u0026#34;Unexpected Status: Http status code is {r.status_code}.\u0026#34;, ) ... Here, I only had to change the make_one_request function to take advantage of the semaphore. First, I initialized an asyncio.Semaphore object with the limit 3. This means the semaphore won\u0026rsquo;t allow more than three concurrent workers to make HTTP GET requests at the same time. The semaphore instance is then used as a context manager. Inside the async with block, the line starting with if limit.locked() makes the workers wait for a second whenever the concurrency limit is reached. If you execute the script, it\u0026rsquo;ll produce the following output:\nMaking request 0 Making request 1 Making request 2 Concurrency limit reached, waiting ... Concurrency limit reached, waiting ... Concurrency limit reached, waiting ... Making request 3 Making request 4 Making request 5 Concurrency limit reached, waiting ... Concurrency limit reached, waiting ... Concurrency limit reached, waiting ... Making request 6 Making request 7 Making request 8 ... Making request 199 ... The output makes it clear that no more than 3 async functions are making concurrent requests to the server at the same time. You can tune the number of concurrent workers by changing the limit in the asyncio.Semaphore object.\nComplete script # src.py from __future__ import annotations import asyncio from http import HTTPStatus from pprint import pprint import httpx # Reusing http client allows us to reuse a pool of TCP connections. client = httpx.AsyncClient() # Initialize a semaphore object with a limit of 3. limit = asyncio.Semaphore(3) async def make_one_request(url: str, num: int) -\u0026gt; httpx.Response: headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} # No more than 3 concurrent workers will be able to make # get request at the same time. async with limit: print(f\u0026#34;Making request {num}\u0026#34;) r = await client.get(url, headers=headers) # When workers hit the limit, they\u0026#39;ll wait for a second # before making more requests. if limit.locked(): print(\u0026#34;Concurrency limit reached, waiting ...\u0026#34;) await asyncio.sleep(1) if r.status_code == HTTPStatus.OK: return r raise ValueError( f\u0026#34;Unexpected Status: Http status code is {r.status_code}.\u0026#34;, ) async def make_many_requests(url: str, count: int) -\u0026gt; list[httpx.Response]: tasks = [] for num in range(count): task = asyncio.create_task(make_one_request(url, num)) tasks.append(task) results = await asyncio.gather(*tasks) # All the results will look the same, so we\u0026#39;re just printing one. print(\u0026#34;\\n\u0026#34;) print(\u0026#34;Final result:\u0026#34;) print(\u0026#34;==============\\n\u0026#34;) pprint(results[0].json()) return results if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(make_many_requests(\u0026#34;https://httpbin.org/get\u0026#34;, count=200)) ","permalink":"https://rednafi.com/python/limit-concurrency-with-semaphore/","summary":"\u003cp\u003eI was working with a rate-limited API endpoint where I continuously needed to send\nshort-polling GET requests without hitting HTTP 429 error. Perusing the API doc, I found out\nthat the API endpoint only allows a maximum of 100 requests per second. So, my goal was to\nfind out a way to send the maximum amount of requests without encountering the\ntoo-many-requests error.\u003c/p\u003e\n\u003cp\u003eI picked up Python\u0026rsquo;s \u003ca href=\"https://docs.python.org/3/library/asyncio.html\"\u003easyncio\u003c/a\u003e and the amazing \u003ca href=\"https://www.python-httpx.org/\"\u003eHTTPx\u003c/a\u003e library by Tom Christie to make the\nrequests. This is the naive version that I wrote in the beginning; it quickly hits the HTTP\n429 error:\u003c/p\u003e","title":"Limit concurrency with semaphore in Python asyncio"},{"content":"Whether you like it or not, the split world of sync and async functions in the Python ecosystem is something we\u0026rsquo;ll have to live with; at least for now. So, having to write things that work with both sync and async code is an inevitable part of the journey. Projects like Starlette, HTTPx can give you some clever pointers on how to craft APIs that are compatible with both sync and async code.\nLately, I\u0026rsquo;ve been calling constructs that are compatible with both synchronous and asynchronous paradigms as Amphibian Constructs.\nSo, I wanted to write an amphibian decorator that\u0026rsquo;d work with both sync and async functions. Let\u0026rsquo;s consider writing a trivial decorator that\u0026rsquo;ll tag the wrapped function. Here, by tagging I mean, the decorator will attach a _tags attribute to the wrapped function where the value of the tag can be passed as the function parameter.\nThis type of tagging can be helpful if you want to write code that\u0026rsquo;ll classify functions based on their tags and do interesting things with them. Locust uses this concept of tagging to select and deselect load-testing routines in the CLI. Also, @pytest.mark.* utilizes a similar concept.\nHere\u0026rsquo;s how you can do that:\n# src.py from __future__ import annotations import inspect # In \u0026lt;Python 3.9, import these from the \u0026#39;typing\u0026#39; module. from collections.abc import Awaitable, Callable from functools import wraps from typing import Any def tag(*names: str) -\u0026gt; Callable: def decorator(func: Callable) -\u0026gt; Callable: # Tagging has to happen in function definition time. # Othewise calling func._tags will raise AttributeError. func._tags = names # type: ignore if inspect.iscoroutinefunction(func): @wraps(func) async def async_wrapped( *args: Any, **kwargs: Any ) -\u0026gt; Awaitable: return await func(*args, **kwargs) return async_wrapped else: @wraps(func) def sync_wrapped(*args: Any, **kwargs: Any) -\u0026gt; Any: return func(*args, **kwargs) return sync_wrapped return decorator In the above snippet —\nThe decorator tag is a variadic function that accepts the names of the tags.\nI attached the tag to a function before dealing with the sync and async functions. The tag attachment is done via func._tags = names statement. Placing them outside of the wrapped function also makes sure that the attachment happens during the definition time of the wrapped function; not during runtime. Otherwise, it\u0026rsquo;ll raise AttributeError if you try to access func._tags to inspect the tags.\nAfterwards, I checked if the function is an async one via iscoroutinefunction function from the inspect module. If the wrapped function is an async function, then it\u0026rsquo;s executed with the await statement. Otherwise, the function is a sync function and is executed as usual.\nYou can play around with the decorator as follows:\nIn [1]: import asyncio In [2]: @tag(\u0026#39;tag_1\u0026#39;, \u0026#39;tag_2\u0026#39;) ...: async def foo(): ...: await asyncio.sleep(1) ...: return 42 In [3]: @tag(\u0026#39;tag_3\u0026#39;, \u0026#39;tag_4\u0026#39;) ...: def bar(): ...: return 24 In [4]: foo._tags Out[4]: (\u0026#39;tag_1\u0026#39;, \u0026#39;tag_2\u0026#39;) In [5]: bar._tags Out[5]: (\u0026#39;tag_3\u0026#39;, \u0026#39;tag_4\u0026#39;) In [6]: asyncio.run(foo()) Out[6]: 42 In [7]: bar() Out[7]: 24 Breadcrumbs Astute readers might notice that the type annotations in this decorator are quite loose and it doesn\u0026rsquo;t take advantage of Python 3.10\u0026rsquo;s typing.ParamSpec type. This is intentional as it adds quite a bit of noise that might obfuscate the primary intent of the code snippet. Also, typing a decorator that returns either a sync or async callable based on the control flow is tricky.\nFurther reading Amphibian decorator in Starlette\u0026rsquo;s source code ","permalink":"https://rednafi.com/python/amphibian-decorators/","summary":"\u003cp\u003eWhether you like it or not, the split world of sync and async functions in the Python\necosystem is something we\u0026rsquo;ll have to live with; at least for now. So, having to write things\nthat work with both sync and async code is an inevitable part of the journey. Projects like\n\u003ca href=\"https://www.starlette.io/\"\u003eStarlette\u003c/a\u003e, \u003ca href=\"https://www.python-httpx.org/\"\u003eHTTPx\u003c/a\u003e can give you some clever pointers on how to craft APIs that are\ncompatible with both sync and async code.\u003c/p\u003e","title":"Amphibian decorators in Python"},{"content":"While grokking Black formatter\u0026rsquo;s codebase, I came across this Rust-influenced error handling model that offers an interesting way of handling exceptions in Python. Exception handling in Python usually follows the EAFP paradigm where it\u0026rsquo;s easier to ask for forgiveness than permission.\nHowever, Rust has this recoverable error handling workflow that leverages generic Enums. I wanted to explore how Black emulates that in Python. This is how it works:\n# src.py from __future__ import annotations from typing import Generic, TypeVar, Union T = TypeVar(\u0026#34;T\u0026#34;) E = TypeVar(\u0026#34;E\u0026#34;, bound=Exception) class Ok(Generic[T]): def __init__(self, value: T) -\u0026gt; None: self._value = value def ok(self) -\u0026gt; T: return self._value class Err(Generic[E]): def __init__(self, e: E) -\u0026gt; None: self._e = e def err(self) -\u0026gt; E: return self._e Result = Union[Ok[T], Err[E]] In the above snippet, two generic types Ok and Err represent the return type and the error types of a callable respectively. These two generics were then combined into one Result generic type. You\u0026rsquo;d use the Result generic to handle exceptions as follows:\n# src.py ... def div(dividend: int, divisor: int) -\u0026gt; Result[int, ZeroDivisionError]: if divisor == 0: return Err(ZeroDivisionError(\u0026#34;Zero division error occurred!\u0026#34;)) return Ok(dividend // divisor) if __name__ == \u0026#34;__main__\u0026#34;: result = div(10, 0) if isinstance(result, Ok): print(result.ok()) else: print(result.err()) This will print:\nZero division error occurred! If you run Mypy on the snippet, it\u0026rsquo;ll succeed as well.\nYou can also apply constraints on the return or exception types as follows:\n# src.py ... # Only int, float, and str types are allowed as input. Convertible = TypeVar(\u0026#34;Convertible\u0026#34;, int, float, str) # Create a more specialized generic type from Result. IntResult = Result[int, TypeError] def to_int(num: Convertible) -\u0026gt; IntResult: \u0026#34;\u0026#34;\u0026#34;Converts a convertible input to an integer.\u0026#34;\u0026#34;\u0026#34; if not isinstance(num, (int, float, str)): return Err( TypeError( \u0026#34;Input type is not convertible to an integer type.\u0026#34;, ) ) return Ok(int(num)) if __name__ == \u0026#34;__main__\u0026#34;: result = to_int(1 + 2j) if isinstance(result, Ok): print(result.ok()) else: print(result.err()) Running the script will give you this:\nInput type is not convertible to an integer type. In this case, Mypy will catch the type inconsistency before runtime.\nBreadcrumbs Black extensively uses this rusty pattern in the transformation part of the codebase. This showed me another way of thinking about handling recoverable exceptions while ensuring type safety in a Python codebase.\nHowever, I wouldn\u0026rsquo;t go about and mindlessly refactor any exception handling logic that I come across to follow this pattern. You might find it useful if you need to handle exceptions in a recoverable fashion and need additional type safety around the logic.\nFurther reading Beginner\u0026rsquo;s guide to error handling in Rust ","permalink":"https://rednafi.com/python/go-rusty-with-exception-handling/","summary":"\u003cp\u003eWhile grokking Black formatter\u0026rsquo;s codebase, I came across this \u003ca href=\"https://github.com/psf/black/blob/main/src/black/rusty.py\"\u003eRust-influenced error\nhandling model\u003c/a\u003e that offers an interesting way of handling exceptions in Python. Exception\nhandling in Python usually follows the EAFP paradigm where it\u0026rsquo;s easier to ask for\nforgiveness than permission.\u003c/p\u003e\n\u003cp\u003eHowever, Rust has this \u003ca href=\"https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html\"\u003erecoverable error\u003c/a\u003e handling workflow that leverages generic Enums. I\nwanted to explore how Black emulates that in Python. This is how it works:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003e__future__\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eannotations\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etyping\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eGeneric\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eTypeVar\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eUnion\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eT\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eTypeVar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;T\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eE\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eTypeVar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;E\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ebound\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"ne\"\u003eException\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eOk\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eGeneric\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eT\u003c/span\u003e\u003cspan class=\"p\"\u003e]):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eT\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003e_value\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evalue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eok\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003eT\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003e_value\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eErr\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eGeneric\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eE\u003c/span\u003e\u003cspan class=\"p\"\u003e]):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eE\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003e_e\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ee\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eerr\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003eE\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003e_e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eResult\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eUnion\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eOk\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eT\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003eErr\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eE\u003c/span\u003e\u003cspan class=\"p\"\u003e]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIn the above snippet, two generic types \u003ccode\u003eOk\u003c/code\u003e and \u003ccode\u003eErr\u003c/code\u003e represent the return type and the\nerror types of a callable respectively. These two generics were then combined into one\n\u003ccode\u003eResult\u003c/code\u003e generic type. You\u0026rsquo;d use the \u003ccode\u003eResult\u003c/code\u003e generic to handle exceptions as follows:\u003c/p\u003e","title":"Go Rusty with exception handling in Python"},{"content":"I\u0026rsquo;ve always had a hard time explaining variance of generic types while working with type annotations in Python. This is an attempt to distill the things I\u0026rsquo;ve picked up on type variance while going through PEP-483.\nA pinch of type theory A generic type is a class or interface that is parameterized over types. Variance refers to how subtyping between the generic types relates to subtyping between their parameters' types.\nThroughout this text, the notation T2 \u0026lt;: T1 denotes T2 is a subtype of T1. A subtype always lives in the pointy end.\nIf T2 \u0026lt;: T1, then a generic type constructor GenType will be:\nCovariant, if GenType[T2] \u0026lt;: GenType[T1] for all such T1 and T2. Contravariant, if GenType[T1] \u0026lt;: GenType[T2] for all such T1 and T2. Invariant, if neither of the above is true. To better understand this definition, let\u0026rsquo;s make an analogy with ordinary functions. Assume that we have:\n# src.py from __future__ import annotations def cov(x: float) -\u0026gt; float: return 2 * x def contra(x: float) -\u0026gt; float: return -x def inv(x: float) -\u0026gt; float: return x * x If x1 \u0026lt; x2, then always cov(x1) \u0026lt; cov(x2), and contra(x2) \u0026lt; contra(x1), while nothing could be said about inv. Replacing \u0026lt; with \u0026lt;:, and functions with generic type constructors, we get examples of covariant, contravariant, and invariant behavior.\nA few practical examples Immutable generic types are usually type covariant For example:\nUnion behaves covariantly in all its arguments. That means: if T2 \u0026lt;: T1, then Union[T2] \u0026lt;: Union[T1] for all such T1 and T2.\nFrozenSet[T] is also covariant. Let\u0026rsquo;s consider int and float in place of T. First, int \u0026lt;: float. Second, a set of values of FrozenSet[int] is clearly a subset of values of FrozenSet[float]. Therefore, FrozenSet[int] \u0026lt;: FrozenSet[float].\nMutable generic types are usually type invariant For example:\nlist[T] is invariant. Although a set of values of list[int] is a subset of values of list[float], only an int could be appended to a list[int]. Therefore, list[int] is not a subtype of list[float]. The callable generic type is covariant in return type but contravariant in the arguments Callable[[], int] \u0026lt;: Callable[[], float] . If Manager \u0026lt;: Employee then Callable[[], Manager] \u0026lt;: Callable[[], Employee]. However, for two callable types that differ only in the type of one argument, the subtype relationship for the callable types goes in the opposite direction as for the argument types. Examples:\nCallable[[float], None] \u0026lt;: Callable[[int], None], where int \u0026lt;: float.\nCallable[[Employee], None] \u0026lt;: Callable[[Manager], None], where Manager \u0026lt;: Employee.\nI found this odd at first. However, this actually makes sense. If a function can calculate the salary for a Manager, it should also be able to calculate the salary of an Employee.\nExamples Covariance # src.py from __future__ import annotations # In \u0026lt;Python 3.9, import this from the \u0026#39;typing\u0026#39; module. from collections.abc import Sequence class Animal: pass class Dog(Animal): pass def action(animals: Sequence[Animal]) -\u0026gt; None: pass if __name__ == \u0026#34;__main__\u0026#34;: action((Animal(),)) # ok action((Dog(),)) # ok Here, Dog \u0026lt;: Animal and notice how Mypy doesn\u0026rsquo;t raise an error when a tuple of Dog instance is passed into the action function that expects a sequence of Animal instances. However, if you make change the action function as follows:\n... def action(animals: Sequence[Dog]) -\u0026gt; None: pass if __name__ == \u0026#34;__main__\u0026#34;: action((Animal(),)) # not ok action((Dog(),)) # ok Mypy will complain about this snippet since now, action expects a sequence of Dog instance or a subtype of it. A sequence of Animal is not a subtype of a sequence of Dog. Hence, the error.\nContravariance The Callable generic type is covariant in return type. Here\u0026rsquo;s how you can test it:\nfrom __future__ import annotations # In \u0026lt;Python 3.9, import this from the \u0026#39;typing\u0026#39; module. from collections.abc import Callable def factory(func: Callable[..., float]) -\u0026gt; Callable[..., float]: return func def foo() -\u0026gt; int: return 42 def bar() -\u0026gt; float: return 42 if __name__ == \u0026#34;__main__\u0026#34;: factory(foo) # ok factory(bar) # ok Here, int \u0026lt;: float and the in the return type, you can see Callable[..., int] \u0026lt;: Callable[float] as Mypy is satisfied when either foo or bar is passed into the factory callable.\nOn the other hand, the Callable generic type is contravariant in the argument type. Here\u0026rsquo;s how you can test it:\nfrom __future__ import annotations # In \u0026lt;Python 3.9, import this from the \u0026#39;typing\u0026#39; module. from collections.abc import Callable def factory(func: Callable[[float], None]) -\u0026gt; Callable[[float], None]: return func def foo(number: int) -\u0026gt; None: pass def bar(number: float) -\u0026gt; None: pass if __name__ == \u0026#34;__main__\u0026#34;: factory(foo) # not ok factory(bar) # ok Here, Mypy will complain in the case of factory(foo) as the factory function expects Callable[[float]], None] or its subtype. However, in the above case, Callable[[float]], None] \u0026lt;: Callable[[int], None] but not the other way around. That causes the error.\nInvariance In general, types defined with the TypeVar construct are invariant. You can mark them as covariant or contravariant as well. However:\nRemember that variance is a property of the generic types; not their parameter types.\nHere\u0026rsquo;s how you can mark types as covariant, contravariant, or invariant:\nfrom __future__ import annotations from typing import Generic, TypeVar T = TypeVar(\u0026#34;T\u0026#34;) T_co = TypeVar(\u0026#34;T_co\u0026#34;, covariant=True) T_contra = TypeVar(\u0026#34;T_contra\u0026#34;, contravariant=True) class HolderInv(Generic[T]): def __init__(self, *args: T) -\u0026gt; None: self.args = args class HolderCov(Generic[T_co]): def __init__(self, *args: T_co) -\u0026gt; None: self.args = args class HolderContra(Generic[T_contra]): def __init__(self, *args: T_contra) -\u0026gt; None: self.args = args def process_holder_inv(holder: HolderInv[float]) -\u0026gt; None: pass def process_holder_cov(holder: HolderCov[float]) -\u0026gt; None: pass def process_holder_contra(holder: HolderContra[float]) -\u0026gt; None: pass if __name__ == \u0026#34;__main__\u0026#34;: holder_inv = HolderInv(1.0) # ok holder_cov = HolderCov(1, 2) # ok holder_contra = HolderContra( 1, 2 ) # raises error because T is contravariant process_holder_inv(holder_inv) process_holder_cov(holder_cov) process_holder_contra(holder_contra) Further reading PEP 483 - The theory of type hints ","permalink":"https://rednafi.com/python/variance-of-generic-types/","summary":"\u003cp\u003eI\u0026rsquo;ve always had a hard time explaining \u003cstrong\u003evariance\u003c/strong\u003e of generic types while working with type\nannotations in Python. This is an attempt to distill the things I\u0026rsquo;ve picked up on type\nvariance while going through PEP-483.\u003c/p\u003e\n\u003ch2 id=\"a-pinch-of-type-theory\"\u003eA pinch of type theory\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA generic type is a class or interface that is parameterized over types. Variance refers\nto how subtyping between the generic types relates to subtyping between their parameters'\ntypes.\u003c/p\u003e","title":"Variance of generic types in Python"},{"content":"How\u0026rsquo;d you create a sub dictionary from a dictionary where the keys of the sub-dict are provided as a list?\nI was reading a tweet by Ned Bachelder on this today and that made me realize that I usually solve it with O(DK) complexity, where K is the length of the sub-dict keys and D is the length of the primary dict. Here\u0026rsquo;s how I usually do that without giving it any thoughts or whatsoever:\n# src.py from __future__ import annotations main_dict = { \u0026#34;this\u0026#34;: 0, \u0026#34;is\u0026#34;: 1, \u0026#34;an\u0026#34;: 2, \u0026#34;example\u0026#34;: 3, \u0026#34;of\u0026#34;: 4, \u0026#34;speech\u0026#34;: 5, \u0026#34;synthesis\u0026#34;: 6, \u0026#34;in\u0026#34;: 7, \u0026#34;english\u0026#34;: 8, } sub_keys = [\u0026#34;this\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;an\u0026#34;, \u0026#34;example\u0026#34;] sub_dict = {k: v for k, v in main_dict.items() if k in sub_keys} print(sub_dict) This prints:\n{\u0026#39;this\u0026#39;: 0, \u0026#39;is\u0026#39;: 1, \u0026#39;an\u0026#39;: 2, \u0026#39;example\u0026#39;: 3} While this works fine, if you look carefully you\u0026rsquo;ll notice that in the above snippet, the complexity of creating the sub-dict is O(DK). This means, in the worst-case scenario, it\u0026rsquo;ll have to traverse the entire length of the main-dict and all the keys of the sub-dict to create the sub-dict. We can do better. Consider this:\n# src.py ... # Only this line is different from the previous snippet. sub_dict = {k: main_dict[k] for k in sub_keys} ... It prints out the same thing as before:\n{\u0026#39;this\u0026#39;: 0, \u0026#39;is\u0026#39;: 1, \u0026#39;an\u0026#39;: 2, \u0026#39;example\u0026#39;: 3} It\u0026rsquo;s quite a bit faster because in the worst case scenario, it\u0026rsquo;ll only have to traverse the entire sub_keys list — O(K) complexity achieved. This is so simple and elegant. How did I miss that! There\u0026rsquo;s another functional but subjectively less readable way of achieving the same thing. Here you go:\n# src.py from operator import itemgetter ... sub_dict = dict(zip(sub_keys, itemgetter(*sub_keys)(main_dict))) ... Benchmarks I ran this naive benchmark in an ipython console:\n... In [3]: %timeit {k: v for k, v in main_dict.items() if k in sub_keys} 886 ns ± 7.68 ns per loop (mean ± std. dev. of 7 runs) In [4]: %timeit {k:main_dict[k] for k in sub_keys} 340 ns ± 2.87 ns per loop (mean ± std. dev. of 7 runs) In [5]: %timeit dict(zip(sub_keys, itemgetter(*sub_keys)(main_dict))) 581 ns ± 2.73 ns per loop (mean ± std. dev. of 7 runs) ... It shows that the solution I was using does suffer from the effects of O(DK) complexity even when the dict size is as small as 9 elements. The second solution is the fastest and the least complex one to understand. While the third one is better than the first solution, it\u0026rsquo;s a gratuitously complex way of doing something so trivial.\nFurther reading Second solution from a comment on the same tweet ","permalink":"https://rednafi.com/python/create-sub-dict/","summary":"\u003cp\u003eHow\u0026rsquo;d you create a sub dictionary from a dictionary where the keys of the sub-dict are\nprovided as a list?\u003c/p\u003e\n\u003cp\u003eI was reading a \u003ca href=\"https://twitter.com/nedbat/status/1487084661163626506\"\u003etweet by Ned Bachelder\u003c/a\u003e on this today and that made me realize that I\nusually solve it with \u003ccode\u003eO(DK)\u003c/code\u003e complexity, where \u003ccode\u003eK\u003c/code\u003e is the length of the sub-dict keys and\n\u003ccode\u003eD\u003c/code\u003e is the length of the primary dict. Here\u0026rsquo;s how I usually do that without giving it any\nthoughts or whatsoever:\u003c/p\u003e","title":"Create a sub dictionary with O(K) complexity in Python"},{"content":"I was reading a tweet about it yesterday and that didn\u0026rsquo;t stop me from pushing a code change in production with the same rookie mistake today. Consider this function:\n# src.py from __future__ import annotations import logging import time from datetime import datetime def log( message: str, /, *, level: str, timestamp: str = datetime.utcnow().isoformat(), ) -\u0026gt; None: logger = getattr(logging, level) # Avoid f-string in logging as it\u0026#39;s not lazy. logger(\u0026#34;Timestamp: %s \\nMessage: %s\\n\u0026#34; % (timestamp, message)) if __name__ == \u0026#34;__main__\u0026#34;: for _ in range(3): time.sleep(1) log(\u0026#34;Reality can often be disappointing.\u0026#34;, level=\u0026#34;warning\u0026#34;) Here, the function log has a parameter timestamp that computes its default value using the built-in datetime.utcnow().isoformat() method. I was under the impression that the timestamp parameter would be computed each time when the log function was called. However, that\u0026rsquo;s not what happens when you try to run it. If you run the above snippet, you\u0026rsquo;ll get this instead:\nWARNING:root:Timestamp: 2022-01-27T19:57:34.147403 Message: Reality can often be disappointing. WARNING:root:Timestamp: 2022-01-27T19:57:34.147403 Message: Reality can often be disappointing. WARNING:root:Timestamp: 2022-01-27T19:57:34.147403 Message: Reality can often be disappointing. In the __main__ block, I\u0026rsquo;m calling the log function 3 times with a 1-second delay between each invocation. But if you take a look at the timestamp of each of the log entries in the output, you\u0026rsquo;ll notice that all 3 of them are exactly the same.\nDefault function arguments are early-bound in Python. That means:\nPython interpreter will bind the default parameters at function definition time and will use that static value at run time. It\u0026rsquo;s also true for methods. This design choice was intentional.\nWe\u0026rsquo;re getting the same value of the timestamp each time because Python is computing the value of the default timestamp parameter once in the function definition time and then reusing the same value across all the function calls. The log function was called 3 times but the timestamp function was invoked only once; during the function definition time.\nThis is easy to fix. Remove the default value of the timestamp and explicitly pass the parameter value while calling the function:\n# src.py from __future__ import annotations import logging import time from datetime import datetime def log( message: str, /, *, level: str, timestamp: str, # No default value here. ) -\u0026gt; None: logger = getattr(logging, level) # Avoid f-string in logging as it\u0026#39;s not lazy. logger(\u0026#34;Timestamp: %s \\nMessage: %s\\n\u0026#34; % (timestamp, message)) if __name__ == \u0026#34;__main__\u0026#34;: for _ in range(3): time.sleep(1) log( \u0026#34;Reality can often be disappointing.\u0026#34;, level=\u0026#34;warning\u0026#34;, # Pass this explicitly. timestamp=datetime.utcnow().isoformat(), ) Now if you run it, you\u0026rsquo;ll get this:\nWARNING:root:Timestamp: 2022-01-27T20:19:47.618326 Message: Reality can often be disappointing. WARNING:root:Timestamp: 2022-01-27T20:19:48.618761 Message: Reality can often be disappointing. WARNING:root:Timestamp: 2022-01-27T20:19:49.620116 Message: Reality can often be disappointing. Notice, how the values of the seconds in the timestamps have roughly a 1-second delay between them. Early-bound defaults can also produce surprising results if you try to use a mutable data structure as the default value of a function/method. Here\u0026rsquo;s an example:\n# src.py from __future__ import annotations # In \u0026lt;Python 3.9, import this from the \u0026#39;typing\u0026#39; module. from collections.abc import MutableSequence from typing import Any def append_to(value: Any, target: MutableSequence = []) -\u0026gt; MutableSequence: target.append(value) return target if __name__ == \u0026#34;__main__\u0026#34;: for i in range(3): ret = append_to(i) print(ret) The function append_to takes any object and appends that to the target mutable sequence. Here, the parameter target has a default value; an empty list. However, running the function reveals something unexpected:\n[0] [0, 1] [0, 1, 2] Whereas, you might expect it to print out the following:\n[0] [1] [2] Python is reusing the same MutableSequence that was defined in the function definition time; just like it was reusing the same return value of the datetime.utcnow().isoformat() in the previous section. To fix this you can do the following:\n# src.py from __future__ import annotations from collections.abc import MutableSequence from typing import Any def append_to(value: Any) -\u0026gt; MutableSequence: target = [] target.append(value) return target if __name__ == \u0026#34;__main__\u0026#34;: for i in range(3): ret = append_to(i) print(ret) Running the snippet will produce the expected result this time:\n[0] [1] [2] Here, I just omitted the target parameter from the append_to function signature. Defining the variable inside the function body can save you from being surprised at the most unfortunate time.\nBreadcrumbs Currently, there\u0026rsquo;s an outstanding PEP (PEP-671) that proposes late-bound function argument defaults. It\u0026rsquo;s still in a draft state and I\u0026rsquo;m quite fond of the syntax that it\u0026rsquo;s proposing. Here\u0026rsquo;s how you\u0026rsquo;d make a default parameter late-bound:\ndef foo(bar, baz =\u0026gt; []): ... The default parameter baz will be late-bound and will produce similar results that we\u0026rsquo;ve seen in the last solution.\nFurther reading Mutable default arguments - The hitchhiker\u0026rsquo;s guide to Python! ","permalink":"https://rednafi.com/python/early-bound-function-defaults/","summary":"\u003cp\u003eI was reading a tweet about it yesterday and that didn\u0026rsquo;t stop me from pushing a code change\nin production with the same rookie mistake today. Consider this function:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003e__future__\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eannotations\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003elogging\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003etime\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003edatetime\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003edatetime\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003elog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"o\"\u003e/\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003elevel\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003etimestamp\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edatetime\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eutcnow\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eisoformat\u003c/span\u003e\u003cspan class=\"p\"\u003e(),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003elogger\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003egetattr\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elogging\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003elevel\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Avoid f-string in logging as it\u0026#39;s not lazy.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Timestamp: \u003c/span\u003e\u003cspan class=\"si\"\u003e%s\u003c/span\u003e\u003cspan class=\"s2\"\u003e \u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003eMessage: \u003c/span\u003e\u003cspan class=\"si\"\u003e%s\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e \u003cspan class=\"o\"\u003e%\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etimestamp\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"vm\"\u003e__name__\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003e_\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003etime\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esleep\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003elog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Reality can often be disappointing.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003elevel\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;warning\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHere, the function \u003ccode\u003elog\u003c/code\u003e has a parameter \u003ccode\u003etimestamp\u003c/code\u003e that computes its default value using\nthe built-in \u003ccode\u003edatetime.utcnow().isoformat()\u003c/code\u003e method. I was under the impression that the\n\u003ccode\u003etimestamp\u003c/code\u003e parameter would be computed each time when the \u003ccode\u003elog\u003c/code\u003e function was called.\nHowever, that\u0026rsquo;s not what happens when you try to run it. If you run the above snippet,\nyou\u0026rsquo;ll get this instead:\u003c/p\u003e","title":"Gotchas of early-bound function argument defaults in Python"},{"content":"I used to use Unittest\u0026rsquo;s self.assertTrue / self.assertFalse to check both literal booleans and truthy/falsy values in Unittest. Committed the same sin while writing tests in Django.\nI feel like assertTrue and assertFalse are misnomers. They don\u0026rsquo;t specifically check literal booleans, only truthy and falsy states respectively.\nConsider this example:\n# src.py import unittest class TestFoo(unittest.TestCase): def setUp(self): self.true_literal = True self.false_literal = False self.truthy = [True] self.falsy = [] def is_true(self): self.assertTrue(self.true_literal, True) def is_false(self): self.assertFalse(self.false_literal, True) def is_truthy(self): self.assertTrue(self.truthy, True) def is_falsy(self): self.assertFalse(self.falsy, True) if __name__ == \u0026#34;__main__\u0026#34;: unittest.main() In the above snippet, I\u0026rsquo;ve used assertTrue and assertFalse to check both literal booleans and truthy/falsy values. However, to test the literal boolean values, assertIs works better and is more explicit. Here\u0026rsquo;s how to do the above test properly:\n# src.py import unittest class TestFoo(unittest.TestCase): def setUp(self): self.true_literal = True self.false_literal = False self.truthy = [True] self.falsy = [] def is_true(self): self.assertIs(self.true_literal, True) def is_false(self): self.assertIs(self.false_literal, False) def is_truthy(self): self.assertTrue(self.truthy, True) def is_falsy(self): self.assertFalse(self.falsy, True) if __name__ == \u0026#34;__main__\u0026#34;: unittest.main() Notice how I\u0026rsquo;ve used self.assertIs in the is_true and is_false methods to explicitly test out the literal boolean values. The is_truthy and is_falsy methods were kept unchanged from the previous snippet.\nFurther reading Tweet by Drewrey Lupton ","permalink":"https://rednafi.com/python/use-assertis-to-check-literal-booleans/","summary":"\u003cp\u003eI used to use Unittest\u0026rsquo;s \u003ccode\u003eself.assertTrue\u003c/code\u003e / \u003ccode\u003eself.assertFalse\u003c/code\u003e to check both \u003cstrong\u003eliteral\nbooleans\u003c/strong\u003e and \u003cstrong\u003etruthy\u003c/strong\u003e/\u003cstrong\u003efalsy\u003c/strong\u003e values in Unittest. Committed the same sin while writing\ntests in Django.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI feel like \u003ccode\u003eassertTrue\u003c/code\u003e and \u003ccode\u003eassertFalse\u003c/code\u003e are misnomers. They don\u0026rsquo;t specifically check\nliteral booleans, only truthy and falsy states respectively.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eConsider this example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eunittest\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eTestFoo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eunittest\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eTestCase\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003esetUp\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etrue_literal\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efalse_literal\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etruthy\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efalsy\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eis_true\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eassertTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etrue_literal\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eis_false\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eassertFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efalse_literal\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eis_truthy\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eassertTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etruthy\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eis_falsy\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eassertFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efalsy\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"vm\"\u003e__name__\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eunittest\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIn the above snippet, I\u0026rsquo;ve used \u003ccode\u003eassertTrue\u003c/code\u003e and \u003ccode\u003eassertFalse\u003c/code\u003e to check both literal\nbooleans and truthy/falsy values. However, to test the literal boolean values, \u003ccode\u003eassertIs\u003c/code\u003e\nworks better and is more explicit. Here\u0026rsquo;s how to do the above test properly:\u003c/p\u003e","title":"Use 'assertIs' to check literal booleans in Python unittest"},{"content":"Accurately static typing decorators in Python is an icky business. The wrapper function obfuscates type information required to statically determine the types of the parameters and the return values of the wrapped function.\nLet\u0026rsquo;s write a decorator that registers the decorated functions in a global dictionary during function definition time. Here\u0026rsquo;s how I used to annotate it:\n# src.py # Import \u0026#39;Callable\u0026#39; from \u0026#39;typing\u0026#39; module in \u0026lt; Py3.9. from collections.abc import Callable from functools import wraps from typing import Any, TypeVar R = TypeVar(\u0026#34;R\u0026#34;) funcs = {} def register(func: Callable[..., R]) -\u0026gt; Callable[..., R]: \u0026#34;\u0026#34;\u0026#34;Register any function at definition time in the \u0026#39;funcs\u0026#39; dict.\u0026#34;\u0026#34;\u0026#34; # Registers the function during function defition time. funcs[func.__name__] = func @wraps(func) def inner(*args: Any, **kwargs: Any) -\u0026gt; Any: return func(*args, **kwargs) return inner @register def hello(name: str) -\u0026gt; str: return f\u0026#34;Hello {name}!\u0026#34; The functools.wraps decorator makes sure that the identity and the docstring of the wrapped function don\u0026rsquo;t get gobbled up by the decorator. This is syntactically correct and if you run Mypy against the code snippet, it\u0026rsquo;ll happily tell you that everything\u0026rsquo;s alright. However, this doesn\u0026rsquo;t exactly do anything. If you call the hello function with the wrong type of parameter, Mypy won\u0026rsquo;t be able to detect the mistake statically. Notice this:\n... hello(1) # Mypy doesn\u0026#39;t complain about it all All this for nothing!\nPEP-612 proposed ParamSpec and Concatenate in the typing module to address this issue. Later on, these were introduced in Python 3.10. The former is required to precisely add type hints to any decorator while the latter is needed to type annotate decorators that change wrapped functions\u0026rsquo; signatures.\nIf you\u0026rsquo;re not on Python 3.10+, you can import ParamSpec and Concatenate from the typing_extensions module. The package gets automatically installed with Mypy.\nUse ParamSpec to type decorators I\u0026rsquo;ll take advantage of both ParamSpec and TypeVar to annotate the register decorator that we\u0026rsquo;ve seen earlier:\n# src.py # Import \u0026#39;Callable\u0026#39; from \u0026#39;typing\u0026#39; module in \u0026lt; Py3.9. from collections.abc import Callable from functools import wraps from typing import ParamSpec, TypeVar P = ParamSpec(\u0026#34;P\u0026#34;) R = TypeVar(\u0026#34;R\u0026#34;) funcs = {} def register(func: Callable[P, R]) -\u0026gt; Callable[P, R]: funcs[func.__name__] = func @wraps(func) def inner(*args: P.args, **kwargs: P.kwargs) -\u0026gt; R: return func(*args, **kwargs) return inner @register def hello(name: str) -\u0026gt; str: return f\u0026#34;Hello {name}!\u0026#34; # Try calling the function with the wrong param type. print(hello(1)) # Mypy will complain here! Above, I\u0026rsquo;ve used ParamSpec to annotate the type of the wrapped function\u0026rsquo;s input parameters and TypeVar to annotate its return value. Underneath, ParamSpec is a type variable similar to TypeVar but with a trick under its sleeve; it can relay type information to a decorator\u0026rsquo;s inner callable.\nNotice the annotations of the inner function inside register. Here, P.args and P.kwargs are transferring the type information from the wrapped func to the inner function. This makes sure that static type checkers like Mypy can now precisely scream at you whenever you call the decorated functions with the wrong type of parameters.\nUse Concatenate to type decorators that change the wrapped functions\u0026rsquo; signatures There\u0026rsquo;s another type of decorator that changes the signature of the wrapped function by adding or removing parameters during runtime. Annotating these can be tricky; as the magic happens mostly during runtime. The Concatenate type allows us to communicate this behavior with the type checker.\nConsider this inject_logger decorator, that adds a logger instance to the decorated function. It sort of acts how Django injects the request instances into the view functions. Here\u0026rsquo;s the typed version of that:\n# src.py import logging # Import \u0026#39;Callable\u0026#39; from \u0026#39;typing\u0026#39; module in \u0026lt; Py3.9. from collections.abc import Callable from functools import wraps from typing import Concatenate, ParamSpec, TypeVar P = ParamSpec(\u0026#34;P\u0026#34;) R = TypeVar(\u0026#34;R\u0026#34;) def inject_logger( func: Callable[Concatenate[logging.Logger, P], R], ) -\u0026gt; Callable[P, R]: # Runs this during function definition time only. logger = logging.getLogger(func.__name__) @wraps(func) def inner(*args: P.args, **kwargs: P.kwargs) -\u0026gt; R: return func(logger, *args, *kwargs) return inner @inject_logger def hello(logger: logging.Logger, name: str) -\u0026gt; None: logger.warning(\u0026#34;Spooky action in distance...\u0026#34;) return f\u0026#34;Hello {name}!\u0026#34; # Notice how you can call the hello function without # inserting the first parameter. The decorator does # that for you. print(hello(\u0026#34;world\u0026#34;)) This is a contrived example and a gratuitously complicated way to achieve a simple goal. Also, it\u0026rsquo;s not recommended to mutate function signatures like this in runtime. But it\u0026rsquo;s allowed and now Python gives you a way to statically type check the decorator and the decorated function.\nThe only thing that\u0026rsquo;s different from the previous section is the annotation of the func parameter of the inject_logger. Notice how the Callable generic now contain Concatenate[logging.Logger, P]. The first parameter of the Concatenate generic is the injected parameter — logging.Logger in this case. Since the instance of logging.Logger gets dynamically injected, an additional paradigm Concatenate is necessary to communicate that with the type checker.\nIf you\u0026rsquo;d defined hello with the wrong types, the type checker would\u0026rsquo;ve complained.\n... @inject_logger def hello(logger: int, name: str) -\u0026gt; str: logger.warning(\u0026#34;Spooky action in distance...\u0026#34;) return f\u0026#34;Hello {name}!\u0026#34; Above, I\u0026rsquo;ve changed the type of the logger parameter from logging.Logger to int. The type checker will now dutifully chastise us for our transgressions.\nUnfortunately, as of writing this post, Mypy doesn\u0026rsquo;t understand Concatenate but Microsoft\u0026rsquo;s Pyright does. You can pip install Pyright and test out the above snippet as follows:\npyright src.py This will return:\n... Parameter 1: type \u0026#34;Logger\u0026#34; cannot be assigned to type \u0026#34;int\u0026#34; \u0026#34;Logger\u0026#34; is incompatible with \u0026#34;int\u0026#34; (reportGeneralTypeIssues) ./src.py:83:12 - error: Cannot access member \u0026#34;warning\u0026#34; for type \u0026#34;int\u0026#34; Further reading Decorator typing (PEP 612) - Anthony explains #386 ","permalink":"https://rednafi.com/python/static-typing-decorators/","summary":"\u003cp\u003eAccurately static typing decorators in Python is an icky business. The \u003cstrong\u003ewrapper\u003c/strong\u003e function\nobfuscates type information required to statically determine the types of the parameters and\nthe return values of the \u003cstrong\u003ewrapped\u003c/strong\u003e function.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s write a decorator that registers the decorated functions in a global dictionary during\nfunction definition time. Here\u0026rsquo;s how I used to annotate it:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Import \u0026#39;Callable\u0026#39; from \u0026#39;typing\u0026#39; module in \u0026lt; Py3.9.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003ecollections.abc\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eCallable\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003efunctools\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003ewraps\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etyping\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eTypeVar\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eR\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eTypeVar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;R\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003efuncs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eregister\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003efunc\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eCallable\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"o\"\u003e...\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eR\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003eCallable\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"o\"\u003e...\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eR\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;Register any function at definition time in\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    the \u0026#39;funcs\u0026#39; dict.\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Registers the function during function defition time.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003efuncs\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003efunc\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"vm\"\u003e__name__\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003efunc\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nd\"\u003e@wraps\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003efunc\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003einner\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003eAny\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003efunc\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003einner\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nd\"\u003e@register\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello \u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e!\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe \u003ccode\u003efunctools.wraps\u003c/code\u003e decorator makes sure that the identity and the docstring of the\nwrapped function don\u0026rsquo;t get gobbled up by the decorator. This is syntactically correct and if\nyou run Mypy against the code snippet, it\u0026rsquo;ll happily tell you that everything\u0026rsquo;s alright.\nHowever, this doesn\u0026rsquo;t exactly do anything. If you call the \u003ccode\u003ehello\u003c/code\u003e function with the wrong\ntype of parameter, Mypy won\u0026rsquo;t be able to detect the mistake statically. Notice this:\u003c/p\u003e","title":"Static typing Python decorators"},{"content":"How come I didn\u0026rsquo;t know about the python -m pydoc command before today!\nIt lets you inspect the docstrings of any modules, classes, functions, or methods in Python.\nI\u0026rsquo;m running the commands from a Python 3.10 virtual environment but it\u0026rsquo;ll work on any Python version. Let\u0026rsquo;s print out the docstrings of the functools.lru_cache function. Run:\npython -m pydoc functools.lru_cache This will print the following on the console:\nHelp on function lru_cache in functools: functools.lru_cache = lru_cache(maxsize=128, typed=False) Least-recently-used cache decorator. If *maxsize* is set to None, the LRU features are disabled and the cache can grow without bound. If *typed* is True, arguments of different types will be cached separately. For example, f(3.0) and f(3) will be treated as distinct calls with distinct results. Arguments to the cached function must be hashable. View the cache statistics named tuple (hits, misses, maxsize, currsize) with f.cache_info(). Clear the cache and statistics with f.cache_clear(). Access the underlying function with f.__wrapped__. Works for third party tools as well:\npython -m pydoc typing_extensions.ParamSpec Also, works for any custom Python structure that is accessible from the current Python path. Let\u0026rsquo;s define a function with docstrings and put that in a module called src.py:\n# src.py def greetings(name: str) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Prints Hello \u0026lt;name\u0026gt;! on the console. Parameters ---------- name : str Name of the person you want to greet \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Hello {name}!\u0026#34;) You can inspect the entire src.py module or the greetings function specifically as follows:\nTo inspect the module, run:\npython -m pydoc src To inspect the greetings function only, run:\npython -m pydoc src.greetings It\u0026rsquo;ll return:\nHelp on function greetings in src: src.greetings = greetings(name: str) -\u0026gt; None Prints Hello \u0026lt;name\u0026gt;! on the console. Parameters ---------- name : str Name of the person you want to greet Instead of inspecting the docstrings one by one, you can also pull up all of them in the current Python path and serve them as HTML pages. To do so, run:\npython -m pydoc -b This will render the docstrings as HTML web pages and automatically open the index page with your default browser. From there you can use the built-in search to find and read your ones you need.\nFurther reading Tweet by Brandon Rhodes ","permalink":"https://rednafi.com/python/inspect-docstring-with-pydoc/","summary":"\u003cp\u003eHow come I didn\u0026rsquo;t know about the \u003ccode\u003epython -m pydoc\u003c/code\u003e command before today!\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eIt lets you inspect the docstrings of any modules, classes, functions, or methods in\nPython.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eI\u0026rsquo;m running the commands from a Python 3.10 virtual environment but it\u0026rsquo;ll work on any Python\nversion. Let\u0026rsquo;s print out the docstrings of the \u003ccode\u003efunctools.lru_cache\u003c/code\u003e function. Run:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epython -m pydoc functools.lru_cache\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis will print the following on the console:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eHelp on function lru_cache in functools:\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003efunctools.lru_cache = lru_cache(maxsize=128, typed=False)\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    Least-recently-used cache decorator.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    If *maxsize* is set to None, the LRU features are disabled and\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    the cache can grow without bound.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    If *typed* is True, arguments of different types will be cached\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    separately. For example, f(3.0) and f(3) will be treated as\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    distinct calls with distinct results.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    Arguments to the cached function must be hashable.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    View the cache statistics named tuple (hits, misses, maxsize,\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    currsize) with f.cache_info().  Clear the cache and statistics\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    with f.cache_clear(). Access the underlying function with\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    f.__wrapped__.\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWorks for third party tools as well:\u003c/p\u003e","title":"Inspect docstrings with Pydoc"},{"content":"To check whether an integer is a power of two, I\u0026rsquo;ve deployed hacks like this:\ndef is_power_of_two(x: int) -\u0026gt; bool: return x \u0026gt; 0 and hex(x)[-1] in (\u0026#34;0\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;4\u0026#34;, \u0026#34;8\u0026#34;) While this hex trick works, I\u0026rsquo;ve never liked explaining the pattern matching hack that\u0026rsquo;s going on here.\nToday, I came across this tweet by Raymond Hettinger where he proposed an elegant solution to the problem. Here\u0026rsquo;s how it goes:\ndef is_power_of_two(x: int) -\u0026gt; bool: return x \u0026gt; 0 and x.bit_count() == 1 This is neat as there\u0026rsquo;s no hack and it uses a mathematical invariant to check whether an integer is a power of 2 or not. Also, it\u0026rsquo;s a tad bit faster.\nExplanation Any integer that\u0026rsquo;s a power of 2, will only contain a single 1 in its binary representation.\nFor example:\n\u0026gt;\u0026gt;\u0026gt; bin(2) \u0026#39;0b10\u0026#39; \u0026gt;\u0026gt;\u0026gt; bin(4) \u0026#39;0b100\u0026#39; \u0026gt;\u0026gt;\u0026gt; bin(8) \u0026#39;0b1000\u0026#39; \u0026gt;\u0026gt;\u0026gt; bin(16) \u0026#39;0b10000\u0026#39; \u0026gt;\u0026gt;\u0026gt; The .bit_count() function checks how many on-bits (1) are there in the binary representation of an integer.\nComplete example with tests import unittest def is_power_of_two(number: int) -\u0026gt; bool: return number \u0026gt; 0 and number.bit_count() == 1 class IsPowerofTwoTest(unittest.TestCase): def setUp(self): self.power_of_twos = [2**x for x in range(2, 25_000)] self.not_power_of_twos = [3**x for x in range(2, 25_000)] def test_is_power_of_two(self): for x, y in zip(self.power_of_twos, self.not_power_of_twos): self.assertIs(is_power_of_two(x), True) self.assertIs(is_power_of_two(y), False) if __name__ == \u0026#34;__main__\u0026#34;: unittest.main() ","permalink":"https://rednafi.com/python/check-is-a-power-of-two/","summary":"\u003cp\u003eTo check whether an integer is a power of two, I\u0026rsquo;ve deployed hacks like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eis_power_of_two\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003ebool\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"nb\"\u003ehex\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)[\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;0\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;2\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;4\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;8\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWhile this \u003ca href=\"https://twitter.com/rednafi/status/1484326191687696391/photo/1\"\u003ehex trick works\u003c/a\u003e, I\u0026rsquo;ve never liked explaining the pattern matching hack that\u0026rsquo;s\ngoing on here.\u003c/p\u003e\n\u003cp\u003eToday, I came across this \u003ca href=\"https://twitter.com/raymondh/status/1483948152906522625\"\u003etweet by Raymond Hettinger\u003c/a\u003e where he proposed an elegant solution\nto the problem. Here\u0026rsquo;s how it goes:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eis_power_of_two\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003ebool\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebit_count\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis is neat as there\u0026rsquo;s no hack and it uses a mathematical invariant to check whether an\ninteger is a power of \u003ccode\u003e2\u003c/code\u003e or not. Also, it\u0026rsquo;s a tad bit faster.\u003c/p\u003e","title":"Check whether an integer is a power of two in Python"},{"content":"Django Rest Framework exposes a neat hook to customize the response payload of your API when errors occur. I was going through Microsoft\u0026rsquo;s REST API guideline and wanted to make the error response of my APIs more uniform and somewhat similar to this example.\nI\u0026rsquo;ll use a modified version of the quickstart example in the DRF docs to show how to achieve that. Also, we\u0026rsquo;ll need a POST API to demonstrate the changes better. Here\u0026rsquo;s the same example with the added POST API. Place this code in the project\u0026rsquo;s urls.py file.\n# urls.py from django.urls import path, include from django.contrib.auth.models import User from rest_framework import routers, serializers, viewsets # Serializers define the API representation. class UserSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = User fields = [\u0026#34;url\u0026#34;, \u0026#34;username\u0026#34;, \u0026#34;email\u0026#34;, \u0026#34;is_staff\u0026#34;] def validate_username(self, username: str) -\u0026gt; str: if len(username) \u0026lt; 10: raise serializers.ValidationError( \u0026#34;Username must be at least 10 characters long.\u0026#34;, ) return username def validate_email(self, email: str) -\u0026gt; str: try: validate_email(email) except ValidationError: raise serializers.ValidationError(\u0026#34;Invalid email format.\u0026#34;) return email def create(self, validated_data: str) -\u0026gt; User: return User.objects.create(**validated_data) # ViewSets define the view behavior. class UserViewSet(viewsets.ModelViewSet): queryset = User.objects.all() serializer_class = UserSerializer # Routers provide an easy way of automatically determining the URL conf. router = routers.DefaultRouter() router.register(r\u0026#34;users\u0026#34;, UserViewSet) # Wire up our API using automatic URL routing. # Additionally, we include login URLs for the browsable API. urlpatterns = [ path(\u0026#34;\u0026#34;, include(router.urls)), path( \u0026#34;api-auth/\u0026#34;, include(\u0026#34;rest_framework.urls\u0026#34;, namespace=\u0026#34;rest_framework\u0026#34;), ), ] If you make a POST request to /users endpoint with the following payload where it\u0026rsquo;ll intentionally fail email and username validation —\n{ \u0026#34;username\u0026#34;: \u0026#34;hello\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;email..\u0026#34;, \u0026#34;is_staff\u0026#34;: false } you\u0026rsquo;ll see the following response:\n{ \u0026#34;username\u0026#34;:[ \u0026#34;Username must be at least 10 characters long.\u0026#34; ], \u0026#34;email\u0026#34;:[ \u0026#34;Enter a valid email address.\u0026#34; ] } While this is okay, there\u0026rsquo;s one gotcha here. The error payload isn\u0026rsquo;t consistent. Depending on the type of error, the shape of the response payload will change. This can be a problem if your system has custom error handling logic that expects a consistent response.\nI wanted the error payload to have a predictable shape while carrying more information like — HTTP error code, error message, etc. You can do it by wrapping the default rest_framework.views.exception_handler function in a custom exception handler function. Let\u0026rsquo;s write the api_exception_handler:\n# urls.py from rest_framework.views import exception_handler from http import HTTPStatus from typing import Any from rest_framework.views import Response ... def api_exception_handler( exc: Exception, context: dict[str, Any] ) -\u0026gt; Response: \u0026#34;\u0026#34;\u0026#34;Custom API exception handler.\u0026#34;\u0026#34;\u0026#34; # Call REST framework\u0026#39;s default exception handler first, # to get the standard error response. response = exception_handler(exc, context) if response is not None: # Using the description\u0026#39;s of the HTTPStatus class as error message. http_code_to_message = {v.value: v.description for v in HTTPStatus} error_payload = { \u0026#34;error\u0026#34;: { \u0026#34;status_code\u0026#34;: 0, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;details\u0026#34;: [], } } error = error_payload[\u0026#34;error\u0026#34;] status_code = response.status_code error[\u0026#34;status_code\u0026#34;] = status_code error[\u0026#34;message\u0026#34;] = http_code_to_message[status_code] error[\u0026#34;details\u0026#34;] = response.data response.data = error_payload return response ... Now, you\u0026rsquo;ll have to register this custom exception handler in the settings.py file. Head over to the REST_FRAMEWORK section and add the following key:\nREST_FRAMEWORK = { ... \u0026#34;EXCEPTION_HANDLER\u0026#34;: \u0026#34;\u0026lt;project\u0026gt;.urls.api_exception_handler\u0026#34;, } If you make a POST request to /users endpoint with an invalid payload as before, you\u0026rsquo;ll see this:\n{ \u0026#34;error\u0026#34;: { \u0026#34;status_code\u0026#34;:400, \u0026#34;message\u0026#34;:\u0026#34;Bad request syntax or unsupported method\u0026#34;, \u0026#34;details\u0026#34;:{ \u0026#34;username\u0026#34;:[ \u0026#34;Username must be at least 10 character long.\u0026#34; ], \u0026#34;email\u0026#34;:[ \u0026#34;Enter a valid email address.\u0026#34; ] } } } Much nicer!\nFurther reading Custom Exception Handling - DRF docs ","permalink":"https://rednafi.com/python/uniform-error-response-in-drf/","summary":"\u003cp\u003eDjango Rest Framework exposes a neat hook to customize the response payload of your API when\nerrors occur. I was going through \u003ca href=\"https://github.com/microsoft/api-guidelines\"\u003eMicrosoft\u0026rsquo;s REST API guideline\u003c/a\u003e and wanted to make the\nerror response of my APIs more uniform and somewhat similar to \u003ca href=\"https://github.com/microsoft/api-guidelines/blob/vNext/Guidelines.md#examples\"\u003ethis example\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ll use a modified version of the \u003ca href=\"https://www.django-rest-framework.org/#example\"\u003equickstart example\u003c/a\u003e in the DRF docs to show how to\nachieve that. Also, we\u0026rsquo;ll need a POST API to demonstrate the changes better. Here\u0026rsquo;s the same\nexample with the added POST API. Place this code in the project\u0026rsquo;s \u003ccode\u003eurls.py\u003c/code\u003e file.\u003c/p\u003e","title":"Uniform error response in Django Rest Framework"},{"content":"If you want to define a variable that can accept values of multiple possible types, using typing.Union is one way of doing that:\nfrom typing import Union U = Union[int, str] However, there\u0026rsquo;s another way you can express a similar concept via constrained TypeVar. You\u0026rsquo;d do so as follows:\nfrom typing import TypeVar T = TypeVar(\u0026#34;T\u0026#34;, int, str) So, what\u0026rsquo;s the difference between these two and when to use which? The primary difference is:\nT\u0026rsquo;s type needs to be consistent across multiple uses within a given scope, while U\u0026rsquo;s doesn\u0026rsquo;t.\nWith a Union type used as function parameters, the arguments, as well as the return type, can all be different:\n# src.py from typing import Union U = Union[int, str] # Native generic tuple requires py3.10 or # \u0026#39;from __future__ import annotations\u0026#39; import. def foo(a: U, b: U) -\u0026gt; tuple[U, ...]: return (a, b) # Use the \u0026#39;foo\u0026#39; function. foo(\u0026#34;apple\u0026#34;, \u0026#34;bazooka\u0026#34;) # This is valid. foo(1, \u0026#34;apple\u0026#34;) # Mypy won\u0026#39;t complain here. foo(\u0026#34;apple\u0026#34;, 1) # Mypy won\u0026#39;t complain here as well. However, the above type definition will be too loose if you need to ensure that all of your function parameters must be of the same type in a single scope. Here\u0026rsquo;s where constrained TypeVar can come in handy:\n# src.py from typing import TypeVar T = TypeVar(\u0026#34;T\u0026#34;, int, str) def add(a: T, b: T) -\u0026gt; T: return a + b add(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) # This is allowed. add(1, 2) # This is fine as well. add(\u0026#34;hello\u0026#34;, 1) # Mypy will complain; also fails in runtime. If you run Mypy against the above snippet, you\u0026rsquo;ll get this:\n$ mypy src.py src.py:12: error: Value of type variable \u0026#34;T\u0026#34; of \u0026#34;add\u0026#34; cannot be \u0026#34;object\u0026#34; add(\u0026#34;hello\u0026#34;, 1) # Mypy will complain, fails at runtime. ^ Found 1 error in 1 file (checked 1 source file) As the comment implies, this error is coming from the line where I called add(\u0026quot;hello\u0026quot;, 1). The function add can take parameters of either integer or string type. However, the type of both the parameters needs to be the same. Also, the type of the input parameters will define the type of the output value. So, the types of the input parameters must match, otherwise, Mypy will complain and in this case, the snippet will also raise a TypeError in runtime. Mypy is statically catching a bug that\u0026rsquo;d otherwise appear in runtime, how convenient!\nFurther reading What\u0026rsquo;s the difference between a constrained TypeVar and a Union? ","permalink":"https://rednafi.com/python/difference-between-typevar-and-union/","summary":"\u003cp\u003eIf you want to define a variable that can accept values of multiple possible types, using\n\u003ccode\u003etyping.Union\u003c/code\u003e is one way of doing that:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etyping\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eUnion\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eU\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eUnion\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHowever, there\u0026rsquo;s another way you can express a similar concept via constrained \u003ccode\u003eTypeVar\u003c/code\u003e.\nYou\u0026rsquo;d do so as follows:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etyping\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eTypeVar\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eT\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eTypeVar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;T\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eSo, what\u0026rsquo;s the difference between these two and when to use which? The primary difference\nis:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eT\u0026rsquo;s type needs to be consistent across multiple uses within a given scope, while U\u0026rsquo;s\ndoesn\u0026rsquo;t.\u003c/p\u003e","title":"Difference between constrained 'TypeVar' and 'Union' in Python"},{"content":"Recently, fell into this trap as I wanted to speed up a slow instance method by caching it.\nWhen you decorate an instance method with functools.lru_cache decorator, the instances of the class encapsulating that method never get garbage collected within the lifetime of the process holding them.\nLet\u0026rsquo;s consider this example:\n# src.py import functools import time from typing import TypeVar Number = TypeVar(\u0026#34;Number\u0026#34;, int, float, complex) class SlowAdder: def __init__(self, delay: int = 1) -\u0026gt; None: self.delay = delay @functools.lru_cache def calculate(self, *args: Number) -\u0026gt; Number: time.sleep(self.delay) return sum(args) def __del__(self) -\u0026gt; None: print(\u0026#34;Deleting instance ...\u0026#34;) # Create a SlowAdder instance. slow_adder = SlowAdder(2) # Measure performance. start_time = time.perf_counter() # ---------------------------------------------- result = slow_adder.calculate(1, 2) # ---------------------------------------------- end_time = time.perf_counter() print(f\u0026#34;Calculation took {end_time-start_time} seconds, result: {result}.\u0026#34;) start_time = time.perf_counter() # ---------------------------------------------- result = slow_adder.calculate(1, 2) # ---------------------------------------------- end_time = time.perf_counter() print(f\u0026#34;Calculation took {end_time-start_time} seconds, result: {result}.\u0026#34;) Here, I\u0026rsquo;ve created a simple SlowAdder class that accepts a delay value; then it sleeps for delay seconds and calculates the sum of the inputs in the calculate method. To avoid this slow recalculation for the same arguments, the calculate method was wrapped in the lru_cache decorator. The __del__ method notifies us when the garbage collection has successfully cleaned up instances of the class.\nIf you run this program, it\u0026rsquo;ll print this:\nCalculation took 2.0021052900010545 seconds, result: 3. Calculation took 5.632002284983173e-06 seconds, result: 3. Deleting instance ... You can see that the lru_cache decorator is doing its job. The second call to the calculate method with the same argument took noticeably less time compared to the first one. In the second case, the lru_cache decorator is just doing a simple dictionary lookup. This is all good but the instances of the ShowAdder class never get garbage collected in the lifetime of the program. Let\u0026rsquo;s prove that in the next section.\nGarbage collector can\u0026rsquo;t clear up the affected instances If you execute the above snippet with an -i flag, we can interactively prove that no garbage collection takes place. Let\u0026rsquo;s do it:\n$ python -i src.py Calculation took 2.002104839997628 seconds, result: 3. Calculation took 5.566998879658058e-06 seconds, result: 3. \u0026gt;\u0026gt;\u0026gt; import gc \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; slow_adder.calculate(1,2) 3 \u0026gt;\u0026gt;\u0026gt; slow_adder = None \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; gc.collect() 0 \u0026gt;\u0026gt;\u0026gt; Here on the REPL, you can see that I\u0026rsquo;ve reassigned slow_adder to None and then explicitly triggered the garbage collector. However, we don\u0026rsquo;t see the message in the __del__ method printed here and the output of gc.collect() is 0. This implies that something is holding a reference to the slow_adder instance and the garbage collector can\u0026rsquo;t clear up the object. Let\u0026rsquo;s inspect who has that reference:\n$ python -i src.py Calculation took 2.00233274600032 seconds, result: 3. Calculation took 5.453999619930983e-06 seconds, result: 3. \u0026gt;\u0026gt;\u0026gt; slow_adder.calculate.cache_info() CacheInfo(hits=1, misses=1, maxsize=128, currsize=1) \u0026gt;\u0026gt;\u0026gt; slow_adder.calculate(1,2) 3 \u0026gt;\u0026gt;\u0026gt; slow_adder.calculate.cache_info() CacheInfo(hits=2, misses=1, maxsize=128, currsize=1) \u0026gt;\u0026gt;\u0026gt; slow_adder.calculate.cache_clear() \u0026gt;\u0026gt;\u0026gt; slow_adder = None Deleting instance ... \u0026gt;\u0026gt;\u0026gt; The cache_info() is showing that the cache container keeps a reference to the instance until it gets cleared. When I manually cleared the cache and reassigned the variable slow_adder to None, only then did the garbage collector remove the instance. By default, the size of the lru_cache is 128 but if I had applied lru_cache(maxsize=None), that would\u0026rsquo;ve kept the cache forever and the garbage collector would wait for the reference count to drop to zero but that\u0026rsquo;d never happen within the lifetime of the process.\nThis can be dangerous if you create millions of instances and they don\u0026rsquo;t get garbage collected naturally. It can overflow your working memory and cause the process to crash! I accidentally did it where the infected class was being instantiated millions of times via HTTP API requests.\nThe solution To solve this, we\u0026rsquo;ll have to make the cache containers local to the instances so that the reference from cache to the instance gets scraped off with the instance. Here\u0026rsquo;s how you can do that:\n# src_2.py import functools import time from typing import TypeVar Number = TypeVar(\u0026#34;Number\u0026#34;, int, float, complex) class SlowAdder: def __init__(self, delay: int = 1) -\u0026gt; None: self.delay = delay self.calculate = functools.lru_cache()(self._calculate) def _calculate(self, *args: Number) -\u0026gt; Number: time.sleep(self.delay) return sum(args) def __del__(self) -\u0026gt; None: print(\u0026#34;Deleting instance ...\u0026#34;) The only difference here is — instead of decorating the method directly, I called the decorator function on the _calculate method just as a regular function and saved the result as an instance variable named calculate. The instances of this class get garbage collected as usual.\n$ python -i src.py \u0026gt;\u0026gt;\u0026gt; slow_adder = SlowAdder(2) \u0026gt;\u0026gt;\u0026gt; slow_adder.calculate(1,2) 3 \u0026gt;\u0026gt;\u0026gt; slow_adder.calculate.cache_info() CacheInfo(hits=0, misses=1, maxsize=128, currsize=1) \u0026gt;\u0026gt;\u0026gt; import gc \u0026gt;\u0026gt;\u0026gt; slow_adder = None \u0026gt;\u0026gt;\u0026gt; gc.collect() Deleting instance ... 11 Notice that this time, clearing out the cache wasn\u0026rsquo;t necessary. I had to call gc.collect() to invoke explicit garbage collection. That\u0026rsquo;s because this shenanigan creates cyclical references and the GC needs to do some special magic to clear the memory. In real code, Python interpreter will clean this up for you in the background without you having to call the GC.\nThe self dilemma Even after applying the solution above, a weird thing happens in the case of instance methods. Let\u0026rsquo;s run the src_2.py script interactively to demonstrate that:\n$ python -i src_2.py \u0026gt;\u0026gt;\u0026gt; slow_adder = SlowAdder(2) \u0026gt;\u0026gt;\u0026gt; slow_adder.calculate(1,2) \u0026gt;\u0026gt;\u0026gt; slow_adder \u0026lt;__main__.SlowAdder object at 0x7f92595f9b40\u0026gt; \u0026gt;\u0026gt;\u0026gt; slow_adder_2 = SlowAdder(2) \u0026gt;\u0026gt;\u0026gt; slow_adder_2.calculate(1,2) 3 \u0026gt;\u0026gt;\u0026gt; slow_adder.calculate.cache_info() CacheInfo(hits=1, misses=2, maxsize=128, currsize=2) \u0026gt;\u0026gt;\u0026gt; slow_adder_2.calculate.cache_info() CacheInfo(hits=1, misses=2, maxsize=128, currsize=2) \u0026gt;\u0026gt;\u0026gt; Here, I\u0026rsquo;ve created another instance of the SlowAdder class and called calculate with the same arguments. But whenever I called the calculate method on the slow_adder_2 instance with the same parameters as before, the first time, it recalculated it instead of returning the result from the cache. How come!\nUnderneath, the lru_cache decorator uses a dictionary to cache the calculated values. A hash function is applied to all the parameters of the target function to build the key of the dictionary, and the value is the return value of the function when those parameters are the inputs. This means, the first argument self also gets included while building the cache key. However, for different instances, this self object is going to be different and that makes the hashed key of the cache different for every instance even if the other parameters are the same.\nBut what about class methods \u0026amp; static methods Class methods and static methods don\u0026rsquo;t suffer from the above issues as they don\u0026rsquo;t have any ties to their respective instances. In their case, the cache container is local to the class, not the instances. Here, you can stack the lru_cache decorator as usual. Let\u0026rsquo;s demonstrate that for classmethod first:\n# src_3.py import functools import time class Foo: @classmethod @functools.lru_cache def bar(cls, delay: int) -\u0026gt; int: # Do something with the cls. cls.delay = delay time.sleep(delay) return 42 def __del__(self) -\u0026gt; None: print(\u0026#34;Deleting instance ...\u0026#34;) foo_1 = Foo() foo_2 = Foo() start_time = time.perf_counter() # ---------------------------------------------- result = foo_1.bar(2) # ---------------------------------------------- end_time = time.perf_counter() print(f\u0026#34;Took {end_time - start_time} seconds, result: {result}.\u0026#34;) start_time = time.perf_counter() # ---------------------------------------------- result = foo_2.bar(2) # ---------------------------------------------- end_time = time.perf_counter() print(f\u0026#34;Took {end_time - start_time} seconds, result: {result}.\u0026#34;) You can inspect the garbage collection behavior here:\n$ python src_3.py Took 2.0022965140015003 seconds, result: 42. Took 4.4819971662946045e-06 seconds, result: 42. \u0026gt;\u0026gt;\u0026gt; foo_1 = None Deleting instance ... \u0026gt;\u0026gt;\u0026gt; Static methods behave exactly the same. You can use the lru_cache decorator in similar fashion as below:\nimport functools import time class Foo: @staticmethod @functools.lru_cache def bar(delay: int) -\u0026gt; int: return 42 def __del__(self) -\u0026gt; None: print(\u0026#34;Deleting instance ...\u0026#34;) Further reading functools.lru_cache - Python Docs Don\u0026rsquo;t lru_cache methods! (intermediate) anthony explains #382 Python LRU cache in a class disregards maxsize limit when decorated with a staticmethod or classmethod decorator ","permalink":"https://rednafi.com/python/lru-cache-on-methods/","summary":"\u003cp\u003eRecently, fell into this trap as I wanted to speed up a slow instance method by caching it.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWhen you decorate an instance method with \u003ccode\u003efunctools.lru_cache\u003c/code\u003e decorator, the instances\nof the class encapsulating that method never get garbage collected within the lifetime of\nthe process holding them.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eLet\u0026rsquo;s consider this example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003efunctools\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003etime\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etyping\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eTypeVar\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eNumber\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eTypeVar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Number\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003ecomplex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eSlowAdder\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edelay\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edelay\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edelay\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nd\"\u003e@functools.lru_cache\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ecalculate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eNumber\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003eNumber\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003etime\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esleep\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edelay\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"nb\"\u003esum\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__del__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Deleting instance ...\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Create a SlowAdder instance.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eslow_adder\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eSlowAdder\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Measure performance.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003estart_time\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etime\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eperf_counter\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# ----------------------------------------------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eresult\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eslow_adder\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecalculate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# ----------------------------------------------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eend_time\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etime\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eperf_counter\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Calculation took \u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003eend_time\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003estart_time\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e seconds, result: \u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003eresult\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003estart_time\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etime\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eperf_counter\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# ----------------------------------------------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eresult\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eslow_adder\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecalculate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# ----------------------------------------------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eend_time\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etime\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eperf_counter\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Calculation took \u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003eend_time\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003estart_time\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e seconds, result: \u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003eresult\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHere, I\u0026rsquo;ve created a simple \u003ccode\u003eSlowAdder\u003c/code\u003e class that accepts a \u003ccode\u003edelay\u003c/code\u003e value; then it sleeps\nfor \u003ccode\u003edelay\u003c/code\u003e seconds and calculates the sum of the inputs in the \u003ccode\u003ecalculate\u003c/code\u003e method. To avoid\nthis slow recalculation for the same arguments, the \u003ccode\u003ecalculate\u003c/code\u003e method was wrapped in the\n\u003ccode\u003elru_cache\u003c/code\u003e decorator. The \u003ccode\u003e__del__\u003c/code\u003e method notifies us when the garbage collection has\nsuccessfully cleaned up instances of the class.\u003c/p\u003e","title":"Don't wrap instance methods with 'functools.lru_cache' decorator in Python"},{"content":"Problem A common interview question that I\u0026rsquo;ve seen goes as follows:\nWrite a function to crop a text corpus without breaking any word.\nTake the length of the text up to which character you should trim. Make sure that the cropped text doesn\u0026rsquo;t have any trailing space. Try to maximize the number of words you can pack in your trimmed text. Your function should look something like this:\ndef crop(text: str, limit: int) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Crops \u0026#39;text\u0026#39; upto \u0026#39;limit\u0026#39; characters.\u0026#34;\u0026#34;\u0026#34; # Crop the text. cropped_text = perform_crop() return cropped_text For example, if text looks like this —\n\u0026#34;A quick brown fox jumps over the lazy dog.\u0026#34; and you\u0026rsquo;re asked to crop it up to 9 characters, then the function crop should return:\n\u0026#34;A quick\u0026#34; and not:\n\u0026#34;A quick \u0026#34; or:\n\u0026#34;A quick b\u0026#34; Solution This is quite easily solvable by using Python\u0026rsquo;s textwrap.shorten function. The shorten function takes quite a few parameters. However, we\u0026rsquo;ll only need the following ones to do our job:\ntext: str: Target text that we\u0026rsquo;re going to operate on. width: int : Desired width after cropping. initial_indent: str: Character to use for the initial indentation. Provide empty string for no initial indentation. subsequent_indent: str: Character to use for the subsequent indentation. Provide empty string for no subsequent indentation. break_long_words: bool: Whether to break long words or not. break_on_hyphens: bool: Whether to break words on hyphens or not. placeholder: bool: Placeholder character. The default here is [...]. However, provide an empty string if you don\u0026rsquo;t want any placeholder after the cropped string. The length of the placeholder is going to be included in the total length of the cropped text. With the descriptions out of the way, let\u0026rsquo;s write the crop function here:\n# src.py import textwrap def crop(text: str, limit: int) -\u0026gt; str: cropped_text = textwrap.shorten( text, width=limit, initial_indent=\u0026#34;\u0026#34;, subsequent_indent=\u0026#34;\u0026#34;, break_long_words=False, break_on_hyphens=False, placeholder=\u0026#34;\u0026#34;, ) return cropped_text if __name__ == \u0026#34;__main__\u0026#34;: cropped_text = crop( text=\u0026#34;A quick brown fox jumps over the lazy dog.\u0026#34;, limit=9, ) print(cropped_text) This prints out the desired output as follows:\nA quick You can see that we achieved our goal of cropping a text corpus without breaking any word. Try playing around with the initial_indent, subsequent_indent, and placeholder parameters and see how they change the output.\nComplete solution with tests # src.py import textwrap import unittest def crop(text: str, limit: int) -\u0026gt; str: cropped_text = textwrap.shorten( text, width=limit, initial_indent=\u0026#34;\u0026#34;, subsequent_indent=\u0026#34;\u0026#34;, break_long_words=False, break_on_hyphens=False, placeholder=\u0026#34;\u0026#34;, ) return cropped_text class TestCrop(unittest.TestCase): def setUp(self): self.text = \u0026#34;This is an example of speech synthesis in English.\u0026#34; self.text_complex = \u0026#34;\u0026#34;\u0026#34; wrap(), fill() and shorten() work by creating a TextWrapper and calling a single method on it. \u0026#34;\u0026#34;\u0026#34; def test_ok(self): cropped_text = crop(self.text, limit=10) self.assertEqual(cropped_text, \u0026#34;This is an\u0026#34;) def test_complex_ok(self): cropped_text = crop(self.text_complex, limit=15) self.assertEqual(cropped_text, \u0026#34;wrap(), fill()\u0026#34;) def test_no_word_break(self): cropped_text = crop(self.text, limit=9) self.assertNotEqual(cropped_text, \u0026#34;This is a\u0026#34;) def test_no_trailing_space(self): cropped_text = crop(self.text, limit=8) self.assertNotEqual(cropped_text, \u0026#34;This is \u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: unittest.main() ","permalink":"https://rednafi.com/python/text-cropping-with-textwrap-shorten/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eA common interview question that I\u0026rsquo;ve seen goes as follows:\u003c/p\u003e\n\u003cp\u003eWrite a function to crop a text corpus without breaking any word.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTake the length of the text up to which \u003cstrong\u003echaracter\u003c/strong\u003e you should trim.\u003c/li\u003e\n\u003cli\u003eMake sure that the cropped text doesn\u0026rsquo;t have any trailing space.\u003c/li\u003e\n\u003cli\u003eTry to maximize the number of words you can pack in your trimmed text.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYour function should look something like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ecrop\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003elimit\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;Crops \u0026#39;text\u0026#39; upto \u0026#39;limit\u0026#39; characters.\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e# Crop the text.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ecropped_text\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eperform_crop\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ecropped_text\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eFor example, if \u003ccode\u003etext\u003c/code\u003e looks like this —\u003c/p\u003e","title":"Cropping texts in Python with 'textwrap.shorten'"},{"content":"I was reading the reference implementation of PEP-661: Sentinel Values and discovered an optimization technique known as String interning. Modern programming languages like Java, Python, PHP, Ruby, Julia, etc, performs string interning to make their string operations more performant.\nString interning String interning makes common string processing operations time and space-efficient by caching them. Instead of creating a new copy of string every time, this optimization method dictates to keep just one copy of string for every appropriate immutable distinct value and use the pointer reference wherever referred.\nConsider this example:\n# src.py x = \u0026#34;This is a string\u0026#34; y = \u0026#34;This is a string\u0026#34; print(x is y) # prints True Running this will print True on the console. The is operator in Python is used to check whether two objects refer to the same memory location or not. If it returns True, it means, the two objects surrounding the operator are actually the same object.\nThis is kind of neat if you think about it. In the above snippet, instead of creating a new copy when y is assigned to a string that has the same value as x, internally, Python points to the same string that is assigned to x. This is only true for smaller strings; larger strings will create individual copies as usual. The exact length that determines whether a string will be interned or not depends on the implementation and you shouldn\u0026rsquo;t rely on this implicit behavior if your code needs interning. See this example:\n# src.py x = \u0026#34;This is a string\u0026#34; * 300 y = \u0026#34;This is a string\u0026#34; * 300 print(x is y) # prints False This will print False on the console and the strings aren\u0026rsquo;t interned.\nExplicit string interning Python\u0026rsquo;s sys module in the standard library has a routine called intern that you can use to intern even large strings. For example:\n# src.py import sys x = sys.intern(\u0026#34;This is a string\u0026#34; * 300) y = sys.intern(\u0026#34;This is a string\u0026#34; * 300) print(x is y) # prints True Here, the strings are interned and running the snippet will print True on the console.\nWhat strings are interned? CPython performs string interning on constants such as Function Names, Variable Names, String Literals, etc. This PyObject snippet from the CPython codebase suggests that when a new Python object is created, the interpreter is interning all the compile-time constants, names, and literals. Also, Dictionary Keys and Object Attributes are interned. Notice this:\n# src.py # Dict key interning. d = {\u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34;} print(d.popitem()[0] is \u0026#34;hello\u0026#34;) # prints True # Object attribute interning. class Foo: def __init__(self, bar, baz): self.bar = bar self.baz = baz foo = Foo(\u0026#34;hello\u0026#34;, \u0026#34;hello\u0026#34;) print(foo.bar is foo.baz) # prints True In both of these above cases, the print statement will print True on the console — confirming the fact that dictionary keys and object attributes are interned. Having interned attributes and keys means that the access operation is faster since the string comparison operation is now just doing a pointer comparison.\nWhen explicit string interning might come in handy? One use case that I\u0026rsquo;ve found is — interning large dictionary keys. Dictionary keys are in general, interned automatically. However, if the key is large — something like a 4097 bytes hash value — Python can choose not to perform interning. Here\u0026rsquo;s an example:\n# src.py # No dict key interning as the key is quite large. d = {} k = \u0026#34;#\u0026#34; * 4097 d[\u0026#34;#\u0026#34; * 4097] = 1 print(d.popitem()[0] is k) # prints False This will print False as the key in this case won\u0026rsquo;t be interned. Dictionary value access is slower if the key isn\u0026rsquo;t interned. Let\u0026rsquo;s test that out:\n# src.py import time # Interned. t0 = time.perf_counter() for _ in range(10000): d = {\u0026#34;#\u0026#34; * 4096: \u0026#34;Interned\u0026#34;} d[\u0026#34;#\u0026#34; * 4096] t1 = time.perf_counter() # Non-interned. t2 = time.perf_counter() for _ in range(10000): d = {\u0026#34;#\u0026#34; * 4097: \u0026#34;Non-interned\u0026#34;} d[\u0026#34;#\u0026#34; * 4097] t3 = time.perf_counter() print(f\u0026#34;Interned dict creation \u0026amp; access: {t1-t0} seconds\u0026#34;) print(f\u0026#34;Non-interned dict creation \u0026amp; access: {t3-t2} seconds\u0026#34;) print(f\u0026#34;Non-interned creation \u0026amp; access is {(t3-t2)/(t1-t0)} times slower\u0026#34;) This prints:\nInterned dict creation \u0026amp; access: 0.0014631289996032137 seconds Non-interned dict creation \u0026amp; access: 0.048660025000572205 seconds Non-interned creation \u0026amp; access is 33.25750840409036 times slower The above script creates and accesses a dictionary with interned and non-interned keys 10000 times. The time difference is quite huge. Non-interned dict creation and accession are in fact, 33 times slower than its interned counterpart.\nWe can circumnavigate this limitation by using explicit string interning via the sys module as follows:\n# src.py import sys import time # Implicitly interned. t0 = time.perf_counter() for _ in range(10000): d = {\u0026#34;#\u0026#34; * 4096: \u0026#34;Implicitly-interned\u0026#34;} d[\u0026#34;#\u0026#34; * 4096] t1 = time.perf_counter() # Explicitly interned. t2 = time.perf_counter() k1 = sys.intern(\u0026#34;#\u0026#34; * 4097) k2 = sys.intern(\u0026#34;#\u0026#34; * 4097) for _ in range(10000): d = {k1: \u0026#34;Explicitly-interned\u0026#34;} d[k2] t3 = time.perf_counter() print(f\u0026#34;Implicitly interned dict creation \u0026amp; access: {t1-t0} seconds\u0026#34;) print(f\u0026#34;Explicitly interned dict creation \u0026amp; access: {t3-t2} seconds\u0026#34;) print(f\u0026#34;Ratio (explicit/implicit): {(t3-t2)/(t1-t0):.2f}\u0026#34;) This prints:\nImplicitly interned dict creation \u0026amp; access: 0.002887188999011414 seconds Explicitly interned dict creation \u0026amp; access: 0.002545474999351427 seconds Ratio (explicit/implicit): 0.88 Here, implicitly and explicitly interned dict creation and key access are almost equally fast.\nFurther reading String interning in Python Python docs: sys.intern ","permalink":"https://rednafi.com/python/string-interning/","summary":"\u003cp\u003eI was reading the \u003ca href=\"https://github.com/taleinat/python-stdlib-sentinels/blob/main/sentinels/sentinels.py\"\u003ereference implementation\u003c/a\u003e of \u003ca href=\"https://hugovk-peps.readthedocs.io/en/latest/pep-0661/#\"\u003ePEP-661: Sentinel Values\u003c/a\u003e and discovered an\noptimization technique known as \u003cstrong\u003eString interning\u003c/strong\u003e. Modern programming languages like\nJava, Python, PHP, Ruby, Julia, etc, performs \u003cem\u003estring interning\u003c/em\u003e to make their string\noperations more performant.\u003c/p\u003e\n\u003ch2 id=\"string-interning\"\u003eString interning\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eString interning\u003c/strong\u003e makes common string processing operations time and space-efficient by\ncaching them. Instead of creating a new copy of string every time, this optimization\nmethod dictates to keep just one copy of string for every appropriate immutable distinct\nvalue and use the pointer reference wherever referred.\u003c/p\u003e","title":"String interning in Python"},{"content":"I love using Go\u0026rsquo;s interface feature to declaratively define my public API structure. Consider this example:\npackage main import ( \u0026#34;fmt\u0026#34; ) // Declare the interface. type Geometry interface { area() float64 perim() float64 } // Struct that represents a rectangle. type rect struct { width, height float64 } // Method to calculate the area of a rectangle instance. func (r *rect) area() float64 { return r.width * r.height } // Method to calculate the perimeter of a rectange instance. func (r *rect) perim() float64 { return 2 * (r.width + r.height) } // Notice that we\u0026#39;re calling the methods on the interface, // not on the instance of the Rectangle struct directly. func measure(g Geometry) { fmt.Println(g) fmt.Println(g.area()) fmt.Println(g.perim()) } func main() { r := \u0026amp;rect{width: 3, height: 4} measure(r) } You can play around with the example on Go Playground. Running it will print:\n\u0026amp;{3 4} 12 14 Even if you don\u0026rsquo;t speak Go, you can just take a look at the Geometry interface and instantly know that the function measure expects a struct that implements the Geometry interface where the Geometry interface is satisfied when the struct implements two methods — area and perim. The function measure doesn\u0026rsquo;t care whether the struct is a rectangle, a circle, or a square. As long as it implements the interface Geometry, measure can work on it and calculate the area and the perimeter.\nThis is extremely powerful as it allows you to achieve polymorphism like dynamic languages without letting go of type safety. If you try to pass a struct that doesn\u0026rsquo;t fully implement the interface, the compiler will throw a type error.\nIn the world of Python, this polymorphism is achieved dynamically. Consider this example:\ndef find(haystack, needle): return needle in haystack Here, the type of the haystack can be anything that supports the in operation. It can be a list, tuple, set, or dict; basically, any type that has the __contains__ method. Python\u0026rsquo;s duck typing is more flexible than any static typing as you won\u0026rsquo;t have to tell the function anything about the type of the parameters and it\u0026rsquo;ll work spectacularly; it\u0026rsquo;s a dynamically typed language, duh! The only problem is the lack of type safety. Since there\u0026rsquo;s no compilation step in Python, it won\u0026rsquo;t stop you from accidentally putting a type that haystack doesn\u0026rsquo;t support and Python will only raise a TypeError when your try to run the code.\nIn bigger codebases, this can often become tricky as it\u0026rsquo;s difficult to tell the types of these uber-dynamic function parameters without reading through all the methods that are being called on them. In this situation, we want the best of the both world; we want the flexibility of dynamic polymorphism and at the same time, we want some sort of type safety. Moreover, the Go code is self-documented to some extent and I\u0026rsquo;d love this kind of polymorphic | type-safe | self-documented trio in Python. Let\u0026rsquo;s try to use nominal type hinting to statically type the following example:\n# src.py from __future__ import annotations from typing import TypeVar T = TypeVar(\u0026#34;T\u0026#34;) def find(haystack: dict, needle: T) -\u0026gt; bool: return needle in haystack if __name__ == \u0026#34;__main__\u0026#34;: haystack = {1, 2, 3, 4} needle = 3 print(contains(haystack, needle)) In this snippet, we\u0026rsquo;re declaring haystack to be a dict and then passing a set to the function parameter. If you try to run this function, it\u0026rsquo;ll happily print True. However, if you run mypy against this file, it\u0026rsquo;ll complain as follows:\nsrc.py:17: error: Argument 1 to \u0026#34;find\u0026#34; has incompatible type \u0026#34;Set[int]\u0026#34;; \u0026#34;Dict[Any, Any]\u0026#34; print(contains(haystack, needle)) ^ Found 1 error in 1 file (checked 4 source files) make: *** [makefile:61: mypy] Error 1 That\u0026rsquo;s because mypy expects the type to be a dict but we\u0026rsquo;re passing a set which is incompatible. During runtime, Python doesn\u0026rsquo;t raise any error because the set that we\u0026rsquo;re passing as the value of haystack, supports in operation. But we\u0026rsquo;re not communicating that with the type checker properly and mypy isn\u0026rsquo;t happy about that. To fix this mypy error, we can use Union type.\n... def contains(haystack: dict | set, needle: T) -\u0026gt; bool: return needle in haystack ... This will make mypy happy. However, it\u0026rsquo;s still not bulletproof. If you try to pass a list object as the value of haystack, mypy will complain again. So, nominal typing can get a bit tedious in this kind of situation, as you\u0026rsquo;d have to explicitly tell the type checker about every type that a variable can expect. There\u0026rsquo;s a better way!\nEnter structural subtyping. We know that the value of haystack can be anything that has the __contains__ method. So, instead of explicitly defining the name of all the allowed types — we can create a class, add the __contains__ method to it, and signal mypy the fact that haystack can be anything that has the __contains__ method. Python\u0026rsquo;s typing.Protocol class allows us to do that. Let\u0026rsquo;s use that:\n# src.py from __future__ import annotations from typing import Protocol, TypeVar, runtime_checkable T = TypeVar(\u0026#34;T\u0026#34;) @runtime_checkable class ProtoHaystack(Protocol): def __contains__(self, obj) -\u0026gt; bool: ... def find(haystack: ProtoHaystack, needle: T) -\u0026gt; bool: return needle in haystack if __name__ == \u0026#34;__main__\u0026#34;: haystack = {1, 2, 3, 4} needle = 3 print(find(haystack, needle)) print(isinstance(ProtoHaystack, haystack)) Here, the ProtoHaystack class statically defines the structure of the type of objects that are allowed to be passed as the value of haystack. The instance method __contains__ accepts an object (obj) as the second parameter and returns a boolean value based on the fact whether that obj exists in the self instance or not. Now if you run mypy on this snippet, it\u0026rsquo;ll be satisfied.\nThe runtime_checkable decorator on the ProtoHaystack class allows you to check whether a target object is an instance of the ProtoHaystack class in runtime via the isinstance() function. Without the decorator, you\u0026rsquo;ll only be able to test the conformity of an object to ProtoHaystack statically but not in runtime.\nThis pattern of strurctural duck typing is so common, that the mixins in the collections.abc module are now compatible with structural type checking. So, in this case, instead of creating a ProtoHaystack class, you can directly use the collections.abc.Container class from the standard library and it\u0026rsquo;ll do the same job.\n... from collections.abc import Container def find(haystack: Container, needle: T) -\u0026gt; bool: return needle in haystack ... Avoid abc inheritance Abstract base classes in Python let you validate the structure of subclasses in runtime. Python\u0026rsquo;s standard library APIs uses abc.ABC in many places. See this example:\n# src.py from __future__ import annotations from abc import ( ABC, abstractmethod, abstractclassmethod, abstractproperty, ) class FooInterface(ABC): @abstractmethod def bar(self) -\u0026gt; str: pass @abstractclassmethod def baz(cls) -\u0026gt; str: pass @abstractproperty def qux(self) -\u0026gt; str: pass class Foo(FooInterface): \u0026#34;\u0026#34;\u0026#34;Foo implements FooInterface.\u0026#34;\u0026#34;\u0026#34; def bar(self) -\u0026gt; str: return \u0026#34;from instance method\u0026#34; @classmethod def baz(cls) -\u0026gt; str: return \u0026#34;from class method\u0026#34; @property def qux(self) -\u0026gt; str: return \u0026#34;from property method\u0026#34; Here, the class FooInterface inherits from abc.ABC and then the methods are decorated with abstract* decorators. The combination of abc.ABC class and these decorators make sure that any class that inherits from FooInterface will have to implement the bar, baz, and qux methods. Failing to do so will raise a TypeError. The Foo class implements the FooInterface.\nThis works well in theory and practice but often time, people inadvertently start to use the *Interface classes to share implementation methods with the subclasses. When you pollute your interface with implementation methods, theoretically, it no longer stays and interface class. Go doesn\u0026rsquo;t even allow you to add implementation methods to interfaces. Abstract base classes have their places and often time, you can\u0026rsquo;t avoid them if you need a dependable runtime interface conformity check.\nHowever, more often than not, using Protocol classes with @runtime_checkable decorator works really well. Here, the Protocol class implicitly (just like Go interfaces) makes sure that your subclasses conform to the structure that you want, and the decorator, along with isinstance check can guarantee the conformity in runtime. Let\u0026rsquo;s replace the abc.ABC and the shenanigans with the decorators with typing.Protocol:\nfrom __future__ import annotations from typing import Protocol, runtime_checkable @runtime_checkable class ProtoFoo(Protocol): def bar(self) -\u0026gt; str: ... @classmethod def baz(cls) -\u0026gt; str: ... @property def qux(self) -\u0026gt; str: ... class Foo: def bar(self) -\u0026gt; str: return \u0026#34;from instance method\u0026#34; @classmethod def baz(cls) -\u0026gt; str: return \u0026#34;from class method\u0026#34; @property def qux(self) -\u0026gt; str: return \u0026#34;from property method\u0026#34; def run(foo: ProtoFoo) -\u0026gt; None: if not isinstance(foo, ProtoFoo): raise Exception(\u0026#34;Foo do not conform to Protofoo interface\u0026#34;) print(foo.bar()) print(foo.baz()) print(foo.qux) if __name__ == \u0026#34;__main__\u0026#34;: foo = Foo() run(foo) Notice that Foo is not inheriting from ProtoFoo and when you run mypy against the snippet, it\u0026rsquo;ll statically check whether Foo conforms to the ProtoFoo interface or not. Voila, we avoided inheritance. The isinstance in the run function later checks whether foo is an instance of ProtoFoo or not.\nComplete example with tests This example employs static duck-typing to check the type of WebhookPayload where the class represents the structure of the payload that is going to be sent to an URL by the send_webhook function.\n# Placeholder python file to test the snippets from __future__ import annotations import json import unittest from dataclasses import asdict, dataclass from typing import Protocol, runtime_checkable @runtime_checkable class ProtoPayload(Protocol): url: str message: str @property def json(self): ... @dataclass class WebhookPayload: url: str = \u0026#34;https://dummy.com/post/\u0026#34; message: str = \u0026#34;Dummy message\u0026#34; @property def json(self) -\u0026gt; str: return json.dumps(asdict(self)) # Notice the type accepted by the function. # Go-like static polymorphism right there! def send_webhook(payload: ProtoPayload) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; This function doesn\u0026#39;t care what type of Payload it gets as long as the payload conforms to \u0026#39;ProtoPayload\u0026#39; structure. \u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Webhook message: {payload.json}\u0026#34;) print(f\u0026#34;Sending webhook to {payload.url}\u0026#34;) class TestWebHookPayload(unittest.TestCase): def setUp(self): self.payload = WebhookPayload() def test_payload(self): # We can do isinstance check because we decorated the # \u0026#39;ProtoPayload\u0026#39; class with the \u0026#39;runtime_checkable\u0026#39; decorator. implements_protocol = isinstance(self.payload, ProtoPayload) self.assertEqual(implements_protocol, True) if __name__ == \u0026#34;__main__\u0026#34;: unittest.main() Disclaimer All the code snippets here are using Python 3.10\u0026rsquo;s type annotation syntax. However, if you\u0026rsquo;re using from __future__ import annotations, you\u0026rsquo;ll be able to run all of them in earlier Python versions, going as far back as Python 3.7.\nFurther reading PEP 544 \u0026ndash; Protocols: Structural subtyping (static duck typing) ","permalink":"https://rednafi.com/python/structural-subtyping/","summary":"\u003cp\u003eI love using Go\u0026rsquo;s interface feature to declaratively define my public API structure.\nConsider this example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003epackage\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003emain\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;fmt\u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Declare the interface.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003etype\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eGeometry\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003einterface\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nf\"\u003earea\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003efloat64\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nf\"\u003eperim\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003efloat64\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Struct that represents a rectangle.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003etype\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003erect\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estruct\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nx\"\u003ewidth\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eheight\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003efloat64\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Method to calculate the area of a rectangle instance.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003er\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"nx\"\u003erect\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003earea\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003efloat64\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003er\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003ewidth\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003er\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eheight\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Method to calculate the perimeter of a rectange instance.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003er\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"nx\"\u003erect\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eperim\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003efloat64\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003er\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003ewidth\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003er\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eheight\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Notice that we\u0026#39;re calling the methods on the interface,\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// not on the instance of the Rectangle struct directly.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003emeasure\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eg\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eGeometry\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nx\"\u003efmt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ePrintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eg\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nx\"\u003efmt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ePrintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eg\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003earea\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nx\"\u003efmt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ePrintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eg\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eperim\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nx\"\u003er\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e:=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"nx\"\u003erect\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"nx\"\u003ewidth\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nx\"\u003eheight\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nf\"\u003emeasure\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003er\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eYou can \u003ca href=\"https://go.dev/play/p/RG82v5Ubdlc\"\u003eplay around with the example\u003c/a\u003e on Go Playground. Running it will print:\u003c/p\u003e","title":"Structural subtyping in Python"},{"content":"While trying to avoid inheritance in an API that I was working on, I came across this neat trick to perform attribute delegation on composed classes. Let\u0026rsquo;s say there\u0026rsquo;s a class called Engine and you want to put an engine instance in a Car. In this case, the car has a classic \u0026lsquo;has a\u0026rsquo; (inheritance usually refers to \u0026lsquo;is a\u0026rsquo; relationships) relationship with the engine. So, composition makes more sense than inheritance here. Consider this example:\n# src.py from typing import Any class Engine: def __init__(self, name: str, sound: str) -\u0026gt; None: self.name = name self.sound = sound def noise(self) -\u0026gt; str: return f\u0026#34;Engine {self.name} goes {self.sound}!\u0026#34; class Car: def __init__(self, engine: Engine, tier: str, price: int) -\u0026gt; None: self.engine = engine self.tier = tier self.price = price def info(self) -\u0026gt; dict[str, Any]: return {\u0026#34;tier\u0026#34;: self.tier, \u0026#34;price\u0026#34;: self.price} Ideally, you\u0026rsquo;d to use the classes as a good citizen as follows:\nengine = Engine(\u0026#34;w16\u0026#34;, \u0026#34;vroom\u0026#34;) car = Car(engine, \u0026#34;supercar\u0026#34;, 3_000_000) # Access \u0026#39;engine\u0026#39; attribute from \u0026#39;car\u0026#39; instance: print(car.engine.name) print(car.engine.sound) This will print the following:\n$ python src.py w16 vroom However, I wanted free attribute access, just like we get in inheritance. We should be able to do car.name, not car.engine.name, and get the name of the engine instance. With a little bit of __getattr__ magic, it\u0026rsquo;s easy to do so:\n# src.py from typing import Any class Engine: def __init__(self, name: str, sound: str) -\u0026gt; None: self.name = name self.sound = sound def noise(self) -\u0026gt; str: return f\u0026#34;Engine {self.name} goes {self.sound}!\u0026#34; class Car: def __init__(self, engine: Engine, tier: str, price: int) -\u0026gt; None: self.engine = engine self.tier = tier self.price = price def info(self) -\u0026gt; dict[str, Any]: return {\u0026#34;tier\u0026#34;: self.tier, \u0026#34;price\u0026#34;: self.price} # NOTE: This is new!! def __getattr__(self, attr: str) -\u0026gt; Any: return getattr(self.engine, attr) This snippet is exactly the same as before and the only thing that was added here is the __getattr__ method in the Car class. Whenever you\u0026rsquo;ll try to access an attribute or a method on an instance of the Car class, the __getattr__ will intervene. It\u0026rsquo;ll first look for the attribute in the instance of the Car class and if it can\u0026rsquo;t find it there, then it\u0026rsquo;ll look for the attribute in the instance of the Engine class; just like type inheritance. This will work in case of method access as well. So now you can use the classes as below:\nengine = Engine(\u0026#34;w16\u0026#34;, \u0026#34;vroom\u0026#34;) car = Car(engine, \u0026#34;supercar\u0026#34;, 3_000_000) print(car.name) # Actually prints the \u0026#39;name\u0026#39; of the engine print(car.sound) # Prints the \u0026#39;sound\u0026#39; of the engine print(car.info()) # Method \u0026#39;info\u0026#39; is in the \u0026#39;Car\u0026#39; instance print(car.noise()) # Method \u0026#39;noise\u0026#39; is in the \u0026#39;Engine\u0026#39; instance This will print:\n$ python src.py w16 vroom {\u0026#39;tier\u0026#39;: \u0026#39;supercar\u0026#39;, \u0026#39;price\u0026#39;: 3000000} Engine w16 goes vroom! While this was all fun and dandy, I don\u0026rsquo;t recommend putting it in any serious code as it can obfuscate the program\u0026rsquo;s intent and can make obvious things not-so-obvious. Also, in case of attributes and methods with the same names in different classes, this can get hairy. I just found this gymnastics intellectually stimulating.\nComplete example with tests # src.py import unittest from typing import Any class Engine: def __init__(self, name: str, sound: str) -\u0026gt; None: self.name = name self.sound = sound def noise(self) -\u0026gt; str: return f\u0026#34;Engine {self.name} goes {self.sound}!\u0026#34; class Car: def __init__(self, engine: Engine, tier: str, price: int) -\u0026gt; None: self.engine = engine self.tier = tier self.price = price def info(self) -\u0026gt; dict[str, Any]: return {\u0026#34;tier\u0026#34;: self.tier, \u0026#34;price\u0026#34;: self.price} def __getattr__(self, attr: str) -\u0026gt; Any: return getattr(self.engine, attr) class Test(unittest.TestCase): def setUp(self): self.engine = Engine(\u0026#34;w16\u0026#34;, \u0026#34;vroom\u0026#34;) self.car = Car(self.engine, \u0026#34;supercar\u0026#34;, 3_000_000) def test_auto_delegation(self): expected_info = {\u0026#34;tier\u0026#34;: \u0026#34;supercar\u0026#34;, \u0026#34;price\u0026#34;: 3000000} expected_noise = \u0026#34;Engine w16 goes vroom!\u0026#34; self.assertEqual(self.car.info(), expected_info) self.assertEqual(self.car.noise(), expected_noise) if __name__ == \u0026#34;__main__\u0026#34;: unittest.main(Test()) ","permalink":"https://rednafi.com/python/attribute-delegation-in-composition/","summary":"\u003cp\u003eWhile trying to avoid inheritance in an API that I was working on, I came across this neat\ntrick to perform attribute delegation on composed classes. Let\u0026rsquo;s say there\u0026rsquo;s a class called\n\u003ccode\u003eEngine\u003c/code\u003e and you want to put an engine instance in a \u003ccode\u003eCar\u003c/code\u003e. In this case, the car has a\nclassic \u0026lsquo;has a\u0026rsquo; (inheritance usually refers to \u0026lsquo;is a\u0026rsquo; relationships) relationship with the\nengine. So, composition makes more sense than inheritance here. Consider this example:\u003c/p\u003e","title":"Automatic attribute delegation in Python composition"},{"content":"I wanted to add a helper method to an Enum class. However, I didn\u0026rsquo;t want to make it a classmethod as property method made more sense in this particular case. Problem is, you aren\u0026rsquo;t supposed to initialize an enum class, and property methods can only be accessed from the instances of a class; not from the class itself.\nWhile sifting through Django 3.2\u0026rsquo;s codebase, I found this neat trick to make a classmethod that acts like a property method and can be accessed directly from the class without initializing it.\n# src.py # This requires Python 3.4+. from enum import Enum, EnumMeta class PlanetsMeta(EnumMeta): @property def choices(cls): return [(v.name, v.value) for v in cls] class Planets(Enum, metaclass=PlanetsMeta): EARTH = \u0026#34;earth\u0026#34; MARS = \u0026#34;mars\u0026#34; # This can be accessed as follows. print(Planets.choices) If you run the script, you\u0026rsquo;ll see the following output:\n$ python3.8 src.py [(\u0026#39;EARTH\u0026#39;, \u0026#39;earth\u0026#39;), (\u0026#39;MARS\u0026#39;, \u0026#39;mars\u0026#39;)] While the previous example is quite impressive, I still don\u0026rsquo;t like the solution as it requires creating a metaclass and doing a bunch of magic to achieve something so simple. Luckily, Python3.9+ makes it possible without any additional magic. Notice the example below:\n# src.py # Requires Python 3.9+ class ModernPlanets(Enum): EARTH = \u0026#34;earth\u0026#34; MARS = \u0026#34;mars\u0026#34; @classmethod @property def choices(cls): return [(v.name, v.value) for v in cls] # This can be accessed as follows. print(ModernPlanets.choices) The only thing that matters here is the order of the property and classmethod decorator. Python applies them from bottom to top. Changing the order will make it behave unexpectedly.\nComplete example with tests # src.py # Requires Python 3.4+ import sys import unittest from enum import Enum, EnumMeta class PlanetsMeta(EnumMeta): @property def choices(cls): return [(v.name, v.value) for v in cls] class Planets(Enum, metaclass=PlanetsMeta): EARTH = \u0026#34;earth\u0026#34; MARS = \u0026#34;mars\u0026#34; # Requires Python 3.9+ class ModernPlanets(Enum): EARTH = \u0026#34;earth\u0026#34; MARS = \u0026#34;mars\u0026#34; @classmethod @property def choices(cls): return [(v.name, v.value) for v in cls] class TestPlanets(unittest.TestCase): python_version = (sys.version_info.major, sys.version_info.minor) def setUp(self): self.expected_result = [(\u0026#34;EARTH\u0026#34;, \u0026#34;earth\u0026#34;), (\u0026#34;MARS\u0026#34;, \u0026#34;mars\u0026#34;)] def test_planets(self): self.assertEqual(Planets.choices, self.expected_result) @unittest.skipIf( python_version \u0026lt; (3, 9), \u0026#34;Not supported under Python 3.9\u0026#34;, ) def test_modern_planets(self): \u0026#34;\u0026#34;\u0026#34;This test method will fail if we try to run it on a version earlier than Python 3.9. So we skip it accordingly.\u0026#34;\u0026#34;\u0026#34; self.assertEqual(ModernPlanets.choices, self.expected_result) if __name__ == \u0026#34;__main__\u0026#34;: unittest.main() Running this will print out the following:\n$ python src.py .. ---------------------------------------------------------------------- Ran 2 tests in 0.000s OK ","permalink":"https://rednafi.com/python/access-classmethod-like-property/","summary":"\u003cp\u003eI wanted to add a helper method to an Enum class. However, I didn\u0026rsquo;t want to make it a\n\u003ccode\u003eclassmethod\u003c/code\u003e as \u003ccode\u003eproperty\u003c/code\u003e method made more sense in this particular case. Problem is, you\naren\u0026rsquo;t supposed to initialize an enum class, and \u003ccode\u003eproperty\u003c/code\u003e methods can only be accessed\nfrom the instances of a class; not from the class itself.\u003c/p\u003e\n\u003cp\u003eWhile sifting through Django 3.2\u0026rsquo;s codebase, I found this neat trick to make a \u003ccode\u003eclassmethod\u003c/code\u003e\nthat acts like a \u003ccode\u003eproperty\u003c/code\u003e method and can be accessed directly from the class without\ninitializing it.\u003c/p\u003e","title":"Access 'classmethod's like 'property' methods in Python"},{"content":"I was browsing through the source code of Tom Christie\u0026rsquo;s typesystem library and discovered that the shell scripts of the project don\u0026rsquo;t have any extensions attached to them. At first, I found it odd, and then it all started to make sense.\nExecutable scripts can be written in any language and the users don\u0026rsquo;t need to care about that.\nGitHub uses this scripts-to-rule-them-all pattern successfully to normalize their scripts. According to the pattern, every project should have a folder named scripts with a subset or superset of the following files:\nscript/bootstrap – installs/updates all dependencies script/setup – sets up a project to be used for the first time script/update – updates a project to run at its current version script/server – starts app script/test – runs tests script/cibuild – invoked by continuous integration servers to run tests script/console – opens a console ","permalink":"https://rednafi.com/misc/do-not-add-extensions-to-bash-executables/","summary":"\u003cp\u003eI was browsing through the source code of Tom Christie\u0026rsquo;s \u003ca href=\"https://github.com/encode/typesystem\"\u003etypesystem\u003c/a\u003e library and discovered\nthat the \u003ca href=\"https://github.com/encode/typesystem/tree/master/scripts\"\u003eshell scripts\u003c/a\u003e of the project don\u0026rsquo;t have any extensions attached to them. At\nfirst, I found it odd, and then it all started to make sense.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eExecutable scripts can be written in any language and the users don\u0026rsquo;t need to care about\nthat.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eGitHub uses this \u003ca href=\"https://github.com/github/scripts-to-rule-them-all\"\u003escripts-to-rule-them-all pattern\u003c/a\u003e successfully to normalize their scripts.\nAccording to the pattern, every project should have a folder named \u003ccode\u003escripts\u003c/code\u003e with a subset\nor superset of the following files:\u003c/p\u003e","title":"Don't add extensions to shell executables"},{"content":"At my workplace, we have a fairly large Celery config file where you\u0026rsquo;re expected to subclass from a base class and extend that if there\u0026rsquo;s a new domain. However, the subclass expects the configuration in a specific schema. So, having a way to enforce that schema in the subclasses and raising appropriate runtime exceptions is nice.\nWrote a fancy Python 3.6+ __init_subclasshook__ to validate the subclasses as below. This is neater than writing a metaclass.\n# main.py from collections.abc import Mapping from typing import Any class Base: def __init_subclass__( cls, validate_config: bool = False, **kwargs: Any, ) -\u0026gt; None: if validate_config: cls._raise_error_for_invalid_config(cls) @staticmethod def _raise_error_for_invalid_config(cls) -\u0026gt; None: if not \u0026#34;config\u0026#34; in cls.__dict__: raise Exception( f\u0026#34;\u0026#39;{cls.__name__}\u0026#39; should define \u0026#39;config\u0026#39; attr\u0026#34;, ) if not isinstance(cls.config, Mapping): raise Exception( \u0026#34;attribute \u0026#39;config\u0026#39; should be of \u0026#39;Mapping\u0026#39; type\u0026#34;, ) config = cls.config config_keys = config.keys() expected_keys = (\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;bazz\u0026#34;) if not tuple(config_keys) == expected_keys: raise Exception( f\u0026#34;\u0026#39;config\u0026#39; should have only {expected_keys} keys\u0026#34;, ) def __repr__(self) -\u0026gt; str: return f\u0026#34;{self.config}\u0026#34; class Sub(Base, validate_config=True): config = {\u0026#34;foo\u0026#34;: 1, \u0026#34;bar\u0026#34;: 2, \u0026#34;bazz\u0026#34;: 3} s = Sub() print(s) Running the script will print:\n{\u0026#39;foo\u0026#39;: 1, \u0026#39;bar\u0026#39;: 2, \u0026#39;bazz\u0026#39;: 3} However, if we initialize the Sub class like this:\nclass Sub(Base): config = {\u0026#34;not\u0026#34;: 1, \u0026#34;allowed\u0026#34;: 2} This will raise an error:\nTraceback (most recent call last): File \u0026#34;main.py\u0026#34;, line 29, in \u0026lt;module\u0026gt; class Sub(Base, validate_config=True): File \u0026#34;main.py\u0026#34;, line 8, in __init_subclass__ cls._raise_error_for_invalid_config(cls) File \u0026#34;main.py\u0026#34;, line 23, in _raise_error_for_invalid_config raise Exception(f\u0026#34;\u0026#39;config\u0026#39; should have only {expected_keys} keys\u0026#34;) Exception: \u0026#39;config\u0026#39; should have only (\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;, \u0026#39;bazz\u0026#39;) keys Test # test_base.py # Install pytest before running the script. import pytest from main import Base def test_base(): # Don\u0026#39;t raise any exception if validate_config is False. class A(Base, validate_config=False): hello = \u0026#34;world\u0026#34; # Raise error when there\u0026#39;s no attribute called config. with pytest.raises( Exception, match=\u0026#34;\u0026#39;B\u0026#39; should define a class attribute named \u0026#39;config\u0026#39;\u0026#34;, ): class B(Base, validate_config=True): hello = \u0026#34;world\u0026#34; # Raise error when config isn\u0026#39;t a Mapping. with pytest.raises( Exception, match=\u0026#34;attribute \u0026#39;config\u0026#39; should be of \u0026#39;Mapping\u0026#39; type\u0026#34;, ): class C(Base, validate_config=True): config = [1, 2, 3] # Raise error when config is empty. with pytest.raises( Exception, match=\u0026#34;\u0026#39;config\u0026#39; map should have only \u0026#39;foo, bar, bazz\u0026#39; keys\u0026#34;, ): class D(Base, validate_config=True): config = {} # Raise error when config doesn\u0026#39;t have `foo, bar, bazz` keys. with pytest.raises( Exception, match=\u0026#34;\u0026#39;config\u0026#39; map should have only \u0026#39;foo, bar, bazz\u0026#39; keys\u0026#34;, ): class E(Base, validate_config=True): config = {\u0026#34;foo\u0026#34;: 1, \u0026#34;bar\u0026#34;: 2, \u0026#34;wrong_attribute\u0026#34;: 3} # Should pass successfully. class F(Base): config = {\u0026#34;foo\u0026#34;: 1, \u0026#34;bar\u0026#34;: 2, \u0026#34;bazz\u0026#34;: 3} # Assert f = F() # Check the repr. assert str(f) == f\u0026#34;{{\u0026#39;foo\u0026#39;: 1, \u0026#39;bar\u0026#39;: 2, \u0026#39;bazz\u0026#39;: 3}}\u0026#34; ","permalink":"https://rednafi.com/python/use-init-subclass-hook-to-validate-subclasses/","summary":"\u003cp\u003eAt my workplace, we have a fairly large Celery config file where you\u0026rsquo;re expected to subclass\nfrom a base class and extend that if there\u0026rsquo;s a new domain. However, the subclass expects the\nconfiguration in a specific schema. So, having a way to enforce that schema in the\nsubclasses and raising appropriate runtime exceptions is nice.\u003c/p\u003e\n\u003cp\u003eWrote a fancy Python 3.6+ \u003ccode\u003e__init_subclasshook__\u003c/code\u003e to validate the subclasses as below. This\nis neater than writing a metaclass.\u003c/p\u003e","title":"Use __init_subclass__ hook to validate subclasses in Python"},{"content":"Making tqdm play nice with multiprocessing requires some additional work. It\u0026rsquo;s not always obvious and I don\u0026rsquo;t want to add another third-party dependency just for this purpose.\nThe following example attempts to make tqdm work with multiprocessing.imap_unordered. However, this should also work with similar mapping methods like — multiprocessing.map, multiprocessing.imap, multiprocessing.starmap, etc.\n\u0026#34;\u0026#34;\u0026#34; Run `pip install tqdm` before running the script. The function `foo` is going to be executed 100 times across `MAX_WORKERS=5` processes. In a single pass, each process will get an iterable of size `CHUNK_SIZE=5`. So 5 processes each consuming 5 elements of an iterable will require (100 / (5*5)) 4 passes to finish consuming the entire iterable of 100 elements. Tqdm progress bar will update every `MAX_WORKERS*CHUNK_SIZE` iterations. \u0026#34;\u0026#34;\u0026#34; # src.py from __future__ import annotations import multiprocessing as mp from tqdm import tqdm import time import random from dataclasses import dataclass MAX_WORKERS = 5 CHUNK_SIZE = 5 @dataclass class StartEnd: start: int end: int def foo(start_end: StartEnd) -\u0026gt; int: time.sleep(0.2) return random.randint(start_end.start, start_end.end) def main() -\u0026gt; None: inputs = [ StartEnd(start, end) for start, end in zip( range(0, 100), range(100, 200), ) ] with mp.Pool(processes=MAX_WORKERS) as pool: results = tqdm( pool.imap_unordered(foo, inputs, chunksize=CHUNK_SIZE), total=len(inputs), ) # \u0026#39;total\u0026#39; is redundant here but can be useful # when the size of the iterable is unobvious for result in results: print(result) if __name__ == \u0026#34;__main__\u0026#34;: main() This will print:\n0%| | 0/100 [00:00\u0026lt;?, ?it/s] 14 1%|▌ | 1/100 [00:01\u0026lt;01:39, 1.00s/it] 6 9 70 ... 26%|██████████████▎ | 26/100 [00:02\u0026lt;00:04, 15.10it/s] 70 42 41 ... 51%|████████████████████████████ | 51/100 [00:03\u0026lt;00:02, 19.61it/s] 114 135 59 ... 76%|█████████████████████████████████████████▊ | 76/100 [00:04\u0026lt;00:01, 21.72it/s] 134 106 167 ... 100%|██████████████████████████████████████████████████████| 100/100 [00:04\u0026lt;00:00] Further reading Using tqdm with multiprocessing ","permalink":"https://rednafi.com/python/tqdm-with-multiprocessing/","summary":"\u003cp\u003eMaking tqdm play nice with multiprocessing requires some additional work. It\u0026rsquo;s not always\nobvious and I don\u0026rsquo;t want to add another third-party dependency just for this purpose.\u003c/p\u003e\n\u003cp\u003eThe following example attempts to make tqdm work with \u003ccode\u003emultiprocessing.imap_unordered\u003c/code\u003e.\nHowever, this should also work with similar mapping methods like — \u003ccode\u003emultiprocessing.map\u003c/code\u003e,\n\u003ccode\u003emultiprocessing.imap\u003c/code\u003e, \u003ccode\u003emultiprocessing.starmap\u003c/code\u003e, etc.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003eRun `pip install tqdm` before running the script.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003eThe function `foo` is going to be executed 100 times across\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e`MAX_WORKERS=5` processes. In a single pass, each process will\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003eget an iterable of size `CHUNK_SIZE=5`. So 5 processes each consuming\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e5 elements of an iterable will require (100 / (5*5)) 4 passes to finish\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003econsuming the entire iterable of 100 elements.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003eTqdm progress bar will update every `MAX_WORKERS*CHUNK_SIZE` iterations.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# src.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003e__future__\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eannotations\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003emultiprocessing\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"nn\"\u003emp\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etqdm\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003etqdm\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003etime\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003erandom\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003edataclasses\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003edataclass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eMAX_WORKERS\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eCHUNK_SIZE\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nd\"\u003e@dataclass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eStartEnd\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003estart\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eend\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003efoo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003estart_end\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eStartEnd\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003etime\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esleep\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mf\"\u003e0.2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003erandom\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erandint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003estart_end\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estart\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003estart_end\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eend\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003einputs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003eStartEnd\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003estart\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eend\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003estart\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eend\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003ezip\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e100\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e100\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e200\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"n\"\u003emp\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ePool\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eprocesses\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eMAX_WORKERS\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003epool\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003eresults\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etqdm\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003epool\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eimap_unordered\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003efoo\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003einputs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003echunksize\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eCHUNK_SIZE\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003etotal\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"nb\"\u003elen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003einputs\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"p\"\u003e)\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# \u0026#39;total\u0026#39; is redundant here but can be useful\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"c1\"\u003e# when the size of the iterable is unobvious\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003eresult\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003eresults\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eresult\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"vm\"\u003e__name__\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis will print:\u003c/p\u003e","title":"Running tqdm with Python multiprocessing"},{"content":"Python\u0026rsquo;s daemon threads are cool. A Python script will stop when the main thread is done and only daemon threads are running. To test a simple hello function that runs indefinitely, you can do the following:\n# test_hello.py from __future__ import annotations import asyncio import threading from functools import partial from unittest.mock import patch async def hello() -\u0026gt; None: while True: await asyncio.sleep(1) print(\u0026#34;hello\u0026#34;) @patch(\u0026#34;asyncio.sleep\u0026#34;, autospec=True) async def test_hello(mock_asyncio_sleep, capsys): run = partial(asyncio.run, hello()) t = threading.Thread(target=run, daemon=True) t.start() t.join(timeout=0.1) out, err = capsys.readouterr() assert err == \u0026#34;\u0026#34; assert \u0026#34;hello\u0026#34; in out mock_asyncio_sleep.assert_awaited() To execute the script, make sure you\u0026rsquo;ve your virtual env actiavated. Also you\u0026rsquo;ll need to install pytest and pytest-asyncio. Then run:\npytest -v -s --asyncio-mode=auto The idea came from this Quora answer.\n","permalink":"https://rednafi.com/python/use-daemon-threads-to-test-infinite-loop/","summary":"\u003cp\u003ePython\u0026rsquo;s daemon threads are cool. A Python script will stop when the main thread is done and\nonly daemon threads are running. To test a simple \u003ccode\u003ehello\u003c/code\u003e function that runs indefinitely,\nyou can do the following:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# test_hello.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003e__future__\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eannotations\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003easyncio\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ethreading\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003efunctools\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003epartial\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003eunittest.mock\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003epatch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003easync\u003c/span\u003e \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ewhile\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eawait\u003c/span\u003e \u003cspan class=\"n\"\u003easyncio\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esleep\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;hello\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nd\"\u003e@patch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;asyncio.sleep\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eautospec\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003easync\u003c/span\u003e \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003etest_hello\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emock_asyncio_sleep\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ecapsys\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003erun\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003epartial\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003easyncio\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ehello\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003et\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ethreading\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eThread\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etarget\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003erun\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edaemon\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estart\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejoin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etimeout\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mf\"\u003e0.1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eerr\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ecapsys\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ereadouterr\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eassert\u003c/span\u003e \u003cspan class=\"n\"\u003eerr\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eassert\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;hello\u0026#34;\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003eout\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003emock_asyncio_sleep\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eassert_awaited\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTo execute the script, make sure you\u0026rsquo;ve your virtual env actiavated. Also you\u0026rsquo;ll need to\ninstall \u003ccode\u003epytest\u003c/code\u003e and \u003ccode\u003epytest-asyncio\u003c/code\u003e. Then run:\u003c/p\u003e","title":"Use daemon threads to test infinite while loops in Python"},{"content":"One thing that came to me as news is that the command which — which is the de-facto tool to find the path of an executable — is not POSIX compliant. The recent Debian which hunt brought it to my attention. The POSIX-compliant way of finding an executable program is command -v, which is usually built into most of the shells.\nSo, instead of doing this:\nwhich python3.12 Do this:\ncommand -v which python3.12 ","permalink":"https://rednafi.com/misc/use-command-v-over-which/","summary":"\u003cp\u003eOne thing that came to me as news is that the command \u003ccode\u003ewhich\u003c/code\u003e — which is the de-facto tool\nto find the path of an executable — is not POSIX compliant. The recent \u003ca href=\"https://lwn.net/Articles/874049/\"\u003eDebian which hunt\u003c/a\u003e\nbrought it to my attention. The POSIX-compliant way of finding an executable program is\n\u003ccode\u003ecommand -v\u003c/code\u003e, which is usually built into most of the shells.\u003c/p\u003e\n\u003cp\u003eSo, instead of doing this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ewhich python3.12\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eDo this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003ecommand\u003c/span\u003e -v which python3.12\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c!-- references --\u003e\n\u003c!-- prettier-ignore-start --\u003e\n\u003c!-- prettier-ignore-end --\u003e","title":"Use 'command -v' over 'which' to find a program's executable"},{"content":"Writing consistent commit messages helps you to weave a coherent story with your git history. Recently, I\u0026rsquo;ve started paying attention to my commit messages. Before this, my commit messages in this repository used to look like this:\ngit log --oneline -5 d058a23 (HEAD -\u0026gt; master) bash strict mode a62e59b Updating functool partials til. 532b21a Added functool partials til ec9191c added unfinished indexing script 18e41c8 Bash tils With all the misuse of letter casings and punctuations, clearly, the message formatting is all over the place. To tame this mayhem, I\u0026rsquo;ve adopted these 7 rules of writing great commit messages:\nThe seven rules of writing consistent git commit messages Separate subject from body with a blank line Limit the subject line to 50 characters (I often break this when there\u0026rsquo;s no message body) Capitalize the subject line Do not end the subject line with a period Use the imperative mood in the subject line Wrap the body at 72 characters Use the body to explain what and why vs. how Now, after rebasing, currently, the commit messages in this repo look like this:\ngit log --oneline -5 d058a23 (HEAD -\u0026gt; master) Employ bash strict mode a62e59b Update functool partials til 532b21a Add functool partials til ec9191c Add unfinished indexing script 18e41c8 Update bash tils Further reading How to write a git commit message ","permalink":"https://rednafi.com/misc/write-git-commit-messages-properly/","summary":"\u003cp\u003eWriting consistent commit messages helps you to weave a coherent story with your git\nhistory. Recently, I\u0026rsquo;ve started paying attention to my commit messages. Before this, my\ncommit messages in this repository used to look like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit log --oneline -5\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ed058a23 (HEAD -\u0026gt; master) bash strict mode\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ea62e59b Updating functool partials til.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e532b21a Added functool partials til\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eec9191c added unfinished indexing script\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e18e41c8 Bash tils\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWith all the misuse of letter casings and punctuations, clearly, the message formatting is\nall over the place. To tame this mayhem, I\u0026rsquo;ve adopted these 7 rules of writing great commit\nmessages:\u003c/p\u003e","title":"Write git commit messages properly"},{"content":"The constructor for functools.partial() detects nesting and automatically flattens itself to a more efficient form. For example:\nfrom functools import partial def f(*, a: int, b: int, c: int) -\u0026gt; None: print(f\u0026#34;Args are {a}-{b}-{c}\u0026#34;) g = partial(partial(partial(f, a=1), b=2), c=3) # Three function calls are flattened into one; free efficiency. print(g) # Bare function can be called as 3 arguments were bound previously. g() This returns:\nfunctools.partial(\u0026lt;function f at 0x7f4fd16c11f0\u0026gt;, a=1, b=2, c=3) Args are 1-2-3 Further reading Tweet by Raymond Hettinger ","permalink":"https://rednafi.com/python/functools-partial-flattens-nestings-automatically/","summary":"\u003cp\u003eThe constructor for \u003ccode\u003efunctools.partial()\u003c/code\u003e detects nesting and automatically flattens itself\nto a more efficient form. For example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003efunctools\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003epartial\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ef\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Args are \u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e-\u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e-\u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eg\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003epartial\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epartial\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epartial\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Three function calls are flattened into one; free efficiency.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eg\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Bare function can be called as 3 arguments were bound previously.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eg\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis returns:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003efunctools.partial(\u0026lt;function f at 0x7f4fd16c11f0\u0026gt;, a=1, b=2, c=3)\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eArgs are 1-2-3\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"further-reading\"\u003eFurther reading\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://twitter.com/raymondh/status/1454865294120325124\"\u003eTweet by Raymond Hettinger\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- references --\u003e\n\u003c!-- prettier-ignore-start --\u003e\n\u003c!-- prettier-ignore-end --\u003e","title":"Python's 'functools.partial' flattens nestings Automatically"},{"content":"Pasting shell commands can be a pain when they include hidden return \\n characters. In such a case, your shell will try to execute the command immediately. To prevent that, use curly braces { \u0026lt;cmd\u0026gt; } while pasting the command. Your command should look like the following:\n{ dig +short google.com } Here, the spaces after the braces are significant.\n","permalink":"https://rednafi.com/misc/use-curly-braces-while-pasting-shell-commands/","summary":"\u003cp\u003ePasting shell commands can be a pain when they include hidden return \u003ccode\u003e\\n\u003c/code\u003e characters. In\nsuch a case, your shell will try to execute the command immediately. To prevent that, use\ncurly braces \u003ccode\u003e{ \u0026lt;cmd\u0026gt; }\u003c/code\u003e while pasting the command. Your command should look like the\nfollowing:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e{\u003c/span\u003e dig +short google.com \u003cspan class=\"o\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHere, the spaces after the braces are significant.\u003c/p\u003e","title":"Use curly braces while pasting shell commands"},{"content":"Use unofficial bash strict mode while writing scripts. Bash has a few gotchas and this helps you to avoid that. For example:\n#!/bin/bash set -euo pipefail echo \u0026#34;Hello\u0026#34; Where,\n-e Exit immediately if a command exits with a non-zero status. -u Treat unset variables as an error when substituting. -o pipefail The return value of a pipeline is the status of the last command to exit with a non-zero status, or zero if no command exited with a non-zero status. Further reading Unofficial bash strict mode ","permalink":"https://rednafi.com/misc/use-strict-mode-while-running-bash-scripts/","summary":"\u003cp\u003eUse unofficial bash strict mode while writing scripts. Bash has a few gotchas and this helps\nyou to avoid that. For example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cp\"\u003e#!/bin/bash\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eset\u003c/span\u003e -euo pipefail\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eecho\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Hello\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWhere,\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e-e              Exit immediately if a command exits with a non-zero status.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e-u              Treat unset variables as an error when substituting.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e-o pipefail     The return value of a pipeline is the status of\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                the last command to exit with a non-zero status,\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                or zero if no command exited with a non-zero status.\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"further-reading\"\u003eFurther reading\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://redsymbol.net/articles/unofficial-bash-strict-mode/\"\u003eUnofficial bash strict mode\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- references --\u003e\n\u003c!-- prettier-ignore-start --\u003e\n\u003c!-- prettier-ignore-end --\u003e","title":"Use strict mode while running bash scripts"},{"content":"Managing configurations in your Python applications isn\u0026rsquo;t something you think about much often, until complexity starts to seep in and forces you to re-architect your initial approach. Ideally, your config management flow shouldn\u0026rsquo;t change across different applications or as your application begins to grow in size and complexity.\nEven if you\u0026rsquo;re writing a library, there should be a consistent config management process that scales up properly. Since I primarily spend my time writing data-analytics, data-science applications and expose them using Flask or FastAPI framework, I\u0026rsquo;ll be tacking config management from an application development perspective.\nFew ineffective approaches In the past, while exposing APIs with Flask, I used to use .env, .flaskenv and Config class approach to manage configs which is pretty much a standard in the Flask realm. However, it quickly became cumbersome to maintain and juggle between configs depending on development, staging or production environments.\nThere were additional application specific global constants to deal with too. So I tried using *.json, *.yaml or *.toml based config management approaches but those too, quickly turned into a tangled mess. I was constantly accessing variables buried into 3-4 levels of nested toml data structure and it wasn\u0026rsquo;t pretty.\nThen there are config management libraries like Dynaconf or environ-config that aim to ameliorate the issue. While these are all amazing tools but they also introduce their own custom workflow that can feel over-engineered while dealing with maintenance and extension.\nA pragmatic wishlist I wanted to take a declarative approach while designing a config management pipleline that\u0026rsquo;ll be modular, scalable and easy to maintain. To meet my requirements, the system should be able to:\nRead configs from .env files and shell environment at the same time. Handle dependency injection for introducing passwords or secrets. Convert variable types automatically in the appropriate cases, e.g. string to integer conversion. Keep development, staging and production configs separate. Switch between the different environments e.g development, staging effortlessly. Inspect the active config values Create arbitrarily nested config structure if required (Not encouraged though). Building the config management pipeline Preparation The code block that appears in this section is self contained. It should run without any modifications. If you want to play along, then just spin up a Python virtual environment and install Pydantic and python-dotenv. The following commands works on any *nix based system:\npython3.10 -m venv venv source venv/bin/activate pip install pydantic python-dotenv Make sure you have fairly a recent version of Python 3 installed, preferably Python 3.10+. You might need to install python3.10 venv.\nIntroduction to Pydantic To check off all the boxes of the wishlist above, I made a custom config management flow using pydantic, python-dotenv and the .env file. Pydantic is a fantastic data validation library that can be used for validating and implicitly converting data types using Python\u0026rsquo;s type hints. Type hinting is a formal solution to statically indicate the type of a value within your Python code. It was specified in PEP-484 and introduced in Python 3.5. Let\u0026rsquo;s define and validate the attributes of a class named User:\nfrom Pydantic import BaseModel class User(BaseModel): name: str username: str password: int user = User(name=\u0026#34;Redowan Delowar\u0026#34;, username=\u0026#34;rednafi\u0026#34;, password=\u0026#34;123\u0026#34;) print(user) This will give you:\n\u0026gt;\u0026gt;\u0026gt; User(name=\u0026#39;Redowan Delowar\u0026#39;, username=\u0026#39;rednafi\u0026#39;, password=123) In the above example, I defined a simple class named User and used Pydantic for data validation. Pydantic will make sure that the data you assign to the class attributes conform with the types you\u0026rsquo;ve annotated. Notice, how I\u0026rsquo;ve assigned a string type data in the password field and Pydantic converted it to integer type without complaining. That\u0026rsquo;s because the corresponding type annotation suggests that the password attribute of the User class should be an integer. When implicit conversion is not possible or the hinted value of an attribute doesn\u0026rsquo;t conform to its assigned type, Pydantic will throw a ValidationError.\nThe orchestration Now let\u0026rsquo;s see how you can orchestrate your config management flow with the tools mentioned above. For simplicity, let\u0026rsquo;s say you\u0026rsquo;ve 3 sets of configurations.\nConfigs of your app\u0026rsquo;s internal logic Development environment configs Production environment configs In this case, other than the first set of configs, all should go into the .env file.\nI\u0026rsquo;ll be using this .env file for demonstration. If you\u0026rsquo;re following along, then go ahead, create an empty .env file there and copy the variables mentioned below:\n#.env ENV_STATE=\u0026#34;dev\u0026#34; # or prod DEV_REDIS_HOST=\u0026#34;127.0.0.1\u0026#34; DEV_REDIS_PORT=\u0026#34;4000\u0026#34; PROD_REDIS_HOST=\u0026#34;127.0.0.2\u0026#34; PROD_REDIS_PORT=\u0026#34;5000\u0026#34; Notice how I\u0026rsquo;ve used the DEV_ and PROD_ prefixes before the environment specific configs. These help you discern between the variables designated for different environments.\nConfigs related to your application\u0026rsquo;s internal logic should either be explicitly mentioned in the same configs.py or imported from a different app_configs.py file. You shouldn\u0026rsquo;t pollute your .env files with the internal global variables necessitated by your application\u0026rsquo;s core logic.\nNow let\u0026rsquo;s dump the entire config orchestration and go though the building blocks one by one:\n# configs.py from typing import Optional from pydantic import BaseSettings, Field, BaseModel class AppConfig(BaseModel): \u0026#34;\u0026#34;\u0026#34;Application configurations.\u0026#34;\u0026#34;\u0026#34; VAR_A: int = 33 VAR_B: float = 22.0 class GlobalConfig(BaseSettings): \u0026#34;\u0026#34;\u0026#34;Global configurations.\u0026#34;\u0026#34;\u0026#34; # These variables will be loaded from the .env file. However, if # there is a shell environment variable having the same name, # that will take precedence. APP_CONFIG: AppConfig = AppConfig() # define global variables with the Field class ENV_STATE: Optional[str] = Field(None, env=\u0026#34;ENV_STATE\u0026#34;) # environment specific variables do not need the Field class REDIS_HOST: Optional[str] = None REDIS_PORT: Optional[int] = None REDIS_PASS: Optional[str] = None class Config: \u0026#34;\u0026#34;\u0026#34;Loads the dotenv file.\u0026#34;\u0026#34;\u0026#34; env_file: str = \u0026#34;.env\u0026#34; class DevConfig(GlobalConfig): \u0026#34;\u0026#34;\u0026#34;Development configurations.\u0026#34;\u0026#34;\u0026#34; class Config: env_prefix: str = \u0026#34;DEV_\u0026#34; class ProdConfig(GlobalConfig): \u0026#34;\u0026#34;\u0026#34;Production configurations.\u0026#34;\u0026#34;\u0026#34; class Config: env_prefix: str = \u0026#34;PROD_\u0026#34; class FactoryConfig: \u0026#34;\u0026#34;\u0026#34;Returns a config instance dependending on the ENV_STATE variable.\u0026#34;\u0026#34;\u0026#34; def __init__(self, env_state: Optional[str]): self.env_state = env_state def __call__(self): if self.env_state == \u0026#34;dev\u0026#34;: return DevConfig() elif self.env_state == \u0026#34;prod\u0026#34;: return ProdConfig() cnf = FactoryConfig(GlobalConfig().ENV_STATE)() print(cnf.__repr__()) The print statement of the last line in the above code block is to inspect the active configuration class. You\u0026rsquo;ll soon learn what I meant by the term active configuration. You can comment out the last line while using the code in production. Let\u0026rsquo;s explain what\u0026rsquo;s going on with each of the classes defined above.\nAppConfig The AppConfig class defines the config variables required for you API\u0026rsquo;s internal logic. In this case I\u0026rsquo;m not loading the variables from the .env file, rather defining them directly in the class. You can also define and import them from another app_configs.py file if necessary but they shouldn\u0026rsquo;t be placed in the .env file. For data validation to work, you have to inherit from Pydantic\u0026rsquo;s BaseModel and annotate the attributes using type hints while constructing the AppConfig class. Later, this class is called from the GlobalConfig class to build a nested data structure.\nGlobalConfig GlobalConfig defines the variables that propagates through other environment classes and the attributes of this class are globally accessible from all other environments. In this class, the variables are loaded from the .env file. In the .env file, global variables don\u0026rsquo;t have any environment specific prefixes like DEV_ or PROD_ before them. The class GlobalConfig inherits from Pydantic\u0026rsquo;s BaseSettings which helps to load and read the variables from the .env file. The .env file itself is loaded in the nested Config class. Although the environment variables are loaded from the .env file, Pydantic also loads your actual shell environment variables at the same time. From the Pydantic Settings management documentation:\nEven when using a dotenv file, Pydantic will still read environment variables as well as the dotenv file, environment variables will always take priority over values loaded from a dotenv file.\nThis means you can keep the sensitive variables in your .bashrc or zshrc and Pydantic will inject them during runtime. It\u0026rsquo;s a powerful feature, as it implies that you can easily keep the insensitive variables in your .env file and include that to the version control system. Meanwhile the sensitive information should be injected as a shell environment variable. For example, although I\u0026rsquo;ve defined an attribute called REDIS_PASS in the GlobalConfig class, there is no mention of any REDIS_PASS variable in the .env file. So normally, it returns None but you can easily inject a password into the REDIS_PASS variable from the shell. Assuming that you\u0026rsquo;ve set up your venv and installed the dependencies, you can test it by copying the contents of the above code snippet in file called configs.py and running the commands below:\nexport DEV_REDIS_PASS=ubuntu python configs.py This should printout:\n\u0026gt;\u0026gt;\u0026gt; DevConfig( ... ENV_STATE=\u0026#39;dev\u0026#39;, ... APP_CONFIG=AppConfig(VAR_A=33, VAR_B=22.0), ... REDIS_PASS=\u0026#39;ubuntu\u0026#39;, ... REDIS_HOST=\u0026#39;127.0.0.1\u0026#39;, REDIS_PORT=4000) Notice how your injected REDIS_PASS has appeared in the printed config class instance. Although I injected DEV_REDIS_PASS into the environment variable, it appeared as REDIS_PASS inside the DevConfig instance. This is convenient because you won\u0026rsquo;t need to change the name of the variables in your codebase when you change the environment. To understand why it printed an instance of the DevConfig class, refer to the FactoryConfig section.\nDevConfig DevConfig class inherits from the GlobalConfig class and it can define additional variables specific to the development environment. It inherits all the variables defined in the GlobalConfig class. In this case, the DevConfig class doesn\u0026rsquo;t define any new variable.\nThe nested Config class inside DevConfig defines an attribute env_prefix and assigns DEV_ prefix to it. This helps Pydantic to read your prefixed variables like DEV_REDIS_HOST, DEV_REDIS_PORT etc without you having to explicitly mention them.\nProdConfig ProdConfig class also inherits from the GlobalConfig class and it can define additional variables specific to the production environment. It inherits all the variables defined in the GlobalConfig class. In this case, like DevConfig this class doesn\u0026rsquo;t define any new variable.\nThe nested Config class inside ProdConfig defines an attribute env_prefix and assigns PROD_ prefix to it. This helps Pydantic to read your prefixed variables like PROD_REDIS_HOST, PROD_REDIS_PORT etc without you having to explicitly mention them.\nFactoryConfig FactoryConfig is the controller class that dictates which config class should be activated based on the environment state defined as ENV_STATE in the .env file. If it finds ENV_STATE=\u0026quot;dev\u0026quot; then the control flow statements in the FactoryConfig class will activate the development configs (DevConfig). Similarly, if ENV_STATE=\u0026quot;prod\u0026quot; is found then the control flow will activate the production configs (ProdConfig). Since the current environment state is ENV_STATE=\u0026quot;dev\u0026quot;, when you run the code, it prints an instance of the activated DevConfig class. This way, you can assign different values to the same variable based on different environment contexts.\nYou can also dynamically change the environment by changing the value of ENV_STATE on your shell. Run:\nEXPORT ENV_STATE=\u0026#34;prod\u0026#34; python configs.py This time the config instance should change and print the following:\n\u0026gt;\u0026gt;\u0026gt; ProdConfig( ... ENV_STATE=\u0026#39;prod\u0026#39;, ... APP_CONFIG=AppConfig(VAR_A=33, VAR_B=22.0), ... REDIS_PASS=\u0026#39;ubuntu\u0026#39;, REDIS_HOST=\u0026#39;127.0.0.2\u0026#39;, ... REDIS_PORT=5000) Accessing the configs Using the config variables is easy. Suppose you want use the variables in file called app.py. You can easily do so as shown in the following code block:\n# app.py from configs import cnf APP_CONFIG = cnf.APP_CONFIG VAR_A = APP_CONFIG.VAR_A # this is a nested config VAR_B = APP_CONFIG.VAR_B REDIS_HOST = cnf.REDIS_HOST # this is a top-level config REDIS_PORT = cnf.REDIS_PORT print(APP_CONFIG) print(VAR_A) print(VAR_B) print(REDIS_HOST) print(REDIS_PORT) This should print out:\n\u0026gt;\u0026gt;\u0026gt; ProdConfig( ... ENV_STATE=\u0026#39;prod\u0026#39;, ... APP_CONFIG=AppConfig(VAR_A=33, VAR_B=22.0), ... REDIS_PASS=\u0026#39;ubuntu\u0026#39;, ... REDIS_HOST=\u0026#39;127.0.0.2\u0026#39;, ... REDIS_PORT=5000) VAR_A=33 VAR_B=22.0 33 22.0 127.0.0.2 5000 Extending the pipeline The modular design demonstrated above is easy to maintain and extend in my opinion. Previously, for simplicity, I\u0026rsquo;ve defined only two environment scopes; development and production. Let\u0026rsquo;s say you want to add the configs for your staging environment.\nFirst you\u0026rsquo;ll need to add those staging variables to the .env file. ... STAGE_REDIS_HOST=\u0026#34;127.0.0.3\u0026#34; STAGE_REDIS_PORT=\u0026#34;6000\u0026#34; ... Then you\u0026rsquo;ve to create a class named StageConfig that inherits from the GlobalConfig class. The architecture of the class is similar to that of the DevConfig or ProdConfig class. # configs.py ... class StageConfig(GlobalConfig): \u0026#34;\u0026#34;\u0026#34;Staging configurations.\u0026#34;\u0026#34;\u0026#34; class Config: env_prefix: str = \u0026#34;STAGE_\u0026#34; ... Finally, you\u0026rsquo;ll need to insert an ENV_STATE logic into the control flow of the FactoryConfig class. See how I\u0026rsquo;ve appended another if-else block to the previous (prod) block. # configs.py ... class FactoryConfig: \u0026#34;\u0026#34;\u0026#34;Returns a config instance depending on the ENV_STATE variable.\u0026#34;\u0026#34;\u0026#34; def __init__(self, env_state: Optional[str]): self.env_state = env_state def __call__(self): if self.env_state == \u0026#34;dev\u0026#34;: return DevConfig() elif self.env_state == \u0026#34;prod\u0026#34;: return ProdConfig() elif self.env_state == \u0026#34;stage\u0026#34; return StageConfig() ... To see your new addition in action just change the ENV_STATE to \u0026ldquo;stage\u0026rdquo; in the .env file or export it to your shell environment.\nexport ENV_STATE=\u0026#34;stage\u0026#34; python configs.py This will print out an instance of the class StageConfig.\nRemarks The above workflow works perfectly for my usage scenario. So subjectively, I feel like it\u0026rsquo;s an elegant solution to a very icky problem. Your mileage will definitely vary.\nFurther reading Settings management with Pydantic Flask config management ","permalink":"https://rednafi.com/python/config-management-with-pydantic/","summary":"\u003cp\u003eManaging configurations in your Python applications isn\u0026rsquo;t something you think about much\noften, until complexity starts to seep in and forces you to re-architect your initial\napproach. Ideally, your config management flow shouldn\u0026rsquo;t change across different\napplications or as your application begins to grow in size and complexity.\u003c/p\u003e\n\u003cp\u003eEven if you\u0026rsquo;re writing a library, there should be a consistent config management process\nthat scales up properly. Since I primarily spend my time writing data-analytics,\ndata-science applications and expose them using \u003ca href=\"https://github.com/pallets/flask\"\u003eFlask\u003c/a\u003e or \u003ca href=\"https://github.com/tiangolo/fastapi\"\u003eFastAPI\u003c/a\u003e framework, I\u0026rsquo;ll be\ntacking config management from an application development perspective.\u003c/p\u003e","title":"Pedantic configuration management with Pydantic"},{"content":"Imagine a custom set-like data structure that doesn\u0026rsquo;t perform hashing and trades performance for tighter memory footprint. Or imagine a dict-like data structure that automatically stores data in a PostgreSQL or Redis database the moment you initialize it; also it lets you get-set-delete key-value pairs using the usual retrieval-assignment-deletion syntax associated with built-in dictionaries. Custom data structures can give you the power of choice and writing them will make you understand how the built-in data structures in Python are constructed.\nOne way to understand how built-in objects like dictionary, list, set etc work is to build custom data structures based on them. Python provides several mixin classes in the collection.abc module to design custom data structures that look and act like built-in structures with additional functionalities baked in.\nConcepts To understand how all these work, you\u0026rsquo;ll need a fair bit of knowledge about Interfaces, Abstract Base Classes, Mixin Classes etc. I\u0026rsquo;ll build the concept edifice layer by layer where you\u0026rsquo;ll learn about interfaces first and how they can be created and used via the abc.ABC class. Then you\u0026rsquo;ll learn how abstract base classes differ from interfaces. After that I\u0026rsquo;ll introduce mixins and explain how all these concepts can be knitted together to architect custom data structures with amazing capabilities. Let\u0026rsquo;s dive in.\nInterfaces Python interfaces can help you write classes based on common structures. They ensure that classes that provide similar functionalities will also have similar footprints. Interfaces aren\u0026rsquo;t as popular in Python as they are in other statically typed languages. The dynamic nature and duck-typing capabilities of Python often make them redundant.\nHowever, in larger applications, interfaces can make you avoid writing code that is poorly encapsulated or build classes that look awfully similar but provide completely unrelated functionalities. Moreover, interfaces implicitly spawn other powerful techniques like mixin classes which can help you achieve DRY nirvana.\nOverview At a high level, an interface acts as a blueprint for designing classes. In Python, an interface is a specialized abstract class that defines one or more abstract methods. Abstract classes differ from concrete classes in the sense that they aren\u0026rsquo;t intended to stand on their own and the methods they define shouldn\u0026rsquo;t have any implementation.\nUsually, you inherit from an interface and implement the methods defined in the abstract class in a concrete subclass. Interfaces provide the skeletons and concrete classes provide the implementation of the methods based on those skeletons. Depending on the ways you can architect interfaces, they can be segmented into two primary categories.\nInformal Interfaces Formal Interfaces Informal interfaces Informal interfaces are classes which define methods that can be overridden, but there\u0026rsquo;s no strict enforcement.\nLet\u0026rsquo;s write an informal interface for a simple calculator class:\nclass ICalc: \u0026#34;\u0026#34;\u0026#34;Informal Interface: Abstract calculator class.\u0026#34;\u0026#34;\u0026#34; def add(self, a, b): raise NotImplementedError def sub(self, a, b): raise NotImplementedError def mul(self, a, b): raise NotImplementedError def div(self, a, b): raise NotImplementedError Notice that the ICalc class has four different methods that don\u0026rsquo;t give you any implementation. It\u0026rsquo;s an informal interface because you can still instantiate the class but the methods will raise NotImplementedError if you try to apply them. You\u0026rsquo;ve to subclass the interface to use it. Let\u0026rsquo;s do it:\nclass Calc(ICalc): \u0026#34;\u0026#34;\u0026#34;Concrete Class: Calculator\u0026#34;\u0026#34;\u0026#34; def add(self, a, b): return a + b def sub(self, a, b): return a - b def mul(self, a, b): return a * b def div(self, a, b): return a / b # Using the class c = Calc() print(c.add(1, 2)) print(c.sub(2, 3)) print(c.mul(4, 5)) print(c.div(5, 6)) 3 -1 20 0.8333333333333334 Now, you might be wondering why you even need all of these boilerplate code and inheritance when you can directly define the concrete Calc class and call it a day.\nConsider the following scenario where you want to add additional functionalities to each of the method of the Calc class. Here, you\u0026rsquo;ve two options. Either you can mutate the original class and add those extra functionalities to the methods or you can create another class with similar footprint and implement all the methods with the added functionalities.\nThe first option isn\u0026rsquo;t always viable and can cause regression in real life scenario. The second approach ensures modularity and is generally quicker to implement since you won\u0026rsquo;t have to worry about messing up the original concrete class. However, figuring out which methods you\u0026rsquo;ll need to implement in the extended class can be hard because the concrete class might have additional methods that you don\u0026rsquo;t want in the extended class.\nIn this case, instead of figuring out the methods from the concrete Calc class, it\u0026rsquo;s easier to do so from an established structure defined in the ICalc interface. Interfaces make the process of extending class functionalities more tractable. Let\u0026rsquo;s make another class that will add logging to all of the methods of the Calc class:\nimport logging logging.basicConfig(level=logging.INFO) class CalcLog(ICalc): \u0026#34;\u0026#34;\u0026#34;Concrete Class: Calculator with logging\u0026#34;\u0026#34;\u0026#34; def add(self, a, b): logging.info(f\u0026#34;Operation: Addition, Arguments: {(a, b)}\u0026#34;) return a + b def sub(self, a, b): logging.info(f\u0026#34;Operation: Subtraction, Arguments: {(a, b)}\u0026#34;) return a - b def mul(self, a, b): logging.info(f\u0026#34;Operation: Multiplication, Arguments: {(a, b)}\u0026#34;) return a * b def div(self, a, b): logging.info(f\u0026#34;Operation: Division, Arguments: {(a, b)}\u0026#34;) return a / b # Using the class clog = CalcLog() print(clog.add(1, 2)) print(clog.sub(2, 3)) print(clog.mul(4, 5)) print(clog.div(5, 6)) INFO:root:Operation: Addition, Arguments: (1, 2) INFO:root:Operation: Subtraction, Arguments: (2, 3) INFO:root:Operation: Multiplication, Arguments: (4, 5) INFO:root:Operation: Division, Arguments: (5, 6) 3 -1 20 0.8333333333333334 In the above class, I\u0026rsquo;ve defined another class called CalcLog that basically extends the functionalities of the previously defined Calc class. Here, I\u0026rsquo;ve inherited from the informal interface ICalc and implemented all the methods with additional info logging capability.\nAlthough writing informal interfaces is trivial, there are multiple issues that plague them. The user of the interface class can still instantiate it like a normal class and won\u0026rsquo;t be able to tell the difference between it and a concrete class until she tries to use any of the methods define inside the interface. Only then the methods will throw exceptions. This can have unintended side effects.\nMoreover, informal interfaces won\u0026rsquo;t compel you to implement all the methods in the subclasses. You can easily get away without implementing a particular method defined in the interface. It won\u0026rsquo;t complain about the unimplemented methods in the subclasses. However, if you try to use a method that hasn\u0026rsquo;t been implemented in the subclass, you\u0026rsquo;ll get an error. This means even if issubclass(ConcreteSubClass, Interface) shows True, you can\u0026rsquo;t rely on it since it doesn\u0026rsquo;t give you the guarantee that the ConcreteSubClass has implemented all the methods defined in the Interface.\nLet\u0026rsquo;s create another class FakeCalc and only implement one method defined in the ICalc abstract class:\nclass FakeCalc(ICalc): \u0026#34;\u0026#34;\u0026#34;Concrete Class: Fake calculator that doesn\u0026#39;t implement all the methods defined in the interface.\u0026#34;\u0026#34;\u0026#34; def add(self, a, b): return a + b # Using the class cfake = FakeCalc() print(cfake.add(1, 2)) print(cfake.sub(2, 3)) 3 --------------------------------------------------------------------------- NotImplementedError Traceback (most recent call last) \u0026lt;ipython-input-48-035c519cee55\u0026gt; in \u0026lt;module\u0026gt; 10 cfake = FakeCalc() 11 print(cfake.add(1,2)) ---\u0026gt; 12 print(cfake.sub(2,3)) \u0026lt;ipython-input-45-255c6a2093b0\u0026gt; in sub(self, a, b) 6 7 def sub(self, a, b): ----\u0026gt; 8 raise NotImplementedError 9 10 def mul(self, a, b): NotImplementedError: Despite not implementing all the methods defined in the ICalc class, I was still able to instantiate the FakeCalc concrete class. However, when I tried to apply a method sub that wasn\u0026rsquo;t implemented in the concrete class, it gave me an error. Also, issubclass(FakeCalc, ICalc) returns True which can mislead you into thinking that all the methods of the subclass FakeCalc are usable. It can cause subtle bugs can be difficult to detect. Formal interfaces try to overcome these issues.\nFormal interfaces Formal interfaces don\u0026rsquo;t suffer from the problems that plague informal interfaces. So if you want to implement an interface that the users can\u0026rsquo;t initiate independently and that forces them to implement all the methods in the concrete sub classes, formal interface is the way to go. In Python, the idiomatic way to define formal interfaces is via the abc module. Let\u0026rsquo;s transform the previously mentioned ICalc interface into a formal one:\nfrom abc import ABC, abstractmethod class ICalc(ABC): \u0026#34;\u0026#34;\u0026#34;Formal interface: Abstract calculator class.\u0026#34;\u0026#34;\u0026#34; @abstractmethod def add(self, a, b): pass @abstractmethod def sub(self, a, b): pass @abstractmethod def mul(self, a, b): pass @abstractmethod def div(self, a, b): pass Here, I\u0026rsquo;ve imported ABC class and abstractmethod decorator from the abc module of Python\u0026rsquo;s standard library. The name ABC stands for Abstract Base Class. The interface class needs to inherit from this ABCclass and all the abstract methods need to be decorated using the abstractmethod decorator. If your knowledge on decorators is fuzzy, checkout this in-depth article on Python decorators.\nAlthough, it seems like ICalc has merely inherited from the ABC class, under the hood, a metaclass ABCMeta gets attached to the interface which essentially makes sure that you can\u0026rsquo;t instantiate this class independently. Let\u0026rsquo;s try to do so and see what happens:\ni = ICalc() --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-118-a3cb2945d943\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 i = ICalc() TypeError: Can\u0026#39;t instantiate abstract class ICalc with abstract methods add, div, mul, sub The error message clearly states that you can\u0026rsquo;t instantiate the class ICalc directly at all. You have to make a subclass of ICalc and implement all the abstract methods and only then you\u0026rsquo;ll be able to make an instance of the subclass. The subclassing and implementation part is same as before.\nclass Calc(ICalc): \u0026#34;\u0026#34;\u0026#34;Concrete calculator class\u0026#34;\u0026#34;\u0026#34; def add(self, a, b): return a + b def sub(self, a, b): return a - b def mul(self, a, b): return a * b def div(self, a, b): return a / b # Using the class c = Calc() print(c.add(1, 2)) print(c.sub(2, 3)) print(c.mul(4, 5)) print(c.div(5, 6)) In the case of formal interface, failing to implement even one abstract method in the subclass will raise TypeError. So you can never write something the like the FakeCalc with a formal interface. This approach is more explicit and if there is an issue, it fails early.\nInterfaces vs abstract base classes You\u0026rsquo;ve probably seen the term Interface and Abstract Base Class being used interchangeably. However, conceptually they\u0026rsquo;re different. Interfaces can be thought of as a special case of Abstract Base Classes.\nIt\u0026rsquo;s imperative that all the methods of an interface are abstract methods and the classes don\u0026rsquo;t store any state (instance variables). However, in case of abstract base classes, the methods are generally abstract but there can also be methods that provide implementation (concrete methods) and also, these classes can have instance variables. These generic abstract base classes can get very interesting and they can be used as mixins but more on that in the later sections.\nBoth interfaces and abstract base classes are similar in the sense that they can\u0026rsquo;t stand on their own, that means these classes aren\u0026rsquo;t meant to be instantiated independently. Pay attention to the following snippet to understand how interfaces and abstract base classes differ.\nInterface\nfrom abc import ABC, abstractmethod class InterfaceExample(ABC): @abstractmethod def method_a(self): pass @abstractmethod def method_b(self): pass Here, all the methods must have to be abstract.\nAbstract Base Class\nfrom abc import ABC, abstractmethod class AbstractBaseClassExample(ABC): @abstractmethod def method_a(self): pass @abstractmethod def method_b(self): pass def method_c(self): # implement something pass Notice how method_c in the above class is a concrete method and can have implementation.\nThe two examples above establish the fact that\nAll interfaces are abstract base classes but not all abstract base classes are interfaces.\nA complete example Before moving on to the next section, let\u0026rsquo;s see another contrived example to get the idea about the cases where interfaces can come handy. I\u0026rsquo;ll define an interface called AutoMobile and create three concrete classes called Car, Truck and Bus from it. The interface defines three abstract methods start, accelerate and stop that the concrete classes will need to implement later.\nfrom abc import ABC, abstractmethod class Automobile(ABC): \u0026#34;\u0026#34;\u0026#34;Formal interface: Abstract automobile class.\u0026#34;\u0026#34;\u0026#34; @abstractmethod def start(self): pass @abstractmethod def accelerate(self): pass @abstractmethod def stop(self): pass class Car(Automobile): \u0026#34;\u0026#34;\u0026#34;Concrete Class: Car\u0026#34;\u0026#34;\u0026#34; def start(self): return \u0026#34;The car is starting\u0026#34; def accelerate(self): return \u0026#34;The car is accelerating\u0026#34; def stop(self): return \u0026#34;The car is stopping\u0026#34; class Truck(Automobile): \u0026#34;\u0026#34;\u0026#34;Concrete Class: Truck\u0026#34;\u0026#34;\u0026#34; def start(self): return \u0026#34;The truck is starting\u0026#34; def accelerate(self): return \u0026#34;The truck is accelerating\u0026#34; def stop(self): return \u0026#34;The truck is stopping\u0026#34; class Bus(Automobile): \u0026#34;\u0026#34;\u0026#34;Concrete Class: Bus\u0026#34;\u0026#34;\u0026#34; def start(self): return \u0026#34;The bus is starting\u0026#34; def accelerate(self): return \u0026#34;The bus is accelerating\u0026#34; def stop(self): return \u0026#34;The bus is stopping\u0026#34; car = Car() truck = Truck() bus = Bus() print(car.start()) print(car.accelerate()) print(car.stop()) print(truck.start()) print(truck.accelerate()) print(truck.stop()) print(bus.start()) print(bus.accelerate()) print(bus.stop()) The car is starting The car is accelerating The car is stopping The truck is starting The truck is accelerating The truck is stopping The bus is starting The bus is accelerating The bus is stopping The above example delineates the use cases for interfaces. When you need to create multiple similar classes, interfaces can provide a basic foundation for the subclasses to build upon. In the next section, I\u0026rsquo;ll be using formal interfaces to create Mixin classes. So, before understanding mixin classes and how they can be used to inject additional plugins to your classes, it\u0026rsquo;s important that you understand interfaces and abstract base classes properly.\nMixins Imagine you\u0026rsquo;re baking chocolate brownies. Now, you can have them without any extra fluff which is fine or you can top them with cream cheese, caramel sauce, chocolate chips etc. Usually you don\u0026rsquo;t make the extra toppings yourself, rather you prepare the brownies and use off the shelf toppings. This also gives you the ability to mix and match different combinations of toppings to spruce up the flavors quickly. However, making the toppings from scratch would be a lengthy process and doing it over and over again can ruin the fun of baking.\nWhile creating software, there\u0026rsquo;s sometimes a limit to the depth we should go. When pieces of what we\u0026rsquo;d like to achieve have already been executed well by others, it makes a lot of sense to reuse them. One way to achieve modularity and reusability in object-oriented programming is through a concept called a mixin. Different languages implement the concept of mixin in different ways. In Python, mixins are supported via multiple inheritance.\nOverview In the context of Python especially, a mixin is a parent class that provides functionality to subclasses but isn\u0026rsquo;t intended to be instantiated itself. This should already incite deja vu in you since classes that aren\u0026rsquo;t intended to be instantiated and can have both concrete and abstract methods are basically abstract base classes. Mixins can be regarded as a specific strain of abstract base classes where they can house both concrete and abstract methods but don\u0026rsquo;t keep any internal states.\nThese can help you when -\nYou want to provide a lot of optional features for a class. You want to provide a lot of not-optional features for a class, but you want the features in separate classes so that each of them is about one feature (behavior). You want to use one particular feature in many different classes. Let\u0026rsquo;s see a contrived example. Consider Werkzeug\u0026rsquo;s request and response system. Werkzeug is a small library that Flask depends on. I can make a plain old request object by saying:\nfrom werkzeug import BaseRequest class Request(BaseRequest): pass If I want to add accept header support, I would make that:\nfrom werkzeug import BaseRequest, AcceptMixin class Request(AcceptMixin, BaseRequest): pass If I wanted to make a request object that supports accept headers, etags, user agent and authentication support, I could do this:\nfrom werkzeug import ( BaseRequest, AcceptMixin, ETagRequestMixin, UserAgentMixin, AuthenticationMixin, ) class Request( AcceptMixin, ETagRequestMixin, UserAgentMixin, AuthenticationMixin, BaseRequest, ): pass The above example might cause you to say, \u0026ldquo;that\u0026rsquo;s just multiple inheritance, not really a mixin\u0026rdquo;, which is can be true in a special case. Indeed, the differences between plain old multiple inheritance and mixin based inheritance collapse when the parent class can be instantiated. Understanding the subtlety in the differences between a mixin class, an abstract base class, an interface and the scope of multiple inheritance is important, so I\u0026rsquo;ll explore them in a dedicated section.\nDifferences between interfaces, abstract classes and mixins In order to better understand mixins, it\u0026rsquo;d be useful to compare mixins with abstract classes and interfaces from a code/implementation perspective:\nInterfaces\nInterfaces can contain abstract methods only, no concrete methods and no internal states (instance variables).\nAbstract Classes\nAbstract classes can contain abstract methods, concrete methods and internal state.\nMixins\nLike interfaces, mixins don\u0026rsquo;t contain any internal state. But like abstract classes, they can contain one or more concrete methods. So mixins are basically abstract classes without any internal states.\nIn Python, these are just conventions because all of the above are defined as classes. However, one trait that is common among interfaces, abstract classes and mixins is that they shouldn\u0026rsquo;t exist on their own, i.e. shouldn\u0026rsquo;t be instantiated independently.\nA complete example Before diving into the real-life examples and how mixins can be used to construct custom data structures, let\u0026rsquo;s have a look at a self-contained example of a mixin class at work:\nimport inspect from abc import ABC, abstractmethod from pprint import pprint class DisplayFactorMult(ABC): \u0026#34;\u0026#34;\u0026#34;Mixin class that reveals factor calculation details.\u0026#34;\u0026#34;\u0026#34; @abstractmethod def multiply(self, x): pass def multiply_show(self, x): result = self.multiply(x) print(f\u0026#34;Factor: {self.factor}, Argument: {x}, Result: {result}\u0026#34;) return result class FactorMult(DisplayFactorMult): \u0026#34;\u0026#34;\u0026#34;Concrete class that uses the DisplayFactorMult mixin.\u0026#34;\u0026#34;\u0026#34; def __init__(self, factor): self.factor = factor def multiply(self, x): return x * self.factor # Putting the FactorMult class to use f = FactorMult(10) f.multiply_show(20) # Use the inspect.getmembers method to inspect the methods pprint(inspect.getmembers(f, predicate=inspect.ismethod)) Factor: 10, Argument: 20, Result: 200 [(\u0026#39;__init__\u0026#39;, \u0026lt;bound method FactorMult.__init__ of \u0026lt;__main__.FactorMult object at 0x7f0f0546bf40\u0026gt;\u0026gt;), (\u0026#39;multiply\u0026#39;, \u0026lt;bound method FactorMult.multiply of \u0026lt;__main__.FactorMult object at 0x7f0f0546bf40\u0026gt;\u0026gt;), (\u0026#39;multiply_show\u0026#39;, \u0026lt;bound method DisplayFactorMult.multiply_show of \u0026lt;__main__.FactorMult object at 0x7f0f0546bf40\u0026gt;\u0026gt;)] The FactorMult class takes in a number as a factor and the multiply method simply multiplies an argument with the factor. The mixin class DisplayFactorMult provides an additional method multiply_show that enhances the multiply method of the concrete class. Method multiply_show prints the value of the factor, arguments and the result before returning the result. Here, DisplayFactoryMult is a mixin since it houses an abstract method multiply, a concrete method multiply_show and doesn\u0026rsquo;t store any instance variable.\nIf you really want to dive deeper into mixins and their real-life use cases, checkout the codebase of the requests library. It defines and employs many powerful mixin classes to bestow superpowers upon different concrete classes.\nBuilding powerful custom data structures with mixins You\u0026rsquo;ve reached the hall of fame where I\u0026rsquo;ll be building custom data structures using the mixin classes from the collections.abc module.\nVerbose tuple This is a tuple-like data structure that acts exactly like the built-in tuple but with one exception. It\u0026rsquo;ll print out the special methods underneath when you perform any operation with it.\nfrom collections.abc import Sequence class VerboseTuple(Sequence): \u0026#34;\u0026#34;\u0026#34;Custom class that is exactly like a tuple but does some extra magic. Sequence: ------------------- Inherits From: Reversible, Collection Abstract Methods: __getitem__, __len__ Mixin Methods: __contains__, __iter__, __reversed__, index, and count \u0026#34;\u0026#34;\u0026#34; def __init__(self, *args): self.args = args @classmethod def _classname(cls): # This method just returns the name of the class return cls.__name__ def __getitem__(self, index): print(f\u0026#34;Method: __getitem__, Index: {index}\u0026#34;) return self.args[index] def __len__(self): print(f\u0026#34;Method: __len__\u0026#34;) return len(self.args) def __repr__(self): return f\u0026#34;{self._classname()}{tuple(self.args)}\u0026#34; vt = VerboseTuple(1, 3, 4) print(vt) print(f\u0026#34;Abstract Methods: {set(Sequence.__abstractmethods__)}\u0026#34;) print( f\u0026#34;Mixin Methods: { {k for k, v in Sequence.__dict__.items() if callable(v)} }\u0026#34; ) VerboseTuple(1, 3, 4) Abstract Methods: {\u0026#39;__len__\u0026#39;, \u0026#39;__getitem__\u0026#39;} Mixin Methods: {\u0026#39;__iter__\u0026#39;, \u0026#39;__contains__\u0026#39;, \u0026#39;index\u0026#39;, \u0026#39;count\u0026#39;, \u0026#39;__getitem__\u0026#39;, \u0026#39;__reversed__\u0026#39;} To build the VerboseTuple data structure, first, I\u0026rsquo;ve inherited the Sequence mixin class from the collections.abc module. The docstring mentions all the abstract and mixin methods provided by the Sequence class. To build the new data structure, you\u0026rsquo;ll have to implement all the abstract methods defined in the Sequence class and you\u0026rsquo;ll get all the mixin methods implemented automatically. Notice that the print statement above also reveals the abstract and the mixin methods.\nIn the following snippet I\u0026rsquo;ve used some of the functionalities offered by tuple and printed them in a way that will reveal the special methods when they perform any action.\n# check __getitem__ print(\u0026#34;\\n ==== Checking __getitem__ ====\u0026#34;) print(vt[2]) # check __len__ print(\u0026#34;\\n ==== Checking __len__ ====\u0026#34;) print(len(vt)) # check __contains__ print(\u0026#34;\\n ==== Checking __contains__ ====\u0026#34;) print(3 in vt) # check __len__ print(\u0026#34;\\n ==== Checking __iter__ ====\u0026#34;) for elem in vt: print(elem) # check reverse print(f\u0026#34;\\n ==== Checking reverse ====\u0026#34;) print(list(reversed(vt))) # check count print(\u0026#34;\\n ==== Checking count ====\u0026#34;) print(vt.count(1)) ==== Checking __getitem__ ==== Method: __getitem__, Index: 2 4 ==== Checking __len__ ==== Method: __len__ 3 ==== Checking __contains__ ==== Method: __getitem__, Index: 0 Method: __getitem__, Index: 1 True ==== Checking __iter__ ==== Method: __getitem__, Index: 0 1 Method: __getitem__, Index: 1 3 Method: __getitem__, Index: 2 4 Method: __getitem__, Index: 3 ==== Checking reverse ==== Method: __len__ Method: __getitem__, Index: 2 Method: __getitem__, Index: 1 Method: __getitem__, Index: 0 [4, 3, 1] ==== Checking count ==== Method: __getitem__, Index: 0 Method: __getitem__, Index: 1 Method: __getitem__, Index: 2 Method: __getitem__, Index: 3 1 The printed statements reveal the corresponding special methods used internally when a particular tuple operation occurs.\nVerbose list This is a list-like data structure that acts exactly like the built-in list but with one exception. Like VerboseTuple, it\u0026rsquo;ll also print out the special methods underneath when you perform any operation on or with it.\nfrom collections.abc import MutableSequence class VerboseList(MutableSequence): \u0026#34;\u0026#34;\u0026#34;Custom class that is exactly like a list but does some extra magic. MutableSequence: ----------------- Inherits From: Sequence Abstract Methods: __getitem__, __setitem__, __delitem__, __len__, insert Mixin Methods: Inherited Sequence methods and append, reverse, extend, pop, remove, and __iadd__ \u0026#34;\u0026#34;\u0026#34; def __init__(self, *args): self.args = list(args) @classmethod def _classname(cls): # This method just returns the name of the class return cls.__name__ def __getitem__(self, index): print(f\u0026#34;Method: __getitem__, Index: {index}\u0026#34;) return self.args[index] def __setitem__(self, index, value): print(f\u0026#34;Method: __setitem__, Index: {index}, Value: {value}\u0026#34;) self.args[index] = value def __delitem__(self, index): print(f\u0026#34;Method: __delitem__, Index: {index}\u0026#34;) del self.args[index] def __len__(self): print(f\u0026#34;Method: __len__\u0026#34;) return len(self.args) def __repr__(self): return f\u0026#34;{self._classname()}{tuple(self.args)}\u0026#34; def insert(self, index, value): self.args.insert(index, value) vl = VerboseList(4, 5, 6) vl2 = VerboseList(7, 8, 9) print(vl) print(f\u0026#34;Abstract Methods: {set(MutableSequence.__abstractmethods__)}\u0026#34;) print( f\u0026#34;Mixin Methods: { {k for k, v in MutableSequence.__dict__.items() if callable(v)} }\u0026#34; ) VerboseList(4, 5, 6) Abstract Methods: {\u0026#39;__delitem__\u0026#39;, \u0026#39;__len__\u0026#39;, \u0026#39;__getitem__\u0026#39;, ...} Mixin Methods: {\u0026#39;__iadd__\u0026#39;, \u0026#39;__setitem__\u0026#39;, \u0026#39;pop\u0026#39;, \u0026#39;append\u0026#39;, ...} In the above segment, I\u0026rsquo;ve inherited the MutableSequence mixin class from the collections.abc module. This ensures that the VerboseList object will be mutable. All the abstract methods mentioned in the docstring have been implemented and the output print statements reveal the structure of the custom data structure as well as all the abstract and mixin methods.\nIn the following snippet, I\u0026rsquo;ve used some of the functionalities offered by list and printed them in a way that will reveal the special methods when they perform any action.\n# check __setitem__ print(\u0026#34;\\n ==== Checking __setitem__ ====\u0026#34;) vl[1] = 44 print(vl) # check remove (__delitem__) print(\u0026#34;\\n ==== Checking remove ====\u0026#34;) vl.remove(6) print(vl) # check extend print(\u0026#34;\\n ==== Checking extend ====\u0026#34;) vl.extend([0, 0]) print(vl) # check pop print(\u0026#34;\\n ==== Checking pop ====\u0026#34;) vl.pop(-1) print(vl) # check __iadd__ print(\u0026#34;\\n ==== Checking __iadd__\u0026#34;) vl += vl2 print(vl) ==== Checking __setitem__ ==== Method: __setitem__, Index: 1, Value: 44 VerboseList(4, 44, 6) ==== Checking remove ==== Method: __getitem__, Index: 0 Method: __getitem__, Index: 1 Method: __getitem__, Index: 2 Method: __delitem__, Index: 2 VerboseList(4, 44) ==== Checking extend ==== Method: __len__ Method: __len__ VerboseList(4, 44, 0, 0) ==== Checking pop ==== Method: __getitem__, Index: -1 Method: __delitem__, Index: -1 VerboseList(4, 44, 0) ==== Checking __iadd__ Method: __getitem__, Index: 0 Method: __len__ Method: __getitem__, Index: 1 Method: __len__ Method: __getitem__, Index: 2 Method: __len__ Method: __getitem__, Index: 3 VerboseList(4, 44, 0, 7, 8, 9) Verbose frozen dict Here, VerboseFrozenDict is an immutable data structure that is similar to the built-in dictionaries. Like the previous structures, this also reveals the internal special methods while performing different operations.\nfrom collections.abc import Mapping class VerboseFrozenDict(Mapping): \u0026#34;\u0026#34;\u0026#34;Custom class that is exactly like an immutable dict but does some extra magic. Mapping: ----------------- Inherits From: Collection Abstract Methods: __getitem__, __iter__, __len__ Mixin Methods: __contains__, keys, items, values, get, __eq__, and __ne__ \u0026#34;\u0026#34;\u0026#34; def __init__(self, **kwargs): self.kwargs = kwargs @classmethod def _classname(cls): # This method just returns the name of the class return cls.__name__ def __getitem__(self, key): print(f\u0026#34;Method: __getitem__, Key: {key}\u0026#34;) return self.kwargs[key] def __iter__(self): print(f\u0026#34;Method: __iter__\u0026#34;) return iter(self.kwargs) def __len__(self): print(f\u0026#34;Method: __len__\u0026#34;) return len(self.kwargs) def __repr__(self): return f\u0026#34;{self._classname()}({self.kwargs})\u0026#34; vf = VerboseFrozenDict(**{\u0026#34;a\u0026#34;: \u0026#34;apple\u0026#34;}) vf2 = VerboseFrozenDict(**{\u0026#34;b\u0026#34;: \u0026#34;orange\u0026#34;, \u0026#34;c\u0026#34;: \u0026#34;mango\u0026#34;}) print(vf) print(f\u0026#34;Abstract Methods: {set(Mapping.__abstractmethods__)}\u0026#34;) print( f\u0026#34;Mixin Methods: { {k for k, v in Mapping.__dict__.items() if callable(v)} }\u0026#34; ) VerboseFrozenDict({\u0026#39;a\u0026#39;: \u0026#39;apple\u0026#39;}) Abstract Methods: {\u0026#39;__len__\u0026#39;, \u0026#39;__getitem__\u0026#39;, \u0026#39;__iter__\u0026#39;} Mixin Methods: {\u0026#39;items\u0026#39;, \u0026#39;__contains__\u0026#39;, \u0026#39;values\u0026#39;, \u0026#39;__eq__\u0026#39;, \u0026#39;keys\u0026#39;, \u0026#39;get\u0026#39;, \u0026#39;__getitem__\u0026#39;} In the above segment, I\u0026rsquo;ve inherited the Mapping mixin class from the collections.abc module. This ensures that the output sequence will be immutable. Just like before, all the abstract methods mentioned in the docstring have been implemented and the output print statements reveal the structure of the custom data structure, all the abstract and mixin methods.\nBelow the printed output will reveal the special methods used internally when the VerboseFrozenDict objects perform any operation.\n# check __getitem__ print(\u0026#34;\\n ==== Checking __getitem__ ====\u0026#34;) print(vf[\u0026#34;a\u0026#34;]) # check __iter__ print(\u0026#34;\\n ==== Checking __iter__ ====\u0026#34;) for elem in vf: print(elem) # check __len__ print(\u0026#34;\\n ==== Checking __len__ ====\u0026#34;) print(len(vf)) # check __contains__ print(\u0026#34;\\n ==== Checking __iter__ ====\u0026#34;) print(\u0026#34;a\u0026#34; in vf) # check keys, values print(f\u0026#34;\\n ==== Checking items, keys, values ====\u0026#34;) print(vf.items()) print(vf.keys()) print(vf.values()) # check get print(\u0026#34;\\n ==== Checking get ====\u0026#34;) print(vf.get(\u0026#34;b\u0026#34;, None)) # check eq \u0026amp; nq print(\u0026#34;\\n ==== Checking __eq__, __nq__ ====\u0026#34;) print(vf == vf2) print(vf != vf2) ==== Checking __getitem__ ==== Method: __getitem__, Key: a apple ==== Checking __iter__ ==== Method: __iter__ a ==== Checking __len__ ==== Method: __len__ 1 ==== Checking __iter__ ==== Method: __getitem__, Key: a True ==== Checking items, keys, values ==== ItemsView(VerboseFrozenDict({\u0026#39;a\u0026#39;: \u0026#39;apple\u0026#39;})) KeysView(VerboseFrozenDict({\u0026#39;a\u0026#39;: \u0026#39;apple\u0026#39;})) ValuesView(VerboseFrozenDict({\u0026#39;a\u0026#39;: \u0026#39;apple\u0026#39;})) ==== Checking get ==== Method: __getitem__, Key: b None ==== Checking __eq__, __nq__ ==== Method: __iter__ Method: __getitem__, Key: a Method: __iter__ Method: __getitem__, Key: b Method: __getitem__, Key: c False Method: __iter__ Method: __getitem__, Key: a Method: __iter__ Method: __getitem__, Key: b Method: __getitem__, Key: c True Verbose dict The VerboseDict data structure is the mutable version of VerboseFrozenDict. It supports all the operations of VerboseFrozenDict with some additional features like adding and deleting key-value pairs, updating values corresponding to different keys etc.\nfrom collections.abc import MutableMapping class VerboseDict(MutableMapping): \u0026#34;\u0026#34;\u0026#34;Custom class that is exactly like a dict but does some extra magic. MutableMapping: ----------------- Inherits From: Mapping Abstract Methods: __getitem__, __setitem__, __delitem__, __iter__, __len__ Mixin Methods: Inherited Mapping methods and pop, popitem, clear, update, and setdefault \u0026#34;\u0026#34;\u0026#34; def __init__(self, **kwargs): self.kwargs = kwargs @classmethod def _classname(cls): # This method just returns the name of the class return cls.__name__ def __getitem__(self, key): print(f\u0026#34;Method: __getitem__, Key: {key}\u0026#34;) return self.kwargs[key] def __setitem__(self, key, value): print(f\u0026#34;Method: __setitem__, Key: {key}\u0026#34;) self.kwargs[key] = value def __delitem__(self, key): print(f\u0026#34;Method: __delitem__, Key: {key}\u0026#34;) del self.kwargs[key] def __iter__(self): print(f\u0026#34;Method: __iter__\u0026#34;) return iter(self.kwargs) def __len__(self): print(f\u0026#34;Method: __len__\u0026#34;) return len(self.kwargs) def __repr__(self): return f\u0026#34;{self._classname()}({self.kwargs})\u0026#34; vd = VerboseDict(**{\u0026#34;a\u0026#34;: \u0026#34;apple\u0026#34;, \u0026#34;b\u0026#34;: \u0026#34;ball\u0026#34;, \u0026#34;c\u0026#34;: \u0026#34;cat\u0026#34;}) print(vd) print(f\u0026#34;Abstract Methods: {set(MutableMapping.__abstractmethods__)}\u0026#34;) print( f\u0026#34;Mixin Methods: { {k for k, v in MutableMapping.__dict__.items() if callable(v)} }\u0026#34; ) VerboseDict({\u0026#39;a\u0026#39;: \u0026#39;apple\u0026#39;, \u0026#39;b\u0026#39;: \u0026#39;ball\u0026#39;, \u0026#39;c\u0026#39;: \u0026#39;cat\u0026#39;}) Abstract Methods: { \u0026#39;__delitem__\u0026#39;, \u0026#39;__len__\u0026#39;, \u0026#39;__iter__\u0026#39;, \u0026#39;__getitem__\u0026#39;, \u0026#39;__setitem__\u0026#39; } Mixin Methods: { \u0026#39;__setitem__\u0026#39;, \u0026#39;pop\u0026#39;, \u0026#39;popitem\u0026#39;, \u0026#39;__delitem__\u0026#39;, \u0026#39;setdefault\u0026#39;, \u0026#39;update\u0026#39;, \u0026#39;clear\u0026#39; } The output statements reveal the structure of the VeboseDict class and the abstract and mixin methods associated with it. The following snippet will print the special methods used internally by the custom data structure (also in the built-in one) while performing different operations.\n# check __getitem__ print(\u0026#34;\\n ==== Checking __setitem__ ====\u0026#34;) vd[\u0026#34;a\u0026#34;] = \u0026#34;orange\u0026#34; print(vd) # check popitem print(\u0026#34;\\n ==== Checking popitem ====\u0026#34;) vd.popitem() print(vd) # check update print(\u0026#34;\\n ==== Checking update ====\u0026#34;) vd.update({\u0026#34;d\u0026#34;: \u0026#34;dog\u0026#34;}) print(vd) # check clear print(\u0026#34;\\n ==== Checking clear ====\u0026#34;) vd.clear() print(vd) # check setdefault print(f\u0026#34;\\n ==== Checking setdefault ====\u0026#34;) x = vd.setdefault(\u0026#34;a\u0026#34;, \u0026#34;pepsi\u0026#34;) print(x) print(vd) ==== Checking __setitem__ ==== Method: __setitem__, Key: a VerboseDict({\u0026#39;a\u0026#39;: \u0026#39;orange\u0026#39;, \u0026#39;b\u0026#39;: \u0026#39;ball\u0026#39;, \u0026#39;c\u0026#39;: \u0026#39;cat\u0026#39;}) ==== Checking popitem ==== Method: __iter__ Method: __getitem__, Key: a Method: __delitem__, Key: a VerboseDict({\u0026#39;b\u0026#39;: \u0026#39;ball\u0026#39;, \u0026#39;c\u0026#39;: \u0026#39;cat\u0026#39;}) ==== Checking update ==== Method: __setitem__, Key: d VerboseDict({\u0026#39;b\u0026#39;: \u0026#39;ball\u0026#39;, \u0026#39;c\u0026#39;: \u0026#39;cat\u0026#39;, \u0026#39;d\u0026#39;: \u0026#39;dog\u0026#39;}) ==== Checking clear ==== Method: __iter__ Method: __getitem__, Key: b Method: __delitem__, Key: b Method: __iter__ Method: __getitem__, Key: c Method: __delitem__, Key: c Method: __iter__ Method: __getitem__, Key: d Method: __delitem__, Key: d Method: __iter__ VerboseDict({}) ==== Checking setdefault ==== Method: __getitem__, Key: a Method: __setitem__, Key: a pepsi VerboseDict({\u0026#39;a\u0026#39;: \u0026#39;pepsi\u0026#39;}) Going ballistic with custom data structures This section discusses two advanced data structures that I mentioned at the beginning of the post.\nBitSet : Mutable set-like data structure that doesn\u0026rsquo;t perform hashing. SQLAlchemyDict: Mutable dict-like data structure that can store key-value pairs in any SQLAlchemy supported relational database. BitSet This mutable set-like data structure doesn\u0026rsquo;t perform hashing to store data. It can store integers in a fixed range. While storing integers, BitSet objects use less memory compared to built-in sets.\nHowever, since no hashing happens, it\u0026rsquo;s slower to perform addition and retrieval compared to built-in sets. The following code snippet was taken directly from Raymond Hettinger\u0026rsquo;s 2019 PyCon Russia talk on advanced data structures.\nfrom collections.abc import MutableSet class BitSet(MutableSet): \u0026#34;Ordered set with compact storage for integers in a fixed range\u0026#34; def __init__(self, limit, iterable=()): self.limit = limit num_bytes = (limit + 7) // 8 self.data = bytearray(num_bytes) self |= iterable def _get_location(self, elem): if elem \u0026lt; 0 or elem \u0026gt;= self.limit: raise ValueError( f\u0026#34;{elem!r} must be in range 0 \u0026lt;= elem \u0026lt; {self.limit}\u0026#34; ) return divmod(elem, 8) def __contains__(self, elem): bytenum, bitnum = self._get_location(elem) return bool((self.data[bytenum] \u0026gt;\u0026gt; bitnum) \u0026amp; 1) def add(self, elem): bytenum, bitnum = self._get_location(elem) self.data[bytenum] |= 1 \u0026lt;\u0026lt; bitnum def discard(self, elem): bytenum, bitnum = self._get_location(elem) self.data[bytenum] \u0026amp;= ~(1 \u0026lt;\u0026lt; bitnum) def __iter__(self): for elem in range(self.limit): if elem in self: yield elem def __len__(self): return sum(1 for elem in self) def __repr__(self): return ( f\u0026#34;{type(self).__name__}\u0026#34; f\u0026#34;(limit={self.limit}, iterable={list(self)})\u0026#34; ) def _from_iterable(self, iterable): return type(self)(self.limit, iterable) Let\u0026rsquo;s inspect the above data structure to understand exactly how much memory we can save. I\u0026rsquo;ll digress a little here. Normally, you\u0026rsquo;d use sys.getsizeof to measure the memory footprint of an object where the function reveals the size in bytes.\nBut there\u0026rsquo;s a problem. The function sys.getsizeof only reveals the size of the target object, excluding the objects the target objects might be referring to. To understand what I mean, consider the following situation:\nSuppose, you have a nested list that looks like this:\nlst = [[1], [2, 3], [[4, 5], 6, 7], 8, 9] When you apply sys.getsizeof function on the list, it shows 96 bytes. This means only the outermost list consumes 96 bytes of memory. Here, sys.getsizeof doesn\u0026rsquo;t include the size of the nested lists.\nThe same is true for other data structures. In case of nested dictionaries, sys.getsizeof will not include the size of nested data structures. I\u0026rsquo;ll only reveal the size of the outermost dictionary object. The following snippet will traverse through the reference tree of a nested object and reveal the true size of it.\nfrom collections.abc import Mapping, Container from sys import getsizeof def deep_getsizeof(o: object, ids: None = None) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Find the memory footprint of a Python object. This is a recursive function that drills down a Python object graph like a dictionary holding nested dictionaries with lists of lists and tuples and sets. The sys.getsizeof function does a shallow size of only. It counts each object inside a container as pointer only regardless of how big it really is. Params ------ o: object The object ids: None Later an iterable is assigned to store the object ids Returns -------- int Returns the size of object in bytes \u0026#34;\u0026#34;\u0026#34; if ids is None: ids = set() d = deep_getsizeof if id(o) in ids: return 0 r = getsizeof(o) ids.add(id(o)) if isinstance(o, str): return r if isinstance(o, Mapping): return r + sum(d(k, ids) + d(v, ids) for k, v in o.iteritems()) if isinstance(o, Container): return r + sum(d(x, ids) for x in o) return r Let\u0026rsquo;s use the deep_getsizeof to inspect the size differences between built-in set and BitSet objects.\nbs = BitSet(limit=5, iterable=[0, 4]) s = {0, 4} print(f\u0026#34;Normal Set object: {s}\u0026#34;) print(f\u0026#34;BitSet object: {bs}\u0026#34;) print(f\u0026#34;Size of a normal Set object: {deep_getsizeof(s)} bytes\u0026#34;) print(f\u0026#34;Size of a BitSet object: {deep_getsizeof(bs)} bytes\u0026#34;) Normal Set object: {0, 4} BitSet object: BitSet(limit=5, iterable=[0, 4]) Size of a normal Set object: 268 bytes Size of a BitSet object: 100 bytes The output of the print statements reveal that the BitSet object uses less than half the memory compared to its built-in counterpart!\nSQLAlchemyDict Here goes the second type of custom data structure that I mentioned in the introduction. It\u0026rsquo;s also a mutable dict-like structure that can automatically store key-value pairs to any SQLAlchemy supported relational database when initialized.\nI was inspired to write this one from the same Raymond Hettinger talk that I mentioned before. For demonstration purposes, I\u0026rsquo;ve chosen SQLite database to store the key value pairs.\nThis structure gives you immense power since you can abstract away the entire process of database communication inside the custom object. You\u0026rsquo;ll perform get-set-delete operations on the object just like you\u0026rsquo;d do so with built-in dictionary objects and the custom object will take care of storing and updating the data to the target database.\nBefore running the code snippet below, you\u0026rsquo;ll need to install SQLAlchemy as an external dependency.\n# sqla_dict.py from collections.abc import MutableMapping from contextlib import contextmanager from operator import itemgetter from sqlalchemy import create_engine, text from sqlalchemy.exc import OperationalError from sqlalchemy.orm import sessionmaker def create_transaction_session(dburl): # an engine, which the Session will use for connection resources some_engine = create_engine(dburl) # create a configured \u0026#34;Session\u0026#34; class Session = sessionmaker(bind=some_engine) @contextmanager def session_scope(): \u0026#34;\u0026#34;\u0026#34;Provide a transactional scope around a series of operations.\u0026#34;\u0026#34;\u0026#34; session = Session() try: yield session session.commit() except OperationalError: pass except Exception: session.rollback() raise finally: session.close() return session_scope session_scope = create_transaction_session(\u0026#34;sqlite:///foo.db\u0026#34;) class SQLAlechemyDict(MutableMapping): def __init__(self, dbname, session_scope, items=None, **kwargs): self.dbname = dbname self.session_scope = session_scope if items is None: items = [] with self.session_scope() as session: session.execute(\u0026#34;CREATE TABLE Dict (key text, value text)\u0026#34;) session.execute(\u0026#34;CREATE UNIQUE INDEX KIndx ON Dict (key)\u0026#34;) self.update(items, **kwargs) def __setitem__(self, key, value): if key in self: del self[key] with self.session_scope() as session: session.execute( text(\u0026#34;INSERT INTO Dict VALUES (:key, :value)\u0026#34;), {\u0026#34;key\u0026#34;: key, \u0026#34;value\u0026#34;: value}, ) def __getitem__(self, key): with self.session_scope() as session: r = session.execute( text(\u0026#34;SELECT value FROM Dict WHERE key=:key\u0026#34;), {\u0026#34;key\u0026#34;: key}, ) row = r.fetchone() if row is None: raise KeyError(key) return row[0] def __delitem__(self, key): if key not in self: raise KeyError(key) with self.session_scope() as session: session.execute( text(\u0026#34;DELETE FROM Dict WHERE key=:key\u0026#34;), {\u0026#34;key\u0026#34;: key} ) def __len__(self): with self.session_scope() as session: r = session.execute(\u0026#34;SELECT COUNT(*) FROM Dict\u0026#34;) return next(r)[0] def __iter__(self): with self.session_scope() as session: r = session.execute(\u0026#34;SELECT key FROM Dict\u0026#34;) return map(itemgetter(0), r.fetchall()) def __repr__(self): return ( f\u0026#34;{type(self).__name__}\u0026#34; f\u0026#34;(dbname={self.dbname!r}, items={list(self.items())})\u0026#34; ) def vacuum(self): with self.session_scope() as session: session.execute(\u0026#34;VACUUM;\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: # test the struct sqladict = SQLAlechemyDict( dbname=\u0026#34;foo.db\u0026#34;, session_scope=session_scope, items={\u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34;}, ) print(sqladict) sqladict[\u0026#34;key\u0026#34;] = \u0026#34;val\u0026#34; for key in sqladict: print(key) # \u0026gt;\u0026gt;\u0026gt; SQLAlechemyDict(dbname=\u0026#39;foo.db\u0026#39;, items=[...]) # \u0026gt;\u0026gt;\u0026gt; hello # \u0026gt;\u0026gt;\u0026gt; key SQLAlechemyDict(dbname=\u0026#39;foo.db\u0026#39;, items=[(\u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39;), (\u0026#39;key\u0026#39;, \u0026#39;val\u0026#39;)]) hello key Running the above code snippet will create a SQLite database named foo.db in your current working directory. You can inspect the database with any database viewer and find your key-value pairs there. Everything else is the same as a built-in dictionary object.\nFurther reading Implementing an interface in Python - Real Python What is a mixin, and why are they useful? - Stackoverflow Mixins for fun and profit - Dan Hillard ","permalink":"https://rednafi.com/python/mixins/","summary":"\u003cp\u003eImagine a custom \u003cem\u003eset-like\u003c/em\u003e data structure that doesn\u0026rsquo;t perform hashing and trades\nperformance for tighter memory footprint. Or imagine a \u003cem\u003edict-like\u003c/em\u003e data structure that\nautomatically stores data in a PostgreSQL or Redis database the moment you initialize it;\nalso it lets you \u003cem\u003eget-set-delete\u003c/em\u003e key-value pairs using the usual\n\u003cem\u003eretrieval-assignment-deletion\u003c/em\u003e syntax associated with built-in dictionaries. Custom data\nstructures can give you the power of choice and writing them will make you understand how\nthe built-in data structures in Python are constructed.\u003c/p\u003e","title":"Interfaces, mixins and building powerful custom data structures in Python"},{"content":"Updated on 2023-09-11: Fix broken URLs.\nIn Python, metaclass is one of the few tools that enables you to inject metaprogramming capabilities into your code. The term metaprogramming refers to the potential for a program to manipulate itself in a self referential manner. However, messing with metaclasses is often considered an arcane art that\u0026rsquo;s beyond the grasp of the plebeians. Heck, even Tim Peters advises you to tread carefully while dealing with these.\nMetaclasses are deeper magic than 99% of users should ever worry about. If you wonder whether you need them, you don\u0026rsquo;t (the people who actually need them know with certainty that they need them, and don\u0026rsquo;t need an explanation about why).\nMetaclasses are an esoteric OOP concept, lurking behind virtually all Python code. Every Python class that you create is attached to a default metaclass and Python cleverly abstracts away all the meta-magics. So, you\u0026rsquo;re indirectly using them all the time whether you are aware of it or not. For the most part, you don\u0026rsquo;t need to be aware of it. Most Python programmers rarely, if ever, have to think about metaclasses. This makes metaclasses exciting for me and I want to explore them in this post to formulate my own judgement.\nMetaclasses A metaclass is a class whose instances are classes. Like an \u0026ldquo;ordinary\u0026rdquo; class defines the behavior of the instances of the class, a metaclass defines the behavior of classes and their instances.\nMetaclasses aren\u0026rsquo;t supported by every object oriented programming language. Those programming language, which support metaclasses, considerably vary in way they implement them. Python provides you a way to get under the hood and define custom metaclasses.\nUnderstanding type and class In Python, everything is an object. Classes are objects as well. As a result, all classes must have corresponding types. You deal with built in types like int, float, list etc all the time. Consider this example:\na = 5 print(type(a)) print(type(int)) \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; \u0026lt;class \u0026#39;type\u0026#39;\u0026gt; In the above example, variable a is an instance of the built in class int. Type of a is int and the type of int is type. User defined classes also show similar behavior. For example:\nclass Foo: pass a = Foo() print(type(a)) print(type(Foo)) \u0026lt;class \u0026#39;__main__.Foo\u0026#39;\u0026gt; \u0026lt;class \u0026#39;type\u0026#39;\u0026gt; Here, I\u0026rsquo;ve defined another class named Foo and created an instance a of the class. Applying type on instance a reveals its type as __main__.Foo and applying type on class Foo reveals the type as type. So here, we can use the term class and type interchangeably. This brings up the question:\nWhat on earth this type (function? class?) thing actually is and what is the type of type?\nLet\u0026rsquo;s apply type on type:\nprint(type(type)) \u0026lt;class \u0026#39;type\u0026#39;\u0026gt; Weird. The type of any class (not instance of a class) in Python is type and the type of type is also type. By now, you\u0026rsquo;ve probably guessed that type is a very special class in Python that can reveal the type of itself and of any other class or object. In fact, type is a metaclass and all the classes in Python are instances of it. You can inspect that easily:\nclass Foo: pass for klass in [int, float, list, dict, Foo, type]: print(type(klass)) print(isinstance(Foo, type)) print(isinstance(type, type)) \u0026lt;class \u0026#39;type\u0026#39;\u0026gt; \u0026lt;class \u0026#39;type\u0026#39;\u0026gt; \u0026lt;class \u0026#39;type\u0026#39;\u0026gt; \u0026lt;class \u0026#39;type\u0026#39;\u0026gt; \u0026lt;class \u0026#39;type\u0026#39;\u0026gt; \u0026lt;class \u0026#39;type\u0026#39;\u0026gt; True True The the last line of the above code snippet demonstrates that type is also an instance of metaclass type. Normally, you can\u0026rsquo;t write self referential classes like that in pure Python. However, you can circumvent this limitation by subclassing from type. This enables you to write custom metaclasses that you can use to dictate and mutate the way classes are created and instantiated. From now on, I\u0026rsquo;ll be referring to the instance class of a metaclass as target class. Let\u0026rsquo;s create a custom metaclass that just prints the name of the target class while creating it:\nclass PrintMeta(type): def __new__(metacls, cls, bases, classdict): \u0026#34;\u0026#34;\u0026#34;__new__ gets executed before the target is created. Parameters ---------- metacls : PrintMeta Instance of the the PrintMeta class itself cls : str Name of the class being defined (Point in this example) bases : tuple Base classes of the constructed class, empty tuple in this case classdict : dict Dict containing methods and fields defined in the class Returns ------- instance class of this metaclass \u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Name of this class is: {cls}\u0026#34;) return super().__new__(metacls, cls, bases, classdict) class A(metaclass=PrintMeta): pass Name of this class is A Despite the fact that we haven\u0026rsquo;t called class A or created an instance of it, the __new__ method of metaclass PrintMeta was executed and printed the name of the target class. In the return statement of __new__ method, super() was used to call the __new__ method of the base class (type) of the metaclass PrintMeta.\nSpecial methods used by metaclasses Type type, as the default metaclass in Python, defines a few special methods that new metaclasses can override to implement unique code generation behavior. Here is a brief overview of these \u0026ldquo;magic\u0026rdquo; methods that exist on a metaclass:\n__new__: This method is called on the Metaclass before an instance of a class based on the metaclass is created __init__: This method is called to set up values after the instance/object is created __prepare__: Defines the class namespace in a mapping that stores the attributes __call__: This method is called when the constructor of the new class is to be used to create an object These are the methods to override in your custom metaclass to give your classes behaviors different from that of type. The following example shows the default behaviors of these special methods and their execution order.\nSome people immediately think of __init__, and I\u0026rsquo;ve occasionally called it \u0026ldquo;the constructor\u0026rdquo; myself; but in actuality, as its name indicates, it\u0026rsquo;s an initializer and by the time it\u0026rsquo;s invoked, the object has already been created, seeing as it\u0026rsquo;s passed in as self. The real constructor is a far less famous function: __new__. The reason you might never hear about it or use it — is that allocation doesn\u0026rsquo;t mean that much in Python, which manages memory for you. So if you do override __new__, it\u0026rsquo;d be just like your __init__ — except you\u0026rsquo;ll have to call into Python to actually create the object, and then return that object afterward.\nclass ExampleMeta(type): \u0026#34;\u0026#34;\u0026#34;Simple metaclass showing the execution flow of the special methods.\u0026#34;\u0026#34;\u0026#34; @classmethod def __prepare__(metacls, cls, bases): \u0026#34;\u0026#34;\u0026#34;Defines the class namespace in a mapping that stores the attributes Parameters ---------- cls : str Name of the class being defined (Point in this example) bases : tuple Base classes for constructed class, empty tuple in this case \u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Calling __prepare__ method of {super()}!\u0026#34;) return super().__prepare__(cls, bases) def __new__(metacls, cls, bases, classdict): \u0026#34;\u0026#34;\u0026#34;__new__ is a classmethod, even without @classmethod decorator Parameters ---------- cls : str Name of the class being defined (Point in this example) bases : tuple Base classes for constructed class, empty tuple in this case classdict : dict Dict containing methods and fields defined in the class \u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Calling __new__ method of {super()}!\u0026#34;) return super().__new__(metacls, cls, bases, classdict) def __init__(self, cls, bases, classdict): \u0026#34;\u0026#34;\u0026#34;This method is called to set up values after the instance/object is created.\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Calling __init__ method of {super()}!\u0026#34;) super().__init__(cls, bases, classdict) def __call__(self, *args, **kwargs): \u0026#34;\u0026#34;\u0026#34;This method is called when the constructor of the new class is to be used to create an object Parameters ---------- args : tuple Position only arguments of the new class kwargs : dict Keyward only arguments of the new class \u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Calling __call__ method of {super()}!\u0026#34;) print(f\u0026#34;Printing {self} args:\u0026#34;, args) print(f\u0026#34;Printing {self} kwargs\u0026#34;, kwargs) return super().__call__(*args, **kwargs) class A(metaclass=ExampleMeta): def __init__(self, x, y): self.x = x self.y = y print(f\u0026#34;Calling __init__ method of {self}\u0026#34;) a = A(1, 3) Calling __prepare__ method of \u0026lt;super: \u0026lt;class \u0026#39;ExampleMeta\u0026#39;\u0026gt;, ...\u0026gt;! Calling __new__ method of \u0026lt;super: \u0026lt;class \u0026#39;ExampleMeta\u0026#39;\u0026gt;, ...\u0026gt;! Calling __init__ method of \u0026lt;super: \u0026lt;class \u0026#39;ExampleMeta\u0026#39;\u0026gt;, ...\u0026gt;! Calling __call__ method of \u0026lt;super: \u0026lt;class \u0026#39;ExampleMeta\u0026#39;\u0026gt;, ...\u0026gt;! Printing \u0026lt;class \u0026#39;__main__.A\u0026#39;\u0026gt; args: (1, 3) Printing \u0026lt;class \u0026#39;__main__.A\u0026#39;\u0026gt; kwargs {} Calling __init__ method of \u0026lt;__main__.A object at 0x7febe710a130\u0026gt; Pay attention to the execution order of the special methods of the custom metaclass ExampleMeta. The __prepare__ method is called first and is followed by __new__, __init__ and __call__ respectively. Only after that the first method __init__ of the target class A is called. This is important since it\u0026rsquo;ll help you to decide how you\u0026rsquo;ll mutate and change the behavior of your target class.\nMetaclass conflicts Note that the metaclass argument is singular – you can\u0026rsquo;t attach more than one metaclass to a class. However, through multiple inheritance you can accidentally end up with more than one metaclass, and this produces a conflict which must be resolved.\nclass FirstMeta(type): pass class SecondMeta(type): pass class First(metaclass=FirstMeta): pass class Second(metaclass=SecondMeta): pass class Third(First, Second): pass third = Third() --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-340-6afe6bc8f8bc\u0026gt; in \u0026lt;module\u0026gt; 11 pass 12 ---\u0026gt; 13 class Third(First, Second): 14 pass 15 TypeError: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases Class First and Second are attached to different metaclasses and class Third inherits from both of them. Since metaclasses trickle down to subclasses, class Third is now effective attached to two metaclasses (FirstMeta and SecondMeta). This will raise TypeError. Attachment with only one metaclass is allowed here.\nExamples in the wild In this section, I\u0026rsquo;ll go through a few real life examples where metaclasses can provide viable solutions to several tricky problems that you might encounter during software development. The solutions might appear over-engineered in some cases and almost all of them can be solved without using metaclasses. However, the purpose is to peek into the inner wirings of metaclasses and see how they can offer alternative solutions.\nSimple logging with metaclasses The goal here is to log a few basic information about a class without directly adding any logging statements to it. Instead, you can whip up a custom metaclass to perform some metaprogramming and add those statements to the target class without mutating it explicitly.\nimport logging logging.basicConfig(level=logging.INFO) class LittleMeta(type): def __new__(metacls, cls, bases, classdict): logging.info(f\u0026#34;classname: {cls}\u0026#34;) logging.info(f\u0026#34;baseclasses: {bases}\u0026#34;) logging.info(f\u0026#34;classdict: {classdict}\u0026#34;) return super().__new__(metacls, cls, bases, classdict) class Point(metaclass=LittleMeta): def __init__(self, x: float, y: float) -\u0026gt; None: self.x = x self.y = y def __repr__(self) -\u0026gt; str: return f\u0026#34;Point({self.x}, {self.y})\u0026#34; p = Point(5, 10) print(p) INFO:root:classname: Point INFO:root:baseclasses: () INFO:root:attrs: {\u0026#39;__module__\u0026#39;: \u0026#39;__main__\u0026#39;, ...} Point(5, 10) In the above example, I\u0026rsquo;ve created a metaclass called LittleMeta and added the necessary logging statements to record the information about the target class. Since the logging statements are residing in the __new__ method of the metaclass, these informations will be logged before the creation of the target class. In the target class Point, LittleMeta replaces the default type metaclass and produces the desired result by mutating the class.\nReturning class attributes in a custom list In this case, I want to dynamically attach a new attribute to the target class called __attrs_ordered__. Accessing this attribute from the target class (or instance) will give out an alphabetically sorted list of the attribute names. Here, the __prepare__ method inside the metaclass AttrsListMeta returns an OrderDict instead of a simple dict — so all attributes gathered before the __new__ method call will be ordered. Just like the previous example, here, the __new__ method inside the metaclass implements the logic required to get the sorted list of the attribute names.\nfrom collections import OrderedDict class AttrsListMeta(type): @classmethod def __prepare__(metacls, cls, bases): return OrderedDict() def __new__(metacls, cls, bases, classdict, **kwargs): attrs_names = [k for k in classdict.keys()] attrs_names_ordered = sorted(attrs_names) classdict[\u0026#34;__attrs_ordered__\u0026#34;] = attrs_names_ordered return super().__new__(metacls, cls, bases, classdict, **kwargs) class A(metaclass=AttrsListMeta): def __init__(self, x, y): self.y = y self.x = x a = A(1, 2) print(a.__attrs_ordered__) [\u0026#39;__init__\u0026#39;, \u0026#39;__module__\u0026#39;, \u0026#39;__qualname__\u0026#39;] You can access the __attrs_ordered__ attribute from both class A and an instance of class A. Try removing the sorted() function inside the __new__ method of the metaclass and see what happens!\nCreating a singleton class In OOP term, a singleton class is a class that can have only one object (an instance of the class) at a time.\nAfter the first time, if you try to instantiate a Singleton class, it will basically return the same instance of the class that was created before. So any modifications done to this apparently new instance will mutate the original instance since they\u0026rsquo;re basically the same instance.\nclass Singleton(type): _instance = {} def __call__(cls, *args, **kwargs): if cls not in cls._instance: cls._instance[cls] = super().__call__(*args, **kwargs) return cls._instance[cls] class A(metaclass=Singleton): pass a = A() b = A() a is b True In the above example, at first, I\u0026rsquo;ve created a singleton class A by attaching the Singleton metaclass to it. Secondly, I\u0026rsquo;ve instantiated class A and assigned the instance of the class to a variable a. Thirdly, I\u0026rsquo;ve instantiated the class again and assigned variable a b to this seemingly new instance. Checking the identity of the two variables a and b reveals that both of them are actually the same object.\nImplementing a class that can\u0026rsquo;t be subclassed Suppose you want to create a base class where the users of your class won\u0026rsquo;t be able to create any subclasses from the base class. In that case, you can write a metaclass and attach that your base class. The base class will raise RuntimeError if someone tries to create a subclass from it.\nclass TerminateMeta(type): def __new__(metacls, cls, bases, classdict): type_list = [type(base) for base in bases] for typ in type_list: if typ is metacls: raise RuntimeError( f\u0026#34;Subclassing a class that has \u0026#34; + f\u0026#34;{metacls.__name__} metaclass is prohibited\u0026#34; ) return super().__new__(metacls, cls, bases, classdict) class A(metaclass=TerminateMeta): pass class B(A): pass a = A() --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) \u0026lt;ipython-input-438-ccba42f1f95b\u0026gt; in \u0026lt;module\u0026gt; 20 21 ---\u0026gt; 22 class B(A): 23 pass 24 ... RuntimeError: Subclassing a class that has TerminateMeta metaclass is prohibited Disallowing multiple inheritance Multiple inheritance can be fragile and error prone. So, if you don\u0026rsquo;t want to allow the users to use a base class with any other base classes to form multiple inheritance, you can do so by attaching a metaclass to that target base class.\nclass NoMultiMeta(type): def __new__(metacls, cls, bases, classdict): if len(bases) \u0026gt; 1: raise TypeError(\u0026#34;Inherited multiple base classes!\u0026#34;) return super().__new__(metacls, cls, bases, classdict) class Base(metaclass=NoMultiMeta): pass # no error is raised class A(Base): pass # no error is raised class B(Base): pass # This will raise an error! class C(A, B): pass --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-404-36c323db1ea0\u0026gt; in \u0026lt;module\u0026gt; 18 19 # This will raise an error! ---\u0026gt; 20 class C(A, B): 21 pass ... TypeError: Inherited multiple base classes! Timing classes with metaclasses Suppose you want to measure the execution time of different methods of a class. One way of doing that is to define a timer decorator and decorating all the methods to measure and show the execution time. However, by using a metaclass, you can avoid decorating the methods in the class individually and the metaclass will dynamically apply the timer decorator to all of the methods of your target class. This can reduce code repetition and improve code readability.\nimport inspect import time from functools import wraps def timefunc(func): @wraps(func) def wrapper(*args, **kwargs): start_time = time.time() ret = func(*args, **kwargs) end_time = time.time() run_time = end_time - start_time print(f\u0026#34;Executing {func.__qualname__} took {run_time} seconds.\u0026#34;) return ret return wrapper class TimerMeta(type): def __new__(metacls, cls, bases, classdict): new_cls = super().__new__(metacls, cls, bases, classdict) # key is attr name and val is attr value in attribute dict for key, val in classdict.items(): if inspect.isfunction(val) or inspect.ismethoddescriptor(val): setattr(new_cls, key, timefunc(val)) return new_cls class Shouter(metaclass=TimerMeta): def __init__(self): pass def intro(self): time.sleep(1) print(\u0026#34;I shout!\u0026#34;) s = Shouter() s.intro() Executing Shouter.__init__ took 9.5367431640625e-07 seconds. I shout! Executing Shouter.intro took 1.0011515617370605 seconds. Registering plugins with metaclasses Suppose a specific single class represents a plugin in your code. You can write a metaclass to keep track of all of the plugins so than you don\u0026rsquo;t have to count them manually.\nregistry = {} class RegisterMeta(type): def __new__(metacls, cls, bases, classdict): new_cls = super().__new__(metacls, cls, bases, classdict) registry[new_cls.__name__] = new_cls return new_cls class A(metaclass=RegisterMeta): pass class B(A): pass class C(A): pass class D(B): pass b = B() print(registry) {\u0026#39;A\u0026#39;: __main__.A, \u0026#39;B\u0026#39;: __main__.B, \u0026#39;C\u0026#39;: __main__.C, \u0026#39;D\u0026#39;: __main__.D} Debugging methods with metaclasses Debugging a class often involves inspecting the individual methods and adding extra debugging logic to those. However, this can get tedious if you\u0026rsquo;ve do this over an over again. Instead, you can write an inspection decorator and use a metaclass to dynamically apply the decorator to all of the methods of your target class. Later on, you can simply detach the metaclass once you\u0026rsquo;re done with debugging and don\u0026rsquo;t want the extra logic in your target class.\nimport inspect from functools import wraps def debug(func): \u0026#34;\u0026#34;\u0026#34;Decorator for debugging passed function.\u0026#34;\u0026#34;\u0026#34; @wraps(func) def wrapper(*args, **kwargs): print(\u0026#34;Full name of this method:\u0026#34;, func.__qualname__) return func(*args, **kwargs) return wrapper class DebugMeta(type): def __new__(metacls, cls, bases, classdict): new_cls = super().__new__(metacls, cls, bases, classdict) # key is attr name and val is attr value in the attrs dict for key, val in classdict.items(): if inspect.isfunction(val) or inspect.ismethoddescriptor(val): setattr(new_cls, key, debug(val)) return new_cls class Base(metaclass=DebugMeta): pass class Calc(Base): def add(self, x, y): return x + y class CalcAdv(Calc): def mul(self, x, y): return x * y mycal = CalcAdv() print(mycal.mul(2, 3)) Full name of this method: CalcAdv.mul 6 Exception handling with metaclasses Sometimes you need to handle exceptions in multiple methods of a class in a generic manner. That means all the methods of the class have the same exception handling, logging logic etc. Metaclasses can help you avoid adding repetitive exception handling and logging logics to your methods.\nimport inspect from functools import wraps def exc_handler(func): \u0026#34;\u0026#34;\u0026#34;Decorator for custom exception handling.\u0026#34;\u0026#34;\u0026#34; @wraps(func) def wrapper(*args, **kwargs): try: ret = func(*args, **kwargs) except Exception: print(\u0026#34;Exception Occured!\u0026#34;) print(f\u0026#34;Method name: {func.__qualname__}\u0026#34;) print(f\u0026#34;Args: {args}, Kwargs: {kwargs}\u0026#34;) raise return ret return wrapper class ExceptionMeta(type): def __new__(metacls, cls, bases, classdict): new_cls = super().__new__(metacls, cls, bases, classdict) # key is attr name and val is attr value in attribute dict for key, val in classdict.items(): if inspect.isfunction(val) or inspect.ismethoddescriptor(val): setattr(new_cls, key, exc_handler(val)) return new_cls class Base(metaclass=ExceptionMeta): pass class Calc(Base): def add(self, x, y): return x + y class CalcAdv(Calc): def div(self, x, y): return x / y mycal = CalcAdv() print(mycal.div(2, 0)) Exception Occured! Method name: CalcAdv.div Args: (\u0026lt;__main__.CalcAdv object at 0x7febe692d1c0\u0026gt;, 2, 0), Kwargs: {} --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) \u0026lt;ipython-input-467-accaebe919a8\u0026gt; in \u0026lt;module\u0026gt; 43 44 mycal = CalcAdv() ---\u0026gt; 45 print(mycal.div(2, 0)) ... ZeroDivisionError: division by zero Abstract base classes An abstract class can be regarded as a blueprint for other classes. It allows you to provide a set of methods that must be implemented within any child classes built from the abstract class. Abstract classes usually house multiple abstract methods. An abstract method is a method that has a declaration but does not have an implementation.\nWhen you want to provide a common interface for different implementations of a component, abstract classes are the way to go. You can\u0026rsquo;t directly initialize or use an abstract class. Rather, you\u0026rsquo;ve to subclass the abstract base class and provide concrete implementations of all the abstract methods. Python has a dedicated abc module to help you create abstract classes. Let\u0026rsquo;s see how you can define a simple abstract class that provides four abstract methods:\nfrom abc import ABC, abstractmethod class ICalc(ABC): \u0026#34;\u0026#34;\u0026#34;Interface for a simple calculator.\u0026#34;\u0026#34;\u0026#34; @abstractmethod def add(self, a, b): pass @abstractmethod def sub(self, a, b): pass @abstractmethod def mul(self, a, b): pass @abstractmethod def div(self, a, b): pass intrf = ICalc() --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-21-7be58e3a2a92\u0026gt; in \u0026lt;module\u0026gt; 21 22 ---\u0026gt; 23 intrf = ICalc() TypeError: Can\u0026#39;t instantiate abstract class ICalc with abstract methods add, div, mul, sub Although it seems like interface ICalc is simply inheriting from the class ABC, in fact, ABC is attaching a metaclass ABCMeta to ICalc. This metaclass transforms the ICalc class into an abstract class. You can see that the class ICalc gives TypeError when you take an attempt to initialize it. The only way to use this interface is via creating subclasses from ICalc base class and implementing all the abstract methods there. The snippet below shows that:\nclass Calc(ICalc): \u0026#34;\u0026#34;\u0026#34;Concrete class that uses Icalc interface.\u0026#34;\u0026#34;\u0026#34; def add(self, a, b): return a + b def sub(self, a, b): return a - b def mul(self, a, b): return a * b def div(self, a, b): return a / b calc = Calc() print(calc.add(1, 2)) print(calc.sub(2, 3)) print(calc.mul(3, 4)) print(calc.div(4, 5)) 3 -1 12 0.8 Metaclasses \u0026amp; dataclasses Data classes were introduced to python in version 3.7. Basically they can be regarded as code generators that reduce the amount of boilerplate you need to write while generating generic classes. Dataclasses automatically create __init__, __repr__, __eq__, __gt__, __lt__ etc methods without you having to add them explicitly. This can be very handy when you need to create custom collections for your data. You can create dataclasses in the following manner:\nCreating multiple dataclasses from dataclasses import dataclass from datetime import datetime @dataclass(unsafe_hash=True, frozen=True) class Event: created_at: datetime @dataclass(unsafe_hash=True, frozen=True) class InvoiceIssued(Event): invoice_uuid: int customer_uuid: int total_amount: float due_date: datetime @dataclass(unsafe_hash=True, frozen=True) class InvoiceOverdue(Event): invoice_uuid: int customer_uuid: int inv = InvoiceIssued( **{ \u0026#34;invoice_uuid\u0026#34;: 22, \u0026#34;customer_uuid\u0026#34;: 34, \u0026#34;total_amount\u0026#34;: 100.0, \u0026#34;due_date\u0026#34;: datetime(2020, 6, 19), \u0026#34;created_at\u0026#34;: datetime.now(), } ) print(inv) InvoiceIssued( created_at=datetime.datetime(2020, 6, 20, 1, 3, 24, 967633), invoice_uuid=22, customer_uuid=34, total_amount=100.0, due_date=datetime.datetime(2020, 6, 19, 0, 0) ) Avoiding dataclass decorator with metaclasses Now, one thing that I find cumbersome while creating multiple dataclasses is having to attach the @dataclasses.dataclass decorator to each of the dataclasses. Also, the decorator takes multiple arguments to customize the dataclass behavior and it can quickly get cumbersome when you\u0026rsquo;ve to create multiple dataclasses with custom behavior. Moreover, this goes against the DRY (Don\u0026rsquo;t Repeat Yourself) principle in software engineering.\nTo avoid this, you can write a metaclass that will automatically apply the customized dataclass decorator to all of the target classes implicitly. All you have to do is to attach the metaclass to a base dataclass and inherit from it in the later dataclasses that need to be created.\nfrom dataclasses import dataclass from datetime import datetime class EventMeta(type): def __new__(metacls, cls, bases, classdict): \u0026#34;\u0026#34;\u0026#34;__new__ is a classmethod, even without @classmethod decorator Parameters ---------- cls : str Name of the class being defined (Event in this example) bases : tuple Base classes of the constructed class, empty tuple in this case attrs : dict Dict containing methods and fields defined in the class \u0026#34;\u0026#34;\u0026#34; new_cls = super().__new__(metacls, cls, bases, classdict) return dataclass(unsafe_hash=True, frozen=True)(new_cls) class Event(metaclass=EventMeta): created_at: datetime class InvoiceIssued(Event): invoice_uuid: int customer_uuid: int total_amount: float due_date: datetime class InvoiceOverdue(Event): invoice_uuid: int customer_uuid: int inv = InvoiceIssued( **{ \u0026#34;invoice_uuid\u0026#34;: 22, \u0026#34;customer_uuid\u0026#34;: 34, \u0026#34;total_amount\u0026#34;: 100.0, \u0026#34;due_date\u0026#34;: datetime(2020, 6, 19), \u0026#34;created_at\u0026#34;: datetime.now(), } ) print(inv) InvoiceIssued( created_at=datetime.datetime(2020, 6, 24, 12, 57, 22, 543328), invoice_uuid=22, customer_uuid=34, total_amount=100.0, due_date=datetime.datetime(2020, 6, 19, 0, 0) ) Should you use it? Almost all of the problems you\u0026rsquo;ve encountered above can be solved without using metaclasses. Decorators can also be exclusively used to perform metaprogramming in a more manageable and subjectively cleaner way. One case where you absolutely have to use metaclasses is to avoid applying decorators to multiple classes or methods repetitively.\nAlso, metaclasses can easily veer into the realm of being a \u0026ldquo;solution in search of a problem\u0026rdquo;. If the problem at hand can be solved in a simpler way, it probably should be. However, I still think that you should at least try to understand how metaclasses work to have a better grasp on how Python classes work in general and can recognize when a metaclass really is the appropriate tool to use.\nRemarks Wrapping your mind around metaclasses can be tricky. So, to avoid any unnecessary confusion, I\u0026rsquo;ve entirely evaded any discussion regarding the behavioral difference between old style classes and new style classes in Python. Also, I\u0026rsquo;ve intentionally excluded mentioning the differences between type in Python 2 and type in Python 3 entirely. Python 2.x has reached its EOL. Save yourself some trouble and switch to Python 3.x if you already haven\u0026rsquo;t done so.\nThis article assumes familiarity with decorators, dataclasses etc. If your knowledge on them is rusty, checkout these posts on decorators and dataclasses.\nFurther reading Understanding Python\u0026rsquo;s metaclasses What are metaclasses in Python - Stackoverflow Python metaclasses - Real Python Metaclasses - Python course EU When to use metaclasses in Python A primer on Python metaclasses ","permalink":"https://rednafi.com/python/metaclasses/","summary":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eUpdated on 2023-09-11\u003c/em\u003e\u003c/strong\u003e: \u003cem\u003eFix broken URLs.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eIn Python, metaclass is one of the few tools that enables you to inject metaprogramming\ncapabilities into your code. The term metaprogramming refers to the potential for a program\nto manipulate itself in a self referential manner. However, messing with metaclasses is\noften considered an arcane art that\u0026rsquo;s beyond the grasp of the plebeians. Heck, even \u003ca href=\"https://en.wikipedia.org/wiki/Tim_Peters_(software_engineer)\"\u003eTim\nPeters\u003c/a\u003e advises you to tread carefully while dealing with these.\u003c/p\u003e","title":"Deciphering Python's metaclasses"},{"content":"In Python, there\u0026rsquo;s a saying that \u0026ldquo;design patterns are anti-patterns\u0026rdquo;. Also, in the realm of dynamic languages, design patterns have the notoriety of injecting additional abstraction layers to the core logic and making the flow gratuitously obscure. Python\u0026rsquo;s dynamic nature and the treatment of functions as first-class objects often make Java-ish design patterns redundant.\nInstead of littering your code with seemingly over-engineered patterns, you can almost always take the advantage of Python\u0026rsquo;s first-class objects, duck-typing, monkey-patching etc to accomplish the task at hand. However, recently there is one design pattern that I find myself using over and over again to write more maintainable code and that is the Proxy pattern. So I thought I\u0026rsquo;d document it here for future reference.\nThe proxy pattern Before diving into the academic definition, let\u0026rsquo;s try to understand the Proxy pattern from an example.\nHave you ever used an access card to go through a door? There are multiple options to open that door i.e. it can be opened either using access card or by pressing a button that bypasses the security. The door\u0026rsquo;s main functionality is to open but there is a proxy added on top of it to add some functionality. Let me better explain it using the code example below:\n# src.py class Door: def open_method(self) -\u0026gt; None: pass class SecuredDoor: def __init__(self) -\u0026gt; None: self._klass = Door() def open_method(self) -\u0026gt; None: print(f\u0026#34;Adding security measure to the method of {self._klass}\u0026#34;) secured_door = SecuredDoor() secured_door.open_method() \u0026gt;\u0026gt;\u0026gt; Adding security measure to the method of \u0026lt;__main__.Door object ...\u0026gt; The above code snippet concretizes the example given before. Here, the Door class has a single method called open_method which denotes the action of opening on the Door object. This method gets extended in the SecuredDoor class and in this case, I\u0026rsquo;ve just added a print statement to the method of the latter class.\nNotice how the class Door was called from SecuredDoor via composition. In the case of proxy pattern, you can substitute primary object with the proxy object without any additional changes in the code. This conforms to the Liskov Substitution Principle. It states:\nObjects of a superclass shall be replaceable with objects of its subclasses without breaking the application. That requires the objects of your subclasses to behave in the same way as the objects of your superclass.\nThe Door object can be replaced by the SecuredDoor and the SecuredDoor class doesn\u0026rsquo;t introduce any new methods, it only extends the functionality of the open_method of the Door class.\nIn plain words:\nUsing the proxy pattern, a class represents the functionality of another class.\nWikipedia says:\nA proxy, in its most general form, is a class functioning as an interface to something else. A proxy is a wrapper or agent object that is being called by the client to access the real serving object behind the scenes. Use of the proxy can simply be forwarding to the real object, or can provide additional logic. In the proxy extra functionality can be provided, for example caching when operations on the real object are resource intensive, or checking preconditions before operations on the real object are invoked.\nPedagogically, the proxy pattern belongs to a family of patterns called the structural pattern.\nWhy use it? Loose coupling Proxy pattern lets you easily decouple your core logic from the added functionalities that might be needed on top of that. The modular nature of the code makes maintaining and extending the functionalities of your primary logic a lot quicker and easier.\nSuppose, you\u0026rsquo;re defining a division function that takes two integers as arguments and returns the result of the division between them. It also handles edge cases like ZeroDivisionError or TypeError and logs them properly.\n# src.py from __future__ import annotations import logging from typing import Union logging.basicConfig(level=logging.INFO) Number = Union[int, float] def division(a: Number, b: Number) -\u0026gt; float: try: result = a / b return result except ZeroDivisionError: logging.error(f\u0026#34;Argument b cannot be {b}\u0026#34;) except TypeError: logging.error(\u0026#34;Arguments must be integers/floats\u0026#34;) print(division(1.9, 2)) \u0026gt;\u0026gt;\u0026gt; 0.95 You can see this function is already doing three things at once which violates the Single Responsibility Principle. SRP says that a function or class should have only one reason to change. In this case, a change in any of the three responsibilities can force the function to change. Also this means, changing or extending the function can be difficult to keep track of.\nInstead, you can write two classes. The primary class Division will only implement the core logic while another class ProxyDivision will extend the functionality of Division by adding exception handlers and loggers.\n# src.py from __future__ import annotations import logging from typing import Union logging.basicConfig(level=logging.INFO) Number = Union[int, float] class Division: def div(self, a: Number, b: Number) -\u0026gt; Number: return a / b class ProxyDivision: def __init__(self) -\u0026gt; None: self._klass = Division() def div(self, a: Number, b: Number) -\u0026gt; Number: try: result = self._klass.div(a, b) return result except ZeroDivisionError: logging.error(f\u0026#34;Argument b cannot be {b}\u0026#34;) except TypeError: logging.error(\u0026#34;Arguments must be integers/floats\u0026#34;) klass = ProxyDivision() print(klass.div(2, 0)) \u0026gt;\u0026gt;\u0026gt; ERROR:root:Argument b cannot be 0 None In the example above, since both Division and ProxyDivision class implement the same interface, you can swap out the Division class with ProxyDivision and vice versa. The second class neither inherits directly from the first class nor it adds any new method to it. This means you can easily write another class to extend the functionalities of Division or DivisionProxy class without touching their internal logics directly.\nEnhanced testability Another great advantage of using the proxy pattern is enhanced testability. Since your core logic is loosely coupled with the extended functionalities, you can test them out separately. This makes the test more succinct and modular. It\u0026rsquo;s easy to demonstrate the benefits with our previously mentioned Division and ProxyDivision classes. Here, the logic of the primary class is easy to follow and since this class only holds the core logic, it\u0026rsquo;s crucial to write unit test for this before testing the added functionalities.\nTesting out the Division class is much cleaner than testing the previously defined division function that tries to do multiple things at once. Once you\u0026rsquo;re done testing the primary class, you can proceed with the additional functionalities. Usually, this decoupling of core logic from the cruft and the encapsulation of additional functionalities result in more reliable and rigorous unit tests.\nProxy pattern with interface In the real world, your class won\u0026rsquo;t look like the simple Division class having only a single method. Usually your primary class will have multiple methods and they will carry out multiple sophisticated tasks. By now, you probably have grasped the fact that the proxy classes need to implement all of the methods of the primary class. While writing a proxy class for a complicated primary class, the author of that class might forget to implement all the methods of the primary class. This will lead to a violation of the proxy pattern. Also, it can be hard to follow all the methods of the primary class if the class is large and complicated.\nHere, the solution is an interface that can signal the author of the proxy class about all the methods that need to be implemented. An interface is nothing but an abstract class that dictates all the methods a concrete class needs to implement. However, interfaces can\u0026rsquo;t be initialized independently. You\u0026rsquo;ll have to make a subclass of the interface and implement all the methods defined there. Your subclass will raise error if it fails to implement any of the methods of the interface. Let\u0026rsquo;s look at a minimal example of how you can write an interface using Python\u0026rsquo;s abc.ABC and abc.abstractmethod and achieve proxy pattern with that.\n# src.py from abc import ABC, abstractmethod class Interface(ABC): \u0026#34;\u0026#34;\u0026#34;Interfaces of Interface, Concrete \u0026amp; Proxy should be the same, because the client should be able to use Concrete or Proxy without any change in their internals. \u0026#34;\u0026#34;\u0026#34; @abstractmethod def job_a(self, user: str) -\u0026gt; None: pass @abstractmethod def job_b(self, user: str) -\u0026gt; None: pass class Concrete(Interface): \u0026#34;\u0026#34;\u0026#34;This is the main job doer. External services like payment gateways can be a good example. \u0026#34;\u0026#34;\u0026#34; def job_a(self, user: str) -\u0026gt; None: print(f\u0026#34;I am doing the job_a for {user}\u0026#34;) def job_b(self, user: str) -\u0026gt; None: print(f\u0026#34;I am doing the job_b for {user}\u0026#34;) class Proxy(Interface): def __init__(self) -\u0026gt; None: self._concrete = Concrete() def job_a(self, user: str) -\u0026gt; None: print(f\u0026#34;I\u0026#39;m extending job_a for user {user}\u0026#34;) def job_b(self, user: str) -\u0026gt; None: print(f\u0026#34;I\u0026#39;m extending job_b for user {user}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: klass = Proxy() print(klass.job_a(\u0026#34;red\u0026#34;)) print(klass.job_b(\u0026#34;nafi\u0026#34;)) \u0026gt;\u0026gt;\u0026gt; I\u0026#39;m extending job_a for user red None I\u0026#39;m extending job_b for user nafi None It\u0026rsquo;s evident from the above workflow that you\u0026rsquo;ll need to define an Interface class first. Python provides abstract base classes as ABC in the abc module. Abstract class Interface inherits from ABC and defines all the methods that the concrete class will have to implement later. Concrete class inherits from the interface and implements all the methods defined in it. Notice how each method in the Interface class is decorated with the @abstractmethod decorator. The @abstractmethod decorator turns a normal method into an abstract method which means that the method is nothing but a blueprint of the required methods that the concrete subclass will have to implement later. If your knowledge on decorators is fuzzy, check out my Python decorators post. You can\u0026rsquo;t directly instantiate Interface or use any of the abstract methods without making subclasses of the interface and implementing the methods.\nThe second class Concrete is the actual class that inherits from the abstract base class (interface) Interface and implements all the methods mentioned as abstract methods. This is a real class that you can instantiate and the methods can be used directly. However, if you forget to implement any of the abstract methods defined in the Interface then you\u0026rsquo;ll invoke TypeError.\nThe third class Proxy extends the functionalities of the base concrete class Concrete. It calls the Concrete class using the composition pattern and implements all the methods. However, in this case, I used the results from the concrete methods and extended their functionalities without code duplication.\nAnother practical example Let\u0026rsquo;s play around with one last real-world example to concretize the concept. Suppose, you want to collect data from an external API endpoint. To do so, you hit the endpoint with GET requests from your HTTP client and collect the responses in json format. Then say, you also want to inspect the response header and the arguments that were passed while making the request.\nNow, in the real world, public APIs will often impose rate limits and when you go over the limit with multiple get requests, your client will likely throw an http connection-timeout error. Say, you want to handle this exceptions outside of the core logic that will send the HTTP GET requests.\nAgain, let\u0026rsquo;s say you also want to cache the responses if the client has seen the arguments in the requests before. This means, when you send requests with the same arguments multiple times, instead of hitting the APIs with redundant requests, the client will show you the responses from the cache. Caching improves API response time dramatically.\nFor this demonstration, I\u0026rsquo;ll be using Postman\u0026rsquo;s publicly available GET API.\nhttps://postman-echo.com/get?foo1=bar_1\u0026amp;foo2=bar_2 This API is perfect for the demonstration since it has a rate limiter that kicks in arbitrarily and make the client throw ConnectTimeOut and ReadTimeOutError. See how this workflow is going to look like:\nDefine an interface called IFetchUrl that will implement three abstract methods. The first method get_data will fetch data from the URL and serialize them into json format. The second method get_headers will probe the data and return the header as a dictionary. The third method get_args will also probe the data like the second method but this time it will return the query arguments as a dictionary. However, in the interface, you won\u0026rsquo;t be implementing anything inside the methods.\nMake a concrete class named FetchUrl that will derive from interface IFetchUrl. This time you\u0026rsquo;ll implement all three methods defined in the abstract class. However, you shouldn\u0026rsquo;t handle any edge cases here. The method should contain pure logic flow without any extra fluff.\nMake a proxy class called ExcFetchUrl. It will also inherit from the interface but this time you\u0026rsquo;ll add your exception handling logics here. This class also adds logging functionality to all the methods. Here you call the concrete class FetchUrl in a composition format and avoid code repetition by using the methods that\u0026rsquo;s been already implemented in the concrete class. Like the FetchUrl class, here too, you\u0026rsquo;ve to implement all the methods found in the abstract class.\nThe fourth and the final class will extend the ExcFetchUrl and add caching functionality to the get_data method. It will follow the same pattern as the ExcFetchUrl class.\nSince, by now, you\u0026rsquo;re already familiar with the workflow of the proxy pattern, let\u0026rsquo;s dump the entire 110 line solution all at once.\nfrom __future__ import annotations import functools import logging import sys from abc import ABC, abstractmethod from datetime import datetime from pprint import pprint import httpx from httpx import ConnectTimeout, ReadTimeout from typing import Any logging.basicConfig(level=logging.INFO) D = dict[str, Any] class IFetchUrl(ABC): \u0026#34;\u0026#34;\u0026#34;Abstract base class. You can\u0026#39;t instantiate this independently.\u0026#34;\u0026#34;\u0026#34; @abstractmethod def get_data(self, url: str) -\u0026gt; D: pass @abstractmethod def get_headers(self, data: D) -\u0026gt; D: pass @abstractmethod def get_args(self, data: D) -\u0026gt; D: pass class FetchUrl(IFetchUrl): \u0026#34;\u0026#34;\u0026#34;Concrete class that doesn\u0026#39;t handle exceptions and loggings.\u0026#34;\u0026#34;\u0026#34; def get_data(self, url: str) -\u0026gt; D: with httpx.Client() as client: response = client.get(url) data = response.json() return data def get_headers(self, data: D) -\u0026gt; D: return data[\u0026#34;headers\u0026#34;] def get_args(self, data: D) -\u0026gt; D: return data[\u0026#34;args\u0026#34;] class ExcFetchUrl(IFetchUrl): \u0026#34;\u0026#34;\u0026#34;This class can be swapped out with the FetchUrl class. It provides additional exception handling and logging.\u0026#34;\u0026#34;\u0026#34; def __init__(self) -\u0026gt; None: self._fetch_url = FetchUrl() def get_data(self, url: str) -\u0026gt; D: try: data = self._fetch_url.get_data(url) return data except ConnectTimeout: logging.error(\u0026#34;Connection time out. Try again later.\u0026#34;) sys.exit(1) except ReadTimeout: logging.error(\u0026#34;Read timed out. Try again later.\u0026#34;) sys.exit(1) def get_headers(self, data: D) -\u0026gt; D: headers = self._fetch_url.get_headers(data) logging.info(f\u0026#34;Getting the headers at {datetime.now()}\u0026#34;) return headers def get_args(self, data: D) -\u0026gt; D: args = self._fetch_url.get_args(data) logging.info(f\u0026#34;Getting the args at {datetime.now()}\u0026#34;) return args class CacheFetchUrl(IFetchUrl): def __init__(self) -\u0026gt; None: self._fetch_url = ExcFetchUrl() self.get_data = functools.lru_cache()(self.get_data) # noqa def get_data(self, url: str) -\u0026gt; D: data = self._fetch_url.get_data(url) return data def get_headers(self, data: D) -\u0026gt; D: headers = self._fetch_url.get_headers(data) return headers def get_args(self, data: D) -\u0026gt; D: args = self._fetch_url.get_args(data) return args if __name__ == \u0026#34;__main__\u0026#34;: # url = \u0026#34;https://postman-echo.com/get?foo1=bar_1\u0026amp;foo2=bar_2\u0026#34; fetch = CacheFetchUrl() for arg1, arg2 in zip([1, 2, 3, 1, 2, 3], [1, 2, 3, 1, 2, 3]): url = f\u0026#34;https://postman-echo.com/get?foo1={arg1}\u0026amp;foo2={arg2}\u0026#34; print(f\u0026#34;\\n {\u0026#39;-\u0026#39;*75}\\n\u0026#34;) data = fetch.get_data(url) print(f\u0026#34;Cache Info: {fetch.get_data.cache_info()}\u0026#34;) # type: ignore pprint(fetch.get_headers(data)) pprint(fetch.get_args(data)) --------------------------------------------------------------------------- INFO:root:Getting the headers at 2022-01-31 16:54:36.214562 INFO:root:Getting the args at 2022-01-31 16:54:36.220221 Cache Info: CacheInfo(hits=0, misses=1, maxsize=32, currsize=1) {\u0026#39;accept\u0026#39;: \u0026#39;*/*\u0026#39;, \u0026#39;accept-encoding\u0026#39;: \u0026#39;gzip, deflate\u0026#39;, \u0026#39;content-length\u0026#39;: \u0026#39;0\u0026#39;, \u0026#39;host\u0026#39;: \u0026#39;postman-echo.com\u0026#39;, \u0026#39;user-agent\u0026#39;: \u0026#39;python-httpx/0.13.1\u0026#39;, \u0026#39;x-amzn-trace-id\u0026#39;: \u0026#39;Root=1-5ee8a4eb-4341ae58365e4090660dfaa4\u0026#39;, \u0026#39;x-b3-parentspanid\u0026#39;: \u0026#39;044bd10726921994\u0026#39;, \u0026#39;x-b3-sampled\u0026#39;: \u0026#39;0\u0026#39;, \u0026#39;x-b3-spanid\u0026#39;: \u0026#39;503e6ceaa2a4f493\u0026#39;, \u0026#39;x-b3-traceid\u0026#39;: \u0026#39;77d5b03fe98fcc1a044bd10726921994\u0026#39;, \u0026#39;x-envoy-external-address\u0026#39;: \u0026#39;10.100.91.201\u0026#39;, \u0026#39;x-forwarded-client-cert\u0026#39;: \u0026#39;By=spiffe://...;Hash=...;URI=...\u0026#39;, \u0026#39;x-forwarded-port\u0026#39;: \u0026#39;443\u0026#39;, \u0026#39;x-forwarded-proto\u0026#39;: \u0026#39;http\u0026#39;, \u0026#39;x-request-id\u0026#39;: \u0026#39;295d0b6c-7aa0-4481-aa4d-f47f5eac7d57\u0026#39;} {\u0026#39;foo1\u0026#39;: \u0026#39;bar_1\u0026#39;, \u0026#39;foo2\u0026#39;: \u0026#39;bar_1\u0026#39;} .... In the get_data method of the FetchUrl class, I\u0026rsquo;ve used HTTPx to fetch the data from the URL. Pay attention to the fact that I\u0026rsquo;ve practically ignored all the additional logics of error handling and logging here. The exception handling and logging logic were added via ExcFetchUrl proxy class. Another class CacheFetchUrl further extends the proxy class ExcFetchUrl by adding cache functionality to the get_data method.\nIn the main section, you can use any of the FetchUrl, ExcFetchUrl or CacheFetchUrl without any additional changes to the logic of these classes. The FetchUrl is the barebone class that will fail in case of the occurrence of any exceptions. The latter classes appends additional functionalities while maintaining the same interface.\nThe output basically prints out the results returned by the get_headers and get_args methods. Also notice, how I picked the endpoint arguments to simulate caching. The Cache Info: on the third line of the output shows when data is served from the cache. Here, hits=0 means data is served directly from the external API. However, if you inspect the later outputs, you\u0026rsquo;ll see when the query arguments get repeated ([1, 2, 3, 1, 2, 3]), Cache Info: will show higher hit counts. This means that the data is being served from the cache.\nShould you use it? Well, yes obviously. But not always. You see, you need a little bit of planning before orchestrating declarative solution with the proxy pattern. It\u0026rsquo;s not viable to write code in this manner in a throwaway script that you don\u0026rsquo;t have to maintain in the long run. Also, this OOP-cursed additional layers of abstraction can make your code subjectively unreadable. So use the pattern wisely. On the flip side, proxy pattern can come in handy when you need to extend the functionality of some class arbitrarily as it can work as a gateway to the El Dorado of loose coupling.\nFurther reading Proxy pattern Design patterns for humans - proxy pattern ","permalink":"https://rednafi.com/python/proxy-pattern/","summary":"\u003cp\u003eIn Python, there\u0026rsquo;s a saying that \u0026ldquo;design patterns are anti-patterns\u0026rdquo;. Also, in the realm of\ndynamic languages, design patterns have the notoriety of injecting additional abstraction\nlayers to the core logic and making the flow gratuitously obscure. Python\u0026rsquo;s dynamic nature\nand the treatment of functions as first-class objects often make Java-ish design patterns\nredundant.\u003c/p\u003e\n\u003cp\u003eInstead of littering your code with seemingly over-engineered patterns, you can almost\nalways take the advantage of Python\u0026rsquo;s first-class objects, duck-typing, monkey-patching etc\nto accomplish the task at hand. However, recently there is one design pattern that I find\nmyself using over and over again to write more maintainable code and that is the Proxy\npattern. So I thought I\u0026rsquo;d document it here for future reference.\u003c/p\u003e","title":"Implementing proxy pattern in Python"},{"content":"Updated on 2023-09-11: Fix broken URLs.\nRecently, I was working with Mapbox\u0026rsquo;s Route optimization API. It tries to solve the traveling salesman problem where you provide the API with coordinates of multiple places and it returns a duration-optimized route between those locations. This is a perfect usecase where Redis caching can come handy. Redis is a fast and lightweight in-memory database with additional persistence options; making it a perfect candidate for the task at hand. Here, caching can save you from making redundant API requests and also, it can dramatically improve the response time as well.\nI found that in my country, the optimized routes returned by the API do not change dramatically for at least for a couple of hours. So the workflow will look something like this:\nCaching the API response in Redis using the key-value data structure. Here the requested coordinate-string will be the key and the response will be the corresponding value. Setting a timeout on the records. Serving new requests from cache if the records exist. Only send a new request to MapBox API if the response is not cached and then add that response to cache. Setting up Redis \u0026amp; RedisInsight To proceed with the above workflow, you\u0026rsquo;ll need to install and setup Redis database on your system. For monitoring the database, I\u0026rsquo;ll be using RedisInsight. The easiest way to setup Redis and RedisInsight is through Docker. Here\u0026rsquo;s a docker-compose that you can use to setup everything with a single command.\n# docker-compose.yml version: \u0026#34;3.9\u0026#34; services: redis: container_name: redis-cont image: \u0026#34;redis:alpine\u0026#34; environment: - REDIS_PASSWORD=ubuntu - REDIS_REPLICATION_MODE=master ports: - \u0026#34;6379:6379\u0026#34; volumes: # save redisearch data to your current working directory - ./redis-data:/data command: # Save if 100 keys are added in every 10 seconds - \u0026#34;--save 10 100\u0026#34; # Set password - \u0026#34;--requirepass ubuntu\u0026#34; redisinsight: # redis db visualization dashboard container_name: redisinsight-cont image: redislabs/redisinsight ports: - 8001:8001 volumes: - redisinsight:/db volumes: redis-data: redisinsight: The above docker-compose file has two services, redis and redisinsight. I\u0026rsquo;ve set up the database with a dummy password ubuntu and made it persistent using a folder named redis-data in the current working directory. The database listens in localhost\u0026rsquo;s port 6379. You can monitor the database using redisinsight in port 8000. To spin up Redis and RedisInsight containers, run:\ndocker compose up -d This command will start the database and monitor accordingly. You can go to this localhost:8000 link using your browser and connect redisinsight to your database. After connecting your database, you should see a dashboard like this in your redisinsight panel:\nPreparing Python environment For local development, you can set up your python environment and install the dependencies using pip. Here, I\u0026rsquo;m on a Linux machine and using virtual environment for isolation. The following commands will work if you\u0026rsquo;re on a *nix based system and have python 3.12 installed on your system. This will install the necessary dependencies in a virtual environment:\npython3.12 -m venv .venv source .venv/bin/activate pip install redis httpx Workflow Connecting Python client to Redis Assuming the database server is running and you\u0026rsquo;ve installed the dependencies, the following snippet connects redis-py client to the database.\nimport redis import sys def redis_connect() -\u0026gt; redis.client.Redis: try: client = redis.Redis( host=\u0026#34;localhost\u0026#34;, port=6379, password=\u0026#34;ubuntu\u0026#34;, db=0, socket_timeout=5, ) ping = client.ping() if ping is True: return client except redis.AuthenticationError: print(\u0026#34;AuthenticationError\u0026#34;) sys.exit(1) client = redis_connect() The above excerpt tries to connect to the Redis database server using the port 6379. Notice, how I\u0026rsquo;m providing the password ubuntu via the password argument. Here, client.ping() helps you determine if a connection has been established successfully. It returns True if a successful connection can be established or raises specific errors in case of failures. The above function handles AuthenticationError and prints out an error message if the error occurs. If everything goes well, running the redis_connect() function will return an instance of the redis.client.Redis class. This instance will be used later to set and retrieve data to and from the redis database.\nGetting route data from MapBox API The following function strikes the MapBox Route Optimization API and collects route data.\nimport httpx def get_routes_from_api(coordinates: str) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Data from mapbox api.\u0026#34;\u0026#34;\u0026#34; with httpx.Client() as client: base = \u0026#34;https://api.mapbox.com/optimized-trips/v1/mapbox/driving\u0026#34; geometries = \u0026#34;geojson\u0026#34; access_token = \u0026#34;Your-MapBox-API-token\u0026#34; url = ( f\u0026#34;{base}/{coordinates}?geometries={geometries}\u0026#34; f\u0026#34;\u0026amp;access_token={access_token}\u0026#34; ) response = client.get(url) return response.json() The above code uses Python\u0026rsquo;s HTTPx library to make the get request. HTTPx is almost a drop-in replacement for the ubiquitous requests library but way faster and has async support. Here, I\u0026rsquo;ve used context manager httpx.Client() for better resource management while making the get request. You can read more about context managers and how to use them for hassle free resource management.\nThe base_url is the base url of the route optimization API and the you\u0026rsquo;ll need to provide your own access token in the access_token field. Notice, how the url variable builds up the final request url. The coordinates are provided using the lat0,lon0;lat1,lon1;lat2,lon2... format. Rest of the function sends the http requests and converts the response into a native dictionary object using the response.json() method.\nSetting \u0026amp; retrieving data to \u0026amp; from Redis database The following two functions retrieves data from and sets data to redis database respectively.\nfrom datetime import timedelta def get_routes_from_cache(key: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Get data from redis.\u0026#34;\u0026#34;\u0026#34; val = client.get(key) return val def set_routes_to_cache(key: str, value: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Set data to redis.\u0026#34;\u0026#34;\u0026#34; state = client.setex( key, timedelta(seconds=3600), value=value, ) return state Here, both the keys and the values are strings. In the second function, set_routes_to_cache, the client.setex() method sets a timeout of 1 hour on the key. After that the key and its associated value get deleted automatically.\nThe central orchestration The route_optima function is the primary agent that orchestrates and executes the caching and returning of responses against requests. It roughly follows the execution flow shown below.\nWhen a new request arrives, the function first checks if the return-value exists in the Redis cache. If the value exists, it shows the cached value, otherwise, it sends a new request to the MapBox API, cache that value and then shows the result.\ndef route_optima(coordinates: str) -\u0026gt; dict: # First it looks for the data in redis cache data = get_routes_from_cache(key=coordinates) # If cache is found then serves the data from cache if data is not None: data = json.loads(data) data[\u0026#34;cache\u0026#34;] = True return data else: # If cache is not found then sends request to the MapBox API data = get_routes_from_api(coordinates) # This block sets saves the respose to redis and serves it directly if data.get(\u0026#34;code\u0026#34;) == \u0026#34;Ok\u0026#34;: data[\u0026#34;cache\u0026#34;] = False data = json.dumps(data) state = set_routes_to_cache(key=coordinates, value=data) if state is True: return json.loads(data) return data Exposing as an API This part of the code wraps the original Route Optimization API and exposes that as a new endpoint. I\u0026rsquo;ve used FastAPI to build the wrapper API. Doing this also hides the underlying details of authentication and the actual endpoint of the MapBox API.\nfrom fastapi import FastAPI app = FastAPI() @app.get(\u0026#34;/route-optima/{coordinates}\u0026#34;) def view(coordinates): \u0026#34;\u0026#34;\u0026#34;This will wrap our original route optimization API and incorporate Redis Caching. You\u0026#39;ll only expose this API to the end user.\u0026#34;\u0026#34;\u0026#34; # coordinates = \u0026#34;90.3866,23.7182;90.3742,23.7461\u0026#34; return route_optima(coordinates) Putting it all together # app.py import json import sys from datetime import timedelta import httpx import redis from fastapi import FastAPI def redis_connect() -\u0026gt; redis.client.Redis: try: client = redis.Redis( host=\u0026#34;localhost\u0026#34;, port=6379, password=\u0026#34;ubuntu\u0026#34;, db=0, socket_timeout=5, ) ping = client.ping() if ping is True: return client except redis.AuthenticationError: print(\u0026#34;AuthenticationError\u0026#34;) sys.exit(1) client = redis_connect() def get_routes_from_api(coordinates: str) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;Data from mapbox api.\u0026#34;\u0026#34;\u0026#34; with httpx.Client() as client: base = \u0026#34;https://api.mapbox.com/optimized-trips/v1/mapbox/driving\u0026#34; geometries = \u0026#34;geojson\u0026#34; access_token = \u0026#34;Your-MapBox-API-token\u0026#34; url = ( f\u0026#34;{base}/{coordinates}?geometries={geometries}\u0026#34; f\u0026#34;\u0026amp;access_token={access_token}\u0026#34; ) response = client.get(url) return response.json() def get_routes_from_cache(key: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Data from redis.\u0026#34;\u0026#34;\u0026#34; val = client.get(key) return val def set_routes_to_cache(key: str, value: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Data to redis.\u0026#34;\u0026#34;\u0026#34; state = client.setex( key, timedelta(seconds=3600), value=value, ) return state def route_optima(coordinates: str) -\u0026gt; dict: # First it looks for the data in redis cache data = get_routes_from_cache(key=coordinates) # If cache is found then serves the data from cache if data is not None: data = json.loads(data) data[\u0026#34;cache\u0026#34;] = True return data else: # If cache is not found then sends request to the MapBox API data = get_routes_from_api(coordinates) # This block sets saves the respose to redis and serves it directly if data.get(\u0026#34;code\u0026#34;) == \u0026#34;Ok\u0026#34;: data[\u0026#34;cache\u0026#34;] = False data = json.dumps(data) state = set_routes_to_cache(key=coordinates, value=data) if state is True: return json.loads(data) return data app = FastAPI() @app.get(\u0026#34;/route-optima/{coordinates}\u0026#34;) def view(coordinates: str) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;This will wrap our original route optimization API and incorporate Redis Caching. You\u0026#39;ll only expose this API to the end user.\u0026#34;\u0026#34;\u0026#34; # coordinates = \u0026#34;90.3866,23.7182;90.3742,23.7461\u0026#34; return route_optima(coordinates) You can copy the complete code to a file named app.py and run the app using the command below (assuming redis, redisinsight is running and you\u0026rsquo;ve installed the dependencies beforehand):\nuvicorn app.app:app --host 0.0.0.0 --port 5000 --reload This will run a local server where you can send new request with coordinates.\nGo to your browser and hit the endpoint with a set of new coordinates. For example:\nhttp://localhost:5000/route-optima/90.3866,23.7182;90.3742,23.7461 This should return a response with the coordinates of the optimized route.\n{ \u0026#34;code\u0026#34;:\u0026#34;Ok\u0026#34;, \u0026#34;waypoints\u0026#34;:[ { \u0026#34;distance\u0026#34;:26.041809241776583, \u0026#34;name\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;location\u0026#34;:[ 90.386855, 23.718213 ], \u0026#34;waypoint_index\u0026#34;:0, \u0026#34;trips_index\u0026#34;:0 }, { \u0026#34;distance\u0026#34;:6.286653078791968, \u0026#34;name\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;location\u0026#34;:[ 90.374253, 23.746129 ], \u0026#34;waypoint_index\u0026#34;:1, \u0026#34;trips_index\u0026#34;:0 } ], \u0026#34;trips\u0026#34;:[ { \u0026#34;geometry\u0026#34;:{ \u0026#34;coordinates\u0026#34;:[ [ 90.386855, 23.718213 ], \u0026#34;... ...\u0026#34; ], \u0026#34;type\u0026#34;:\u0026#34;LineString\u0026#34; }, \u0026#34;legs\u0026#34;:[ { \u0026#34;summary\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;weight\u0026#34;:3303.1, \u0026#34;duration\u0026#34;:2842.8, \u0026#34;steps\u0026#34;:[ ], \u0026#34;distance\u0026#34;:5250.2 }, { \u0026#34;summary\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;weight\u0026#34;:2536.5, \u0026#34;duration\u0026#34;:2297, \u0026#34;steps\u0026#34;:[ ], \u0026#34;distance\u0026#34;:4554.8 } ], \u0026#34;weight_name\u0026#34;:\u0026#34;routability\u0026#34;, \u0026#34;weight\u0026#34;:5839.6, \u0026#34;duration\u0026#34;:5139.8, \u0026#34;distance\u0026#34;:9805 } ], \u0026#34;cache\u0026#34;:false } If you\u0026rsquo;ve hit the above URL for the first time, the cache attribute of the json response should show false. This means that the response is being served from the original MapBox API. However, hitting the same URL with the same coordinates again will show the cached response and this time the cache attribute should show true.\nInspection Once you\u0026rsquo;ve got everything up and running you can inspect the cache via redis insight. To do so, go to the link below while your app server is running:\nhttp://localhost:8000/ Select the Browser panel from the left menu and click on a key of your cached data. It should show something like this:\nAlso you can play around with the API in the swagger UI. To do so, go to the following link:\nhttp://localhost:5000/docs This will take you to the swagger dashboard. Here you can make requests using the interactive UI. Go ahead and inspect how the caching works for new coordinates.\nRemarks You can find the complete source code of the app in my HTTP request caching with Redis repository.\nDisclaimer This app has been made for demonstration purpose only. So it might not reflect the best practices of production ready applications.\n","permalink":"https://rednafi.com/python/redis-cache/","summary":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eUpdated on 2023-09-11\u003c/em\u003e\u003c/strong\u003e: \u003cem\u003eFix broken URLs.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eRecently, I was working with Mapbox\u0026rsquo;s \u003ca href=\"https://docs.mapbox.com/api/navigation/#optimization\"\u003eRoute optimization API\u003c/a\u003e. It tries to solve the\n\u003ca href=\"https://en.wikipedia.org/wiki/Travelling_salesman_problem\"\u003etraveling salesman problem\u003c/a\u003e where you provide the API with coordinates of multiple places\nand it returns a duration-optimized route between those locations. This is a perfect usecase\nwhere \u003ca href=\"https://redis.io/\"\u003eRedis\u003c/a\u003e caching can come handy. Redis is a fast and lightweight in-memory database\nwith additional persistence options; making it a perfect candidate for the task at hand.\nHere, caching can save you from making redundant API requests and also, it can dramatically\nimprove the response time as well.\u003c/p\u003e","title":"Effortless API response caching with Python \u0026 Redis"},{"content":"Updated on 2022-02-13: Change functools import style.\nWhen I first learned about Python decorators, using them felt like doing voodoo magic. Decorators can give you the ability to add new functionalities to any callable without actually touching or changing the code inside it. This can typically yield better encapsulation and help you write cleaner and more understandable code. However, decorator is considered as a fairly advanced topic in Python since understanding and writing it requires you to have command over multiple additional concepts like first class objects, higher order functions, closures etc. First, I\u0026rsquo;ll try to introduce these concepts as necessary and then unravel the core concept of decorator layer by layer. So let\u0026rsquo;s dive in.\nFirst class objects In Python, basically everything is an object and functions are regarded as first-class objects. It means that functions can be passed around and used as arguments, just like any other object (string, int, float, list, and so on). You can assign functions to variables and treat them like any other objects. Consider this example:\ndef func_a(): return \u0026#34;I was angry with my friend.\u0026#34; def func_b(): return \u0026#34;I told my wrath, my wrath did end\u0026#34; def func_c(*funcs): for func in funcs: print(func()) main_func = func_c main_func(func_a, func_b) \u0026gt;\u0026gt;\u0026gt; I was angry with my friend. \u0026gt;\u0026gt;\u0026gt; I told my wrath, my wrath did end The above example demonstrates how Python treats functions as first class citizens. First, I defined two functions, func_a and func_b and then func_c takes them as parameters. func_c runs the functions taken as parameters and prints the results. Then we assign func_c to variable main_func. Finally, we run main_func and it behaves just like func_c.\nHigher order functions Python also allows you to use functions as return values. You can take in another function and return that function or you can define a function within another function and return the inner function.\ndef higher(func): \u0026#34;\u0026#34;\u0026#34;This is a higher order function. It returns another function. \u0026#34;\u0026#34;\u0026#34; return func def lower(): return \u0026#34;I\u0026#39;m hunting high and low\u0026#34; higher(lower) \u0026gt;\u0026gt;\u0026gt; \u0026lt;function __main__.lower()\u0026gt; Now you can assign the result of higher to another variable and execute the output function.\nh = higher(lower) h() \u0026gt;\u0026gt;\u0026gt; \u0026#34;I\u0026#39;m hunting high and low\u0026#34; Let\u0026rsquo;s look into another example where you can define a nested function within a function and return the nested function instead of its result.\ndef outer(): \u0026#34;\u0026#34;\u0026#34;Define and return a nested function from another function.\u0026#34;\u0026#34;\u0026#34; def inner(): return \u0026#34;Hello from the inner func\u0026#34; return inner inn = outer() inn() \u0026gt;\u0026gt;\u0026gt; \u0026#39;Hello from the inner func\u0026#39; Notice how the nested function inner was defined inside the outer function and then the return statement of the outer function returned the nested function. After definition, to get to the nested function, first we called the outer function and received the result as another function. Then executing the result of the outer function prints out the message from the inner function.\nClosures You saw examples of inner functions at work in the previous section. Nested functions can access variables of the enclosing scope. In Python, these non-local variables are read only by default and we must declare them explicitly as non-local (using nonlocal keyword) in order to modify them. Following is an example of a nested function accessing a non-local variable.\ndef burger(name): def ingredients(): if name == \u0026#34;deli\u0026#34;: return (\u0026#34;steak\u0026#34;, \u0026#34;pastrami\u0026#34;, \u0026#34;emmental\u0026#34;) elif name == \u0026#34;smashed\u0026#34;: return (\u0026#34;chicken\u0026#34;, \u0026#34;nacho cheese\u0026#34;, \u0026#34;jalapeno\u0026#34;) else: return None return ingredients Now run the function,\ningr = burger(\u0026#34;deli\u0026#34;) ingr() \u0026gt;\u0026gt;\u0026gt; (\u0026#39;steak\u0026#39;, \u0026#39;pastrami\u0026#39;, \u0026#39;emmental\u0026#39;) Well, that\u0026rsquo;s unusual.\nThe burger function was called with the string deli and the returned function was bound to the name ingr. On calling ingr(), the message was still remembered and used to derive the outcome although the outer function burger had already finished its execution.\nThis technique by which some data (\u0026ldquo;deli\u0026rdquo;) gets attached to the code is called closure in Python. The value in the enclosing scope is remembered even when the variable goes out of scope or the function itself is removed from the current namespace. Decorators uses the idea of non-local variables multiple times and soon you\u0026rsquo;ll see how.\nWriting a basic decorator With these prerequisites out of the way, let\u0026rsquo;s go ahead and create your first simple decorator.\ndef deco(func): def wrapper(): print(\u0026#34;This will get printed before the function is called.\u0026#34;) func() print(\u0026#34;This will get printed after the function is called.\u0026#34;) return wrapper Before using the decorator, let\u0026rsquo;s define a simple function without any parameters.\ndef ans(): print(42) Treating the functions as first-class objects, you can use your decorator like this:\nans = deco(ans) ans() \u0026gt;\u0026gt;\u0026gt; This will get printed before the function is called. 42 This will get printed after the function is called. In the above two lines, you can see a very simple decorator in action. Our deco function takes in a target function, manipulates the target function inside a wrapper function and then returns the wrapper function. Running the function returned by the decorator, you\u0026rsquo;ll get your modified result. To put it simply, decorators wraps a function and modifies its behavior.\nThe decorator function runs at the time the decorated function is imported/defined, not when it is called.\nBefore moving onto the next section, let\u0026rsquo;s see how we can get the return value of target function instead of just printing it.\ndef deco(func): \u0026#34;\u0026#34;\u0026#34;This modified decorator also returns the result of func.\u0026#34;\u0026#34;\u0026#34; def wrapper(): print(\u0026#34;This will get printed before the function is called.\u0026#34;) ret = func() print(\u0026#34;This will get printed after the function is called.\u0026#34;) return ret return wrapper def ans(): return 42 In the above example, the wrapper function returns the result of the target function and the wrapper itself. This makes it possible to get the result of the modified function.\nans = deco(ans) print(ans()) \u0026gt;\u0026gt;\u0026gt; This will get printed before the function is called. This will get printed after the function is called. 42 Can you guess why the return value of the decorated function appeared in the last line instead of in the middle like before?\nThe @ syntactic sugar The way you\u0026rsquo;ve used decorator in the last section might feel a little clunky. First, you have to type the name ans three times to call and use the decorator. Also, it becomes harder to tell apart where the decorator is actually working. So Python allows you to use decorator with the special syntax @. You can apply your decorators while defining your functions, like this:\n@deco def func(): ... # Now call your decorated function just like a normal one func() Sometimes the above syntax is called the pie syntax and it\u0026rsquo;s just a syntactic sugar for func = deco(func).\nDecorating functions with arguments The naive decorator that we\u0026rsquo;ve implemented above will only work for functions that take no arguments. It\u0026rsquo;ll fail and raise TypeError if your try to decorate a function having arguments with deco. Now let\u0026rsquo;s create another decorator called yell which will take in a function that returns a string value and transform that string value to uppercase.\ndef yell(func): def wrapper(*args, **kwargs): ret = func(*args, **kwargs) ret = ret.upper() + \u0026#34;!\u0026#34; return ret return wrapper Create the target function that returns string value.\n@yell def hello(name): return f\u0026#34;Hello {name}\u0026#34; hello(\u0026#34;redowan\u0026#34;) \u0026gt;\u0026gt;\u0026gt; \u0026#39;HELLO REDOWAN!\u0026#39; Function hello takes a name:string as parameter and returns a message as string. Look how the yell decorator is modifying the original return string, transforming that to uppercase and adding an extra ! sign without directly changing any code in the hello function.\nSolving identity crisis In Python, you can introspect any object and its properties via the interactive shell. A function knows its identity, docstring etc. For instance, you can inspect the built in print function in the following ways:\nprint \u0026gt;\u0026gt;\u0026gt; \u0026lt;function print\u0026gt; print.__name__ \u0026gt;\u0026gt;\u0026gt; \u0026#39;print\u0026#39; print.__doc__ \u0026gt;\u0026gt;\u0026gt; \u0026#34;print(value, ..., sep=\u0026#39; \u0026#39;, end=\u0026#39;\\\\n\u0026#39;, file=sys.stdout, flush=False)\\n\\nPrints the values to a stream, or to sys.stdout by default.\\nOptional keyword arguments:\\nfile: a file-like object (stream); defaults to the current sys.stdout.\\nsep: string inserted between values, default a space.\\nend: string appended after the last value, default a newline.\\nflush: whether to forcibly flush the stream.\u0026#34; help(print) \u0026gt;\u0026gt;\u0026gt; Help on built-in function print in module builtins: print(...) print(value, ..., sep=\u0026#39; \u0026#39;, end=\u0026#39;\\n\u0026#39;, file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream. This introspection works similarly for functions that you defined yourself. I\u0026rsquo;ll be using the previously defined hello function.\nhello.__name__ \u0026gt;\u0026gt;\u0026gt; \u0026#39;wrapper\u0026#39; help(hello) \u0026gt;\u0026gt;\u0026gt; Help on function wrapper in module __main__: wrapper(*args, **kwargs) Now what\u0026rsquo;s going on there. The decorator yell has made the function hello confused about its own identity. Instead of reporting its own name, it takes the identity of the inner function wrapper. This can be confusing while doing debugging. You can fix this by using builtin wraps decorator from the functools module. This will make sure that the original identity of the decorated function stays preserved.\nfrom functools import wraps def yell(func): @wraps(func) def wrapper(*args, **kwargs): ret = func(*args, **kwargs) ret = ret.upper() + \u0026#34;!\u0026#34; return ret return wrapper @yell def hello(name): \u0026#34;Hello from the other side.\u0026#34; return f\u0026#34;Hello {name}\u0026#34; hello(\u0026#34;Galaxy\u0026#34;) \u0026gt;\u0026gt;\u0026gt; \u0026#39;HELLO GALAXY!\u0026#39; Introspecting the hello function decorated with modified decorator will give you the desired result.\nhello.__name__ \u0026gt;\u0026gt;\u0026gt; \u0026#39;hello\u0026#39; help(hello) \u0026gt;\u0026gt;\u0026gt; Help on function hello in module __main__: hello(name) Hello from the other side. Decorators in the wild Before moving on to the next section let\u0026rsquo;s see a few real world examples of decorators. To define all the decorators, we\u0026rsquo;ll be using the following template that we\u0026rsquo;ve perfected so far.\nfrom functools import wraps def decorator(func): @wraps(func) def wrapper(*args, **kwargs): # Do something before ret = func(*args, **kwargs) # Do something after return ret return wrapper Timer Timer decorator will help you time your callables in a non-intrusive way. It can help you while debugging and profiling your functions.\nfrom functools import wraps from time import perf_counter def timer(func): \u0026#34;\u0026#34;\u0026#34;This decorator prints out the execution time of a callable.\u0026#34;\u0026#34;\u0026#34; @wraps(func) def wrapper(*args, **kwargs): start_time = perf_counter() ret = func(*args, **kwargs) end_time = perf_counter() run_time = end_time - start_time print(f\u0026#34;Finished {func.__name__} in {run_time:.4f} seconds.\u0026#34;) return ret return wrapper @timer def dothings(n_times): for _ in range(n_times): return sum((i**3 for i in range(100_000))) In the above way, we can introspect the time it requires for function dothings to complete its execution.\ndothings(100_000) \u0026gt;\u0026gt;\u0026gt; Finished dothings in 0.0353 seconds. 24999500002500000000 Exception logger Just like the timer decorator, we can define a logger decorator that will log the state of a callable. For this demonstration, I\u0026rsquo;ll be defining a exception logger that will show additional information like timestamp, argument names when an exception occurs inside of the decorated callable.\nfrom functools import wraps from datetime import datetime def logexc(func): @wraps(func) def wrapper(*args, **kwargs): # Stringify the arguments args_rep = [repr(arg) for arg in args] kwargs_rep = [f\u0026#34;{k}={v!r}\u0026#34; for k, v in kwargs.items()] sig = \u0026#34;, \u0026#34;.join(args_rep + kwargs_rep) # Try running the function try: return func(*args, **kwargs) except Exception as e: print( \u0026#34;Time: \u0026#34;, datetime.now().strftime(\u0026#34;%Y-%m-%d [%H:%M:%S]\u0026#34;), ) print(\u0026#34;Arguments: \u0026#34;, sig) print(\u0026#34;Error:\\n\u0026#34;) raise return wrapper @logexc def divint(a, b): return a / b Let\u0026rsquo;s invoke ZeroDivisionError to see the logger in action.\ndivint(1, 0) \u0026gt;\u0026gt;\u0026gt; Time: 2020-05-12 [12:03:31] Arguments: 1, 0 Error: ------------------------------------------------------------ ZeroDivisionError Traceback (most recent call last) .... The decorator first prints a few info regarding the function and then raises the original error.\nValidation \u0026amp; runtime checks Python\u0026rsquo;s type system is strongly typed, but very dynamic. For all its benefits, this means some bugs can try to creep in, which more statically typed languages (like Java) would catch at compile time. Looking beyond even that, you may want to enforce more sophisticated, custom checks on data going in or out. Decorators can let you easily handle all of this, and apply it to many functions at once.\nImagine this: you have a set of functions, each returning a dictionary, which (among other fields) includes a field called \u0026ldquo;summary.\u0026rdquo; The value of this summary must not be more than 30 characters long; if violated, that\u0026rsquo;s an error. Here is a decorator that raises a ValueError if that happens:\nfrom functools import wraps def validate_summary(func): @wraps(func) def wrapper(*args, **kwargs): ret = func(*args, **kwargs) if len(ret[\u0026#34;summary\u0026#34;]) \u0026gt; 30: raise ValueError(\u0026#34;Summary exceeds 30 character limit.\u0026#34;) return ret return wrapper @validate_summary def short_summary(): return {\u0026#34;summary\u0026#34;: \u0026#34;This is a short summary\u0026#34;} @validate_summary def long_summary(): return {\u0026#34;summary\u0026#34;: \u0026#34;This is a long summary that exceeds limit.\u0026#34;} print(short_summary()) print(long_summary()) \u0026gt;\u0026gt;\u0026gt; {\u0026#39;summary\u0026#39;: \u0026#39;This is a short summary\u0026#39;} ------------------------------------------------------------------- ValueError Traceback (most recent call last) \u0026lt;ipython-input-178-7375d8e2a623\u0026gt; in \u0026lt;module\u0026gt; 19 20 print(short_summary()) ---\u0026gt; 21 print(long_summary()) ... Retry Imagine a situation where your defined callable fails due to some I/O related issues and you\u0026rsquo;d like to retry that again. Decorator can help you to achieve that in a reusable manner. Let\u0026rsquo;s define a retry decorator that will rerun the decorated function multiple times if an HTTP error occurs.\nimport requests from functools import wraps def retry(func): \u0026#34;\u0026#34;\u0026#34;This will rerun the decorated callable 3 times if the callable encounters http 500/404 error.\u0026#34;\u0026#34;\u0026#34; @wraps(func) def wrapper(*args, **kwargs): n_tries = 3 tries = 0 while True: resp = func(*args, **kwargs) if ( resp.status_code == 500 or resp.status_code == 404 and tries \u0026lt; n_tries ): print(f\u0026#34;retrying... ({tries})\u0026#34;) tries += 1 continue break return resp return wrapper @retry def getdata(url): resp = requests.get(url) return resp resp = getdata(\u0026#34;https://httpbin.org/get/1\u0026#34;) resp.text \u0026gt;\u0026gt;\u0026gt; retrying... (0) retrying... (1) retrying... (2) \u0026#39;\u0026lt;!DOCTYPE HTML PUBLIC \u0026#34;-//W3C//DTD HTML 3.2 Final//EN\u0026#34;\u0026gt;\\n\u0026lt;title\u0026gt;404 Not Found\u0026lt;/ title\u0026gt;\\n\u0026lt;h1\u0026gt;Not Found\u0026lt;/h1\u0026gt;\\n\u0026lt;p\u0026gt;The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.\u0026lt;/p\u0026gt;\\n\u0026#39; Applying multiple decorators You can apply multiple decorators to a function by stacking them on top of each other. Let\u0026rsquo;s define two simple decorators and use them both on a function.\nfrom functools import wraps def greet(func): \u0026#34;\u0026#34;\u0026#34;Greet in English.\u0026#34;\u0026#34;\u0026#34; @wraps(func) def wrapper(*args, **kwargs): ret = func(*args, **kwargs) return \u0026#34;Hello \u0026#34; + ret + \u0026#34;!\u0026#34; return wrapper def flare(func): \u0026#34;\u0026#34;\u0026#34;Add flares to the string.\u0026#34;\u0026#34;\u0026#34; @wraps(func) def wrapper(*args, **kwargs): ret = func(*args, **kwargs) return \u0026#34;🎉 \u0026#34; + ret + \u0026#34; 🎉\u0026#34; return wrapper @flare @greet def getname(name): return name getname(\u0026#34;Nafi\u0026#34;) \u0026gt;\u0026gt;\u0026gt; \u0026#39;🎉 Hello Nafi! 🎉\u0026#39; The decorators are called in a bottom up order. First, the decorator greet gets applied on the result of getname function and then the result of greet gets passed to the flare decorator. The decorator stack above can be written as flare(greet(getname(name))). Change the order of the decorators and see what happens!\nDecorators with arguments While defining the retry decorator in the previous section, you may have noticed that I\u0026rsquo;ve hard coded the number of times I\u0026rsquo;d like the function to retry if an error occurs. It\u0026rsquo;d be handy if you could inject the number of tries as a parameter into the decorator and make it work accordingly. This isn\u0026rsquo;t a trivial task and you\u0026rsquo;ll need three levels of nested functions to achieve that.\nBefore doing that let\u0026rsquo;s cook up a trivial example of how you can define decorators with parameters.\nfrom functools import wraps def joinby(delimiter=\u0026#34; \u0026#34;): \u0026#34;\u0026#34;\u0026#34;This decorator splits the string output of the decorated function by a single space and then joins them using a user specified delimiter.\u0026#34;\u0026#34;\u0026#34; def outer_wrapper(func): @wraps(func) def inner_wrapper(*args, **kwargs): ret = func(*args, **kwargs) ret = ret.split(\u0026#34; \u0026#34;) ret = delimiter.join(ret) return ret return inner_wrapper return outer_wrapper @joinby(delimiter=\u0026#34;,\u0026#34;) def hello(name): return f\u0026#34;Hello {name}!\u0026#34; @joinby(delimiter=\u0026#34;\u0026gt;\u0026#34;) def greet(name): return f\u0026#34;Greetings {name}!\u0026#34; @joinby() def goodbye(name): return f\u0026#34;Goodbye {name}!\u0026#34; print(hello(\u0026#34;Nafi\u0026#34;)) print(greet(\u0026#34;Redowan\u0026#34;)) print(goodbye(\u0026#34;Delowar\u0026#34;)) \u0026gt;\u0026gt;\u0026gt; Hello,Nafi! Greetings\u0026gt;Redowan! Goodbye Delowar! The decorator joinby takes a single parameter called delimiter. It splits the string output of the decorated function by a single space and then joins them using the user defined delimiter specified in the delimiter argument. The three layer nested definition looks scary but we\u0026rsquo;ll get to that in a moment. Notice how you can use the decorator with different parameters. In the above example, I\u0026rsquo;ve defined three different functions to demonstrate the usage of joinby. It\u0026rsquo;s important to note that in case of a decorator that takes parameters, you\u0026rsquo;ll always need to pass something to it and even if you don\u0026rsquo;t want to pass any parameter (run with the default), you\u0026rsquo;ll still need to decorate your function with deco() instead of deco. Try changing the decorator on the goodbye function from joinby() to joinby and see what happens.\nTypically, a decorator creates and returns an inner wrapper function but here in the repeat decorator, there is an inner function within another inner function. This almost looks like a dream within a dream from the movie Inception.\nThere are a few subtle things happening in the joinby() function:\nDefining outer_wrapper() as an inner function means that repeat() will refer to a function object outer_wrapper.\nThe delimiter argument is seemingly not used in joinby() itself. But by passing delimiter a closure is created where the value of delimiter is stored until it will be used later by inner_wrapper()\nDecorators with \u0026amp; without arguments You saw earlier that a decorator specifically designed to take parameters can\u0026rsquo;t be used without parameters; you need to at least apply parenthesis after the decorator deco() to use it without explicitly providing the arguments. But what if you want to design one that can used both with and without arguments. Let\u0026rsquo;s redefine the joinby decorator so that you can use it with parameters or just like an ordinary parameter-less decorator that we\u0026rsquo;ve seen before.\nfrom functools import wraps def joinby(_func=None, *, delimiter=\u0026#34; \u0026#34;): \u0026#34;\u0026#34;\u0026#34;This decorator splits the string output of a function by a single space and then joins that using a user specified delimiter.\u0026#34;\u0026#34;\u0026#34; def outer_wrapper(func): @wraps(func) def inner_wrapper(*args, **kwargs): ret = func(*args, **kwargs) ret = ret.split(\u0026#34; \u0026#34;) ret = delimiter.join(ret) return ret return inner_wrapper # This part enables you to use the decorator with/without arguments if _func is None: return outer_wrapper else: return outer_wrapper(_func) @joinby(delimiter=\u0026#34;,\u0026#34;) def hello(name): return f\u0026#34;Hello {name}!\u0026#34; @joinby def greet(name): return f\u0026#34;Greetings {name}!\u0026#34; print(hello(\u0026#34;Nafi\u0026#34;)) print(greet(\u0026#34;Redowan\u0026#34;)) \u0026gt;\u0026gt;\u0026gt; Hello,Nafi! Greetings Redowan! Here, the _func argument acts as a marker, noting whether the decorator has been called with arguments or not:\nIf joinby has been called without arguments, the decorated function will be passed in as _func. If it has been called with arguments, then _func will be None. The * in the argument list means that the remaining arguments can\u0026rsquo;t be called as positional arguments. This time you can use joinby with or without arguments and function hello and greet above demonstrate that.\nA generic pattern Personally, I find it cumbersome how you need three layers of nested functions to define a generalized decorator that can be used with or without arguments. David Beazley in his Python Cookbook shows an excellent way to define generalized decorators without writing three levels of nested functions. It uses the built in functools.partial function to achieve that. The following is a pattern you can use to define generalized decorators in a more elegant way:\nfrom functools import partial, wraps def decorator(func=None, foo=\u0026#34;spam\u0026#34;): if func is None: return partial(decorator, foo=foo) @wraps(func) def wrapper(*args, **kwargs): # Do something with `func` and `foo`, if you\u0026#39;re so inclined pass return wrapper # Applying decorator without any parameter @decorator def f(*args, **kwargs): pass # Applying decorator with extra parameter @decorator(foo=\u0026#34;buzz\u0026#34;) def f(*args, **kwargs): pass Let\u0026rsquo;s redefine our retry decorator using this pattern.\nfrom functools import partial, wraps def retry(func=None, n_tries=4): if func is None: return partial(retry, n_tries=n_tries) @wraps(func) def wrapper(*args, **kwargs): tries = 0 while True: ret = func(*args, **kwargs) if ( ret.status_code == 500 or ret.status_code == 404 and tries \u0026lt; n_tries ): print(f\u0026#34;retrying... ({tries})\u0026#34;) tries += 1 continue break return ret return wrapper @retry def getdata(url): resp = requests.get(url) return resp @retry(n_tries=2) def getdata_(url): resp = requests.get(url) return resp resp1 = getdata(\u0026#34;https://httpbin.org/get/1\u0026#34;) print(\u0026#34;-----------------------\u0026#34;) resp2 = getdata_(\u0026#34;https://httpbin.org/get/1\u0026#34;) \u0026gt;\u0026gt;\u0026gt; retrying... (0) retrying... (1) retrying... (2) retrying... (3) ----------------------- retrying... (0) retrying... (1) In this case, you don\u0026rsquo;t have to write three level nested functions and the functools. partial takes care of that. Partials can be used to make new derived functions that have some input parameters pre-assigned.Roughly partial does the following:\ndef partial(func, *part_args): def wrapper(*extra_args): args = list(part_args) args.extend(extra_args) return func(*args) return wrapper This eliminates the need to write multiple layers of nested factory function get a generalized decorator.\nDefining decorators with classes This time, I\u0026rsquo;ll be using a class to compose a decorator. Classes can be handy to avoid nested architecture while writing decorators. Also, it can be helpful to use a class while writing stateful decorators. You can follow the pattern below to compose decorators with classes.\nimport functools class ClassDeco: def __init__(self, func): # Does the work of the \u0026#39;functools.wraps\u0026#39; in methods. functools.update_wrapper(self, func) self.func = func def __call__(self, *args, **kwargs): # You can add some code before the function call ret = self.func(*args, **kwargs) # You can also add some code after the function call return ret Let\u0026rsquo;s use the above template to write a decorator named Emphasis that will add bold tags \u0026lt;b\u0026gt;\u0026lt;/b\u0026gt;to the string output of a function.\nimport functools class Emphasis: def __init__(self, func): functools.update_wrapper(self, func) self.func = func def __call__(self, *args, **kwargs): ret = self.func(*args, **kwargs) return \u0026#34;\u0026lt;b\u0026gt;\u0026#34; + ret + \u0026#34;\u0026lt;/b\u0026gt;\u0026#34; @Emphasis def hello(name): return f\u0026#34;Hello {name}\u0026#34; print(hello(\u0026#34;Nafi\u0026#34;)) print(hello(\u0026#34;Redowan\u0026#34;)) \u0026gt;\u0026gt;\u0026gt; \u0026lt;b\u0026gt;Hello Nafi\u0026lt;/b\u0026gt; \u0026lt;b\u0026gt;Hello Redowan\u0026lt;/b\u0026gt; The __init__() method stores a reference to the function num_calls and can do other necessary initialization. The __call__() method will be called instead of the decorated function. It does essentially the same thing as the wrapper() function in our earlier examples. Note that you need to use the functools.update_wrapper() function instead of @functools.wraps.\nBefore moving on, let\u0026rsquo;s write a stateful decorator using classes. Stateful decorators can remember the state of their previous run. Here\u0026rsquo;s a stateful decorator called Tally that\u0026rsquo;ll keep track of the number of times decorated functions are called in a dictionary. The keys of the dictionary will hold the names of the functions and the corresponding values will hold the call count.\nimport functools class Tally: def __init__(self, func): functools.update_wrapper(self, func) self.func = func self.tally = {} self.n_calls = 0 def __call__(self, *args, **kwargs): self.n_calls += 1 self.tally[self.func.__name__] = self.n_calls print(\u0026#34;Callable Tally:\u0026#34;, self.tally) return self.func(*args, **kwargs) @Tally def hello(name): return f\u0026#34;Hello {name}!\u0026#34; print(hello(\u0026#34;Redowan\u0026#34;)) print(hello(\u0026#34;Nafi\u0026#34;)) \u0026gt;\u0026gt;\u0026gt; Callable Tally: {\u0026#39;hello\u0026#39;: 1} Hello Redowan! Callable Tally: {\u0026#39;hello\u0026#39;: 2} Hello Nafi! A few more examples Caching return values Decorators can provide an elegant way of memoizing function return values. Imagine you have an expensive API and you\u0026rsquo;d like call that as few times as possible. The idea is to save and cache values returned by the API for particular arguments, so that if those arguments appear again, you can serve the results from the cache instead of calling the API again. This can dramatically improve your applications\u0026rsquo; performance. Here I\u0026rsquo;ve simulated an expensive API call and provided caching with a decorator.\nimport time def api(a): \u0026#34;\u0026#34;\u0026#34;API takes an integer and returns the square value of it. To simulate a time consuming process, I\u0026#39;ve added some time delay to it. \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;The API has been called...\u0026#34;) # This will delay 3 seconds time.sleep(3) return a * a api(3) \u0026gt;\u0026gt;\u0026gt; The API has been called... 9 You\u0026rsquo;ll see that running this function takes roughly 3 seconds. To cache the result, we can use Python\u0026rsquo;s built in functools.lru_cache to save the result against an argument in a dictionary and serve that when it encounters the same argument again. The only drawback here is, all the arguments need to be hashable.\nimport functools @functools.lru_cache(maxsize=32) def api(a): \u0026#34;\u0026#34;\u0026#34;API takes an integer and returns the square value of it. To simulate a time consuming process, I\u0026#39;ve added some time delay to it. \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;The API has been called...\u0026#34;) # This will delay 3 seconds time.sleep(3) return a * a api(3) \u0026gt;\u0026gt;\u0026gt; 9 Least Recently Used (LRU) Cache organizes items in order of use, allowing you to quickly identify which item hasn\u0026rsquo;t been used for the longest amount of time. In the above case, the parameter max_size refers to the maximum numbers of responses to be saved up before it starts deleting the earliest ones. While you run the decorated function, you\u0026rsquo;ll see first time it\u0026rsquo;ll take roughly 3 seconds to return the result. But if you rerun the function again with the same parameter it\u0026rsquo;ll spit the result from the cache almost instantly.\nUnit Conversion The following decorator converts length from SI units to multiple other units without polluting your target function with conversion logics.\nfrom functools import wraps def convert(func=None, convert_to=None): \u0026#34;\u0026#34;\u0026#34;This converts value from meter to others.\u0026#34;\u0026#34;\u0026#34; if func is None: return partial(convert, convert_to=convert_to) @wraps(func) def wrapper(*args, **kwargs): print(f\u0026#34;Conversion unit: {convert_to}\u0026#34;) ret = func(*args, **kwargs) # Adding conversion rules if convert_to is None: return ret elif convert_to == \u0026#34;km\u0026#34;: return ret / 1000 elif convert_to == \u0026#34;mile\u0026#34;: return ret * 0.000621371 elif convert_to == \u0026#34;cm\u0026#34;: return ret * 100 elif convert_to == \u0026#34;mm\u0026#34;: return ret * 1000 else: raise ValueError(\u0026#34;Conversion unit is not supported.\u0026#34;) return wrapper Let\u0026rsquo;s use that on a function that returns the area of a rectangle.\n@convert(convert_to=\u0026#34;mile\u0026#34;) def area(a, b): return a * b area(1, 2) \u0026gt;\u0026gt;\u0026gt; Conversion unit: mile 0.001242742 Using the convert decorator on the area function shows how it prints out the transformation unit before returning the desired result. Experiment with other conversion units and see what happens.\nFunction registration The following is an example of registering logger function in Flask framework. The decorator register_logger doesn\u0026rsquo;t make any change to the decorated logger function. Rather it takes the function and registers it in a list called logger_list every time it\u0026rsquo;s invoked.\nfrom flask import Flask, request app = Flask(__name__) logger_list = [] def register_logger(func): logger_list.append(func) return func def run_loggers(request): for logger in logger_list: logger(request) @register_logger def logger(request): print(request.method, request.path) @app.route(\u0026#34;/\u0026#34;) def index(): run_loggers(request) return \u0026#34;Hello World!\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: app.run(host=\u0026#34;localhost\u0026#34;, port=\u0026#34;5000\u0026#34;) If you run the server and hit the http://localhost:5000/ url, it\u0026rsquo;ll greet you with a Hello World! message. Also you\u0026rsquo;ll able to see the printed method and path of your HTTP request on the terminal. Moreover, if you inspect the logger_list, you\u0026rsquo;ll find the registered logger there. You\u0026rsquo;ll find a lot more real life usage of decorators in the Flask framework.\nFurther reading Primer on Python decorator - Real Python Decorators in Python - Datacamp 5 reasons you need to write python decorators ","permalink":"https://rednafi.com/python/decorators/","summary":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eUpdated on 2022-02-13\u003c/em\u003e\u003c/strong\u003e: \u003cem\u003eChange functools import style.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWhen I first learned about Python decorators, using them felt like doing voodoo magic.\nDecorators can give you the ability to add new functionalities to any callable without\nactually touching or changing the code inside it. This can typically yield better\nencapsulation and help you write cleaner and more understandable code. However, \u003cem\u003edecorator\u003c/em\u003e\nis considered as a fairly advanced topic in Python since understanding and writing it\nrequires you to have command over multiple additional concepts like first class objects,\nhigher order functions, closures etc. First, I\u0026rsquo;ll try to introduce these concepts as\nnecessary and then unravel the core concept of decorator layer by layer. So let\u0026rsquo;s dive in.\u003c/p\u003e","title":"Untangling Python decorators"},{"content":"Writing concurrent code in Python can be tricky. Before you even start, you have to worry about all this icky stuff like whether the task at hand is I/O or CPU bound or whether putting the extra effort to achieve concurrency is even going to give you the boost you need. Also, the presence of Global Interpreter Lock, GIL foists further limitations on writing truly concurrent code. But for the sake of sanity, you can oversimplify it like this without being blatantly incorrect:\nIn Python, if the task at hand is I/O bound, you can use the standard library\u0026rsquo;s threading module or if the task is CPU bound then multiprocessing module can be your friend. These APIs give you a lot of control and flexibility but they come at the cost of having to write relatively low-level verbose code that adds extra layers of complexity on top of your core logic. Sometimes when the target task is complicated, it\u0026rsquo;s often impossible to avoid complexity while adding concurrency. However, a lot of simpler tasks can be made concurrent without adding too much verbosity.\nPython standard library also houses a module called the concurrent.futures. This module was added in Python 3.2 for providing the developers a high-level interface to launch asynchronous tasks. It\u0026rsquo;s a generalized abstraction layer on top of threading and multiprocessing modules for providing an interface to run tasks concurrently using pools of threads or processes. It\u0026rsquo;s the perfect tool when you just want to run a piece of eligible code concurrently and don\u0026rsquo;t need the added modularity that the threading and multiprocessing APIs expose.\nAnatomy of concurrent.futures From the official docs,\nThe concurrent.futures module provides a high-level interface for asynchronously executing callables.\nWhat it means is you can run your subroutines asynchronously using either threads or processes through a common high-level interface. Basically, the module provides an abstract class called Executor. You can\u0026rsquo;t instantiate it directly, rather you need to use one of two subclasses that it provides to run your tasks.\nExecutor (Abstract Base Class) │ ├── ThreadPoolExecutor │ │ │A concrete subclass of the Executor class to │ │manage I/O bound tasks with threading underneath │ ├── ProcessPoolExecutor │ │ │A concrete subclass of the Executor class to │ │manage CPU bound tasks with multiprocessing underneath Internally, these two classes interact with the pools and manage the workers. Futures are used for managing results computed by the workers. To use a pool of workers, an application creates an instance of the appropriate executor class and then submits them for it to run. When each task is started, a Future instance is returned. When the result of the task is needed, an application can use the Future object to block until the result is available. Various APIs are provided to make it convenient to wait for tasks to complete, so that the Future objects don\u0026rsquo;t need to be managed directly.\nExecutor objects Since both ThreadPoolExecutor and ProcessPoolExecutor have the same API interface, in both cases I\u0026rsquo;ll primarily talk about two methods that they provide. Their descriptions have been collected from the official docs verbatim.\nsubmit(fn, *args, **kwargs) Schedules the callable, fn, to be executed as fn(*args **kwargs) and returns a Future object representing the execution of the callable.\nwith ThreadPoolExecutor(max_workers=1) as executor: future = executor.submit(pow, 323, 1235) print(future.result()) map(func, *iterables, timeout=None, chunksize=1) Similar to map(func, *iterables) except:\nthe iterables are collected immediately rather than lazily;\nfunc is executed asynchronously and several calls to func may be made concurrently.\nThe returned iterator raises a concurrent.futures.TimeoutError if __next__() is called and the result isn\u0026rsquo;t available after timeout seconds from the original call to Executor.map(). Timeout can be an int or a float. If timeout is not specified or None, there is no limit to the wait time.\nIf a func call raises an exception, then that exception will be raised when its value is retrieved from the iterator.\nWhen using ProcessPoolExecutor, this method chops iterables into a number of chunks which it submits to the pool as separate tasks. The (approximate) size of these chunks can be specified by setting chunksize to a positive integer. For very long iterables, using a large value for chunksize can significantly improve performance compared to the default size of 1. With ThreadPoolExecutor, chunksize has no effect.\nGeneric workflows for running tasks concurrently A lot of my scripts contains some variants of the following:\nfor task in get_tasks(): perform(task) Here, get_tasks returns an iterable that contains the target tasks or arguments on which a particular task function needs to applied. Tasks are usually blocking callables and they run one after another, with only one task running at a time. The logic is simple to reason with because of its sequential execution flow. This is fine when the number of tasks is small or the execution time requirement and complexity of the individual tasks is low. However, this can quickly get out of hands when the number of tasks is huge or the individual tasks are time consuming.\nA general rule of thumb is using ThreadPoolExecutor when the tasks are primarily I/O bound, like — sending multiple http requests to many urls, saving a large number of files to disk etc. ProcessPoolExecutor should be used in tasks that are primarily CPU bound, like — running callables that are computation heavy, applying pre-process methods over a large number of images, manipulating many text files at once etc.\nRunning tasks with executor.submit When you have a number of tasks, you can schedule them in one go and wait for them all to complete and then you can collect the results.\nimport concurrent.futures with concurrent.futures.Executor() as executor: futures = {executor.submit(perform, task) for task in get_tasks()} for fut in concurrent.futures.as_completed(futures): print(f\u0026#34;The outcome is {fut.result()}\u0026#34;) Here you start by creating an Executor, which manages all the tasks that are running — either in separate processes or threads. Using the with statement creates a context manager, which ensures any stray threads or processes get cleaned up via calling the executor.shutdown() method implicitly when you\u0026rsquo;re done.\nIn real code, you\u0026rsquo;d would need to replace the Executor with ThreadPoolExecutor or a ProcessPoolExecutor depending on the nature of the callables. Then a set comprehension has been used here to start all the tasks. The executor.submit() method schedules each task. This creates a Future object, which represents the task to be done. Once all the tasks have been scheduled, the method concurrent.futures_as_completed() is called, which yields the futures as they\u0026rsquo;re done – that is, as each task completes. The fut.result() method gives you the return value of perform(task), or throws an exception in case of failure.\nThe executor.submit() method schedules the tasks asynchronously and doesn\u0026rsquo;t hold any contexts regarding the original tasks. So if you want to map the results with the original tasks, you need to track those yourself.\nimport concurrent.futures with concurrent.futures.Executor() as executor: futures = { executor.submit(perform, task): task for task in get_tasks() } for fut in concurrent.futures.as_completed(futures): original_task = futures[fut] print(f\u0026#34;The result of {original_task} is {fut.result()}\u0026#34;) Notice the variable futures where the original tasks are mapped with their corresponding futures using a dictionary.\nRunning tasks with executor.map Another way the results can be collected in the same order they\u0026rsquo;re scheduled is via using executor.map() method.\nimport concurrent.futures with concurrent.futures.Executor() as executor: for arg, res in zip(get_tasks(), executor.map(perform, get_tasks())): print(f\u0026#34;The result of {arg} is {res}\u0026#34;) Notice how the map function takes the entire iterable at once. It spits out the results immediately rather than lazily and in the same order they\u0026rsquo;re scheduled. If any unhandled exception occurs during the operation, it\u0026rsquo;ll also be raised immediately and the execution won\u0026rsquo;t go any further.\nIn Python 3.5+, executor.map() receives an optional argument: chunksize. While using ProcessPoolExecutor, for very long iterables, using a large value for chunksize can significantly improve performance compared to the default size of 1. With ThreadPoolExecutor, chunksize has no effect.\nA few real world examples Before proceeding with the examples, let\u0026rsquo;s write a small decorator that\u0026rsquo;ll be helpful to measure and compare the execution time between concurrent and sequential code.\nimport time from functools import wraps def timeit(method): @wraps(method) def wrapper(*args, **kwargs): start_time = time.time() result = method(*args, **kwargs) end_time = time.time() print(f\u0026#34;{method.__name__} =\u0026gt; {(end_time-start_time)*1000} ms\u0026#34;) return result return wrapper The decorator can be used like this:\n@timeit def func(n): return list(range(n)) This will print out the name of the method and how long it took to execute it.\nDownload \u0026amp; save files from URLs with multi-threading First, let\u0026rsquo;s download some pdf files from a bunch of URLs and save them to the disk. This is presumably an I/O bound task and we\u0026rsquo;ll be using the ThreadPoolExecutor class to carry out the operation. But before that, let\u0026rsquo;s do this sequentially first.\nfrom pathlib import Path import urllib.request def download_one(url): \u0026#34;\u0026#34;\u0026#34; Downloads the specified URL and saves it to disk \u0026#34;\u0026#34;\u0026#34; req = urllib.request.urlopen(url) fullpath = Path(url) fname = fullpath.name ext = fullpath.suffix if not ext: raise RuntimeError(\u0026#34;URL does not contain an extension\u0026#34;) with open(fname, \u0026#34;wb\u0026#34;) as handle: while True: chunk = req.read(1024) if not chunk: break handle.write(chunk) msg = f\u0026#34;Finished downloading {fname}\u0026#34; return msg @timeit def download_all(urls): return [download_one(url) for url in urls] if __name__ == \u0026#34;__main__\u0026#34;: urls = ( \u0026#34;http://www.irs.gov/pub/irs-pdf/f1040.pdf\u0026#34;, \u0026#34;http://www.irs.gov/pub/irs-pdf/f1040a.pdf\u0026#34;, \u0026#34;http://www.irs.gov/pub/irs-pdf/f1040ez.pdf\u0026#34;, \u0026#34;http://www.irs.gov/pub/irs-pdf/f1040es.pdf\u0026#34;, \u0026#34;http://www.irs.gov/pub/irs-pdf/f1040sb.pdf\u0026#34;, ) results = download_all(urls) for result in results: print(result) \u0026gt;\u0026gt;\u0026gt; download_all =\u0026gt; 22850.6863117218 ms ... Finished downloading f1040.pdf ... Finished downloading f1040a.pdf ... Finished downloading f1040ez.pdf ... Finished downloading f1040es.pdf ... Finished downloading f1040sb.pdf In the above code snippet, I have primary defined two functions. The download_one function downloads a pdf file from a given URL and saves it to the disk. It checks whether the file in URL has an extension and in the absence of an extension, it raises RunTimeError. If an extension is found in the file name, it downloads the file chunk by chunk and saves to the disk. The second function download_all just iterates through a sequence of URLs and applies the download_one function on each of them. The sequential code takes about 22.8 seconds to run. Now let\u0026rsquo;s see how our threaded version of the same code performs.\nfrom pathlib import Path import urllib.request from concurrent.futures import ThreadPoolExecutor, as_completed def download_one(url): \u0026#34;\u0026#34;\u0026#34; Downloads the specified URL and saves it to disk \u0026#34;\u0026#34;\u0026#34; req = urllib.request.urlopen(url) fullpath = Path(url) fname = fullpath.name ext = fullpath.suffix if not ext: raise RuntimeError(\u0026#34;URL does not contain an extension\u0026#34;) with open(fname, \u0026#34;wb\u0026#34;) as handle: while True: chunk = req.read(1024) if not chunk: break handle.write(chunk) msg = f\u0026#34;Finished downloading {fname}\u0026#34; return msg @timeit def download_all(urls): \u0026#34;\u0026#34;\u0026#34; Create a thread pool and download specified urls \u0026#34;\u0026#34;\u0026#34; with ThreadPoolExecutor(max_workers=13) as executor: return executor.map(download_one, urls, timeout=60) if __name__ == \u0026#34;__main__\u0026#34;: urls = ( \u0026#34;http://www.irs.gov/pub/irs-pdf/f1040.pdf\u0026#34;, \u0026#34;http://www.irs.gov/pub/irs-pdf/f1040a.pdf\u0026#34;, \u0026#34;http://www.irs.gov/pub/irs-pdf/f1040ez.pdf\u0026#34;, \u0026#34;http://www.irs.gov/pub/irs-pdf/f1040es.pdf\u0026#34;, \u0026#34;http://www.irs.gov/pub/irs-pdf/f1040sb.pdf\u0026#34;, ) results = download_all(urls) for result in results: print(result) \u0026gt;\u0026gt;\u0026gt; download_all =\u0026gt; 5042.651653289795 ms ... Finished downloading f1040.pdf ... Finished downloading f1040a.pdf ... Finished downloading f1040ez.pdf ... Finished downloading f1040es.pdf ... Finished downloading f1040sb.pdf The concurrent version of the code takes only about 1/4 th the time of it\u0026rsquo;s sequential counterpart. Notice in this concurrent version, the download_one function is the same as before but in the download_all function, a ThreadPoolExecutor context manager wraps the executor.map() method. The download_one function is passed into the map along with the iterable containing the URLs. The timeout parameter determines how long a thread will spend before giving up on a single task in the pipeline. The max_workers means how many worker you want to deploy to spawn and manage the threads. A general rule of thumb is using 2 * multiprocessing.cpu_count() + 1. My machine has 6 physical cores with 12 threads. So 13 is the value I chose.\nNote: You can also try running the above functions with ProcessPoolExecutor via the same interface and notice that the threaded version performs slightly better than due to the nature of the task.\nThere is one small problem with the example above. The executor.map() method returns a generator which allows to iterate through the results once ready. That means if any error occurs inside map, it\u0026rsquo;s not possible to handle that and resume the generator after the exception occurs. From PEP-255:\nIf an unhandled exception\u0026ndash; including, but not limited to, StopIteration \u0026ndash;is raised by, or passes through, a generator function, then the exception is passed on to the caller in the usual way, and subsequent attempts to resume the generator function raise StopIteration. In other words, an unhandled exception terminates a generator\u0026rsquo;s useful life.\nTo get around that, you can use the executor.submit() method to create futures, accumulated the futures in a list, iterate through the futures and handle the exceptions manually. See the following example:\nfrom pathlib import Path import urllib.request from concurrent.futures import ThreadPoolExecutor def download_one(url): \u0026#34;\u0026#34;\u0026#34; Downloads the specified URL and saves it to disk \u0026#34;\u0026#34;\u0026#34; req = urllib.request.urlopen(url) fullpath = Path(url) fname = fullpath.name ext = fullpath.suffix if not ext: raise RuntimeError(\u0026#34;URL does not contain an extension\u0026#34;) with open(fname, \u0026#34;wb\u0026#34;) as handle: while True: chunk = req.read(1024) if not chunk: break handle.write(chunk) msg = f\u0026#34;Finished downloading {fname}\u0026#34; return msg @timeit def download_all(urls): \u0026#34;\u0026#34;\u0026#34; Create a thread pool and download specified urls \u0026#34;\u0026#34;\u0026#34; futures_list = [] results = [] with ThreadPoolExecutor(max_workers=13) as executor: for url in urls: futures = executor.submit(download_one, url) futures_list.append(futures) for future in futures_list: try: result = future.result(timeout=60) results.append(result) except Exception: results.append(None) return results if __name__ == \u0026#34;__main__\u0026#34;: urls = ( \u0026#34;http://www.irs.gov/pub/irs-pdf/f1040.pdf\u0026#34;, \u0026#34;http://www.irs.gov/pub/irs-pdf/f1040a.pdf\u0026#34;, \u0026#34;http://www.irs.gov/pub/irs-pdf/f1040ez.pdf\u0026#34;, \u0026#34;http://www.irs.gov/pub/irs-pdf/f1040es.pdf\u0026#34;, \u0026#34;http://www.irs.gov/pub/irs-pdf/f1040sb.pdf\u0026#34;, ) results = download_all(urls) for result in results: print(result) The above snippet should print out similar messages as before.\nRunning multiple CPU bound subroutines with multi-processing The following example shows a CPU bound hashing function. The primary function will sequentially run a compute intensive hash algorithm multiple times. Then another function will again run the primary function multiple times. Let\u0026rsquo;s run the function sequentially first.\nimport hashlib def hash_one(n): \u0026#34;\u0026#34;\u0026#34;A somewhat CPU-intensive task.\u0026#34;\u0026#34;\u0026#34; for i in range(1, n): hashlib.pbkdf2_hmac(\u0026#34;sha256\u0026#34;, b\u0026#34;password\u0026#34;, b\u0026#34;salt\u0026#34;, i * 10000) return \u0026#34;done\u0026#34; @timeit def hash_all(n): \u0026#34;\u0026#34;\u0026#34;Function that does hashing in serial.\u0026#34;\u0026#34;\u0026#34; for i in range(n): hsh = hash_one(n) return \u0026#34;done\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: hash_all(20) \u0026gt;\u0026gt;\u0026gt; hash_all =\u0026gt; 18317.330598831177 ms If you analyze the hash_one and hash_all functions, you can see that together, they are actually running two compute intensive nested for loops. The above code takes roughly 18 seconds to run in sequential mode. Now let\u0026rsquo;s run it parallelly using ProcessPoolExecutor.\nimport hashlib from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor def hash_one(n): \u0026#34;\u0026#34;\u0026#34;A somewhat CPU-intensive task.\u0026#34;\u0026#34;\u0026#34; for i in range(1, n): hashlib.pbkdf2_hmac(\u0026#34;sha256\u0026#34;, b\u0026#34;password\u0026#34;, b\u0026#34;salt\u0026#34;, i * 10000) return \u0026#34;done\u0026#34; @timeit def hash_all(n): \u0026#34;\u0026#34;\u0026#34;Function that does hashing in serial.\u0026#34;\u0026#34;\u0026#34; with ProcessPoolExecutor(max_workers=10) as executor: for arg, res in zip( range(n), executor.map(hash_one, range(n), chunksize=2) ): pass return \u0026#34;done\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: hash_all(20) \u0026gt;\u0026gt;\u0026gt; hash_all =\u0026gt; 1673.842430114746 ms If you look closely, even in the concurrent version, the for loop in hash_one function is running sequentially. However, the other for loop in the hash_all function is being executed through multiple processes. Here, I have used 10 workers and a chunksize of 2. The number of workers and chunksize were adjusted to achieve maximum performance. As you can see the concurrent version of the above CPU intensive operation is about 11 times faster than its sequential counterpart.\nAvoiding concurrency pitfalls Since the concurrent.futures provides such a simple API, you might be tempted to apply concurrency to every simple tasks at hand. However, that\u0026rsquo;s not a good idea. First, the simplicity has its fair share of constraints. In this way, you can apply concurrency only to the simplest of the tasks, usually mapping a function to an iterable or running a few subroutines simultaneously. If your task at hand requires queuing, spawning multiple threads from multiple processes then you will still need to resort to the lower level threading and multiprocessing modules.\nAnother pitfall of using concurrency is deadlock situations that might occur while using ThreadPoolExecutor. When a callable associated with a Future waits on the results of another Future, they might never release their control of the threads and cause deadlock. Let\u0026rsquo;s see a slightly modified example from the official docs.\nimport time from concurrent.futures import ThreadPoolExecutor def wait_on_b(): time.sleep(5) print(b.result()) # b will never complete because it is waiting on a. return 5 def wait_on_a(): time.sleep(5) print(a.result()) # a will never complete because it is waiting on b. return 6 with ThreadPoolExecutor(max_workers=2) as executor: # here, the future from a depends on the future from b # and vice versa # so this is never going to be completed a = executor.submit(wait_on_b) b = executor.submit(wait_on_a) print(\u0026#34;Result from wait_on_b\u0026#34;, a.result()) print(\u0026#34;Result from wait_on_a\u0026#34;, b.result()) In the above example, function wait_on_b depends on the result (result of the Future object) of function wait_on_a and at the same time the later function\u0026rsquo;s result depends on that of the former function. So the code block in the context manager will never execute due to having inter dependencies. This creates the deadlock situation. Let\u0026rsquo;s explain another deadlock situation from the official docs.\nfrom concurrent.futures import ThreadPoolExecutor def wait_on_future(): f = executor.submit(pow, 5, 2) # This will never complete because there is only one worker thread and # it is executing this function. print(f.result()) with ThreadPoolExecutor(max_workers=1) as executor: future = executor.submit(wait_on_future) print(future.result()) The above situation usually happens when a subroutine produces nested Future object and runs on a single thread. In the function wait_on_future, the executor.submit(pow, 5, 2) creates another Future object. Since I\u0026rsquo;m running the entire thing using a single thread, the internal future object is blocking the thread and the external executor.submit() method inside the context manager can not use any threads. This situation can be avoided using multiple threads but in general, this is a bad design itself.\nThen there\u0026rsquo;re situations when you might be getting lower performance with concurrent code than its sequential counterpart. This could happen for multiple reasons:\nThreads were used to perform CPU bound tasks. Multiprocessing were used to perform I/O bound tasks. The tasks were too trivial to justify using either threads or multiple processes. Spawning and squashing multiple threads or processes bring extra overheads. Usually threads are much faster than processes to spawn and squash. However, using the wrong type of concurrency can actually slow down your code rather than making it any performant. Below is a trivial example where both ThreadPoolExecutor and ProcessPoolExecutor perform worse than their sequential counterpart.\nimport math PRIMES = [num for num in range(19000, 20000)] def is_prime(n): if n \u0026lt; 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True @timeit def main(): for number in PRIMES: print(f\u0026#34;{number} is prime: {is_prime(number)}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() \u0026gt;\u0026gt;\u0026gt; 19088 is prime: False ... 19089 is prime: False ... 19090 is prime: False ... ... ... main =\u0026gt; 67.65174865722656 ms The above examples verifies whether a number in a list is prime or not. We ran the function on 1000 numbers to determine if they\u0026rsquo;re prime or not. The sequential version took roughly 67ms to do that. However, look below where the threaded version of the same code takes more than double the time (140ms) to so the same task.\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor import math num_list = [num for num in range(19000, 20000)] def is_prime(n): if n \u0026lt; 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True @timeit def main(): with ThreadPoolExecutor(max_workers=13) as executor: for number, prime in zip(PRIMES, executor.map(is_prime, num_list)): print(f\u0026#34;{number} is prime: {prime}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() \u0026gt;\u0026gt;\u0026gt; 19088 is prime: False ... 19089 is prime: False ... 19090 is prime: False ... ... ... main =\u0026gt; 140.17250061035156 ms The multiprocessing version of the same code is even slower. The tasks doesn\u0026rsquo;t justify opening so many processes.\nfrom concurrent.futures import ProcessPoolExecutor import math num_list = [num for num in range(19000, 20000)] def is_prime(n): if n \u0026lt; 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True @timeit def main(): with ProcessPoolExecutor(max_workers=13) as executor: for number, prime in zip(PRIMES, executor.map(is_prime, num_list)): print(f\u0026#34;{number} is prime: {prime}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() \u0026gt;\u0026gt;\u0026gt; 19088 is prime: False ... 19089 is prime: False ... 19090 is prime: False ... ... ... main =\u0026gt; 311.3126754760742 ms Although intuitively, it may seem like the task of checking prime numbers should be a CPU bound operation, it\u0026rsquo;s also important to determine if the task itself is computationally heavy enough to justify spawning multiple threads or processes. Otherwise, you might end up with complicated code that performs worse than the naive solution.\nFurther reading concurrent.futures - official docs Easy concurrency in Python Adventures in Python with concurrent.futures ","permalink":"https://rednafi.com/python/concurrent-futures/","summary":"\u003cp\u003eWriting concurrent code in Python can be tricky. Before you even start, you have to worry\nabout all this icky stuff like whether the task at hand is I/O or CPU bound or whether\nputting the extra effort to achieve concurrency is even going to give you the boost you\nneed. Also, the presence of Global Interpreter Lock, \u003ca href=\"https://wiki.python.org/moin/GlobalInterpreterLock\"\u003eGIL\u003c/a\u003e foists further limitations on\nwriting truly concurrent code. But for the sake of sanity, you can oversimplify it like this\nwithout being blatantly incorrect:\u003c/p\u003e","title":"Effortless concurrency with Python's concurrent.futures"},{"content":"When I first encountered Python\u0026rsquo;s pathlib module for path manipulation, I brushed it aside assuming it to be just an OOP way of doing what os.path already does quite well. The official doc also dubs it as the Object-oriented filesystem paths. However, back in 2019 when an issue confirmed that Django was replacing os.path with pathlib, I got curious.\nThe os.path module has always been the de facto standard for working with paths in Python. But the API can feel massive as it performs a plethora of other loosely coupled system related jobs. I\u0026rsquo;ve to look things up constantly even to perform some of the most basic tasks like joining multiple paths, listing all the files in a folder having a particular extension, opening multiple files in a directory etc. The pathlib module can do nearly everything that os.path offers and comes with some additional cherries on top.\nProblem with Python\u0026rsquo;s path handling Traditionally, Python has represented file paths as regular text strings. So far, using paths as strings with os.path module has been adequate although a bit cumbersome. However, paths aren\u0026rsquo;t actually strings and this has necessitated the usage of multiple modules to provide disparate functionalities that are scattered all around the standard library, including libraries like os, glob, and shutil. The following code uses three modules just to copy multiple python files from current directory to another directory called src:\nfrom glob import glob import os import shutil for fname in glob(\u0026#34;*.py\u0026#34;): new_path = os.path.join(\u0026#34;src\u0026#34;, fname) shutil.copy(fname, new_path) The above pattern can get complicated fairly quickly and you have to know or look for specific modules and methods in a large search space to perform your path manipulations. Let\u0026rsquo;s have a look at a few more examples of performing the same tasks using os.path and pathlib modules.\nJoining \u0026amp; creating new paths Say you want to achieve the following goals:\nThere is a file named file.txt in your current directory and you want to create the path for another file named file_another.txt in the same directory. Then you want to save the absolute path of file_another.txt in a new variable. Let\u0026rsquo;s see how you\u0026rsquo;d usually do this via the os module.\nfrom os.path import abspath, dirname, join file_path = abspath(\u0026#34;./file.txt\u0026#34;) base_dir = dirname(file_path) file_another_path = join(base_dir, \u0026#34;file_another.txt\u0026#34;) The variables file_path, base_dir, file_another_path look like this on my machine:\nprint(\u0026#34;file_path:\u0026#34;, file_path) print(\u0026#34;base_dir:\u0026#34;, base_dir) print(\u0026#34;file_another_path:\u0026#34;, file_another_path) \u0026gt;\u0026gt;\u0026gt; file_path: /home/rednafi/code/demo/file.txt \u0026gt;\u0026gt;\u0026gt; base_dir: /home/rednafi/code/demo \u0026gt;\u0026gt;\u0026gt; file_another_path: /home/rednafi/code/demo/file_another.txt You can use the usual string methods to transform the paths but generally, that\u0026rsquo;s not a good idea. So, instead of joining two paths with + like regular strings, you should use os.path.join() to join the components of a path. This is because different operating systems don\u0026rsquo;t define paths in the same way. Windows uses \u0026quot;\\\u0026quot; while Mac and *nix based OSes use \u0026quot;/\u0026quot; as a separator. Joining with os.path.join() ensures correct path separator on the corresponding operating system. Pathlib module uses \u0026quot;/\u0026quot; operator overloading and make this a little less painful.\nfrom pathlib import Path file_path = Path(\u0026#34;file.txt\u0026#34;).resolve() base_dir = file_path.parent file_another_path = base_dir / \u0026#34;another_file.txt\u0026#34; print(\u0026#34;file_path:\u0026#34;, file_path) print(\u0026#34;base_dir:\u0026#34;, base_dir) print(\u0026#34;file_another_path:\u0026#34;, file_another_path) \u0026gt;\u0026gt;\u0026gt; file_path: /home/rednafi/code/demo/file.txt \u0026gt;\u0026gt;\u0026gt; base_dir: /home/rednafi/code/demo \u0026gt;\u0026gt;\u0026gt; file_another_path: /home/rednafi/code/demo/file_another.txt The resolve method finds out the absolute path of the file. From there you can use the parent method to find out the base directory and add the another_file.txt accordingly.\nMaking directories \u0026amp; renaming files Here\u0026rsquo;s a piece of code that:\nTries to make a src/stuff/ directory when it already exists. Renames a file in the src directory called .config to .stuffconfig. import os import os.path os.makedirs(os.path.join(\u0026#34;src\u0026#34;, \u0026#34;stuff\u0026#34;), exist_ok=True) os.rename(\u0026#34;src/.config\u0026#34;, \u0026#34;src/.stuffconfig\u0026#34;) Here is the same thing done using the pathlib module:\nfrom pathlib import Path Path(\u0026#34;src/stuff\u0026#34;).mkdir(parents=True, exist_ok=True) Path(\u0026#34;src/.config\u0026#34;).rename(\u0026#34;src/.stuffconfig\u0026#34;) \u0026gt;\u0026gt;\u0026gt; PosixPath(\u0026#39;src/.stuffconfig\u0026#39;) Notice the output where the renamed file path is printed. It\u0026rsquo;s not a simple string, rather a PosixPath object that indicates the type of host system (Linux in this case). You can almost always use stringified path values and the Path objects interchangeably.\nListing specific types of files in a directory Let\u0026rsquo;s say you want to recursively visit nested directories and list .py files in a directory called source. The directory looks like this:\nsrc/ ├── stuff │ ├── __init__.py │ └── submodule.py ├── .stuffconfig ├── somefiles.tar.gz └── module.py Usually, glob module is used to resolve this kind of situation:\nfrom glob import glob top_level_py_files = glob(\u0026#34;src/*.py\u0026#34;) all_py_files = glob(\u0026#34;src/**/*.py\u0026#34;, recursive=True) print(top_level_py_files) print(all_py_files) \u0026gt;\u0026gt;\u0026gt; [\u0026#39;src/module.py\u0026#39;] \u0026gt;\u0026gt;\u0026gt; [\u0026#39;src/module.py\u0026#39;, \u0026#39;src/stuff/__init__.py\u0026#39;, \u0026#39;src/stuff/submodule.py\u0026#39;] The above approach works perfectly. However, if you don\u0026rsquo;t want to use another module just for a single job, pathlib has embedded glob and rglob methods. You can entirely ignore glob and achieve the same result in the following way:\nfrom pathlib import Path top_level_py_files = Path(\u0026#34;src\u0026#34;).glob(\u0026#34;*.py\u0026#34;) all_py_files = Path(\u0026#34;src\u0026#34;).rglob(\u0026#34;*.py\u0026#34;) print(list(top_level_py_files)) print(list(all_py_files)) This will also print the same as before:\n\u0026gt;\u0026gt;\u0026gt; [PosixPath(\u0026#39;src/module.py\u0026#39;)] \u0026gt;\u0026gt;\u0026gt; [PosixPath(\u0026#39;src/module.py\u0026#39;), PosixPath(\u0026#39;src/stuff/__init__.py\u0026#39;), PosixPath(\u0026#39;src/stuff/submodule.py\u0026#39;)] By default, both Path.glob and Path.rglob returns a generator object. Calling list on them gives you the desired result. Notice how rglob method can discover the desired files without you having to mention the directory structure with wildcards explicitly. Pretty neat, huh?\nOpening multiple files \u0026amp; reading their contents Now let\u0026rsquo;s open the .py files and read their contents that you recursively discovered in the previous example:\nfrom glob import glob contents = [] for fname in glob(\u0026#34;src/**/*.py\u0026#34;, recursive=True): with open(fname, \u0026#34;r\u0026#34;) as f: contents.append(f.read()) print(contents) \u0026gt;\u0026gt;\u0026gt; [\u0026#39;from contextlib ...\u0026#39;] The pathlib implementation is almost identical as above:\nfrom pathlib import Path contents = [] for fname in Path(\u0026#34;src\u0026#34;).rglob(\u0026#34;*.py\u0026#34;): with open(fname, \u0026#34;r\u0026#34;) as f: contents.append(f.read()) print(contents) \u0026gt;\u0026gt;\u0026gt; [\u0026#39;from contextlib import ...\u0026#39;] You can also cook up a more robust implementation with generator comprehension and context manager:\nfrom contextlib import ExitStack from pathlib import Path # ExitStack ensures all files are properly closed after o/p with ExitStack() as stack: streams = ( stack.enter_context(open(fname, \u0026#34;r\u0026#34;)) for fname in Path(\u0026#34;src\u0026#34;).rglob(\u0026#34;*.py\u0026#34;) ) contents = [f.read() for f in streams] print(contents) \u0026gt;\u0026gt;\u0026gt; [\u0026#39;from contextlib import ...\u0026#39;] Anatomy of the pathlib module Primarily, pathlib has two high-level components, pure path and concrete path. Pure paths are absolute Path objects that can be instantiated regardless of the host operating system. On the other hand, to instantiate a concrete path, you need to be on the specific type of host expected by the class. These two high level components are made out of six individual classes internally coupled by inheritance. They are:\nPurePath (Useful when you want to work with windows path on a Linux machine) PurePosixPath (Subclass of PurePath) PureWindowsPath (Subclass of PurePath) Path (Concrete path object, most of the time, you\u0026rsquo;ll be dealing with this one) PosixPath (Concrete posix path, subclass of Path) WindowsPath (Concrete windows path, subclass of Path) This UML diagram from the official docs does a better job at explaining the internal relationships between the component classes.\nUnless you are doing cross platform path manipulation, most of the time you\u0026rsquo;ll be working with the concrete Path object. So I\u0026rsquo;ll focus on the methods and properties of Path class only.\nOperators Instead of using os.path.join you can use / operator to create child paths.\nfrom pathlib import Path base_dir = Path(\u0026#34;src\u0026#34;) child_dir = base_dir / \u0026#34;stuff\u0026#34; file_path = child_dir / \u0026#34;__init__.py\u0026#34; print(file_path) \u0026gt;\u0026gt;\u0026gt; PosixPath(\u0026#39;src/stuff/__init__.py\u0026#39;) Attributes \u0026amp; methods The following tree shows an inexhaustive list of attributes and methods that are associated with Path object. I have cherry picked some of the attributes and methods that I use most of the time while doing path manipulation. Head over to the official docs for a more detailed list. We\u0026rsquo;ll linearly traverse through the tree and provide necessary examples to grasp their usage.\nPath │ ├── Attributes │ ├── parts │ ├── parent \u0026amp; parents │ ├── name │ ├── suffix \u0026amp; suffixes │ └── stem │ │ └── Methods ├── joinpath(*other) ├── cwd() ├── home() ├── exists() ├── expanduser() ├── glob() ├── rglob(pattern) ├── is_dir() ├── is_file() ├── is_absolute() ├── iterdir() ├── mkdir(mode=0o777, parents=False, exist_ok=False) ├── open(mode=\u0026#39;r\u0026#39;, buffering=-1, encoding=None, ...) ├── rename(target) ├── replace(target) ├── resolve(strict=False) └── rmdir() Let\u0026rsquo;s dive into their usage one by one. For all the examples, We\u0026rsquo;ll be using the previously seen directory structure.\nsrc/ ├── stuff │ ├── __init__.py │ └── submodule.py ├── .stuffconfig ├── somefile.tar.gz └── module.py Path.parts Returns a tuple containing individual components of a path.\nfrom pathlib import Path file_path = Path(\u0026#34;src/stuff/__init__.py\u0026#34;) file_path.parts \u0026gt;\u0026gt;\u0026gt; (\u0026#39;src\u0026#39;, \u0026#39;stuff\u0026#39;, \u0026#39;__init__.py\u0026#39;) Path.parents \u0026amp; Path.parent Path.parents returns an immutable sequence containing the all logical ancestors of the path. While Path.parent returns the immediate predecessor of the path.\nfile_path = Path(\u0026#34;src/stuff/__init__.py\u0026#34;) for parent in file_path.parents: print(parent) \u0026gt;\u0026gt;\u0026gt; src/stuff ... src ... . file_path.parent \u0026gt;\u0026gt;\u0026gt; PosixPath(\u0026#39;src/stuff\u0026#39;) Path.name Returns the last component of a path as string. Usually used to extract file name from a path.\nfrom pathlib import Path file_path = Path(\u0026#34;src/module.py\u0026#34;) file_path.name \u0026gt;\u0026gt;\u0026gt; \u0026#39;module.py\u0026#39; Path.suffixes \u0026amp; Path.suffix Path.suffixes returns a list of extensions of the final component. Path.suffix only returns the last extension.\nfrom pathlib import Path file_path = Path(\u0026#34;src/stuff/somefile.tar.gz\u0026#34;) file_path.suffixes \u0026gt;\u0026gt;\u0026gt; [\u0026#39;.tar\u0026#39;, \u0026#39;.gz\u0026#39;] file_path.suffix \u0026gt;\u0026gt;\u0026gt;\u0026#39;.gz\u0026#39; Path.stem Returns the final path component without the suffix.\nfrom pathlib import Path file_path = Path(\u0026#34;src/stuff/somefile.tar.gz\u0026#34;) file_path.stem \u0026gt;\u0026gt;\u0026gt; \u0026#39;somefile.tar\u0026#39; Path.is_absolute Checks if a path is absolute or not. Return boolean value.\nfrom pathlib import Path file_path = Path(\u0026#34;src/stuff/somefile.tar.gz\u0026#34;) file_path.is_absolute() \u0026gt;\u0026gt;\u0026gt; False Path.joinpath(*other) This method is used to combine multiple components into a complete path. This can be used as an alternative to \u0026quot;/\u0026quot; operator for joining path components.\nfrom pathlib import Path file_path = Path(\u0026#34;src\u0026#34;).joinpath(\u0026#34;stuff\u0026#34;).joinpath(\u0026#34;__init__.py\u0026#34;) file_path \u0026gt;\u0026gt;\u0026gt; PosixPath(\u0026#39;src/stuff/__init__.py\u0026#39;) Path.cwd() Returns the current working directory.\nfrom pathlib import Path file_path = Path(\u0026#34;src/stuff/somefile.tar.gz\u0026#34;) file_path.cwd() \u0026gt;\u0026gt;\u0026gt; PosixPath(\u0026#39;/home/rednafi/code/demo\u0026#39;) Path.home() Returns home directory.\nfrom pathlib import Path Path.home() \u0026gt;\u0026gt;\u0026gt; PosixPath(\u0026#39;/home/rednafi\u0026#39;) Path.exists() Checks if a path exists or not. Returns boolean value.\nfrom pathlib import Path file_path = Path(\u0026#34;src/stuff/thisisabsent.py\u0026#34;) file_path.exists() \u0026gt;\u0026gt;\u0026gt; False Path.expanduser() Returns a new path with expanded ~ symbol.\nfrom pathlib import Path file_path = Path(\u0026#34;~/code/demo/src/stuff/somefile.tar.gz\u0026#34;) file_path.expanduser() \u0026gt;\u0026gt;\u0026gt; PosixPath(\u0026#39;/home/rednafi/code/demo/src/stuff/somefile.tar.gz\u0026#39;) Path.glob() Globs and yields all file paths matching a specific pattern. Let\u0026rsquo;s discover all the files in src/stuff/ directory that have .py extension.\nfrom pathlib import Path dir_path = Path(\u0026#34;src/stuff/\u0026#34;) file_paths = dir_path.glob(\u0026#34;*.py\u0026#34;) print(list(file_paths)) \u0026gt;\u0026gt;\u0026gt; [PosixPath(\u0026#39;src/stuff/__init__.py\u0026#39;), PosixPath(\u0026#39;src/stuff/submodule.py\u0026#39;)] Path.rglob(pattern) This is like Path.glob method but matches the file pattern recursively.\nfrom pathlib import Path dir_path = Path(\u0026#34;src\u0026#34;) file_paths = dir_path.rglob(\u0026#34;*.py\u0026#34;) print(list(file_paths)) \u0026gt;\u0026gt;\u0026gt; [PosixPath(\u0026#39;src/module.py\u0026#39;), PosixPath(\u0026#39;src/stuff/__init__.py\u0026#39;), PosixPath(\u0026#39;src/stuff/submodule.py\u0026#39;)] Path.is_dir() Checks if a path points to a directory or not. Returns boolean value.\nfrom pathlib import Path dir_path = Path(\u0026#34;src/stuff/\u0026#34;) dir_path.is_dir() \u0026gt;\u0026gt;\u0026gt; True Path.is_file() Checks if a path points to a file. Returns boolean value.\nfrom pathlib import Path dir_path = Path(\u0026#34;src/stuff/\u0026#34;) dir_path.is_file() \u0026gt;\u0026gt;\u0026gt; False Path.is_absolute() Checks if a path is absolute or relative. Returns boolean value.\nfrom pathlib import Path dir_path = Path(\u0026#34;src/stuff/\u0026#34;) dir_path.is_absolute() \u0026gt;\u0026gt;\u0026gt; False Path.iterdir() When the path points to a directory, this yields the content path objects.\nfrom pathlib import Path base_path = Path(\u0026#34;src\u0026#34;) contents = [content for content in base_path.iterdir()] print(contents) \u0026gt;\u0026gt;\u0026gt; [PosixPath(\u0026#39;src/stuff\u0026#39;), PosixPath(\u0026#39;src/module.py\u0026#39;), PosixPath(\u0026#39;src/.stuffconfig\u0026#39;)] Path.mkdir(mode=0o777, parents=False, exist_ok=False) Creates a new directory at this given path.\nParameters:\nmode:(str) Posix permissions (mimicking the POSIX mkdir -p command)\nparents:(boolean) If parents is True, any missing parents of this path are created as needed. Otherwise, if the parent is absent, FileNotFoundError is raised.\nexist_ok: (boolean) If False, FileExistsError is raised if the target directory already exists. If True, FileExistsError is ignored.\nfrom pathlib import Path dir_path = Path(\u0026#34;src/other/side\u0026#34;) dir_path.mkdir(parents=True) Path.open(mode=\u0026lsquo;r\u0026rsquo;, buffering=-1, encoding=None, errors=None, newline=None) This is same as the built in open function.\nfrom pathlib import Path with Path(\u0026#34;src/module.py\u0026#34;) as f: contents = open(f, \u0026#34;r\u0026#34;) for line in contents: print(line) \u0026gt;\u0026gt;\u0026gt; from contextlib import contextmanager ... from time import time ... ... Path.rename(target) Renames this file or directory to the given target and returns a new Path instance pointing to target. This will raise FileNotFoundError if the file is not found.\nfrom pathlib import Path file_path = Path(\u0026#34;src/stuff/submodule.py\u0026#34;) file_path.rename(file_path.parent / \u0026#34;anothermodule.py\u0026#34;) \u0026gt;\u0026gt;\u0026gt; PosixPath(\u0026#39;src/stuff/anothermodule.py\u0026#39;) Path.replace(target) Replaces a file or directory to the given target. Returns the new path instance.\nfrom pathlib import Path file_path = Path(\u0026#34;src/stuff/anothermodule.py\u0026#34;) file_path.replace(file_path.parent / \u0026#34;Dockerfile\u0026#34;) \u0026gt;\u0026gt;\u0026gt; PosixPath(\u0026#39;src/stuff/Dockerfile\u0026#39;) Path.resolve(strict=False) Make the path absolute, resolving any symlinks. A new path object is returned. If strict is True and the path doesn\u0026rsquo;t exist, FileNotFoundError will be raised.\nfrom pathlib import Path file_path = Path(\u0026#34;src/./stuff/Dockerfile\u0026#34;) file_path.resolve() \u0026gt;\u0026gt;\u0026gt; PosixPath(\u0026#39;/home/rednafi/code/demo/src/stuff/Dockerfile\u0026#39;) Path.rmdir() Removes a path pointing to a file or directory. The directory must be empty, otherwise, OSError is raised.\nfrom pathlib import Path file_path = Path(\u0026#34;src/stuff\u0026#34;) file_path.rmdir() So, should you use it? Pathlib was introduced in python 3.4. However, if you are working with python 3.5 or earlier, in some special cases, you might have to convert pathlib.Path objects to regular strings. But since python 3.6, Path objects work almost everywhere you are using stringified paths. Also, the Path object nicely abstracts away the complexity that arises while working with paths in different operating systems.\nThe ability to manipulate paths in an OO way and not having to rummage through the massive os or shutil module can make path manipulation a lot less painful.\nFurther reading pathlib - Object-oriented filesystem paths Python 3\u0026rsquo;s pathlib Module: Taming the File System Why you should be using pathlib ","permalink":"https://rednafi.com/python/pathlib/","summary":"\u003cp\u003eWhen I first encountered Python\u0026rsquo;s \u003ccode\u003epathlib\u003c/code\u003e module for path manipulation, I brushed it aside\nassuming it to be just an OOP way of doing what \u003ccode\u003eos.path\u003c/code\u003e already does quite well. The\nofficial doc also dubs it as the \u003ccode\u003eObject-oriented filesystem paths\u003c/code\u003e. However, back in 2019\nwhen \u003ca href=\"https://code.djangoproject.com/ticket/29983\"\u003ean issue\u003c/a\u003e confirmed that Django was replacing \u003ccode\u003eos.path\u003c/code\u003e with \u003ccode\u003epathlib\u003c/code\u003e, I got curious.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003eos.path\u003c/code\u003e module has always been the de facto standard for working with paths in Python.\nBut the API can feel massive as it performs a plethora of other loosely coupled system\nrelated jobs. I\u0026rsquo;ve to look things up constantly even to perform some of the most basic tasks\nlike joining multiple paths, listing all the files in a folder having a particular\nextension, opening multiple files in a directory etc. The \u003ccode\u003epathlib\u003c/code\u003e module can do nearly\neverything that \u003ccode\u003eos.path\u003c/code\u003e offers and comes with some additional cherries on top.\u003c/p\u003e","title":"No really, Python's pathlib is great"},{"content":"Pre-commit hooks can be a neat way to run automated ad-hoc tasks before submitting a new git commit. These tasks may include linting, trimming trailing whitespaces, running code formatter before code reviews etc. Let\u0026rsquo;s see how multiple Python linters and formatters can be applied automatically before each commit to impose strict conformity on your codebase.\nTo keep my sanity, I only use three linters in all of my python projects:\nIsort: Isort is a Python utility to sort imports alphabetically, and automatically separate them by sections and type. It parses specified files for global level import lines and puts them all at the top of the file grouped together by the type of import:\nFuture\nPython Standard Library\nThird Party\nCurrent Python Project\nExplicitly Local (. before import, as in: from . import x)\nCustom Separate Sections (Defined by forced_separate list in the configuration file)\nCustom Sections (Defined by sections list in configuration file)\nInside each section, the imports are sorted alphabetically. This also automatically removes duplicate python imports, and wraps long from imports to the specified line length (defaults to 79).\nBlack: Black is the uncompromising Python code formatter. It uses consistent rules to format your python code and makes sure that they look the same regardless of the project you\u0026rsquo;re reading.\nFlake8: Flake8 is a wrapper around PyFlakes, pycodestyle, Ned Batchelder\u0026rsquo;s McCabe script. The combination of these three linters makes sure that your code is compliant with PEP-8 and free of some obvious code smells.\nInstalling pre-commit Install using pip:\npip install pre-commit Install via curl:\ncurl https://pre-commit.com/install-local.py | python - Defining the pre-commit config file Pre-commit configuration is a .pre-commit-config.yaml file where you define your hooks (tasks) that you want to run before every commit. Once you have defined your hooks in the config file, they will run automatically every time you say git commit -m \u0026quot;Commit message\u0026quot;. The following example shows how black and a few other linters can be added as hooks to the config:\n# .pre-commit-config.yaml repos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: check-yaml - id: end-of-file-fixer - id: trailing-whitespace - repo: https://github.com/psf/black rev: 19.3b0 hooks: - id: black Installing the git hook scripts Run:\npre-commit install This will set up the git hook scripts and should show the following output in your terminal:\npre-commit installed at .git/hooks/pre-commit Now you\u0026rsquo;ll be able to implicitly or explicitly run the hooks before each commit.\nRunning the hooks against all the files By default, the hooks will run every time you say:\ngit commit -m \u0026#34;Commit message\u0026#34; However, if you wish to run the hooks manually on every file, you can do so via:\npre-commit run --all-files Running the linters as pre-commit hooks To run the above mentioned linters as pre-commit hooks, you need to add their respective settings to the .pre-commit-config.yaml file. However, there\u0026rsquo;re a few minor issues that need to be taken care of.\nThe default line length of black formatter is 88 (you should embrace that) but flake8 caps the line at 79 characters. This raises conflict and can cause failures.\nFlake8 can be overly strict at times. You\u0026rsquo;ll want to ignore basic errors like unused imports, spacing issues etc. However, since your IDE / editor also points out these issues anyway, you should solve them manually. You will need to configure flake8 to ignore some of these minor errors.\nThe following one is an example of how you can define your .pre-commit-config.yaml and configure the individual hooks so that isort, black, flake8 linters can run without any conflicts.\n# .pre-commit-config.yaml # isort - repo: https://github.com/asottile/seed-isort-config rev: v1.9.3 hooks: - id: seed-isort-config - repo: https://github.com/pre-commit/mirrors-isort rev: v4.3.21 hooks: - id: isort # black - repo: https://github.com/ambv/black rev: stable hooks: - id: black args: # arguments to configure black - --line-length=88 - --include=\u0026#39;\\.pyi?$\u0026#39; # these folders wont be formatted by black - --exclude=\u0026#34;\u0026#34;\u0026#34;\\.git | \\.__pycache__| \\.hg| \\.mypy_cache| \\.tox| \\.venv| _build| buck-out| build| dist\u0026#34;\u0026#34;\u0026#34; language_version: python3.6 # flake8 - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: flake8 args: # arguments to configure flake8 # making isort line length compatible with black - \u0026#34;--max-line-length=88\u0026#34; - \u0026#34;--max-complexity=18\u0026#34; - \u0026#34;--select=B,C,E,F,W,T4,B9\u0026#34; # these are errors that will be ignored by flake8 # check out their meaning here # https://flake8.pycqa.org/en/latest/user/error-codes.html - \u0026#34;--ignore=E203,E266,E501,W503,F403,F401,E402\u0026#34; You can add the above lines to your configuration and run:\npre-commit run --all-files This should apply the pre-commit hooks to your code base harmoniously. From now on, before each commit, the hooks will make sure that your code complies with the rules imposed by the linters.\n","permalink":"https://rednafi.com/python/pre-commit/","summary":"\u003cp\u003e\u003ca href=\"https://pre-commit.com/#introduction\"\u003ePre-commit hooks\u003c/a\u003e can be a neat way to run automated ad-hoc \u003cem\u003etasks\u003c/em\u003e before submitting a new\ngit commit. These tasks may include linting, trimming trailing whitespaces, running code\nformatter before code reviews etc. Let\u0026rsquo;s see how multiple Python linters and formatters can\nbe applied automatically before each commit to impose strict conformity on your codebase.\u003c/p\u003e\n\u003cp\u003eTo keep my sanity, I only use three linters in all of my python projects:\u003c/p\u003e","title":"Running Python linters with pre-commit hooks"},{"content":"Updated on 2022-02-13: Change import style of functools.singledispatch.\nRecently, I was refactoring a portion of a Python function that somewhat looked like this:\ndef process(data): if cond0 and cond1: # apply func01 on data that satisfies the cond0 \u0026amp; cond1 return func01(data) elif cond2 or cond3: # apply func23 on data that satisfies the cond2 \u0026amp; cond3 return func23(data) elif cond4 and cond5: # apply func45 on data that satisfies cond4 \u0026amp; cond5 return func45(data) def func01(data): ... def func23(data): ... def func45(data): ... This pattern gets tedious when the number of conditions and actionable functions start to grow. I was looking for a functional approach to avoid defining and calling three different functions that do very similar things. Situations like this is where parametric polymorphism comes into play. The idea is, you have to define a single function that\u0026rsquo;ll be dynamically overloaded with alternative implementations based on the type of the function arguments.\nFunction overloading \u0026amp; generic functions Function overloading is a specific type of polymorphism where multiple functions can have the same name with different implementations. Calling an overloaded function will run a specific implementation of that function based on some prior conditions or appropriate context of the call.\nWhen function overloading happens based on its argument types, the resulting function is known as generic function. Let\u0026rsquo;s see how Python\u0026rsquo;s singledispatch decorator can help to design generic functions and refactor the icky code above.\nSingledispatch Python fairly recently added partial support for function overloading in Python 3.4. They did this by adding a neat little decorator to the functools module called singledispatch. In Python 3.8, there is another decorator for methods called singledispatchmethod. This decorator will transform your regular function into a single dispatch generic function.\nA generic function is composed of multiple functions implementing the same operation for different types. Which implementation should be used during a call is determined by the dispatch algorithm. When the implementation is chosen based on the type of a single argument, this is known as single dispatch.\nAs PEP-443 said, singledispatch only happens based on the first argument\u0026rsquo;s type. Let\u0026rsquo;s take a look at an example to see how this works!\nExample-1: Singledispatch with built-in argument type Let\u0026rsquo;s consider the following code:\n# procedural.py def process(num): if isinstance(num, int): return process_int(num) elif isinstance(num, float): return process_float(num) def process_int(num): # processing interger return f\u0026#34;Integer {num} has been processed successfully!\u0026#34; def process_float(num): # processing float return f\u0026#34;Float {num} has been processed successfully!\u0026#34; # use the function print(process(12.0)) print(process(1)) Running this code will return:\n\u0026gt;\u0026gt;\u0026gt; Float 12.0 has been processed successfully! \u0026gt;\u0026gt;\u0026gt; Integer 1 has been processed successfully! The above code snippet applies process_int or process_float functions on the incoming number based on its type. Now let\u0026rsquo;s see how the same thing can be achieved with singledispatch:\n# single_dispatch.py from functools import singledispatch @singledispatch def process(num=None): raise NotImplementedError(\u0026#34;Implement process function.\u0026#34;) @process.register(int) def sub_process(num): # processing interger return f\u0026#34;Integer {num} has been processed successfully!\u0026#34; @process.register(float) def sub_process(num): # processing float return f\u0026#34;Float {num} has been processed successfully!\u0026#34; # use the function print(process(12.0)) print(process(1)) Running this will return the same result as before.\n\u0026gt;\u0026gt;\u0026gt; Float 12.0 has been processed successfully! \u0026gt;\u0026gt;\u0026gt; Integer 1 has been processed successfully! Example-2: Singledispatch with custom argument type Suppose, you want to dispatch your function based on custom argument type where the type will be deduced from data. Consider this example:\ndef process(data: dict): if data[\u0026#34;genus\u0026#34;] == \u0026#34;Felis\u0026#34; and data[\u0026#34;bucket\u0026#34;] == \u0026#34;cat\u0026#34;: return process_cat(data) elif data[\u0026#34;genus\u0026#34;] == \u0026#34;Canis\u0026#34; and data[\u0026#34;bucket\u0026#34;] == \u0026#34;dog\u0026#34;: return process_dog(data) def process_cat(data: dict): # processing cat return \u0026#34;Cat data has been processed successfully!\u0026#34; def process_dog(data: dict): # processing dog return \u0026#34;Dog data has been processed successfully!\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: cat_data = {\u0026#34;genus\u0026#34;: \u0026#34;Felis\u0026#34;, \u0026#34;species\u0026#34;: \u0026#34;catus\u0026#34;, \u0026#34;bucket\u0026#34;: \u0026#34;cat\u0026#34;} dog_data = {\u0026#34;genus\u0026#34;: \u0026#34;Canis\u0026#34;, \u0026#34;species\u0026#34;: \u0026#34;familiaris\u0026#34;, \u0026#34;bucket\u0026#34;: \u0026#34;dog\u0026#34;} # using process print(process(cat_data)) print(process(dog_data)) Running this snippet will print out:\n\u0026gt;\u0026gt;\u0026gt; Cat data has been processed successfully! \u0026gt;\u0026gt;\u0026gt; Dog data has been processed successfully! To refactor this with singledispatch, you can create two data types Cat and Dog. When you make Cat and Dog objects from the classes and pass them through the process function, singledispatch will take care of dispatching the appropriate implementation of that function.\nfrom dataclasses import dataclass from functools import singledispatch @dataclass class Cat: genus: str species: str @dataclass class Dog: genus: str species: str @singledispatch def process(obj=None): raise NotImplementedError(\u0026#34;Implement process for bucket\u0026#34;) @process.register(Cat) def sub_process(obj): # processing cat return \u0026#34;Cat data has been processed successfully!\u0026#34; @process.register(Dog) def sub_process(obj): # processing dog return \u0026#34;Dog data has been processed successfully!\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: cat_obj = Cat(genus=\u0026#34;Felis\u0026#34;, species=\u0026#34;catus\u0026#34;) dog_obj = Dog(genus=\u0026#34;Canis\u0026#34;, species=\u0026#34;familiaris\u0026#34;) print(process(cat_obj)) print(process(dog_obj)) Running this will print out the same output as before:\n\u0026gt;\u0026gt;\u0026gt; Cat data has been processed successfully! \u0026gt;\u0026gt;\u0026gt; Dog data has been processed successfully! Further reading Transform a function into a single dispatch generic function Function overloading ","permalink":"https://rednafi.com/python/singledispatch/","summary":"\u003cp\u003e\u003cstrong\u003e\u003cem\u003eUpdated on 2022-02-13\u003c/em\u003e\u003c/strong\u003e: \u003cem\u003eChange import style of \u003ccode\u003efunctools.singledispatch\u003c/code\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eRecently, I was refactoring a portion of a Python function that somewhat looked like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eprocess\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003econd0\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003econd1\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"c1\"\u003e# apply func01 on data that satisfies the cond0 \u0026amp; cond1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003efunc01\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eelif\u003c/span\u003e \u003cspan class=\"n\"\u003econd2\u003c/span\u003e \u003cspan class=\"ow\"\u003eor\u003c/span\u003e \u003cspan class=\"n\"\u003econd3\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"c1\"\u003e# apply func23 on data that satisfies the cond2 \u0026amp; cond3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003efunc23\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eelif\u003c/span\u003e \u003cspan class=\"n\"\u003econd4\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003econd5\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"c1\"\u003e# apply func45 on data that satisfies cond4 \u0026amp; cond5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003efunc45\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003efunc01\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"o\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003efunc23\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"o\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003efunc45\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"o\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis pattern gets tedious when the number of conditions and actionable functions start to\ngrow. I was looking for a functional approach to avoid defining and calling three different\nfunctions that do very similar things. Situations like this is where \u003ca href=\"https://en.wikipedia.org/wiki/Parametric_polymorphism\"\u003eparametric\npolymorphism\u003c/a\u003e comes into play. The idea is, you have to define a single function that\u0026rsquo;ll be\ndynamically overloaded with alternative implementations based on the type of the function\narguments.\u003c/p\u003e","title":"Generic functions with Python's singledispatch"},{"content":"Python\u0026rsquo;s context managers are great for resource management and stopping the propagation of leaked abstractions. You\u0026rsquo;ve probably used it while opening a file or a database connection. Usually it starts with a with statement like this:\nwith open(\u0026#34;file.txt\u0026#34;, \u0026#34;wt\u0026#34;) as f: f.write(\u0026#34;contents go here\u0026#34;) In the above case, file.txt gets automatically closed when the execution flow goes out of the scope. This is equivalent to writing:\ntry: f = open(\u0026#34;file.txt\u0026#34;, \u0026#34;wt\u0026#34;) text = f.write(\u0026#34;contents go here\u0026#34;) finally: f.close() Writing custom context managers To write a custom context manager, you need to create a class that includes the __enter__ and __exit__ methods. Let\u0026rsquo;s recreate a custom context manager that will execute the same workflow as above.\nclass CustomFileOpen: \u0026#34;\u0026#34;\u0026#34;Custom context manager for opening files.\u0026#34;\u0026#34;\u0026#34; def __init__(self, filename, mode): self.filename = filename self.mode = mode def __enter__(self): self.f = open(self.filename, self.mode) return self.f def __exit__(self, *args): self.f.close() You can use the above class just like a regular context manager.\nwith CustomFileOpen(\u0026#34;file.txt\u0026#34;, \u0026#34;wt\u0026#34;) as f: f.write(\u0026#34;contents go here\u0026#34;) From generators to context managers Creating context managers by writing a class with __enter__ and __exit__ methods isn\u0026rsquo;t difficult. However, you can achieve better brevity by defining them using contextlib.contextmanager decorator. This decorator converts a generator function into a context manager. The blueprint for creating context manager decorators goes something like this:\n@contextmanager def some_generator(\u0026lt;arguments\u0026gt;): \u0026lt;setup\u0026gt; try: yield \u0026lt;value\u0026gt; finally: \u0026lt;cleanup\u0026gt; When you use the context manager with the with statement:\nwith some_generator(\u0026lt;arguments\u0026gt;) as \u0026lt;variable\u0026gt;: # ...body It roughly translates to:\n\u0026lt;setup\u0026gt; try: \u0026lt;variable\u0026gt; = \u0026lt;value\u0026gt; \u0026lt;body\u0026gt; finally: \u0026lt;cleanup\u0026gt; The setup code goes before the try..finally block. Notice the point where the generator yields. This is where the code block nested in the with statement gets executed. After the completion of the code block, the generator is then resumed. If an unhandled exception occurs in the block, it\u0026rsquo;s re-raised inside the generator at the point where the yield occurred and then the finally block is executed. If no unhandled exception occurs, the code gracefully proceeds to the finally block where you run your cleanup code.\nLet\u0026rsquo;s implement the same CustomFileOpen context manager with contextmanager decorator.\nfrom contextlib import contextmanager @contextmanager def CustomFileOpen(filename, method): \u0026#34;\u0026#34;\u0026#34;Custom context manager for opening a file.\u0026#34;\u0026#34;\u0026#34; f = open(filename, method) try: yield f finally: f.close() Now use it just like before:\nwith CustomFileOpen(\u0026#34;file.txt\u0026#34;, \u0026#34;wt\u0026#34;) as f: f.write(\u0026#34;contents go here\u0026#34;) Writing context managers as decorators You can use context managers as decorators also. To do so, while defining the class, you have to inherit from contextlib.ContextDecorator class. Let\u0026rsquo;s make a RunTime decorator that\u0026rsquo;ll be applied on a file-opening function. The decorator will:\nPrint a user provided description of the function. Print the time it takes to run the function. from contextlib import ContextDecorator from time import time class RunTime(ContextDecorator): \u0026#34;\u0026#34;\u0026#34;Timing decorator.\u0026#34;\u0026#34;\u0026#34; def __init__(self, description): self.description = description def __enter__(self): print(self.description) self.start_time = time() def __exit__(self, *args): self.end_time = time() run_time = self.end_time - self.start_time print(f\u0026#34;The function took {run_time} seconds to run.\u0026#34;) You can use the decorator like this:\n@RunTime(\u0026#34;This function opens a file\u0026#34;) def custom_file_write(filename, mode, content): with open(filename, mode) as f: f.write(content) Using the function like this should return:\nprint(custom_file_write(\u0026#34;file.txt\u0026#34;, \u0026#34;wt\u0026#34;, \u0026#34;jello\u0026#34;)) This function opens a file The function took 0.0005390644073486328 seconds to run. None You can also create the same decorator via contextlib.contextmanager decorator.\nfrom contextlib import contextmanager @contextmanager def runtime(description): print(description) start_time = time() try: yield finally: end_time = time() run_time = end_time - start_time print(f\u0026#34;The function took {run_time} seconds to run.\u0026#34;) Nesting contexts You can nest multiple context managers to manage resources simultaneously. Consider the following dummy manager:\nfrom contextlib import contextmanager @contextmanager def get_state(name): print(\u0026#34;entering:\u0026#34;, name) yield name print(\u0026#34;exiting :\u0026#34;, name) # multiple get_state can be nested like this with get_state(\u0026#34;A\u0026#34;) as A, get_state(\u0026#34;B\u0026#34;) as B, get_state(\u0026#34;C\u0026#34;) as C: print(\u0026#34;inside with statement:\u0026#34;, A, B, C) entering: A entering: B entering: C inside with statement: A B C exiting : C exiting : B exiting : A Notice the order they\u0026rsquo;re closed. Context managers are treated as a stack, and should be exited in reverse order in which they\u0026rsquo;re entered. If an exception occurs, this order matters, as any context manager could suppress the exception, at which point the remaining managers will not even get notified of this. The __exit__ method is also permitted to raise a different exception, and other context managers then should be able to handle that new exception.\nCombining multiple context managers You can combine multiple context managers too. Let\u0026rsquo;s consider these two managers.\nfrom contextlib import contextmanager @contextmanager def a(name): print(\u0026#34;entering a:\u0026#34;, name) yield name print(\u0026#34;exiting a:\u0026#34;, name) @contextmanager def b(name): print(\u0026#34;entering b:\u0026#34;, name) yield name print(\u0026#34;exiting b:\u0026#34;, name) Now combine these two using the decorator syntax. The following function takes the above define managers a and b and returns a combined context manager ab.\n@contextmanager def ab(a, b): with a(\u0026#34;A\u0026#34;) as A, b(\u0026#34;B\u0026#34;) as B: yield (A, B) This can be used as:\nwith ab(a, b) as AB: print(\u0026#34;Inside the composite context manager:\u0026#34;, AB) entering a: A entering b: B Inside the composite context manager: (\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;) exiting b: B exiting a: A If you have variable numbers of context managers and you want to combine them gracefully, contextlib.ExitStack is here to help. Let\u0026rsquo;s rewrite context manager ab using ExitStack. This function takes the individual context managers and their arguments as tuples and returns the combined manager.\nfrom contextlib import contextmanager, ExitStack @contextmanager def ab(cms, args): with ExitStack() as stack: yield [stack.enter_context(cm(arg)) for cm, arg in zip(cms, args)] with ab((a, b), (\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;)) as AB: print(\u0026#34;Inside the composite context manager:\u0026#34;, AB) entering a: A entering b: B Inside the composite context manager: [\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;] exiting b: B exiting a: A ExitStack can be also used in cases where you want to manage multiple resources gracefully. For example, suppose, you need to create a list from the contents of multiple files in a directory. Let\u0026rsquo;s see, how you can do so while avoiding accidental memory leakage with robust resource management.\nfrom contextlib import ExitStack from pathlib import Path # ExitStack ensures all files are properly closed after o/p with ExitStack() as stack: streams = ( stack.enter_context(open(fname, \u0026#34;r\u0026#34;)) for fname in Path(\u0026#34;src\u0026#34;).rglob(\u0026#34;*.py\u0026#34;) ) contents = [f.read() for f in streams] Using context managers to create SQLAlchemy session If you are familiar with SQLALchemy, Python\u0026rsquo;s SQL toolkit and Object Relational Mapper, then you probably know the usage of Session to run a query. A Session basically turns any query into a transaction and make it atomic. Context managers can help you write a transaction session in a very elegant way. A basic querying workflow in SQLAlchemy may look like this:\nfrom sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from contextlib import contextmanager # an Engine, which the Session will use for connection resources some_engine = create_engine(\u0026#34;sqlite://\u0026#34;) # create a configured \u0026#34;Session\u0026#34; class Session = sessionmaker(bind=some_engine) @contextmanager def session_scope(): \u0026#34;\u0026#34;\u0026#34;Provide a transactional scope around a series of operations.\u0026#34;\u0026#34;\u0026#34; session = Session() try: yield session session.commit() except: session.rollback() raise finally: session.close() The excerpt above creates an in memory SQLite connection and a session_scope function with context manager. The session_scope function takes care of committing and rolling back in case of exception automatically. The session_scope function can be used to run queries in the following way:\nwith session_scope() as session: myobject = MyObject(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;) session.add(myobject) Abstract away exception handling monstrosity with context managers This is my absolute favorite use case of context managers. Suppose you want to write a function but want the exception handling logic out of the way. Exception handling logics with sophisticated logging can often obfuscate the core logic of your function. You can write a decorator type context manager that will handle the exceptions for you and decouple these additional code from your main logic. Let\u0026rsquo;s write a decorator that will handle ZeroDivisionError and TypeError simultaneously.\nfrom contextlib import contextmanager @contextmanager def errhandler(): try: yield except ZeroDivisionError: print(\u0026#34;This is a custom ZeroDivisionError message.\u0026#34;) raise except TypeError: print(\u0026#34;This is a custom TypeError message.\u0026#34;) raise Now use this in a function where these exceptions occur.\n@errhandler() def div(a, b): return a // b div(\u0026#34;b\u0026#34;, 0) This is a custom TypeError message. --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-43-65497ed57253\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 div(\u0026#39;b\u0026#39;,0) /usr/lib/python3.8/contextlib.py in inner(*args, **kwds) 73 def inner(*args, **kwds): 74 with self._recreate_cm(): ---\u0026gt; 75 return func(*args, **kwds) 76 return inner 77 \u0026lt;ipython-input-42-b7041bcaa9e6\u0026gt; in div(a, b) 1 @errhandler() 2 def div(a, b): ----\u0026gt; 3 return a // b TypeError: unsupported operand type(s) for //: \u0026#39;str\u0026#39; and \u0026#39;int\u0026#39; You can see that the errhandler decorator is doing the heavylifting for you. Pretty neat, huh?\nThe following one is a more sophisticated example of using context manager to decouple your error handling monstrosity from the main logic. It also hides the elaborate logging logic from the main method.\nimport logging from contextlib import contextmanager import traceback import sys logging.getLogger(__name__) logging.basicConfig( level=logging.INFO, format=\u0026#34;\\n%(asctime)s [%(levelname)s] %(message)s\u0026#34;, handlers=[logging.FileHandler(\u0026#34;./debug.log\u0026#34;), logging.StreamHandler()], ) class Calculation: \u0026#34;\u0026#34;\u0026#34;Demo class for exception decoupling with contextmanager.\u0026#34;\u0026#34;\u0026#34; def __init__(self, a, b): self.a = a self.b = b @contextmanager def errorhandler(self): try: yield except ZeroDivisionError: print( f\u0026#34;Custom handling of Zero Division Error! Printing \u0026#34; \u0026#34;only 2 levels of traceback..\u0026#34; ) logging.exception(\u0026#34;ZeroDivisionError\u0026#34;) def main_func(self): \u0026#34;\u0026#34;\u0026#34;Function we want to save from error handling logic.\u0026#34;\u0026#34;\u0026#34; with self.errorhandler(): return self.a / self.b obj = Calculation(2, 0) print(obj.main_func()) This will return\n(asctime)s [ERROR] ZeroDivisionError Traceback (most recent call last): File \u0026#34;\u0026lt;ipython-input-44-ff609edb5d6e\u0026gt;\u0026#34;, line 25, in errorhandler yield File \u0026#34;\u0026lt;ipython-input-44-ff609edb5d6e\u0026gt;\u0026#34;, line 37, in main_func return self.a / self.b ZeroDivisionError: division by zero Custom handling of Zero Division Error! Printing only 2 levels of traceback.. None Persistent parameters across HTTP requests with context managers Another great use case for context managers is making parameters persistent across multiple HTTP requests. Python\u0026rsquo;s requests library has a Session object that will let you easily achieve this. So, if you\u0026rsquo;re making several requests to the same host, the underlying TCP connection will be reused, which can result in a significant performance increase. The following example is taken directly from the official docs of the requests library. Let\u0026rsquo;s persist some cookies across requests.\nwith requests.Session() as session: session.get(\u0026#34;http://httpbin.org/cookies/set/sessioncookie/123456789\u0026#34;) response = session.get(\u0026#34;http://httpbin.org/cookies\u0026#34;) print(response.text) This should show:\n{ \u0026#34;cookies\u0026#34;: { \u0026#34;sessioncookie\u0026#34;: \u0026#34;123456789\u0026#34; } } Remarks To avoid redundencies, I have purposefully excluded examples of nested with statements and now deprecated contextlib.nested function to create nested context managers.\nFurther reading Python contextlib documentation Python with context manager by Jeff Knupp SQLAlchemy session creation Scipy lectures on context managers Merging context managers ","permalink":"https://rednafi.com/python/contextmanager/","summary":"\u003cp\u003ePython\u0026rsquo;s context managers are great for resource management and stopping the propagation of\nleaked abstractions. You\u0026rsquo;ve probably used it while opening a file or a database connection.\nUsually it starts with a \u003ccode\u003ewith\u003c/code\u003e statement like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"nb\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;file.txt\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;wt\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewrite\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;contents go here\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIn the above case, \u003ccode\u003efile.txt\u003c/code\u003e gets automatically closed when the execution flow goes out of\nthe scope. This is equivalent to writing:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003etry\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ef\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;file.txt\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;wt\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003etext\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewrite\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;contents go here\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003efinally\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eclose\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"writing-custom-context-managers\"\u003eWriting custom context managers\u003c/h2\u003e\n\u003cp\u003eTo write a custom context manager, you need to create a class that includes the \u003ccode\u003e__enter__\u003c/code\u003e\nand \u003ccode\u003e__exit__\u003c/code\u003e methods. Let\u0026rsquo;s recreate a custom context manager that will execute the same\nworkflow as above.\u003c/p\u003e","title":"The curious case of Python's context manager"},{"content":"Recently, my work needed me to create lots of custom data types and draw comparison among them. So, my code was littered with many classes that somewhat looked like this:\nclass CartesianPoint: def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): return f\u0026#34;CartesianPoint(x = {self.x}, y = {self.y}, z = {self.z})\u0026#34; print(CartesianPoint(1, 2, 3)) \u0026gt;\u0026gt;\u0026gt; CartesianPoint(x = 1, y = 2, z = 3) This class only creates a CartesianPoint type and shows a pretty output of the instances created from it. However, it already has two methods inside, __init__ and __repr__ that don\u0026rsquo;t do much.\nDataclasses Let\u0026rsquo;s see how data classes can help to improve this situation. Data classes were introduced to python in version 3.7. Basically they can be regarded as code generators that reduce the amount of boilerplate you need to write while generating generic classes. Rewriting the above class using dataclass will look like this:\nfrom dataclasses import dataclass @dataclass class CartesianPoint: x: float y: float z: float # using the class point = CartesianPoint(1, 2, 3) print(point) \u0026gt;\u0026gt;\u0026gt; CartesianPoint(x=1, y=2, z=3) In the above code, the magic is done by the dataclass decorator. Data classes require you to use explicit type annotations and it automatically implements methods like __init__, __repr__, __eq__ etc beforehand. You can inspect the methods that dataclass auto defines via Python\u0026rsquo;s help.\nhelp(CartesianPoint) Help on class CartesianPoint in module __main__: class CartesianPoint(builtins.object) | CartesianPoint(x:float, y:float, z:float) | | Methods defined here: | | __eq__(self, other) | | __init__(self, x:float, y:float, z:float) -\u0026gt; None | | __repr__(self) | | ---------------------------------------------------------------------- | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | ---------------------------------------------------------------------- | Data and other attributes defined here: | | __annotations__ = {\u0026#39;x\u0026#39;: \u0026lt;class \u0026#39;float\u0026#39;\u0026gt;, \u0026#39;y\u0026#39;: \u0026lt;class \u0026#39;float\u0026#39;\u0026gt;, \u0026#39;z\u0026#39;: \u0026lt;c... | | __dataclass_fields__ = {\u0026#39;x\u0026#39;: Field(name=\u0026#39;x\u0026#39;,type=\u0026lt;class \u0026#39;float\u0026#39;\u0026gt;,defau... | | __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or... | | __hash__ = None Using default values You can provide default values to the fields in the following way:\nfrom dataclasses import dataclass @dataclass class CartesianPoint: x: float = 0 y: float = 0 z: float = 0 Using arbitrary field type If you don\u0026rsquo;t want to specify your field type during type hinting, you can use Any type from python\u0026rsquo;s typing module.\nfrom dataclasses import dataclass from typing import Any @dataclass class CartesianPoint: x: Any y: Any z: Any Instance ordering You can check if two instances are equal without making any modification to the class.\nfrom dataclasses import dataclass @dataclass class CartesianPoint: x: float y: float z: float point_1 = CartesianPoint(1, 2, 3) point_2 = CartesianPoint(1, 2, 5) print(point_1 == point_2) \u0026gt;\u0026gt;\u0026gt; False However, if you want to compare multiple instances of dataclasses, aka add __gt__ or __lt__ methods to your instances, you have to turn on the order flag manually.\nfrom dataclasses import dataclass @dataclass(order=True) class CartesianPoint: x: float y: float z: float # comparing two instances point_1 = CartesianPoint(10, 12, 13) point_2 = CartesianPoint(1, 2, 5) print(point_1 \u0026gt; point_2) \u0026gt;\u0026gt;\u0026gt; True By default, while comparing instances, all of the fields are used. In our above case, all the fields x, y, zof point_1 instance are compared with all the fields of point_2 instance. You can customize this using the field function.\nSuppose you want to acknowledge two instances as equal only when attribute x of both of them are equal. You can emulate this in the following way:\nfrom dataclasses import dataclass, field @dataclass(order=True) class CartesianPoint: x: float y: float = field(compare=False) z: float = field(compare=False) # create instance where only the x attributes are equal point_1 = CartesianPoint(1, 3, 5) point_2 = CartesianPoint(1, 4, 6) # compare the instances print(point_1 == point_2) print(point_1 \u0026lt; point_2) \u0026gt;\u0026gt;\u0026gt; True \u0026gt;\u0026gt;\u0026gt; False You can see the above code prints out True despite the instances have different y and z attributes.\nAdding methods Methods can be added to dataclasses just like normal classes. Let\u0026rsquo;s add another method called dist to our CartesianPoint class. This method calculates the distance of a point from origin.\nfrom dataclasses import dataclass import math @dataclass class CartesianPoint: x: float y: float z: float def dist(self): return math.sqrt(self.x**2 + self.y**2 + self.z**2) # create a new instance and use method `dist` point = CartesianPoint(5, 6, 7) norm = point.dist() print(norm) \u0026gt;\u0026gt;\u0026gt; 10.488088481701515 Making instances immutable By default, instances of dataclasses are mutable. If you want to prevent mutating your instance attributes, you can set frozen=True while defining your dataclass.\nfrom dataclasses import dataclass @dataclass(frozen=True) class CartesianPoint: x: float y: float z: float If you try to mutate the any of the attributes of the above class, it will raise FrozenInstanceError.\npoint = CartesianPoint(2, 4, 6) point.x = 23 --------------------------------------------------------------------------- FrozenInstanceError Traceback (most recent call last) \u0026lt;ipython-input-34-b712968bd0eb\u0026gt; in \u0026lt;module\u0026gt; 1 point = CartesianPoint(2, 4, 6) ----\u0026gt; 2 point.x = 23 \u0026lt;string\u0026gt; in __setattr__(self, name, value) FrozenInstanceError: cannot assign to field \u0026#39;x\u0026#39; Making instances hashable You can turn on the unsafe_hash parameter of the dataclass decorator to make the class instances hashable. This may come in handy when you want to use your instances as dictionary keys or want to perform set operation on them. However, if you\u0026rsquo;re using unsafe_hash make sure that your dataclasses don\u0026rsquo;t contain any mutable data structure in it.\nfrom dataclasses import dataclass @dataclass(unsafe_hash=True) class CartesianPoint: x: float y: float z: float # creating instance point = CartesianPoint(0, 0, 0) # use the class instances as dictionary keys print({f\u0026#34;{point}\u0026#34;: \u0026#34;origin\u0026#34;}) \u0026gt;\u0026gt;\u0026gt; {\u0026#39;CartesianPoint(x=0, y=0, z=0)\u0026#39;: \u0026#39;origin\u0026#39;} Converting instances to dicts The asdict() function converts a dataclass instance to a dict of its fields.\nfrom dataclasses import dataclass, asdict point = CartesianPoint(1, 5, 6) print(asdict(point)) \u0026gt;\u0026gt;\u0026gt; {\u0026#39;x\u0026#39;: 1, \u0026#39;y\u0026#39;: 5, \u0026#39;z\u0026#39;: 6} Post-init processing When dataclass generates the __init__ method, internally it\u0026rsquo;ll call __post_init__ method. You can add additional processing in the __post_init__ method. Here, I\u0026rsquo;ve added another attribute tup that returns the cartesian point as a tuple.\nfrom dataclasses import dataclass @dataclass class CartesianPoint: x: float y: float z: float def __post_init__(self): self.tup = (self.x, self.y, self.z) # checking the tuple point = CartesianPoint(4, 5, 6) print(point.tup) \u0026gt;\u0026gt;\u0026gt; (4, 5, 6) Refactoring the CartesianPoint class The feature rich original CartesianPoint looks something like this:\nimport math class CartesianPoint: \u0026#34;\u0026#34;\u0026#34;Immutable Cartesian point class. Although mathematically incorrect, for demonstration purpose, all the comparisons are done based on the first field only.\u0026#34;\u0026#34;\u0026#34; def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): \u0026#34;\u0026#34;\u0026#34;Print the instance neatly.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;CartesianPoint(x = {self.x}, y = {self.y}, z = {self.z})\u0026#34; def __eq__(self, other): \u0026#34;Checks if equal.\u0026#34; return self.x == other.x def __nq__(self, other): \u0026#34;\u0026#34;\u0026#34;Checks non equality.\u0026#34;\u0026#34;\u0026#34; return self.x != other.x def __gt__(self, other): \u0026#34;\u0026#34;\u0026#34;Checks if greater than.\u0026#34;\u0026#34;\u0026#34; return self.x \u0026gt; other.x def __ge__(self, other): \u0026#34;\u0026#34;\u0026#34;Checks if greater than or equal.\u0026#34;\u0026#34;\u0026#34; return self.x \u0026gt;= other.x def __lt__(self, other): \u0026#34;\u0026#34;\u0026#34;Checks if less than.\u0026#34;\u0026#34;\u0026#34; return self.x \u0026lt; other.x def __le__(self, other): \u0026#34;\u0026#34;\u0026#34;Checks if less than or equal.\u0026#34;\u0026#34;\u0026#34; return self.x \u0026lt;= other.x def __hash__(self): \u0026#34;\u0026#34;\u0026#34;Make the instances hashable.\u0026#34;\u0026#34;\u0026#34; return hash(self) def dist(self): \u0026#34;\u0026#34;\u0026#34;Finds distance of point from origin.\u0026#34;\u0026#34;\u0026#34; return math.sqrt(self.x**2 + self.y**2 + self.z**2) Let\u0026rsquo;s see the class in action:\n# create multiple instances of the class a = CartesianPoint(1, 2, 3) b = CartesianPoint(1, 3, 3) c = CartesianPoint(0, 3, 5) d = CartesianPoint(5, 6, 7) # checking the __repr__ method print(a) # checking the __eq__ method print(a == b) # checking the __nq__ method print(a != c) # checking the __ge__ method print(b \u0026gt;= d) # checking the __lt__ method print(c \u0026lt; a) # checking __hash__ and __dist__ method print({f\u0026#34;{a}\u0026#34;: a.dist()}) CartesianPoint(x = 1, y = 2, z = 3) True True False True {\u0026#39;CartesianPoint(x = 1, y = 2, z = 3)\u0026#39;: 3.7416573867739413} Below is the same class refactored using dataclass.\nfrom dataclasses import dataclass, field @dataclass(unsafe_hash=True, order=True) class CartesianPoint: \u0026#34;\u0026#34;\u0026#34;Immutable Cartesian point class. Although mathematically incorrect, for demonstration purpose, all the comparisons are done based on the first field only.\u0026#34;\u0026#34;\u0026#34; x: float y: float = field(compare=False) z: float = field(compare=False) def dist(self): \u0026#34;\u0026#34;\u0026#34;Finds distance of point from origin.\u0026#34;\u0026#34;\u0026#34; return math.sqrt(self.x**2 + self.y**2 + self.z**2) Use this class like before.\n# create multiple instances of the class a = CartesianPoint(1, 2, 3) b = CartesianPoint(1, 3, 3) c = CartesianPoint(0, 3, 5) d = CartesianPoint(5, 6, 7) # checking the __repr__ method print(a) # checking the __eq__ method print(a == b) # checking the __nq__ method print(a != c) # checking the __ge__ method print(b \u0026gt;= d) # checking the __lt__ method print(c \u0026lt; a) # checking __hash__ and __dist__ method print({f\u0026#34;{a}\u0026#34;: a.dist()}) CartesianPoint(x=1, y=2, z=3) True True False True {\u0026#39;CartesianPoint(x=1, y=2, z=3)\u0026#39;: 3.7416573867739413} Further reading Python dataclasses official docs The ultimate guide to dataclasses in Python 3.7 ","permalink":"https://rednafi.com/python/dataclasses/","summary":"\u003cp\u003eRecently, my work needed me to create lots of custom data types and draw comparison among\nthem. So, my code was littered with many classes that somewhat looked like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eCartesianPoint\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ez\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ez\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ez\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__repr__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;CartesianPoint(x = \u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e, y = \u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e, z = \u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ez\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e)\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eCartesianPoint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-txt\" data-lang=\"txt\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u0026gt;\u0026gt;\u0026gt; CartesianPoint(x = 1, y = 2, z = 3)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis class only creates a \u003ccode\u003eCartesianPoint\u003c/code\u003e type and shows a pretty output of the instances\ncreated from it. However, it already has two methods inside, \u003ccode\u003e__init__\u003c/code\u003e and \u003ccode\u003e__repr__\u003c/code\u003e that\ndon\u0026rsquo;t do much.\u003c/p\u003e","title":"Reduce boilerplate code with Python's dataclasses"},{"content":"June 16 Using sed to extract a patch from a file While running the OpenAI Codex CLI locally, I came across a sed command that prints specific lines from a file. The agent often uses this command to output a portion of a file:\nsed -n \u0026#39;12,15p\u0026#39; \u0026lt;filename\u0026gt; This prints lines 12 through 15 of \u0026lt;filename\u0026gt;.\nMarch 29 dotGo 2014 - John Graham-Cumming - I came for the easy concurrency I stayed for the easy composition I had to watch this twice to fully appreciate it. John Graham-Cumming crams a 40-minute talk into 14 minutes. First, he shows how Go\u0026rsquo;s basic types and stdlib make writing a DNS lookup program quite trivial. Then he walks through how he generalized it with a simple interface. The code can be found here.\nFebruary 23 GopherCon 2019: Socket to me: Where do sockets live in Go? - Gabbi Fisher Gabbi Fisher shows how to write basic TCP and UDP socket servers in Go. It\u0026rsquo;s pretty easy since the net package in the standard library offers some handy abstractions. Plus, making the servers concurrent is quite simple because you can easily spin up a new goroutine to handle each connection.\nI\u0026rsquo;ve written socket servers in Python before, and making them concurrent wasn\u0026rsquo;t fun — even with the nice abstractions of the socketserver library.\nBut holy smokes, why is it so hard in Go to customize socket behavior — like enabling SO_REUSEADDR to let multiple servers bind to the same port?. The last 10 minutes of the talk explores that.\nFebruary 21 GopherCon 2021: Becoming the metaprogrammer: Real world code generation - Alan Shreve This is one of the best talks I\u0026rsquo;ve seen on Go code generation. Alan Shreve explains what code generators are and surveys several popular tools, such as stringer, go mock, ffjson, sqlc, protoc, and babel. He then shows how to write a simple code generator in Go.\nOne thing I learned is how go test works underneath. Instead of running your test functions directly, go test reads your test source code, identifies the tests, and writes a new piece of source code — a test harness — that calls those test functions. This harness is then compiled, executed, and discarded. Similarly, when you run go test -cover, it generates a modified copy of your source code with extra statements inserted to track which parts of your code are executed.\nFebruary 18 GopherCon 2022: Compatibility: How Go programs keep working - Russ Cox Russ Cox talks about Go\u0026rsquo;s compatibility promises and how to write code that makes it easier for the Go team to guarantee compatibility.\nFebruary 15 The official ten-year retrospective of NewSQL databases - Andy Pavlo I still have a hard time explaining the difference between wide-column (column-family) databases like Cassandra/ScyllaDB and NewSQL databases like Spanner/CockroachDB.\nColumn-family databases are generally designed as AP systems — they favor availability and partition tolerance over strong consistency. They let you define a schema upfront but store rows sparsely, so each row can have different columns. This design is especially useful in write-heavy environments where sacrificing strict consistency can improve performance.\nIn contrast, NewSQL databases are typically CP systems that prioritize consistency and partition tolerance. They provide full SQL support and strong ACID guarantees, ensuring every transaction remains consistent across distributed nodes. However, under heavy write loads, if you\u0026rsquo;re willing to relax strong consistency, column-family databases can often outperform NewSQL systems.\nFebruary 14 What goes around comes around\u0026hellip; and around\u0026hellip; - Andy Pavlo (Dijkstra Award 2024) Andy Pavlo\u0026rsquo;s enthusiasm for databases is infectious. I remember reading the paper mentioned here sometime last year, and this talk is a great complement to it. Andy explains why SQL databases work and why RDBMS should be your default choice when building applications. It\u0026rsquo;s also fun to see database vendors trying to move away from SQL, only to add support for it a few years later.\nWhile I default to SQLite/Postgres for my applications, I\u0026rsquo;ve also been fortunate enough to work in places where I\u0026rsquo;ve seen Postgres fail under massive write loads. As a result, I tend to steer clear of the overzealous Hacker News crowd that loves to bash NoSQL and NewSQL databases without considering why they were introduced in the first place.\nFebruary 11 Curious channels - Dave Cheney Dave shows a neat way to notify multiple goroutines using close(ch). Plus, a nil channel can be used to wait for multiple channels to close.\nFebrurary 10 Rob Pike - what we got right, what we got wrong | GopherConAU 2023 Go definitely got concurrency right. However, I still think that channels, wait groups, and mutexes are a bit too low-level, and Go missed out on providing some higher-level building blocks based on these.\nAlso, it\u0026rsquo;s interesting to see Rob Pike admit that dependency management and generics are areas where Go could have done better. I still think generics came to the language a tad too late — but eh, better late than never.\nJanuary 14 On Ousterhout\u0026rsquo;s dichotomy – Alex Kladov For performance, what matters is not so much the code that\u0026rsquo;s executed, but rather the layout of objects in memory. And the high-level dialect locks-in pointer-heavy GC object model! Even if you write your code in assembly, the performance ceiling will be determined by all those pointers GC needs. To actually get full \u0026ldquo;low-level\u0026rdquo; performance, you need to effectively \u0026ldquo;mirror\u0026rdquo; the data across the dialects across a quasi-FFI boundary.\nJanuary 11 GopherCon UK 2019: Fun with pointers – Daniela Petruzalek Daniela explains how Go\u0026rsquo;s pointers are much simpler and safer than those in C/C++, while still providing users the power of indirection. One example surprised me: I was expecting the following snippet to raise a nil pointer dereference error. However, it doesn\u0026rsquo;t. In the StructGoesBoom method, m = nil only modifies the local copy of the pointer to MyLittleStruct. This means the original struct remains unaffected. Running the code confirms that the struct wasn\u0026rsquo;t modified at all:\npackage main import ( \u0026#34;fmt\u0026#34; ) type MyLittleStruct struct { something int } func (m *MyLittleStruct) StructGoesBoom() { fmt.Println(\u0026#34;Boom!\u0026#34;) // This only changes the local copy of pointer m m = nil } func main() { x := MyLittleStruct{1337} fmt.Printf(\u0026#34;address: %p\\n\u0026#34;, \u0026amp;x) // Desugar x.StructGoesBoom() (*MyLittleStruct).StructGoesBoom(\u0026amp;x) fmt.Printf(\u0026#34;%#v\\n\u0026#34;, x) } Output:\naddress: 0xc0000140a0 Boom! main.MyLittleStruct{something:1337} The \u0026ldquo;active enum\u0026rdquo; pattern – Glyph Glyph shows a neat pattern for encoding behaviors in a Python enum. Instead of defining an enum and then handling behaviors in a function like this:\nfrom enum import Enum, auto class SomeNumber(Enum): one = auto() two = auto() three = auto() def behavior(number: SomeNumber) -\u0026gt; int: match number: case SomeNumber.one: print(\u0026#34;one!\u0026#34;) return 1 case SomeNumber.two: print(\u0026#34;two!\u0026#34;) return 2 case SomeNumber.three: print(\u0026#34;three!\u0026#34;) return 3 A better way to do it is:\nfrom dataclasses import dataclass from enum import Enum from typing import Callable @dataclass(frozen=True) class NumberValue: result: int effect: Callable[[], None] class SomeNumber(Enum): one = NumberValue(1, lambda: print(\u0026#34;one!\u0026#34;)) two = NumberValue(2, lambda: print(\u0026#34;two!\u0026#34;)) three = NumberValue(3, lambda: print(\u0026#34;three!\u0026#34;)) def behavior(self) -\u0026gt; int: self.value.effect() return self.value.result January 10 Be aware of the Makefile effect – William Woodruff An excellent name for the situation where a tool is so complex to use that people simply copy existing configurations and tweak them until they work for their specific cases.\nTools and systems that enable this pattern often have less-than-ideal diagnostics or debugging support: the user has to run the tool repeatedly, often with long delays, to get back relatively small amounts of information. Think about CI/CD setups, where users diagnose their copy-pasted CI/CD by doing print-style debugging over the network with a layer of intermediating VM orchestration. Ridiculous!\nJanuary 09 Ghostty by Mitchell Hashimoto How do you pronounce Ghostty?\nGhos-tty Ghost-ty Something else Funny name, but an excellent terminal. It\u0026rsquo;s the first emulator to pull me away from the default macOS/Ubuntu terminal. Since I spend most of my time in VS Code\u0026rsquo;s integrated terminal, the rough edges of Ghostty don\u0026rsquo;t bother me much. In fact, I\u0026rsquo;m quite enjoying it. My config is minimal:\ntheme = \u0026#34;catppuccin-mocha\u0026#34; font-family = \u0026#34;JetBrains Mono\u0026#34; font-size = 20 # Background configuration background-opacity = 0.95 background-blur-radius = 20 GopherCon 2015: Embrace the interface – Tomas Senart One great thing about Go is that you can grok a 10-year-old talk and still find it relevant. Here, Tomas demonstrates the decorator pattern in idiomatic Go, showing how to add logging, instrumentation, or retry functionality to an RPC function without polluting its core logic. The result is a Python-like decorator workflow without syntactic sugar that feels native to Go.\nJanuary 08 Go developer survey 2024 h2 results Similar to previous years, the most common use cases for Go were API/RPC services (75%) and command line tools (62%). More experienced Go developers reported building a wider variety of applications in Go. This trend was consistent across every category of app or service. We did not find any notable differences in what respondents are building based on their organization size. Respondents from the random VS Code and GoLand samples did not display significant differences either.\nJanuary 07 How I program with LLMs – David Crawshaw My LLM workflow is pretty similar to David\u0026rsquo;s. Instead of dumping my whole codebase into the model, I find it way more effective to use the chat UI and tackle a problem piece by piece. The responses are way better when the problem space is smaller and the model needs less out-of-band information to work — probably how a human would perform in a similar context.\nThe reservoir sampler for the quartiles of floats example is a perfect demonstration of why LLMs are so useful in programming, even with their tendency to make things up or get stuck in a rut.\nJanuary 06 GopherCon 2020: Go is boring\u0026hellip; and that\u0026rsquo;s fantastic - Jonathan Bodner When you have a struct in Go that contains other structs, all the data is stored sequentially in memory. This is different from classes in all those other languages, where each field in a class is actually a pointer to some other memory, which means your memory access in those languages is effectively random (random memory access is slower than sequential access).\nJanuary 05 Back to basics: why we chose long polling over WebSockets – Nadeesha Cabral Long polling has its fair share of issues, but in my experience, it\u0026rsquo;s been more reliable than WebSockets in most cases where I\u0026rsquo;ve needed to maintain long-running HTTP connections. Modern databases can handle a surprising number of connections these days, and adding proper indexes can mitigate the risk of overwhelming the database with too many open connections.\nSure, Server-Sent Events (SSE) and WebSockets exist, but reliably detecting changes in the backend and delivering them to the right client still feels like an unsolved problem. Until that\u0026rsquo;s resolved, long polling remains a surprisingly simple and robust solution that just works. It\u0026rsquo;s already used as the fallback solution in most WebSocket setups.\nJanuary 04 Kids can\u0026rsquo;t use computers\u0026hellip; and this is why it should worry you – Marc Scott Damn, the anecdotes just keep getting better and better.\nA sixth-former brings me his laptop, explaining that it is running very slowly and keeps shutting down. The laptop is literally screaming, the processor fans running at full whack and the case is uncomfortably hot to touch. I run Task Manager to see that the CPU is running at 100% despite the only application open being uTorrent (which incidentally had about 200 torrent files actively seeding). I look at what processes are running and there are a lot of them, hogging the CPU and RAM. What\u0026rsquo;s more I can\u0026rsquo;t terminate a single one. \u0026lsquo;What anti-virus are you using?\u0026rsquo; I ask, only to be told that he didn\u0026rsquo;t like using anti-virus because he\u0026rsquo;d heard it slowed his computer down. I hand back the laptop and tell him that it\u0026rsquo;s infected. He asks what he needs to do, and I suggest he reinstalls Windows. He looks at me blankly. He can\u0026rsquo;t use a computer.\nA kid puts her hand up in my lesson. \u0026lsquo;My computer won\u0026rsquo;t switch on,\u0026rsquo; she says, with the air of desperation that implies she\u0026rsquo;s tried every conceivable way of making the thing work. I reach forward and switch on the monitor, and the screen flickers to life, displaying the Windows login screen. She can\u0026rsquo;t use a computer.\nA teacher brings me her school laptop. \u0026lsquo;Bloody thing won\u0026rsquo;t connect to the internet.\u0026rsquo; she says angrily, as if it were my fault. \u0026lsquo;I had tonnes of work to do last night, but I couldn\u0026rsquo;t get on-line at all. My husband even tried and he couldn\u0026rsquo;t figure it out and he\u0026rsquo;s excellent with computers.\u0026rsquo; I take the offending laptop from out of her hands, toggle the wireless switch that resides on the side, and hand it back to her. Neither her nor her husband can use computers.\nJanuary 03 Be a property owner and not a renter on the internet – Den Delimarsky I\u0026rsquo;m loving this renaissance of personal blogs. Den\u0026rsquo;s take on POSSE (Publish Own Site, Syndicate Elsewhere) really resonates — it\u0026rsquo;s the only viable way to create content without getting trapped in the walled gardens of billion-dollar megacorps. About five years ago, Medium burned me, so I followed Simon Willison\u0026rsquo;s advice and started this blog. Easily one of the best decisions I\u0026rsquo;ve made in the past decade.\nI would recommend avoiding any places where there is content lock-in. You want to optimize for future portability. That is, if you can\u0026rsquo;t easily export your full content history (e.g., blog posts) and move them somewhere else, don\u0026rsquo;t use that service. If your content is locked into a service, and at some point that service decides that you are no longer a wanted customer, all that effort you put into making it available to your customers can vanish on a moment\u0026rsquo;s notice. Prefer sites that allow you to publish in open formats, such as Ghost.\nMinifeed Minifeed is a delightful blog aggregator I discovered today. It\u0026rsquo;s full of blogs I\u0026rsquo;d never heard of and uses a smart, vector-driven approach to group similar posts.\nModern for Hacker News I spend an absurd amount of time browsing Hacker News and just stumbled upon this gem. On iOS, Hack is my go-to app for exploring the orange site, but on desktop, I\u0026rsquo;ve mostly stuck with the original UI. Enter Modern — a Chrome extension that transforms the brutalist interface into something sleek, like this:\nJanuary 02 Stop designing languages. Write libraries instead – Patrick S. Li When I first started dabbling with Go, I couldn\u0026rsquo;t help but wonder why it didn\u0026rsquo;t have anything as slick as Python\u0026rsquo;s FastAPI. Generating API docs directly from code required a flexible type system and robust introspection capabilities, which Go just didn\u0026rsquo;t have back then. But now, with generics in the mix, libraries are finally emerging that can generate OpenAPI specs and docs from code without much fuss.\nThe design of the programming language directly determines what sort of libraries you can write and how easy they are to use in the end. In the C language, the only major feature provided for enabling reuse is the ability to declare and call functions. So guess what? The majority of C libraries are basically large collections of functions. Ruby on Rails provides a concise way for expressing: do this when the button is clicked. The \u0026ldquo;do this\u0026rdquo; part is implemented in Ruby as a first-class function. How would it be implemented in languages like Java which don\u0026rsquo;t support them? Well, the behaviour of first-class functions can be mocked by defining a new event handler class with a single perform_action method and then passing an instance of this class to the button object. So guess what? Using a Java library typically entails declaring a humongous number of handler classes. The programming language directly shapes the design of its libraries.\nJanuary 01 Databases in 2024: A year in review – Andy Pavlo From Redis and Elasticsearch\u0026rsquo;s licensing drama to Databricks vs. Snowflake\u0026rsquo;s billion-dollar sparring, to DuckDB\u0026rsquo;s integration into Postgres — this post offers a great overview of the major database events of 2024.\nI\u0026rsquo;ve been reading Andy\u0026rsquo;s database event reviews for the past three years and love his work. However, I\u0026rsquo;m not sure how to feel about the unwarranted shade thrown at Redis. While I don\u0026rsquo;t agree with Redis Ltd.\u0026rsquo;s licensing decisions, I still think it\u0026rsquo;s a marvelous engineering artifact that has shaped the query languages of many similar key-value storage systems.\n","permalink":"https://rednafi.com/feed/2025/","summary":"\u003ch3 id=\"june-16\"\u003eJune 16\u003c/h3\u003e\n\u003ch4 id=\"using-sed-to-extract-a-patch-from-a-file\"\u003eUsing \u003ccode\u003esed\u003c/code\u003e to extract a patch from a file\u003c/h4\u003e\n\u003cp\u003eWhile running the \u003ca href=\"https://openai.com/codex/\"\u003eOpenAI Codex CLI\u003c/a\u003e locally, I came across a \u003ccode\u003esed\u003c/code\u003e command that prints\nspecific lines from a file. The agent often uses this command to output a portion of a file:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esed -n \u003cspan class=\"s1\"\u003e\u0026#39;12,15p\u0026#39;\u003c/span\u003e \u0026lt;filename\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis prints lines 12 through 15 of \u003ccode\u003e\u0026lt;filename\u0026gt;\u003c/code\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"march-29\"\u003eMarch 29\u003c/h3\u003e\n\u003ch4 id=\"dotgo-2014---john-graham-cumming---i-came-for-the-easy-concurrency-i-stayed-for-the-easy-composition\"\u003e\u003ca href=\"https://www.youtube.com/watch?v=woCg2zaIVzQ\"\u003edotGo 2014 - John Graham-Cumming - I came for the easy concurrency I stayed for the easy composition\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eI had to watch this twice to fully appreciate it. John Graham-Cumming crams a 40-minute talk\ninto 14 minutes. First, he shows how Go\u0026rsquo;s basic types and stdlib make writing a DNS lookup\nprogram quite trivial. Then he walks through how he generalized it with a simple interface.\nThe code can be found \u003ca href=\"https://github.com/jgrahamc/dotgo\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"2025"},{"content":"December 31 Exploring network programming by building a Toxiproxy clone – Jordan Neufeld Great talk by Jordan Neufeld on building a toy proxy server in Go that adds latency between upstream and downstream connections. It sits between a client and server, introducing delays, dropping connections, and simulating errors for chaos testing.\nDecember 26 Reflecting on life – Armin Ronacher The best way to completely destroy your long term satisfaction is if the effort you are putting into something, is not reciprocated or the nature of the work feels meaningless. It\u0026rsquo;s an obvious privilege to recommend that one shall not work for exploitative employers but you owe yourself to get this right. With time you build trust in yourself, and the best way to put this trust to use, is to break out of exploitative relationships.\nDecember 24 How I write HTTP services in Go after 13 years – Mat Ryer What I love about the Go ecosystem is its stability. I read an earlier version of this post a few years ago, and the fundamentals haven\u0026rsquo;t changed much since then.\nI like writing HTTP servers with Go\u0026rsquo;s net/http library, and here Matt shows how he organizes his HTTP server codebase for better readability and testability. I\u0026rsquo;ve picked up a few neat patterns from this. His take on testing is spot on — rather than testing each handler individually, it\u0026rsquo;s better to mimic production conditions and test the entire workflow.\nDecember 21 Program your next server in Go – Sameer Ajmani While I don\u0026rsquo;t get to write much Go at work these days, I love using it to build my tools and have been doing so for the past few years. Go doesn\u0026rsquo;t need much evangelizing anymore, but there\u0026rsquo;s one talk I often point people to because it perfectly captures why writing servers in Go makes so much sense.\nSameer starts with a toy search server and gradually makes it concurrent as the talk progresses. It showcases goroutines, buffered and unbuffered channels, and waiting on multiple channels with select, all in one coherent example.\nDecember 10 Taming flaky systems w/o DDoSing yourself in Python – Safe Retries with stamina I\u0026rsquo;ve been dabbling with Hynek\u0026rsquo;s stamina for a while. It\u0026rsquo;s a Python tool for retrying flaky service calls, built on top of the battle-tested tenacity library. It comes with a more ergonomic API, saner defaults, and some cool hooks for testing.\nOne neat pattern I learned from reading the source code is using a context manager in a for-loop to retry a block of code. If you\u0026rsquo;re writing a library that handles retries, you can\u0026rsquo;t add exception handling with try...except inside a user-written for-loop. Using a context manager for this is a clever trick. It allows you to write code like this:\nfor attempt in stamina.retry_context(on=httpx.HTTPError): with attempt: resp = httpx.get(f\u0026#34;https://httpbin.org/status/404\u0026#34;) resp.raise_for_status() December 08 Transactions: myths, surprises and opportunities — Martin Kleppmann This is hands down one of the best talks I\u0026rsquo;ve seen on the topic. Martin points out that in ACID, consistency doesn\u0026rsquo;t carry the same rigid meaning as the other three constituents. It was kinda shoved in there to make the mnemonic work.\nHe also highlighted that, while terms like read uncommitted, read committed, snapshot isolation, and serializable are widely used to describe different isolation levels, few can recall their exact meanings off the top of their heads. This is because the names reflect implementation details from 1970s databases rather than the actual concepts. Beyond clarifying isolation levels, the talk also explores how incredibly hard it is to achieve transactions across multiple services without centralized coordination.\nNovember 13 November ramble — Oz Nova I generally prefer not to comment on software development practices, because of something I\u0026rsquo;ve observed often enough that it feels like a law: for every excellent engineer who swears by a particular practice, there\u0026rsquo;s an even better one who swears by the opposite. Some people couldn\u0026rsquo;t imagine coding without unit tests, or code review, or continuous integration, or step-through debugging, or [your preferred \u0026ldquo;best practice\u0026rdquo;]. Yet, there are people out there who do the exact opposite and outperform us all.\nNovember 11 Brian Kernighan reflects on Unix: A History and a Memoir — Book Overflow So is it possible for, you know, two or three people like us to have a really good idea and do something that does transform our world? I suspect that that still can happen. It\u0026rsquo;s different, and certainly at the time I was in, you know, early days of Unix, the world was smaller and simpler in the computing world, and so it was probably easier, and there was more low-hanging fruit. But I suspect that there\u0026rsquo;s still opportunities like that.\nI think the reason Unix and all of the things that went with it worked so well was there was a big contribution from Doug\u0026rsquo;s ability to improve people\u0026rsquo;s lives so that what we did was described well as well as working well. So I think Doug in that sense would be the unsung person who didn\u0026rsquo;t get as much recognition as perhaps deserved.\n_ — Brian Kernighan_\nNovember 10 Python\u0026rsquo;s finally gotchas Python core developer Irit Katriel recently shared a short piece discussing a few gotchas with Python\u0026rsquo;s finally statement. I don\u0026rsquo;t think I\u0026rsquo;ve ever seen continue, break, or return statements in a finally block, but if you do use them there, avoid it, as they can lead to some unusual behavior.\nThe return statement in the finally block can suppress exceptions implicitly. For example:\ndef foo() -\u0026gt; int: try: 1 / 0 except Exception: raise finally: return 0 Running this function will suppress the exception and return 0. While this might seem surprising, it works this way because Python guarantees that the finally block will always run. This issue can be avoided by removing the finally block and dedenting the return. Similarly, continue and break behaves differently in that block.\nThis behavior is documented in the official docs. However, maintainers are considering making this a warning and, eventually, illegal.\nNovember 9 Software engineering at Google I\u0026rsquo;ve been skimming through Software Engineering at Google over the past few days. It\u0026rsquo;s available online for free, which is a nice bonus. Rather than focusing on specific technologies or operational mechanisms, the book highlights the organization-wide engineering policies that have helped Google scale. The text is sparse and, at times, quite boring, but there are definitely some gems that kept me going. Here are three interesting terms I\u0026rsquo;ve picked up so far:\nBeyoncé Rule – Inspired by Beyoncé\u0026rsquo;s line, \u0026ldquo;If you liked it, then you should have put a ring on it.\u0026rdquo; If you think something\u0026rsquo;s important, write a test for it and make sure it\u0026rsquo;s part of the CI.\nChesterton\u0026rsquo;s Fence – Don\u0026rsquo;t dismantle an established practice without understanding why it exists. Consider why certain legacy systems or rules are in place before changing or removing them.\nHaunted Graveyard – Parts of the codebase no one wants to touch — difficult to maintain or just feel \u0026ldquo;cursed.\u0026rdquo; They\u0026rsquo;re usually left alone because the cost to update them is high, and no one fully understands them.\nI\u0026rsquo;ve always wanted to put names on these things, and now I can!\nNovember 08 Books on engineering policies vs mechanisms The further I got in my career, the less value I gained from books on mechanisms and more from books on policies. But policy books are boring.\nMy 17th book on writing better Python or Go was way more fun to read than Software Engineering at Google but yielded far less value — the age-old strategy vs. operations dichotomy.\nOctober 27 Understanding round robin DNS Round Robin DNS works by adding multiple IP addresses for the same domain in your DNS provider\u0026rsquo;s settings. For example, if you\u0026rsquo;re using a VPS from DigitalOcean or Hetzner, you\u0026rsquo;d add a bunch of A records for the same subdomain (like foo.yourdomain.com) and point each to a different server IP, like:\n203.0.113.45 198.51.100.176 5.62.153.87 89.160.23.104 When a request comes in, the DNS resolver picks one of the IPs and sends the request to that server — basically a poor man\u0026rsquo;s load balancer. But there are some client-side quirks in how browsers pick the IPs, and this blog digs into that.\nWrites and write-nots — Paul Graham These two powerful opposing forces, the pervasive expectation of writing and the irreducible difficulty of doing it, create enormous pressure. This is why eminent professors often turn out to have resorted to plagiarism. The most striking thing to me about these cases is the pettiness of the thefts. The stuff they steal is usually the most mundane boilerplate — the sort of thing that anyone who was even halfway decent at writing could turn out with no effort at all. Which means they\u0026rsquo;re not even halfway decent at writing.\nOctober 14 OpenTelemetry client architecture At the highest architectural level, OpenTelemetry clients are organized into signals. Each signal provides a specialized form of observability. For example, tracing, metrics, and baggage are three separate signals. Signals share a common subsystem – context propagation – but they function independently from each other.\nEach signal provides a mechanism for software to describe itself. A codebase, such as web framework or a database client, takes a dependency on various signals in order to describe itself. OpenTelemetry instrumentation code can then be mixed into the other code within that codebase. This makes OpenTelemetry a cross-cutting concern - a piece of software which is mixed into many other pieces of software in order to provide value. Cross-cutting concerns, by their very nature, violate a core design principle – separation of concerns. As a result, OpenTelemetry client design requires extra care and attention to avoid creating issues for the codebases which depend upon these cross-cutting APIs.\nOpenTelemetry clients are designed to separate the portion of each signal which must be imported as cross-cutting concerns from the portions which can be managed independently. OpenTelemetry clients are also designed to be an extensible framework. To accomplish these goals, each signal consists of four types of packages: API, SDK, Semantic Conventions, and Contrib.\nOctober 05 Private DNS with MagicDNS — Tailscale blog Tailscale runs a DNS server built-in on every node, running at 100.100.100.100.\nYes, Tailscale on your phone includes a DNS server. (We admit that \u0026ldquo;even on your phone!\u0026rdquo; is a little silly when phones are basically supercomputers these days.)\nThe IP 100.100.100.100, usually pronounced \u0026ldquo;quad one hundred,\u0026rdquo; is part of the private Carrier-Grade NAT range. That means, just like IPs in the common private ranges, 192.168.1/24, 172.16/12, and 10/8, it is not routable on the public internet. So when software on your computer sends a traditional, unencrypted UDP packet to 100.100.100.100, no standard router will send it anyway.\nWe then tell your OS that its DNS server is 100.100.100.100. Because operating system DNS clients are largely stuck in 1987, they then forward all their DNS queries over old-school insecure UDP DNS to 100.100.100.100. Tailscale also installs a route to 100.100.100.100/32 back into Tailscale and it then hands those packets over to Tailscale\u0026rsquo;s built-in DNS server, so unencrypted queries don\u0026rsquo;t leave your device.\nOctober 04 Git reset vs revert I misunderstood git revert and made a mess out of my main branch today. Thought it worked like git reset — but they\u0026rsquo;re not quite the same.\nHere\u0026rsquo;s the breakdown:\ngit reset --soft \u0026lt;commit-sha\u0026gt; moves the branch back to the specific commit but keeps your changes. It rewrites history, so you\u0026rsquo;ll need a force push to update the remote.\ngit revert \u0026lt;commit-sha\u0026gt; creates a new commit that undoes the changes from that commit without meddling with history. No force push needed.\nSeems like revert is what you need if you accidentally merge something into main. Keeps things clean without rewriting history.\nSeptember 28 Rails World 2024 opening keynote — David Heinemeier Hansson I was really hyped about this year\u0026rsquo;s Rails World, even though I don\u0026rsquo;t code much in Ruby or Rails. I\u0026rsquo;ve been following 37signals\u0026rsquo; work on simplifying deployment complexity and dogfooding their own tools to show how well they work.\nIt\u0026rsquo;s also refreshing to see someone with more influence acknowledging that the JS ecosystem is unsustainably complex. Not everyone digs that, no matter how hip it might be. Personally, I usually have a higher tolerance for backend and infra complexity than for frontend.\nKamal 2.0 now makes it easy to deploy multiple containers behind SSL on a single VM without dealing with the usual infrastructure idiosyncrasies.\nThen we have Kamal 2. This is how you\u0026rsquo;re going to get your application into the cloud, your own hardware, or any container, because we\u0026rsquo;re not tying ourselves to a PaaS. Kamal 2 levels this up substantially. It does Auto SSL through Let\u0026rsquo;s Encrypt, so you don\u0026rsquo;t even have to know anything about provisioning an SSL certificate. It allows multiple applications to run on a single server, scaling down as well as up. It comes with a simple declaration setup for detailing what your deployment looks like, encapsulated in the fewest possible pieces of information to get as close as possible to no config.\nThe initial trigger for me to get interested in no build for Rails 7 was an infuriating annoyance: being unable to compile a JavaScript project I had carelessly left alone for about five minutes. None of the tools worked; everything was outdated. And when I tried to update it so I could compile it again, I literally couldn\u0026rsquo;t figure it out. I spent half a day wrestling with Webpacker at the time, and I did turn over the table, saying, \u0026lsquo;No, I made the integration for Webpacker to Rails, and I cannot figure out how this works. There\u0026rsquo;s something deeply, fundamentally broken in that model.\u0026rsquo; And that\u0026rsquo;s when I realized the truth: only the browser is forever.\nSeptember 25 The man who killed Google search — Edward Zitron Every single article I\u0026rsquo;ve read about Gomes\u0026rsquo; tenure at Google spoke of a man deeply ingrained in the foundation of one of the most important technologies ever made, who had dedicated decades to maintaining a product with a — to quote Gomes himself — \u0026ldquo;guiding light of serving the user and using technology to do that.\u0026rdquo; And when finally given the keys to the kingdom — the ability to elevate Google Search even further — he was ratfucked by a series of rotten careerists trying to please Wall Street, led by Prabhakar Raghavan.\nSeptember 23 Microservices are technical debt — Matt Ranney, Principal Engineer, Doordash Microservices are technical debt because while they initially allow teams to move faster by working independently, they eventually create a distributed monolith, where services become so intertwined that they require excessive maintenance and coordination, slowing down future development.\nThe real driver for adopting microservices is not necessarily scaling traffic, but scaling teams — when too many developers are working on the same monolith, they step on each other\u0026rsquo;s toes during deployments, forcing the need for smaller, independently deployable services.\nSurely at this point the comment threads are going to explode with people saying that microservices should never share databases — like, can you believe that sacrilege of having two services share the same database? How do you live with yourself?\nSeptember 22 How streaming LLM APIs work — Simon Willison While it\u0026rsquo;s pretty easy to build a simple HTTP streaming endpoint with any basic framework and some generator-like language construct, I\u0026rsquo;ve always been curious about how production-grade streaming LLM endpoints from OpenAI, Anthropic, or Google work. It seems like they\u0026rsquo;re using a similar pattern:\nAll three of the APIs I investigated worked roughly the same: they return data with a content-type: text/event-stream header, which matches the server-sent events mechanism, then stream blocks separated by \\r\\n\\r\\n. Each block has a data: JSON line. Anthropic also include a event: line with an event type.\nAnnoyingly these can\u0026rsquo;t be directly consumed using the browser EventSource API because that only works for GET requests, and these APIs all use POST.\nIt seems like all of them use a somewhat compliant version of Server-Sent Events (SSE) to stream the responses.\nSeptember 17 DHH talks Apple, Linux, and running servers — How About Tomorrow During yesterday evening\u0026rsquo;s walk, I had a lot of fun listening to DHH rant about the Apple ecosystem and the big cloud providers. I can totally get behind how so many people find deployment harder than it actually is, and how the big cloud providers are making bank off that.\nWe were incredibly proud that we were going to take on Gmail with a fresh new system based on thinking from 2020, not 2004, and we thought that was going to be the big boss, right? We\u0026rsquo;re going to take on Google with an actually quite good email system. But we didn\u0026rsquo;t even get to begin that fight because before a bigger boss showed up and just like Apple sat down on our chest and said, \u0026lsquo;Give me your — you\u0026rsquo;re going to give me your lunch money and 30% of everything you own in perpetuity going forward.\u0026rsquo;\nWe used to be in the cloud. We used to be on AWS. We used to be on all this stuff for a bunch of our things with Basecamp and Hey, and we yanked all of it out because cost was just getting ridiculous, and we built a bit of tooling, and now I\u0026rsquo;m on a goddamn mission to make open source as capable, as easy to use as all these AWS resellers against any box running basic Linux with an IP address you can connect to.\nSeptember 16 The many meanings of event-driven architecture — Martin Fowler In this 2017 talk, Martin Fowler untangled a few concepts for me that often get lumped together under the event-driven umbrella. He breaks event-driven systems into four main types:\nEvent notification: A system sends out a signal (event) when something happens but with minimal details. Other systems receive the notification and must request more information if needed. This keeps things simple and decoupled but makes tracking harder since the event doesn\u0026rsquo;t include full data.\nEvent-carried state transfer: Events carry all the necessary data upfront, so no extra requests are needed. This simplifies interactions but can make events bulky and harder to manage as the system scales.\nEvent sourcing: Instead of storing just the current state, the system logs every event that occurs. This allows you to reconstruct the state at any time. It\u0026rsquo;s great for auditing and troubleshooting but adds complexity as log data grows.\nCQRS: Commands (write operations) and queries (read operations) are handled separately, letting each be optimized on its own. It works well for complex domains but introduces more architectural overhead and needs careful planning.\nInterestingly, I\u0026rsquo;ve been using the second one without knowing what it was called.\nSeptember 15 Founder Mode, hackers, and being bored by tech — Ian Betteridge On a micro scale, I think, there\u0026rsquo;s still a lot to be excited about. But on the macro level, this VC-Founder monoculture has been stealing the thunder from what really matters — the great technology that should have been a testament to the hive mind\u0026rsquo;s ingenuity. Instead, all the attention is on the process itself.\nTech has become all Jobs and no Woz. As Dave Karpf rightly identifies, the hacker has vanished from the scene, to be replaced by an endless array of know-nothing hero founders whose main superpower is the ability to bully subordinates (and half of Twitter) into believing they are always right.\nSeptember 14 Simon Willison on the Software Misadventures podcast I spent a delightful 2 hours this morning listening to Simon Willison talk about his creative process and how LLMs have evolved his approach.\nHe shared insights into how he\u0026rsquo;s become more efficient with time, writing consistently on his blog, inspired by things like Duolingo\u0026rsquo;s streak and Tom Scott\u0026rsquo;s weekly video run for a decade. Another thing I found fascinating is how he uses GitHub Issues to record every little detail of a project he\u0026rsquo;s working on. This helps him manage so many projects at once without burning out. Simon even pulled together a summary from the podcast transcript that captured some of the best bits of the discussion.\nAbout 5 years ago, one of Simon\u0026rsquo;s tweets inspired me to start publishing my thoughts and learnings, no matter how trivial they may seem. My career has benefited immensely from that. The process of reifying your ideas and learning on paper seems daunting at first, but it gets easier over time.\nSeptember 09 Canonical log lines — Stripe Engineering Blog I\u0026rsquo;ve been practicing this for a while but didn\u0026rsquo;t know what to call it. Canonical log lines are arbitrarily wide structured log messages that get fired off at the end of a unit of work. In a web app, you could emit a special log line tagged with different IDs and attributes at the end of every request. The benefit is that when debugging, these are the logs you\u0026rsquo;ll check first. Sifting through fewer messages and correlating them with other logs makes investigations much more effective, and the structured nature of these logs allows for easier filtering and automated analysis.\nOut of all the tools and techniques we deploy to help get insight into production, canonical log lines in particular have proven to be so useful for added operational visibility and incident response that we\u0026rsquo;ve put them in almost every service we run — not only are they used in our main API, but there\u0026rsquo;s one emitted every time a webhook is sent, a credit card is tokenized by our PCI vault, or a page is loaded in the Stripe Dashboard.\nSeptember 07 Recognizing the Gell-Mann Amnesia effect in my use of LLM tools It took time for me to recognize the Gell-Mann Amnesia effect shaping how I use LLM tools in my work. When dealing with unfamiliar tech, I\u0026rsquo;m quick to accept suggestions verbatim, but in a domain I know, the patches rarely impress and often get torn to shreds.\nSeptember 04 On the importance of ablation studies in deep learning research — François Chollet This is true for almost any engineering effort. It\u0026rsquo;s always a good idea to ask if the design can be simplified without losing usability. Now I know there\u0026rsquo;s a name for this practice: ablation study.\nThe goal of research shouldn\u0026rsquo;t be merely to publish, but to generate reliable knowledge. Crucially, understanding causality in your system is the most straightforward way to generate reliable knowledge. And there\u0026rsquo;s a very low-effort way to look into causality: ablation studies. Ablation studies consist of systematically trying to remove parts of a system — making it simpler — to identify where its performance actually comes from. If you find that X + Y + Z gives you good results, also try X, Y, Z, X + Y, X + Z, and Y + Z, and see what happens.\nIf you become a deep learning researcher, cut through the noise in the research process: do ablation studies for your models. Always ask, \u0026ldquo;Could there be a simpler explanation? Is this added complexity really necessary? Why?\nSeptember 01 Why A.I. Isn\u0026rsquo;t Going to Make Art — Ted Chiang, The New Yorker I indiscriminately devour almost everything Ted Chiang puts out, and this piece is no exception. It\u0026rsquo;s one of the most articulate arguments I\u0026rsquo;ve read on the sentimental value of human-generated artifacts, even when AI can make perfect knockoffs.\nI\u0026rsquo;m pro-LLMs and use them to aid my work all the time. While they\u0026rsquo;re incredibly useful for a certain genre of tasks, buying into the Silicon Valley idea that these are soon going to replace every type of human-generated content is incredibly naive and redolent of the hubris within the tech bubble.\nArt is notoriously hard to define, and so are the differences between good art and bad art. But let me offer a generalization: art is something that results from making a lot of choices. This might be easiest to explain if we use fiction writing as an example. When you are writing fiction, you are — consciously or unconsciously — making a choice about almost every word you type; to oversimplify, we can imagine that a ten-thousand-word short story requires something on the order of ten thousand choices. When you give a generative-A.I. program a prompt, you are making very few choices; if you supply a hundred-word prompt, you have made on the order of a hundred choices.\nGenerative A.I. appeals to people who think they can express themselves in a medium without actually working in that medium. But the creators of traditional novels, paintings, and films are drawn to those art forms because they see the unique expressive potential that each medium affords. It is their eagerness to take full advantage of those potentialities that makes their work satisfying, whether as entertainment or as art.\nAny writing that deserves your attention as a reader is the result of effort expended by the person who wrote it. Effort during the writing process doesn\u0026rsquo;t guarantee the end product is worth reading, but worthwhile work cannot be made without it.\nSome individuals have defended large language models by saying that most of what human beings say or write isn\u0026rsquo;t particularly original. That is true, but it\u0026rsquo;s also irrelevant. When someone says \u0026ldquo;I\u0026rsquo;m sorry\u0026rdquo; to you, it doesn\u0026rsquo;t matter that other people have said sorry in the past; it doesn\u0026rsquo;t matter that \u0026ldquo;I\u0026rsquo;m sorry\u0026rdquo; is a string of text that is statistically unremarkable. If someone is being sincere, their apology is valuable and meaningful, even though apologies have previously been uttered. Likewise, when you tell someone that you\u0026rsquo;re happy to see them, you are saying something meaningful, even if it lacks novelty.\nAugust 31 How to Be a Better Reader — Tina Jordan, The NY Times To read more deeply, to do the kind of reading that stimulates your imagination, the single most important thing to do is take your time. You can\u0026rsquo;t read deeply if you\u0026rsquo;re skimming. As the writer Zadie Smith has said, \u0026ldquo;When you practice reading, and you work at a text, it can only give you what you put into it.\u0026rdquo;\nAt a time when most of us read in superficial, bite-size chunks that prize quickness — texts, tweets, emails — it can be difficult to retrain your brain to read at an unhurried pace, but it is essential. In \u0026ldquo;Slow Reading in a Hurried Age,\u0026rdquo; David Mikics writes that \u0026ldquo;slow reading changes your mind the way exercise changes your body: A whole new world will open up, you will feel and act differently, because books will be more open and alive to you.\u0026rdquo;\nAugust 26 Dark Matter — Blake Crouch I just finished the book. It\u0026rsquo;s an emotional rollercoaster of a story, stemming from a MacGuffin that enables quantum superposition in the macro world, bringing the Copenhagen interpretation of quantum mechanics to life.\nWhile the book starts off with a bang, it becomes a bit more predictable as the story progresses. I still enjoyed how well the author reified the probable dilemma that having access to the multiverse might pose. Highly recommened. I\u0026rsquo;m already beyond excited to read his next book, Recursion.\n","permalink":"https://rednafi.com/feed/2024/","summary":"\u003ch3 id=\"december-31\"\u003eDecember 31\u003c/h3\u003e\n\u003ch4 id=\"exploring-network-programming-by-building-a-toxiproxy-clone--jordan-neufeld\"\u003e\u003ca href=\"https://www.youtube.com/watch?v=8z6okCgdREo\"\u003eExploring network programming by building a Toxiproxy clone – Jordan Neufeld\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eGreat talk by Jordan Neufeld on building a toy proxy server in Go that adds latency between\nupstream and downstream connections. It sits between a client and server, introducing\ndelays, dropping connections, and simulating errors for chaos testing.\u003c/p\u003e\n\u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n      \u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen\" loading=\"eager\" referrerpolicy=\"strict-origin-when-cross-origin\" src=\"https://www.youtube.com/embed/8z6okCgdREo?autoplay=0\u0026amp;controls=1\u0026amp;end=0\u0026amp;loop=0\u0026amp;mute=0\u0026amp;start=0\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" title=\"YouTube video\"\u003e\u003c/iframe\u003e\n    \u003c/div\u003e\n\n\u003chr\u003e\n\u003ch3 id=\"december-26\"\u003eDecember 26\u003c/h3\u003e\n\u003ch4 id=\"reflecting-on-life--armin-ronacher\"\u003e\u003ca href=\"https://lucumr.pocoo.org/2024/12/26/reflecting-on-life/\"\u003eReflecting on life – Armin Ronacher\u003c/a\u003e\u003c/h4\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eThe best way to completely destroy your long term satisfaction is if the effort you are\nputting into something, is not reciprocated or the nature of the work feels meaningless.\nIt\u0026rsquo;s an obvious privilege to recommend that one shall not work for exploitative employers\nbut you owe yourself to get this right. With time you build trust in yourself, and the\nbest way to put this trust to use, is to break out of exploitative relationships.\u003c/em\u003e\u003c/p\u003e","title":"2024"},{"content":" Hey there! I\u0026rsquo;m Redowan Delowar - also go by the handle \u0026ldquo;rednafi\u0026rdquo; on the web.\nI dabble in platform engineering, which mostly means working with giant distributed balls of mud and fighting to keep the house of cards from falling over. Lately, I\u0026rsquo;ve been focused on persistence, resilience, and observability - or whatever else is currently on fire. When not at the keyboard, I\u0026rsquo;m probably running or reading sci-fi.\nTalking points: Go, Python, persistence, consistency, resilience, HA, and sci-fi.\nCurriculum vitae For the paper trail inclined, here\u0026rsquo;s my CV.\nContact Email Bluesky GitHub LinkedIn Colophon Hugo powers this blog. It\u0026rsquo;s rocking the Papermod theme with some handrolled CSS flair. I write in plain Markdown and push the content to GitHub, triggering the GitHub Actions CI, which then deploys the site to GitHub Pages.\nMore on the stack, deployment, and writing process if you\u0026rsquo;re curious.\nBreadcrumbs The handle \u0026ldquo;rednafi\u0026rdquo; is just a portmanteau of my first name, Redowan, and my nickname Nafi, which my parents and a few close friends still use. I like sci-fis, and my all-time favourite book is Stanislaw Lem\u0026rsquo;s Solaris. I think through writing and not the other way around. Most of the blogs on this site take way longer to write than I\u0026rsquo;d like to admit. Greatest hits Oh my poor business logic The diminishing half-life of knowledge Annotating args and kwargs in Python An ode to the neo-grotesque web HTTP requests via /dev/tcp You probably don\u0026rsquo;t need a DI framework Dotfiles stewardship for the indolent Configuring options in Go Recipes from Python SQLite docs Limit goroutines with buffered channels Pesky little scripts I kind of like rebasing ","permalink":"https://rednafi.com/about/","summary":"\u003cdiv style=\"text-align: center;\"\u003e\n    \u003cfigure style=\"margin: auto;\"\u003e\n        \u003cimg src=\"https://blob.rednafi.com/static/images/about/profile-2025.jpg\"\n             alt=\"Profile picture\"\n             width=\"400\"\n             height=\"400\"\n             class=\"loading\"\n             style=\"display: block; margin: 0 auto; width: 400px; height: auto; border-radius: 20px;\"\u003e\n    \u003c/figure\u003e\n\u003c/div\u003e\n\u003chr\u003e\n\u003cp\u003eHey there! I\u0026rsquo;m Redowan Delowar - also go by the handle \u0026ldquo;rednafi\u0026rdquo; on the web.\u003c/p\u003e\n\u003cp\u003eI dabble in platform engineering, which mostly means working with giant distributed balls\nof mud and fighting to keep the house of cards from falling over. Lately, I\u0026rsquo;ve been focused\non persistence, resilience, and observability - or whatever else is currently on fire. When\nnot at the keyboard, I\u0026rsquo;m probably running or reading sci-fi.\u003c/p\u003e","title":"About"},{"content":"Friends Anton Zhiyanov Matthias Doepmann Preslav Rachev Frequent stops Brandur Charity Majors Dan Luu Drew DeVault\u0026rsquo;s Blog Fabien Sanglard\u0026rsquo;s Website Harmful Stuff Julia Evans Matt T. Proud Peter Bourgon Simon Willison\u0026rsquo;s Weblog ","permalink":"https://rednafi.com/blogroll/","summary":"\u003ch2 id=\"friends\"\u003eFriends\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://antonz.org/\"\u003eAnton Zhiyanov\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://revontulet.dev/\"\u003eMatthias Doepmann\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://preslav.me/\"\u003ePreslav Rachev\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"frequent-stops\"\u003eFrequent stops\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://brandur.org/\"\u003eBrandur\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://charity.wtf\"\u003eCharity Majors\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://danluu.com/\"\u003eDan Luu\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://drewdevault.com/\"\u003eDrew DeVault\u0026rsquo;s Blog\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://fabiensanglard.net/\"\u003eFabien Sanglard\u0026rsquo;s Website\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://harmful.cat-v.org/\"\u003eHarmful Stuff\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://jvns.ca/\"\u003eJulia Evans\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.matttproud.com/blog/index.html\"\u003eMatt T. Proud\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://peter.bourgon.org/\"\u003ePeter Bourgon\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://simonwillison.net/\"\u003eSimon Willison\u0026rsquo;s Weblog\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- references --\u003e\n\u003c!-- prettier-format-start --\u003e\n\u003c!-- friends --\u003e\n\u003c!-- frequent stops --\u003e\n\u003c!-- prettier-format-end --\u003e","title":"Blogroll"},{"content":"","permalink":"https://rednafi.com/misc/sidecar-communication/","summary":"","title":"Docker sidecar communication with Unix Domain Socket (UDS)"},{"content":"Wisdoms, aphorisms, and pointed observations — fragments I find myself frequently quoting in conversations about software, philosophy, and ways of working.\nChesterton\u0026rsquo;s fence Reforms should not be made until the reasoning behind the existing state of affairs is understood.\nGell-Mann amnesia effect The Gell-Mann amnesia effect is a cognitive bias describing the tendency of individuals to critically assess media reports in a domain they are knowledgeable about, yet continue to trust reporting in other areas despite recognizing similar potential inaccuracies.\nGall\u0026rsquo;s law A complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system.\nHanlon\u0026rsquo;s razor Never attribute to malice that which is adequately explained by stupidity.\nHofstadter\u0026rsquo;s law It always takes longer than you expect, even when you take into account Hofstadter\u0026rsquo;s Law.\nHyrum\u0026rsquo;s law (the law of implicit interfaces) With a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviours of your system will be depended on by somebody.\nKernighan\u0026rsquo;s law Debugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it.\nOccam\u0026rsquo;s razor Entities should not be multiplied without necessity.\nParkinson\u0026rsquo;s law Work expands so as to fill the time available for its completion.\nRingelmann effect It\u0026rsquo;s the tendency of an individual to become increasingly inefficient as more and more people are involved in a task. In other words, as more individuals are added to a team, the more the average individual performance decreases.\nSegal\u0026rsquo;s law A man with a watch knows what time it is. A man with two watches is never sure.\nAt surface level, the adage emphasizes the consistency that arises when information comes from a single source and points out the potential pitfalls of having too much conflicting information. However, the underlying message is to question the apparent certainty of anyone who only has one source of information. The man with one watch has no way to identify error or uncertainty.\nThe Beyoncé rule If you want to be confident that a system exhibits a particular behavior, the only way to be sure it will is to write an automated test for it. Google calls it the Beyoncé Rule. Succinctly, it can be stated as follows: \u0026ldquo;If you liked it, then you shoulda put a test on it.\u0026rdquo;\nThe ETTO principle The ETTO (Efficiency–Thoroughness Trade-Off) principle says people and organizations are always balancing doing things quickly with doing things carefully.\nThe law of conservation of complexity (Tesler\u0026rsquo;s law) There is a certain amount of complexity in a system which cannot be reduced.\nThe law of leaky abstractions All non-trivial abstractions, to some degree, are leaky.\nThe law of the instrument If all you have is a hammer, everything looks like a nail.\nThe law of triviality Groups will give far more time and attention to trivial or cosmetic issues rather than serious and substantial ones.\nThe principle of least astonishment People are part of the system. The design should match the user\u0026rsquo;s experience, expectations, and mental models.\nThe Ulysses contract A Ulysses contract is an agreement you make with yourself to resist future temptation by limiting your own choices in advance. It\u0026rsquo;s a way to act on your long-term interests instead of short-term impulses. You know your future self might give in to weakness, so you set up rules or constraints now while you are still in control.\nThe Unix philosophy Software components should be small, and focused on doing one specific thing well. This can make it easier to build systems by composing together small, simple, well-defined units, rather than using large, complex, multi-purpose programs.\n","permalink":"https://rednafi.com/lore/","summary":"\u003cp\u003eWisdoms, aphorisms, and pointed observations — fragments I find myself frequently quoting in\nconversations about software, philosophy, and ways of working.\u003c/p\u003e\n\u003ch3 id=\"chesterton\"\u003e\u003ca href=\"https://github.com/dwmkerr/hacker-laws#chestertons-fence\"\u003eChesterton\u0026rsquo;s fence\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eReforms should not be made until the reasoning behind the existing state of affairs is\nunderstood.\u003c/p\u003e\n\u003ch3 id=\"gell-mann-amnesia-effect\"\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Gell-Mann_amnesia_effect\"\u003eGell-Mann amnesia effect\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe Gell-Mann amnesia effect is a cognitive bias describing the tendency of individuals to\ncritically assess media reports in a domain they are knowledgeable about, yet continue to\ntrust reporting in other areas despite recognizing similar potential inaccuracies.\u003c/p\u003e","title":"Lore"},{"content":"Oratories on paraphernalia.\nGo interface segregation redux - Dec 3, 2025, GDG Berlin Slides Video ","permalink":"https://rednafi.com/talks/","summary":"\u003cp\u003eOratories on paraphernalia.\u003c/p\u003e\n\u003ch2 id=\"go-interface-segregation-redux---dec-3-2025-gdg-berlin\"\u003eGo interface segregation redux - Dec 3, 2025, GDG Berlin\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://docs.google.com/presentation/d/10d4K1V4uJLsanzBS6Jz-Ibnw7kYXnQJJWSEKIimbqnU/edit?usp=sharing\"\u003eSlides\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=AtSutJ2rSr8\"\u003eVideo\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"Talks"}]