[{"content":"When working with Go in an industrial context, I feel like dependency injection (DI) often gets a bad rep because of DI frameworks. But DI as a technique is quite useful. It just tends to get explained with too many OO jargons and triggers PTSD among those who came to Go to escape GoF theology.\nDependency Injection is a 25-dollar term for a 5-cent concept.\n— James Shore\nDI basically means passing values into a constructor instead of creating them inside it. That’s really it. Observe:\ntype server struct { db DB } // NewServer constructs a server instance func NewServer() *server { db := DB{} // The dependency is created here return \u0026server{db: db} } Here, NewServer creates its own DB. Instead, to inject the dependency, build DB elsewhere and pass it in as a constructor parameter:\nfunc NewServer(db DB) *server { return \u0026server{db: db} } Now the constructor no longer decides how a database is built; it simply receives one.\nIn Go, DI is often done using interfaces. You collate the behavior you care about in an interface, and then provide different concrete implementations for different contexts. In production, you pass a real implementation of DB. In unit tests, you pass a fake implementation that behaves the same way from the caller’s perspective but avoids real database calls.\nHere’s how that looks:\n// behaviour we care about type DB interface { Get(id string) (string, error) Save(id, value string) error } type server struct{ db DB } // NewServer accepts a concrete implementation of the DB interface in runtime // and passes it to the server struct. func NewServer(db DB) *server { return \u0026server{db: db} } A real implementation of DB might look like this:\ntype RealDB struct{ url string } func NewDB(url string) *RealDB { return \u0026RealDB{url: url} } func (r *RealDB) Get(id string) (string, error) { // pretend we hit Postgres return \"real value\", nil } func (r *RealDB) Save(id, value string) error { return nil } And a fake implementation for unit tests might be:\ntype FakeDB struct{ data map[string]string } func NewFake() *FakeDB { return \u0026FakeDB{data: map[string]string{}} } func (f *FakeDB) Get(id string) (string, error) { return f.data[id], nil } func (f *FakeDB) Save(id, value string) error { f.data[id] = value; return nil } Use the fake in unit tests like so:\nfunc TestServerGet(t *testing.T) { fake := NewFake() _ = fake.Save(\"42\", \"fake\") srv := NewServer(fake) val, _ := srv.db.Get(\"42\") if val != \"fake\" { t.Fatalf(\"want fake, got %s\", val) } } The compiler guarantees both RealDB and FakeDB satisfy DB, and during tests, we can swap out the implementations without much ceremony.\nWhy frameworks turn mild annoyance into actual pain Once NewServer grows half a dozen dependencies, wiring them by hand can feel noisy. That’s when a DI framework starts looking tempting.\nWith Uber’s dig, you register each constructor as a provider. Provide takes a function, uses reflection to inspect its parameters and return type, and adds it as a node in an internal dependency graph. Nothing is executed yet. Things only run when you call .Invoke() on the container.\nBut that reflection-driven magic is also where the pain starts. As your graph grows, it gets harder to tell which constructor feeds which one. Some constructor takes one parameter, some takes three. There’s no single place you can glance at to understand the wiring. It’s all figured out inside the container at runtime.\nLet the container figure it out!\n— every DI framework ever\nfunc BuildContainer() *dig.Container { c := dig.New() // Each Provide call teaches dig about one node in the graph. c.Provide(NewConfig) // produces *Config c.Provide(NewDB) // wants *Config, produces *DB c.Provide(NewRepo) // wants *DB, produces *Repo c.Provide(NewFlagClient) // produces *FlagClient c.Provide(NewService) // wants *Repo, *FlagClient, produces *Service c.Provide(NewServer) // wants *Service, produces *server return c } func main() { // Invoke kicks off the whole graph. dig topologically sorts, calls each // constructor, and finally hands *server to your callback. if err := BuildContainer().Invoke( func(s *server) { s.Run() }); err != nil { panic(err) } } Now try commenting out NewFlagClient. The code still compiles. There’s no error until runtime, when dig fails to construct NewService due to a missing dependency. And the error message you get?\ndig invoke failed: could not build arguments for function main.main.func1 (prog.go:87) : failed to build *main.Server : could not build arguments for function main.NewServer (prog.go:65) : failed to build *main.Service: missing dependencies for function main.NewService (prog.go:55) : missing type: *main.FlagClient That’s five stack frames deep, far from where the problem started. Now you’re digging through dig’s internals to reconstruct the graph in your head.\nGoogle’s wire takes a different approach: it shifts the graph-building to code generation. You collect your constructors in a wire.NewSet, call wire.Build, and the generator writes a wire_gen.go that wires everything up explicitly.\nvar serverSet = wire.NewSet( NewConfig, NewDB, NewRepo, NewFlagClient, // comment this line out to see Wire complain at compile time NewService, NewServer, ) func InitializeServer() (*server, error) { wire.Build(serverSet) return nil, nil // replaced by generated code } Comment out NewFlagClient and Wire fails earlier—during generation:\nwire: ../../service/wire.go:13:2: cannot find dependency for *flags.Client It’s better than dig’s runtime panic, but still comes with its own headaches:\nYou need to remember to run go generate ./... whenever constructor signatures change. When something breaks, you’re stuck reading through hundreds of lines of autogenerated glue to trace the issue. You have to teach every teammate Wire’s DSL—wire.NewSet, wire.Build, build tags, and sentinel rules. And if you ever switch to something different like dig, you’ll need to learn a completely different set of concepts: Provide, Invoke, scopes, named values, etc. While DI frameworks tend to use vocabularies like provider or container to give you an essense of familiarity, they still reinvent the API surface every time. Switching between them means relearning a new mental model.\nSo the promise of “just register your providers and forget about wiring” ends up trading clear, compile-time control for either reflection or hidden generator logic—and yet another abstraction layer you have to debug.\nThe boring alternative: keep wiring explicit In Go, you can just wire your own dependencies manually. Like this:\nfunc main() { cfg := NewConfig() db := NewDB(cfg.DSN) repo := NewRepo(db) flags := NewFlagClient(cfg.FlagURL) svc := NewService(repo, flags, cfg.APIKey) srv := NewServer(svc, cfg.ListenAddr) srv.Run() } Longer? Yes. But:\nThe call order is the dependency graph.\nErrors are handled right where they happen.\nIf a constructor changes, the compiler points straight at every broken call:\n./main.go:33:39: not enough arguments in call to NewService have (*Repo, *FlagClient) want (*Repo, *FlagClient, string) No reflection, no generated code, no global state. Go type-checks the dependency graph early and loudly, exactly how it should be. And also, it doesn’t confuse your LSP, so your IDE keeps on being useful.\nIf main() really grows unwieldy, split your code:\nfunc buildInfra(cfg *Config) (*DB, *FlagClient, error) { // ... } func buildService(cfg *Config) (*Service, error) { db, flags, err := buildInfra(cfg) if err != nil { return nil, err } return NewService(NewRepo(db), flags, cfg.APIKey), nil } func main() { cfg := NewConfig() svc, err := buildService(cfg) if err != nil { log.Fatal(err) } NewServer(svc, cfg.ListenAddr).Run() } Each helper is a regular function that anyone can skim without reading a framework manual. Also, you usually build all of your dependency in one place and it’s really not that big of a deal if your builder function takes in 20 parameters and builds all the dependencies. Just put each function parameter on their own line and use gofumpt to format the code to make it readable.\nReflection works elsewhere, so why not here? Other languages lean on containers because often times constructors cannot be overloaded and compile times hurt. Go already gives you:\nFirst-class functions so constructors are plain values. Interfaces so implementations swap cleanly in tests. Fast compilation so feedback loops stay tight. A DI framework often fixes problems Go already solved and trades away readability to do it.\nThe most magical thing about Go is how little magic it allows.\n— Some Gopher on Reddit\nYou might still want a framework It’s tempting to make a blanket statement saying that you should never pick up a DI framework, but context matters here.\nI was watching Uber’s talk on how they use Go and how their DI framework Fx (which uses dig underneath) allows them to achieve consistency at scale. If you’re Uber and have all the observability tools in place to get around the downsides, then you’ll know.\nAlso, if you’re working in a codebase that’s already leveraging a framework and it works well, then it doesn’t make sense to refactor it without any incentives.\nOr, you’re writing one of those languages where using a DI framework is the norm, and you’ll be called a weirdo if you try to reinvent the wheel there.\nHowever, in my experience, even in organizations that maintain a substantial number of Go repos, DI frameworks add more confusion than they’re worth. If your experience is otherwise, I’d love to be proven wrong.\nThe post got a fair bit of discussion going around the web. You might find it interesting.\nhackernews r/golang r/experienceddevs r/programming ","permalink":"http://rednafi.com/go/di_frameworks_bleh/","publishDate":"2025-05-24","summary":"When working with Go in an industrial context, I feel like dependency injection (DI) often gets a bad rep because of DI frameworks. But DI as a technique is quite useful. It just tends to get explained with too many OO jargons and triggers PTSD among those who came to Go to escape GoF theology.\nDependency Injection is a 25-dollar term for a 5-cent concept.\n— James Shore\nDI basically means passing values into a constructor instead of creating them inside it. That’s really it. Observe:\n","tags":["Go"],"title":"You probably don't need a DI framework"},{"content":"By default, Go copies values when you pass them around. But sometimes, that can be undesirable. For example, if you accidentally copy a mutex and multiple goroutines work on separate instances of the lock, they won’t be properly synchronized. In those cases, passing a pointer to the lock avoids the copy and works as expected.\nTake this example: passing a sync.WaitGroup by value will break things in subtle ways:\nfunc f(wg sync.WaitGroup) { // ... do something with the waitgroup } func main() { var wg sync.WaitGroup f(wg) // oops! wg is getting copied here! } sync.WaitGroup lets you wait for multiple goroutines to finish some work. Under the hood, it’s a struct with methods like Add, Done, and Wait to sync concurrently running goroutines.\nThat snippet compiles fine but leads to buggy behavior because we’re copying the lock instead of referencing it in the f function.\nLuckily, go vet catches it. If you run vet on that code, you’ll get a warning like this:\nf passes lock by value: sync.WaitGroup contains sync.noCopy call of f copies lock value: sync.WaitGroup contains sync.noCopy This means we’re passing wg by value when we should be passing a reference. Here’s the fix:\nfunc f(wg *sync.WaitGroup) { // pass by reference // ... do something with the waitgroup } func main() { var wg sync.WaitGroup f(\u0026wg) // pass a pointer to wg } Since this kind of incorrect copy doesn’t throw a compile-time error, if you skip go vet, you might never catch it. Another reason to always vet your code.\nI was curious how the Go toolchain enforces this. The clue is in the vet warning:\ncall of f copies lock value: sync.WaitGroup contains sync.noCopy So the sync.noCopy struct inside sync.WaitGroup is doing something to alert go vet when you pass it by value.\nLooking at the implementation of sync.WaitGroup1, you’ll see:\ntype WaitGroup struct { noCopy noCopy state atomic.Uint64 sema uint32 } Then I traced the definition of noCopy in sync/cond.go2:\n// noCopy may be added to structs which must not be copied // after the first use. // Note that it must not be embedded, due to the Lock and Unlock methods. type noCopy struct{} // Lock is a no-op used by -copylocks checker from `go vet`. func (*noCopy) Lock() {} func (*noCopy) Unlock() {} Just having those no-op Lock and Unlock methods on noCopy is enough. This implements the Locker3 interface. Then if you put that struct inside another one, go vet will flag cases where you try to copy the outer struct.\nAlso, note the comment: don’t embed noCopy. Include it explicitly. Embedding would expose Lock and Unlock on the outer struct, which you probably don’t want.\nThe Go toolchain enforces this with the -copylocks checker. It’s part of go vet. You can exclusively invoke it with go vet -copylocks ./.... It looks for value copies of any struct that nests a struct with Lock and Unlock methods. It doesn’t matter what those methods do, just having them is enough.\nWhen vet runs, it walks the AST and applies the checker on assignments, function calls, return values, struct literals, range loops, channel sends, basically anywhere values can get copied. If it sees you copying a struct with noCopy, it yells. You can see the implementation of the check here4.\nInterestingly, if you define noCopy as anything other than a struct and implement the Locker interface, vet ignores that. I tested this on Go 1.24:\ntype noCopy int // this is valid but vet doesn't get triggered func (*noCopy) Lock() {} func (*noCopy) Unlock() {} This doesn’t trigger vet. It only works when noCopy is a struct. The reason is that vet takes a shortcut5 while checking when to trigger the warning. Currently, it explicitly looks for a struct that satisfies the Locker interface and ignores any other type even if it implements the interface.\nYou’ll see this in other parts of the sync package too. sync.Mutex uses the same trick:\ntype Mutex struct { _ noCopy mu isync.Mutex } Same with sync.Once:\ntype Once struct { done uint32 m Mutex noCopy noCopy } Here’s a complete example of abusing -copylocks to prevent copying our own struct:\ntype Svc struct{ _ noCopy } type noCopy struct{} func (*noCopy) Lock() {} func (*noCopy) Unlock() {} // Use this func main() { var svc Svc _ = svc // go vet will complain about this copy op } Running go vet on this gives:\nassignment copies lock value to s: play.Svc contains play.noCopy call of fmt.Println copies lock value: play.Svc contains play.noCopy Someone on Reddit asked me what actually triggers the copylock checker in go vet—is it the struct’s literal name noCopy or the fact that it implements the Locker interface?\nThe name noCopy isn’t special. You can call it whatever you want. As long as it implements the Locker interface, go vet will complain if the surrounding struct gets copied. See this Go Playground snippet6.\nsync.WaitGroup ↩︎\nnoCopy ↩︎\nLocker ↩︎\ncopylock checker ↩︎\ncopylock only checks for structs ↩︎\nThe name noCopy isn’t special ↩︎\n","permalink":"http://rednafi.com/go/prevent_struct_copies/","publishDate":"2025-04-21","summary":"By default, Go copies values when you pass them around. But sometimes, that can be undesirable. For example, if you accidentally copy a mutex and multiple goroutines work on separate instances of the lock, they won’t be properly synchronized. In those cases, passing a pointer to the lock avoids the copy and works as expected.\nTake this example: passing a sync.WaitGroup by value will break things in subtle ways:\nfunc f(wg sync.WaitGroup) { // ... do something with the waitgroup } func main() { var wg sync.WaitGroup f(wg) // oops! wg is getting copied here! } sync.WaitGroup lets you wait for multiple goroutines to finish some work. Under the hood, it’s a struct with methods like Add, Done, and Wait to sync concurrently running goroutines.\n","tags":["Go","TIL"],"title":"Preventing accidental struct copies in Go"},{"content":"Go 1.24 added a new tool directive that makes it easier to manage your project’s tooling.\nI used to rely on Make targets to install and run tools like stringer, mockgen, and linters like gofumpt, goimports, staticcheck, and errcheck. Problem is, these installations were global, and they’d often clash between projects.\nAnother big issue was frequent version mismatch. I ran into cases where people were formatting the same codebase differently because they had different versions of the tools installed. Then CI would yell at everyone because it was always installing the latest version of the tools before running them. Chaos!\nThe tools.go convention To avoid this mess, the Go community came up with a convention where you’d pin your tool versions in a tools.go file. I’ve written about this before1. But the gist is, you’d have a tools.go file in your root directory that imports the tooling and assigns them to _:\n//go:build tools // tools.go package tools import ( _ \"github.com/golangci/golangci-lint/cmd/golangci-lint\" _ \"mvdan.cc/gofumpt\" ) Since these dependencies aren’t used directly in the codebase, the //go:build tools directive ensures they’re excluded from the main build.\nThen running go mod tidy keeps things clean and includes these dev dependencies in the go.mod and go.sum files.\nThis works, but it always felt a bit clunky. You end up polluting your main go.mod with tooling-only dependencies. And sometimes, transitive dependencies of those tools clash with your app’s dependencies.\nThe new tool directive in Go 1.24 solves some of these pain points2.\nEnter the tool directive With Go 1.24, you can now add tooling with the -tool flag when using go get:\ngo get -tool github.com/golangci/golangci-lint/cmd/golangci-lint@latest This adds the dependency to your go.mod like this:\nmodule github.com/rednafi/foo go 1.24.2 tool github.com/golangci/golangci-lint/cmd/golangci-lint // ... other transitive dependencies Notice the tool directive clearly separates these from regular module dependencies.\nThen you can run the tool with:\ngo tool golangci-lint run ./... One thing to keep in mind: the first time you run a tool this way, it might take a second—Go needs to compile it before running if it isn’t already compiled. After that, it’s cached, so subsequent runs are fast.\nWhat about go generate? This also plays nicely with go generate. I’ve started replacing direct tool calls with go tool, so contributors don’t need to install tools globally. Just run go generate and you’re done:\n//go:generate go tool stringer -type=MyEnum No further setup needed, no path issues, and it’s always using the version you pinned.\nStill not perfect That said, one thing still bugs me: go get -tool adds these dev tools to the main go.mod file. That means your application and dev dependencies are still mixed together. Same problem the tools.go hack had.\nThere’s no built-in way to avoid this yet. So your options are:\nAccept that dev and app deps will live in the same go.mod file. Create a separate tools module to isolate your tooling. A bit clunky, but doable. I went with the second option.\nMy layout looks like this:\n. ├── go.mod ├── go.sum └── tools └── go.mod Then I install tools like this:\ncd tools go get -tool github.com/golangci/golangci-lint/cmd/golangci-lint@latest And run them from the root directory as follows:\ngo tool -modfile tools/go.mod golangci-lint run ./... The go tool command supports a -modfile flag that you can use to specify where to pull the tool version from. I really wish go get supported -modfile too—that way you wouldn’t need to manage the dependencies in such a wonky manner. This was close to being perfect. Well, maybe in a future release.\nOnly works with Go-native tools Another limitation is that it only works with tools written in Go. So if you’re using stuff like eslint, prettier, or jq, you’re on your own. But for most of my projects, the dev tooling is written in Go anyway, so this setup has been working okay.\nOmitting dev dependencies in Go binaries ↩︎\nGo toolchain still sticks the dev dependencies into the main go.mod file ↩︎\n","permalink":"http://rednafi.com/go/tool_directive/","publishDate":"2025-04-13","summary":"Go 1.24 added a new tool directive that makes it easier to manage your project’s tooling.\nI used to rely on Make targets to install and run tools like stringer, mockgen, and linters like gofumpt, goimports, staticcheck, and errcheck. Problem is, these installations were global, and they’d often clash between projects.\nAnother big issue was frequent version mismatch. I ran into cases where people were formatting the same codebase differently because they had different versions of the tools installed. Then CI would yell at everyone because it was always installing the latest version of the tools before running them. Chaos!\n","tags":["Go","TIL"],"title":"Go 1.24's \"tool\" directive"},{"content":"Ideally, every function that writes to the stdout probably should ask for a io.Writer and write to it instead. However, it’s common to encounter functions like this: func frobnicate() { fmt.Println(\"do something\") } This would be easier to test if frobnicate would ask for a writer to write to. For instance: func frobnicate(w io.Writer) { fmt.Fprintln(w, \"do something\") } You could pass os.Stdout to frobnicate explicitly to write to the console: func main() { frobnicate(os.Stdout) } This behaves exactly the same way as the first version of frobnicate. During test, instead of os.Stdout, you’d just pass a bytes.Buffer and assert its content as follows: func TestFrobnicate(t *testing.T) { // Create a buffer to capture the output var buf bytes.Buffer // Call the function with the buffer frobnicate(\u0026buf) // Check if the output is as expected expected := \"do something\\n\" if buf.String() != expected { t.Errorf(\"Expected %q, got %q\", expected, buf.String()) } } This is all good. But many functions or methods that emit logs just do that directly to stdout. So we want to test the first version of frobnicate without making any changes to it. I found this neat pattern to test functions that write to stdout without accepting a writer. The idea is to write a helper function named captureStdout that looks like this: // captureStdout replaces os.Stdout with a buffer and returns the captured output. func captureStdout(f func()) string { old := os.Stdout r, w, _ := os.Pipe() os.Stdout = w f() // run the function that writes to stdout _ = w.Close() var buf bytes.Buffer _, _ = io.Copy(\u0026buf, r) os.Stdout = old return buf.String() } Here’s what’s happening under the hood: We use os.Pipe() to create a pipe: a connected pair of file descriptors—a reader (r) and a writer (w). Think of it like a temporary tunnel. Whatever we write to w, we can read back from r. Since both are just files as far as Go is concerned, we can temporarily replace os.Stdout with the writer end of the pipe: os.Stdout = w This means anything printed to stdout during the function run actually goes into our pipe. After the function runs, we close the writer to signal that we’re done writing, then read from the reader into a buffer and restore the original stdout. Now we can test frobnicate without touching its implementation: func TestFrobnicate(t *testing.T) { output := captureStdout(func() { frobnicate() }) expected := \"do something\\n\" if output != expected { t.Errorf(\"Expected %q, got %q\", expected, output) } } No need to refactor frobnicate. This works great for quick tests when you don’t control the code or just want to assert some printed output. A more robust capture out The above version of captureStdout works fine for simple cases. But in practice, functions might also write to stderr, especially if they’re using Go’s log package or if a panic happens. For example, this would not be captured by the simple captureStdout helper: log.Println(\"something went wrong\") Even though it looks like a normal print statement, log writes to stderr by default. So if you want to catch that output too, or generally capture everything that’s printed to the console during a function call, we need to upgrade our helper a bit. I found this example from the immudb1 repo. Here’s a more complete version: // captureOut captures both stdout and stderr. func captureOut(f func()) string { // Create a pipe to capture stdout custReader, custWriter, err := os.Pipe() if err != nil { panic(err) } // Save the original stdout and stderr to restore later origStdout := os.Stdout origStderr := os.Stderr // Restore stdout and stderr when done defer func() { os.Stdout = origStdout os.Stderr = origStderr }() // Set the stdout and stderr to the pipe os.Stdout, os.Stderr = custWriter, custWriter log.SetOutput(custWriter) // Create a channel to read the output from the pipe out := make(chan string) // Use a goroutine to read from the pipe and send the output to the channel var wg sync.WaitGroup wg.Add(1) go func() { var buf bytes.Buffer wg.Done() io.Copy(\u0026buf, custReader) out \u003c- buf.String() }() wg.Wait() // Call the function that writes to stdout f() // Close the writer to signal that we're done _ = custWriter.Close() // Wait for the goroutine to finish reading from the pipe return \u003c-out } This version does a few more things: Captures everything: It redirects both os.Stdout and os.Stderr to ensure all standard output streams are captured. It also explicitly redirects the standard log package’s output, which often bypasses os.Stderr. Prevents deadlocks: Output is read concurrently in a separate goroutine. This is crucial because if f generates more output than the internal pipe buffer can hold, writing would block without a concurrent reader, causing a deadlock. Ensure reader readiness: A sync.WaitGroup guarantees the reading goroutine is active before f starts executing. This prevents a potential race condition where initial output could be lost if f writes before the reader is ready. Guarantees cleanup: Using defer, the original os.Stdout and os.Stderr are always restored, even if f panics. This prevents the function from permanently altering the program’s standard output streams. You’d use captureOut the same way as the naive captureStdout. This version is safer and more complete, and works well when you’re testing CLI commands, log-heavy code, or anything that might write to the terminal in unexpected ways. It’s not a replacement for writing functions that accept io.Writer, but when you’re dealing with existing code or want to quickly assert on terminal output, it gets the job done. Capture out ↩︎ ","permalink":"http://rednafi.com/go/capture_console_output/","publishDate":"2025-04-12","summary":"Ideally, every function that writes to the stdout probably should ask for a io.Writer and write to it instead. However, it’s common to encounter functions like this:\nfunc frobnicate() { fmt.Println(\"do something\") } This would be easier to test if frobnicate would ask for a writer to write to. For instance:\nfunc frobnicate(w io.Writer) { fmt.Fprintln(w, \"do something\") } You could pass os.Stdout to frobnicate explicitly to write to the console:\nfunc main() { frobnicate(os.Stdout) } This behaves exactly the same way as the first version of frobnicate.\n","tags":["Go","Testing","TIL"],"title":"Capturing console output in Go tests"},{"content":"While watching Mitchell Hashimoto’s excellent talk1 on Go testing, I came across this neat technique for deferring teardown to the caller. Let’s say you have a helper function in a test that needs to perform some cleanup afterward.\nYou can’t run the teardown inside the helper itself because the test still needs the setup. For example, in the following case, the helper runs its teardown immediately:\nfunc TestFoo(t *testing.T) { helper(t) // Test logic here: resources may already be cleaned up! } func helper(t *testing.T) { t.Helper() // Setup code here. // Teardown code here. defer func() { // Clean up something. }() } When helper is called, it defers its teardown—which executes at the end of the helper function, not the test. But the test logic still depends on whatever the helper set up. So this approach doesn’t work.\nThe next working option is to move the teardown logic into the test itself:\nfunc TestFoo(t *testing.T) { helper(t) // Run the teardown of helper. defer func() { // Clean up something. }() // Test logic here. } func helper(t *testing.T) { t.Helper() // Setup code here. // No teardown here; we move it to the caller. } This works fine if you have only one helper. But with multiple helpers, it quickly becomes messy—you now have to manage multiple teardown calls manually, like this:\nfunc TestFoo(t *testing.T) { helper1(t) helper2(t) defer func() { // Clean up helper2. }() defer func() { // Clean up helper1. }() // Test logic here. } You also need to be careful with the order: defer statements are executed in LIFO (last-in, first-out) order. So if teardown order matters, this can be a problem. Ideally, your tests shouldn’t depend on teardown order—but sometimes they do.\nSo rather than manually handling cleanup inside the test, have helpers return a teardown function that the test can defer itself. Here’s how:\nfunc TestFoo(t *testing.T) { teardown1 := helper1(t) defer teardown1() teardown2 := helper2(t) defer teardown2() // Test logic here. } func helper1(t *testing.T) func() { t.Helper() // Setup code here. // Maybe create a temp dir, start a mock server, etc. return func() { // Teardown code here. } } func helper2(t *testing.T) func() { t.Helper() // Setup code here. return func() { // Teardown code here. } } Each helper is self-contained: it sets something up and returns a function to clean up whatever resource it has spun up. The test controls when teardown happens by calling the cleanup function at the appropriate time. Another benefit is that the returned teardown closure has access to the local variables of the helper. So func() can access the helper’s *testing.T without us having to pass it explicitly as a parameter.\nHere’s how I’ve been using this pattern.\nCreating a temporary file to test file I/O The setupTempFile helper creates a temporary file, writes some content to it, and returns the file name along with a teardown function that removes the file.\nfunc setupTempFile(t *testing.T, content string) (string, func()) { t.Helper() tmpFile, err := os.CreateTemp(\"\", \"temp-*.txt\") if err != nil { t.Fatalf(\"failed to create temp file: %v\", err) } if _, err := tmpFile.WriteString(content); err != nil { t.Fatalf(\"failed to write to temp file: %v\", err) } tmpFile.Close() return tmpFile.Name(), func() { if err := os.Remove(tmpFile.Name()); err != nil { t.Errorf(\"failed to remove temp file %s: %v\", tmpFile.Name(), err) } else { t.Logf(\"cleaned up temp file: %s\", tmpFile.Name()) } } } In the main test:\nfunc TestReadFile(t *testing.T) { path, cleanup := setupTempFile(t, \"hello world\") defer cleanup() data, err := os.ReadFile(path) if err != nil { t.Fatalf(\"failed to read file: %v\", err) } t.Logf(\"file contents: %s\", data) } Running the test displays:\n=== RUN TestReadFile prog_test.go:18: file contents: hello world prog_test.go:38: cleaned up temp file: /tmp/temp-30176446.txt --- PASS: TestReadFile (0.00s) PASS Starting and stopping a mock HTTP server Sometimes you want to test code that makes HTTP calls. Here’s a helper that starts an in-memory mock server and returns its URL and a cleanup function that shuts it down:\nfunc setupMockServer(t *testing.T) (string, func()) { t.Helper() handler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { w.WriteHeader(http.StatusOK) w.Write([]byte(\"mock response\")) }) server := httptest.NewServer(handler) return server.URL, func() { server.Close() t.Log(\"mock server shut down\") } } And in the test:\nfunc TestHTTPRequest(t *testing.T) { url, cleanup := setupMockServer(t) defer cleanup() resp, err := http.Get(url) if err != nil { t.Fatalf(\"failed to make HTTP request: %v\", err) } defer resp.Body.Close() body, _ := io.ReadAll(resp.Body) t.Logf(\"response body: %s\", body) } Running the test prints:\n=== RUN TestHTTPRequest prog_test.go:34: response body: mock response prog_test.go:20: mock server shut down --- PASS: TestHTTPRequest (0.00s) PASS Setting up and tearing down a database table In tests that hit a real (or test) database, you often need to create and drop tables. Here’s a helper that sets up a test table and returns a teardown function to drop it:\nfunc setupTestTable(t *testing.T, db *sql.DB) func() { t.Helper() query := `CREATE TABLE IF NOT EXISTS users ( id INTEGER PRIMARY KEY, name TEXT )` _, err := db.Exec(query) if err != nil { t.Fatalf(\"failed to create table: %v\", err) } return func() { _, err := db.Exec(`DROP TABLE IF EXISTS users`) if err != nil { t.Errorf(\"failed to drop table: %v\", err) } else { t.Log(\"dropped test table\") } } } And the test:\nfunc TestInsertUser(t *testing.T) { db := getTestDB(t) // This opens a test DB connection; defined elsewhere. cleanup := setupTestTable(t, db) defer cleanup() _, err := db.Exec(`INSERT INTO users (name) VALUES (?)`, \"Alice\") if err != nil { t.Fatalf(\"failed to insert user: %v\", err) } } The t.Cleanup() method P.S. I learned about this after the blog went live.\nGo 1.20 added the t.Cleanup() method, which lets you avoid returning the teardown closures from helper functions altogether. It also runs the cleanup logic in the correct order (LIFO). So, you could rewrite the first example in this post as follows:\nfunc TestFoo(t *testing.T) { // The testing package will ensure that the cleanup runs at the end of // this test function. helper(t) // Test logic here. } func helper(t *testing.T) { t.Helper() // We register the teardown logic with t.Cleanup(). t.Cleanup(func() { // Teardown logic here. }) } Now the testing package will handle calling the cleanup logic in the correct order. You can add multiple teardown functions like this:\nt.Cleanup(func() {}) t.Cleanup(func() {}) The functions will run in LIFO order. Similarly, the database setup example can be rewritten like this:\nfunc setupTestTable(t *testing.T, db *sql.DB) func() { t.Helper() // Logic as before. // Instead of returning the teardown function, we register // it with t.Cleanup(). t.Cleanup(func() { _, err := db.Exec(`DROP TABLE IF EXISTS users`) if err != nil { t.Errorf(\"failed to drop table: %v\", err) } else { t.Log(\"dropped test table\") } }) } Then the helper function is used like this:\nfunc TestInsertUser(t *testing.T) { db := getTestDB(t) // Opens a test DB connection; defined elsewhere. // This sets up the DB, and t.Cleanup will execute the teardown // logic once this test function finishes. setupTestTable(t, db) // Rest of the test logic. } Fin!\nGopherCon 2017: Advanced testing with Go - Mitchell Hashimoto ↩︎\n","permalink":"http://rednafi.com/go/deferred_teardown_closure/","publishDate":"2025-03-28","summary":"While watching Mitchell Hashimoto’s excellent talk1 on Go testing, I came across this neat technique for deferring teardown to the caller. Let’s say you have a helper function in a test that needs to perform some cleanup afterward.\nYou can’t run the teardown inside the helper itself because the test still needs the setup. For example, in the following case, the helper runs its teardown immediately:\nfunc TestFoo(t *testing.T) { helper(t) // Test logic here: resources may already be cleaned up! } func helper(t *testing.T) { t.Helper() // Setup code here. // Teardown code here. defer func() { // Clean up something. }() } When helper is called, it defers its teardown—which executes at the end of the helper function, not the test. But the test logic still depends on whatever the helper set up. So this approach doesn’t work.\n","tags":["Go","Testing"],"title":"Deferred teardown closure in Go testing"},{"content":"There are primarily three ways of sorting slices in Go. Early on, we had the verbose but flexible method of implementing sort.Interface to sort the elements in a slice. Later, Go 1.8 introduced sort.Slice to reduce boilerplate with inline comparison functions. Most recently, Go 1.21 brought generic sorting via the slices package, which offers a concise syntax and compile-time type safety. These days, I mostly use the generic sorting syntax, but I wanted to document all three approaches for posterity. Using sort.Interface The oldest technique is based on sort.Interface. You create a custom type that wraps your slice and implement three methods—Len, Less, and Swap—to satisfy the interface. Then you pass this custom type to sort.Sort(). Sorting a slice of integers The following example defines an IntSlice type. Passing an IntSlice to sort.Sort arranges its integers in ascending order: import ( \"fmt\" \"sort\" ) // Define a custom IntSlice so that we can implement the sort.Interface type IntSlice []int // Len, Less, and Swap methods need to be implemented to conform to sort.Interface func (s IntSlice) Len() int { return len(s) } func (s IntSlice) Less(i, j int) bool { return s[i] \u003c s[j] } func (s IntSlice) Swap(i, j int) { s[i], s[j] = s[j], s[i] } func main() { nums := IntSlice{4, 1, 3, 2} sort.Sort(nums) fmt.Println(nums) // [1 2 3 4] } To reverse the order, invert the comparison in the Less method and define a new type: import ( \"fmt\" \"sort\" ) // Define a custom IntSlice for descending order sorting. type DescIntSlice []int func (s DescIntSlice) Len() int { return len(s) } func (s DescIntSlice) Less(i, j int) bool { return s[i] \u003e s[j] } // Inverted comp func (s DescIntSlice) Swap(i, j int) { s[i], s[j] = s[j], s[i] } func main() { nums := DescIntSlice{4, 1, 3, 2} sort.Sort(nums) fmt.Println(nums) // [4 3 2 1] } Just reversing the order requires you to define a separate type and implement the three methods again! Luckily, for the basic types, the sort package provides sort.IntSlice, sort.Float64Slice, and sort.StringSlice—which already implement sort.Interface. So you don’t have to do the above for sorting a slice of primitive elements. Instead, you can do this: ints := sort.IntSlice{4, 1, 3, 2} floats := sort.Float64Slice{3.1, 2.7, 5.0} strings := sort.StringSlice{\"banana\", \"apple\", \"cherry\"} sort.Sort(ints) // ints: [1 2 3 4] sort.Sort(floats) // floats: [2.7 3.1 5] sort.Sort(strings) // strings: [apple banana cherry] To reverse the order, you can use sort.Reverse as follows: sort.Sort(sort.Reverse(ints)) // ints: [4 3 2 1] sort.Sort(sort.Reverse(floats)) // floats: [5 3.1 2.7] sort.Sort(sort.Reverse(strings)) // strings: [cherry banana apple] Sorting a slice of structs by age However, if you’re dealing with a slice of structs, then you do have to implement sort.Interface manually. Here, we sort by the Age field in ascending order: import ( \"fmt\" \"sort\" ) type User struct { Name string Age int } type ByAge []User func (s ByAge) Len() int { return len(s) } func (s ByAge) Less(i, j int) bool { return s[i].Age \u003c s[j].Age } func (s ByAge) Swap(i, j int) { s[i], s[j] = s[j], s[i] } func main() { users := ByAge{ {\"Alice\", 32}, {\"Bob\", 27}, {\"Carol\", 40}, } sort.Sort(users) fmt.Println(users) // [{Bob 27} {Alice 32} {Carol 40}] } We can leverage sort.Reverse to reverse the order: sort.Sort(sort.Reverse(users)) // [{Carol 40} {Alice 32} {Bob 27}] Although sort.Interface can handle just about any sorting logic, you must create a new custom type (or significantly modify an existing one) each time you want to sort a different slice or the same slice in a different way. It’s powerful but verbose, and can be cumbersome to maintain if you have many different sorts in your code. Using sort.Slice Go 1.8 introduced sort.Slice to minimize the amount of boilerplate needed for sorting. Instead of creating a new type and implementing three methods, you provide an inline comparison function that receives the two indices you’re comparing. Sorting a slice of float64 Here’s a simple example that sorts floats in ascending order: import ( \"fmt\" \"sort\" ) func main() { floats := []float64{2.5, 0.1, 3.9, 1.2} sort.Slice(floats, func(i, j int) bool { return floats[i] \u003c floats[j] }) fmt.Println(floats) // [0.1 1.2 2.5 3.9] } Inverting the comparison sorts them in descending order: import ( \"fmt\" \"sort\" ) func main() { floats := []float64{2.5, 0.1, 3.9, 1.2} sort.Slice(floats, func(i, j int) bool { return floats[i] \u003e floats[j] // Reverse the comp }) fmt.Println(floats) // [3.9 2.5 1.2 0.1] } Sorting a slice of structs by age For structs, the inline comparator can access struct fields: import ( \"fmt\" \"sort\" ) type User struct { Name string Age int } func main() { users := []User{ {\"Alice\", 32}, {\"Bob\", 27}, {\"Carol\", 40}, } sort.Slice(users, func(i, j int) bool { return users[i].Age \u003c users[j].Age }) fmt.Println(users) // [{Bob 27} {Alice 32} {Carol 40}] } Switching \u003e for \u003c will reverse the sort: import ( \"fmt\" \"sort\" ) type User struct { Name string Age int } func main() { users := []User{ {\"Alice\", 32}, {\"Bob\", 27}, {\"Carol\", 40}, } sort.Slice(users, func(i, j int) bool { return users[i].Age \u003e users[j].Age }) fmt.Println(users) // [{Carol 40} {Alice 32} {Bob 27}] } While sort.Slice is much simpler than sort.Interface, it’s still not strictly type-safe: the slice parameter is defined as an interface{}, and you provide a comparator that uses indices. Go won’t necessarily stop you from doing something incorrect in the comparison at compile time. For example, this code compiles but will panic at runtime because other is referenced inside the comparator of a different slice ints, and the indices i or j can go out of bounds in other: import ( \"fmt\" \"sort\" ) func main() { ints := []int{3, 1, 2} other := []int{10, 20} sort.Slice(ints, func(i, j int) bool { // Using 'other' here compiles, but i or j might be out of range. return other[i] \u003c other[j] }) fmt.Println(ints) } You won’t find out you’ve made a mistake until runtime, when a panic occurs. There is no compiler-enforced guarantee that the func(i, j int) bool actually compares two values of the intended slice. Note: In sort.Slice, the comparison function parameters i and j are indices. Inside the function, you must reference slice[i] and slice[j] to get the actual elements being compared. Using generics with the slices package Go 1.21 introduced the slices package, which provides generic sorting functions. These new functions combine the convenience of sort.Slice with the ability to detect type errors at compile time. For basic numeric or string slices that satisfy Go’s “ordered” constraints, you can just call slices.Sort. For more complex or custom sorting, slices.SortFunc accepts a comparator function that returns an integer (negative if a \u003c b, zero if they’re equal, and positive if a \u003e b). Sorting primitive slices When you’re dealing with basic types like int, float64, or string, you can sort them immediately using slices.Sort, which arranges them in ascending order: import ( \"fmt\" \"slices\" ) func main() { ints := []int{4, 1, 3, 2} floats := []float64{2.5, 0.1, 3.9, 1.2} slices.Sort(ints) slices.Sort(floats) fmt.Println(ints) // [1 2 3 4] fmt.Println(floats) // [0.1 1.2 2.5 3.9] } For descending order, you can use slices.SortFunc and invert the usual comparison: import ( \"fmt\" \"slices\" ) func main() { ints := []int{4, 1, 3, 2} floats := []float64{2.5, 0.1, 3.9, 1.2} slices.SortFunc(ints, func(a, b int) int { switch { case a \u003e b: return -1 case a \u003c b: return 1 default: return 0 } }) slices.SortFunc(floats, func(a, b float64) int { switch { case a \u003e b: return -1 case a \u003c b: return 1 default: return 0 } }) fmt.Println(ints) // [4 3 2 1] fmt.Println(floats) // [3.9 2.5 1.2 0.1] } Sorting a slice of structs by age When dealing with more complex structures, you can define precisely how two elements should be compared: import ( \"fmt\" \"slices\" ) type User struct { Name string Age int } func main() { users := []User{ {\"Alice\", 32}, {\"Bob\", 27}, {\"Carol\", 40}, } slices.SortFunc(users, func(a, b User) int { return a.Age - b.Age }) fmt.Println(users) // [{Bob 27} {Alice 32} {Carol 40}] } To reverse the order, invert the numerical comparison: import ( \"fmt\" \"slices\" ) type User struct { Name string Age int } func main() { users := []User{ {\"Alice\", 32}, {\"Bob\", 27}, {\"Carol\", 40}, } slices.SortFunc(users, func(a, b User) int { switch { case a.Age \u003e b.Age: return -1 case a.Age \u003c b.Age: return 1 default: return 0 } }) fmt.Println(users) // [{Carol 40} {Alice 32} {Bob 27}] } Note: Unlike sort.Slice, which passes indices to the comparison function, slices.SortFunc passes the actual elements (a and b) to your comparator. Moreover, the comparator must return an int (negative, zero, or positive), rather than a boolean. Compile-time safety One of the major benefits of the slices package is compile-time type safety, which you don’t get with sort.Sort or sort.Slice. Those older APIs use interface{} parameters or index-based comparators and don’t strictly verify that your comparator operates on the right types. As shown previously, you can accidentally reference a different slice in the comparator and your code will compile but crash at runtime. By contrast, slices.Sort and slices.SortFunc are fully generic. The compiler enforces that you pass a slice of a valid type (e.g., []int, []string, or a custom struct slice), and that your comparator’s signature matches the element type. This means you get errors at compile time instead of at runtime. For instance, if you attempt to pass an array instead of a slice: import \"slices\" func main() { arr := [4]int{10, 20, 30, 40} // compile-time error: cannot use arr (value of type [4]int) as type []int slices.Sort(arr) } Go will refuse to compile this code because arr is not a slice. Similarly, if your comparator for slices.SortFunc returns a type other than int, the compiler will produce an error. This helps you detect mistakes immediately, rather than discovering them in runtime. For a practical illustration, consider sorting a slice by a case-insensitive string field: import ( \"fmt\" \"slices\" \"strings\" ) type Animal struct { Name string Species string } func main() { animals := []Animal{ {\"Bob\", \"Giraffe\"}, {\"alice\", \"Zebra\"}, {\"Dave\", \"Elephant\"}, } // Sort by Name, ignoring case slices.SortFunc(animals, func(a, b Animal) int { aLower := strings.ToLower(a.Name) bLower := strings.ToLower(b.Name) switch { case aLower \u003c bLower: return -1 case aLower \u003e bLower: return 1 default: return 0 } }) fmt.Println(animals) // Output: [{alice Zebra} {Bob Giraffe} {Dave Elephant}] } Because your comparator expects an Animal for both a and b, you can’t accidentally compare two different types or reference the wrong fields without hitting a compile-time error. ","permalink":"http://rednafi.com/go/sort_slice/","publishDate":"2025-03-22","summary":"There are primarily three ways of sorting slices in Go. Early on, we had the verbose but flexible method of implementing sort.Interface to sort the elements in a slice. Later, Go 1.8 introduced sort.Slice to reduce boilerplate with inline comparison functions. Most recently, Go 1.21 brought generic sorting via the slices package, which offers a concise syntax and compile-time type safety.\nThese days, I mostly use the generic sorting syntax, but I wanted to document all three approaches for posterity.\n","tags":["Go"],"title":"Three flavors of sorting Go slices"},{"content":"Comparing interface values in Go has caught me off guard a few times, especially with nils. Often, I’d expect a comparison to evaluate to true but got false instead. Many moons ago, Russ Cox wrote a fantastic blog post1 on interface internals that clarified my confusion. This post is a distillation of my exploration of interfaces and nil comparisons. Interface internals Roughly speaking, an interface in Go has three components: A static type A dynamic type A dynamic value For example: var n any // The static type of n is any (interface{}) n = 1 // Upon assignment, the dynamic type becomes int // And the dynamic value becomes 1 Here, the static type of n is any, which tells the compiler what operations are allowed on the variable. In the case of any, any operation is allowed. When we assign 1 to n, it adopts the dynamic type int and the dynamic value 1. Internally, every interface value is implemented as a two-word2 structure: One word holds a pointer to the dynamic type (i.e., a type descriptor). The other word holds the data associated with that type. This data word might directly contain the value if it’s small enough, or it might hold a pointer to the actual data. Note that this internal representation is distinct from the interface’s declared or “static” type—the type you wrote in the code (any in the example above). At runtime, what gets stored is only the pair of dynamic type and dynamic value. Here’s a crude diagram: +-----------------------+ | Interface | +-----------------------+ | Pointer to type info | ---\u003e [Dynamic type descriptor] +-----------------------+ | Data | ---\u003e [Dynamic value or pointer to the value] +-----------------------+ Comparing nils with interface variables Nil comparisons can be tricky because an interface value is considered nil only when both its dynamic type and dynamic value are nil. A few examples. Comparing a nil pointer directly var p *int // p is a nil pointer of type *int if p == nil { fmt.Println(\"p is nil\") } // Output: p is nil Here, p is a pointer to an int and is explicitly nil, so the comparison works as expected. This doesn’t have anything to do with explicit interfaces, but it’s important to demo basic nil comparison to understand how comparisons work with interfaces. An interface variable explicitly set to nil var r io.Reader // The static type of r is io.Reader r = nil // The dynamic type is nil // The dynamic value is nil // Since both the dynamic type and value evaluate to nil, r == nil is true if r == nil { fmt.Println(\"r is nil\") } // Output: r is nil In this case, r is directly set to nil. Since both the dynamic type and the dynamic value are nil, the interface compares equal to nil. Assigning a nil pointer to an interface variable var b *bytes.Buffer // b is a nil pointer of type *bytes.Buffer var r io.Reader = b // The static type of r is io.Reader. // The dynamic type of r is *bytes.Buffer. // The dynamic value of r is nil. // Although b is nil, r != nil because r holds type information (*bytes.Buffer). if r == nil { fmt.Println(\"r is nil\") } else { fmt.Println(\"r is not nil\") } // Output: r is not nil Even though b is nil, assigning it to the interface variable r gives r a non-nil dynamic type (*bytes.Buffer) with a nil dynamic value. Since r still holds type information, r == nil returns false, even though the underlying value is nil. When comparing an interface variable, Go checks both the dynamic type and the value. The variable evaluates to nil only if both are nil. Using type assertions for reliable nil checks In cases where an interface variable might hold a nil pointer, we’ve seen that comparing the interface directly to nil may not yield the expected result. A type assertion can help extract the underlying value so that you can perform a more reliable nil check. This approach is especially useful when you know the expected underlying type. Below, we define a simple type myReader that implements the Read method to satisfy the io.Reader interface. type myReader struct{} func (mr *myReader) Read(p []byte) (int, error) { return 0, nil } Now, consider the following example: var mr *myReader // mr is a nil pointer of type *myReader var r io.Reader = mr // The static type of r is io.Reader // The dynamic type of r is *myReader // The dynamic value of r is nil // Use a type assertion to extract the underlying *myReader value. if underlying, ok := r.(*myReader); ok \u0026\u0026 underlying == nil { fmt.Println(\"r holds a nil pointer\") } else { fmt.Println(\"r does not hold a nil pointer\") } // Output: r holds a nil pointer Here, we assert that r holds a value of type *myReader. If the assertion succeeds (indicated by ok being true) and the underlying value is nil, we can conclude that the interface variable holds a nil pointer—even though the interface itself is not nil due to its dynamic type. This type assertion trick only works when you know the underlying type of the interface value. If the type might vary, consider using the reflect package to examine the underlying value. Writing a generic nil checker with reflect The following function introspects any variable and checks whether it’s nil: func isNil(i any) bool { if i == nil { return true } // Note: Arrays are not nilable, so we don't check for reflect.Array. switch reflect.TypeOf(i).Kind() { case reflect.Ptr, reflect.Map, reflect.Chan, reflect.Slice, reflect.Func: return reflect.ValueOf(i).IsNil() } return false } The switch on .Kind() is necessary because directly calling reflect.ValueOf().IsNil() on a non-pointer value will cause a panic. Calling this function on any value, including an interface, reliably checks whether it’s nil. Fin! Go data structures: interfaces ↩︎ A word is a fixed-size unit of data that a CPU processes in a single operation, typically matching the system’s pointer size (8 bytes on a 64-bit system, 4 bytes on a 32-bit system). ↩︎ ","permalink":"http://rednafi.com/go/nil_interface_comparison/","publishDate":"2025-03-12","summary":"Comparing interface values in Go has caught me off guard a few times, especially with nils. Often, I’d expect a comparison to evaluate to true but got false instead.\nMany moons ago, Russ Cox wrote a fantastic blog post1 on interface internals that clarified my confusion. This post is a distillation of my exploration of interfaces and nil comparisons.\nInterface internals Roughly speaking, an interface in Go has three components:\n","tags":["Go"],"title":"Nil comparisons and Go interface"},{"content":"Middleware is usually the go-to pattern in Go HTTP servers for tweaking request behavior. Typically, you wrap your base handler with layers of middleware—one might log every request, while another intercepts specific routes like /special to serve a custom response. However, I often find the indirections introduced by this pattern a bit hard to read and debug. I recently came across the embedded delegation pattern while browsing the Gin1 repo. Here, I explore both patterns and explain why I usually start with delegation whenever I need to modify HTTP requests in my Go services. Middleware stacking Here’s an example where the logging middleware records each request, and the special middleware intercepts requests to /special: package main import ( \"log\" \"net/http\" ) // loggingMiddleware logs incoming requests. func loggingMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { log.Println(\"Middleware: received request for\", r.URL.Path) next.ServeHTTP(w, r) }) } // specialMiddleware intercepts requests for \"/special\" and handles them. func specialMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { if r.URL.Path == \"/special\" { w.Write([]byte(\"Special middleware handling request\")) return } next.ServeHTTP(w, r) }) } func main() { mux := http.NewServeMux() mux.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"Hello, world!\")) }) // The middleware chain applies special handling then logs every request. handler := loggingMiddleware(specialMiddleware(mux)) http.ListenAndServe(\":8080\", handler) } In this setup, every incoming request is first handled by the special middleware, which checks for the /special route, and then by the logging middleware that logs the request details. We’re effectively stacking the middleware functions. If you hit the server with: curl localhost:8080/ curl localhost:8080/special the server logs will look like this: 2025/03/06 21:24:44 Middleware: received request for / 2025/03/06 21:24:47 Middleware: received request for /special Stacking middleware functions like middleware3(middleware2(middleware1(mux))) can get messy when you have many of them. That’s why people usually write a wrapper function to apply the middlewares to the mux: func applyMiddleware( handler http.Handler, middlewares ...func(http.Handler) http.Handler) http.Handler { // Apply middlewares in reverse order to preserve LIFO. for i := len(middlewares) - 1; i \u003e= 0; i-- { handler = middlewares[i](handler) } return handler } applyMiddleware takes an http.Handler and a variadic list of middleware functions (...func(http.Handler) http.Handler). It loops over the middleware in reverse order so each one wraps the next properly. This avoids deep nesting like middleware3(middleware2(middleware1(mux))) and keeps the middleware chain tidy. You’d then use it like this: func main() { mux := http.NewServeMux() mux.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"Hello, world!\")) }) // The middleware chain applies special handling then logs every request. // specialMiddleware is applied before loggingMiddleware, just like before. handler := applyMiddleware(mux, loggingMiddleware, specialMiddleware) http.ListenAndServe(\":8080\", handler) } This behaves just like the manual middleware stacking, but it’s a bit cleaner. While this is the canonical way to handle request-response modifications in Go, it can sometimes be hard to reason about, especially when debugging or dealing with many middleware layers. There’s another way to achieve the same result without dealing with a soup of nested functions. The next section talks about that. Embedded delegation Embedded delegation (or the delegation pattern) means you embed the standard HTTP multiplexer inside your own struct and override its ServeHTTP method. It’s a bit like inheritance—overriding a method in a subclass to add extra functionality and then delegating the call to the original method. Although Go doesn’t have a class hierarchy, you can still delegate responsibilities to the embedded type’s method. The following example implements the same behavior—logging every request and intercepting the /special route—directly within a custom mux: package main import ( \"log\" \"net/http\" ) // CustomMux embeds http.ServeMux to override ServeHTTP. type CustomMux struct { *http.ServeMux } // ServeHTTP logs the request and intercepts \"/special\" before // delegating to the embedded mux. func (cm *CustomMux) ServeHTTP(w http.ResponseWriter, r *http.Request) { // Log all requests. log.Println(\"CustomMux: received request for\", r.URL.Path) // Handle \"/special\" differently. if r.URL.Path == \"/special\" { w.Write([]byte(\"Special handling in CustomMux\")) return } cm.ServeMux.ServeHTTP(w, r) } func main() { mux := http.NewServeMux() mux.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"Hello, world!\")) }) // Wrap the standard mux with our custom delegation. customMux := \u0026CustomMux{ServeMux: mux} http.ListenAndServe(\":8080\", customMux) } In this example, the custom mux centralizes both logging and special-case route handling within one ServeHTTP method. This approach cuts out the extra function calls in a middleware chain and can simplify tracking the request flow. I find it a bit easier on the eyes too. If you have a bunch of extra functionality to add inside cm.ServeHTTP, you can wrap them in utility functions like this: // logRequest logs incoming HTTP requests. func logRequest(r *http.Request) { log.Println(\"CustomMux: received request for\", r.URL.Path) } // handleSpecialRequest handles requests to \"/special\" // and returns true if handled. func handleSpecialRequest(w http.ResponseWriter, r *http.Request) bool { if r.URL.Path != \"/special\" { return false // Not handled, continue processing. } w.Write([]byte(\"Special handling in CustomMux\")) return true // Handled; no further processing needed. } Then, simply call these functions inside your cm.ServeHTTP method: func (cm *CustomMux) ServeHTTP(w http.ResponseWriter, r *http.Request) { logRequest(r) if handleSpecialRequest(w, r) { return } cm.ServeMux.ServeHTTP(w, r) } This keeps all the request modifications in a single ServeHTTP method. Mixing the two approaches You can also mix both techniques. For example, you might use direct delegation for special route handling and then wrap the resulting handler with middleware for logging. Here’s how a hybrid solution might look: package main import ( \"log\" \"net/http\" ) // CustomMux embeds http.ServeMux and intercepts \"/special\". type CustomMux struct { *http.ServeMux } // ServeHTTP intercepts \"/special\" and delegates other routes. func (cm *CustomMux) ServeHTTP(w http.ResponseWriter, r *http.Request) { if r.URL.Path == \"/special\" { w.Write([]byte(\"Special handling in CustomMux\")) return } cm.ServeMux.ServeHTTP(w, r) } // loggingMiddleware logs incoming requests. func loggingMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { log.Println(\"Middleware: received request for\", r.URL.Path) next.ServeHTTP(w, r) }) } func main() { mux := http.NewServeMux() mux.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"Hello, world!\")) }) // Use direct delegation for special routing. customMux := \u0026CustomMux{ServeMux: mux} // Wrap the custom mux with logging middleware. handler := loggingMiddleware(customMux) http.ListenAndServe(\":8080\", handler) } In this hybrid approach, the specialized behavior (intercepting the /special path) is handled via direct delegation, while logging stays modular as middleware. This gives you the best of both worlds. I usually start with the embedded delegation and gradually introduce the middleware pattern if I need it later. It’s easier to adopt the middleware pattern if you start with delegation than the other way around. Gin ↩︎ ","permalink":"http://rednafi.com/go/middleware_vs_delegation/","publishDate":"2025-03-06","summary":"Middleware is usually the go-to pattern in Go HTTP servers for tweaking request behavior. Typically, you wrap your base handler with layers of middleware—one might log every request, while another intercepts specific routes like /special to serve a custom response.\nHowever, I often find the indirections introduced by this pattern a bit hard to read and debug. I recently came across the embedded delegation pattern while browsing the Gin1 repo. Here, I explore both patterns and explain why I usually start with delegation whenever I need to modify HTTP requests in my Go services.\n","tags":["Go","API"],"title":"Stacked middleware vs embedded delegation in Go"},{"content":"I’ve always found the signature of io.Reader a bit odd:\ntype Reader interface { Read(p []byte) (n int, err error) } Why take a byte slice and write data into it? Wouldn’t it be simpler to create the slice inside Read, load the data, and return it instead?\n// Hypothetical; what I *thought* it should be Read() (p []byte, err error) This felt more intuitive to me—you call Read, and it gives you a slice filled with data, no need to pass anything.\nI found out why it’s designed this way while watching this1 excellent GopherCon Singapore talk on understanding allocations by Jacob Walker. It mainly boils down to two reasons.\nReducing heap allocations If Read created and returned a new slice every time, the memory would always end up on the heap.\nHeap allocations are slower because they require garbage collection, while stack allocations are faster since they are freed automatically when a function returns. By taking a caller-provided slice, Read lets the caller control memory and reuse buffers, keeping them on the stack whenever possible.\nThis matters a lot when reading large amounts of data. If each Read call created a new slice, you’d constantly be allocating memory, leading to more work for the garbage collector. Instead, the caller can allocate a buffer once and reuse it across multiple reads:\nbuf := make([]byte, 4096) // Single allocation n, err := reader.Read(buf) // Read into existing buffer Go’s escape analysis tool (go build -gcflags=-m) can confirm this. If Read returned a new slice, the tool would likely show:\nbuf escapes to heap meaning Go has to allocate it dynamically. But by reusing a preallocated slice, we avoid unnecessary heap allocations—only if the buffer is small enough to fit in the stack. How small? Only the compiler knows, and you shouldn’t depend on it. Use the escape analysis tool to see that. But most of the time, you don’t need to worry about this at all.\nReusing buffers in streaming The second issue is correctness. When reading from a stream, you usually call Read multiple times to get all the data. If Read returned a fresh slice every time, you’d have no control over memory usage across calls. Worse, you couldn’t efficiently handle partial reads, making buffer management unpredictable.\nWith the hypothetical version of Read, every call would allocate a new slice. If you needed to read a large stream of data, you’d have to manually piece everything together using append, like this:\nvar allData []byte for { buf, err := reader.Read() // New allocation every call if err != nil { break } allData = append(allData, buf...) // Growing slice every time, more allocation } process(allData) This is a mess. Every time append runs out of space, Go will have to allocate a larger slice and copy the existing data over, piling on unnecessary GC pressure.\nBy contrast, io.Reader’s actual design avoids this problem:\nbuf := make([]byte, 4096) // Allocate once for { n, err := reader.Read(buf) if err != nil { break } process(buf[:n]) } This avoids unnecessary allocations and produces less garbage for the GC to clean up.\nUnderstanding allocations: the stack and the heap - GopherCon SG 2019 ↩︎\n","permalink":"http://rednafi.com/go/io_reader_signature/","publishDate":"2025-02-08","summary":"I’ve always found the signature of io.Reader a bit odd:\ntype Reader interface { Read(p []byte) (n int, err error) } Why take a byte slice and write data into it? Wouldn’t it be simpler to create the slice inside Read, load the data, and return it instead?\n// Hypothetical; what I *thought* it should be Read() (p []byte, err error) This felt more intuitive to me—you call Read, and it gives you a slice filled with data, no need to pass anything.\n","tags":["Go","TIL"],"title":"Why does Go's io.Reader have such a weird signature?"},{"content":"Just like any other dynamically growable container structure, Go slices come with a few gotchas. I don’t always remember all the rules I need to be aware of. So this is an attempt to list some of the most common mistakes I’ve made at least once. Slices are views over arrays In Go, a slice is a lightweight wrapper around an array. Instead of storing data itself, it keeps track of three things: a pointer to an underlying array where the data is stored, the number of elements it currently holds, and the total capacity before it needs more space. The Go runtime defines it like this: // src/runtime/slice.go type slice struct { array unsafe.Pointer // pointer to data array len int // slice length cap int // slice capacity } When you create a slice from an array or another slice, Go doesn’t copy the data—it simply points to a section of the existing array. Slice Header Underlying Array +-------------+ +-------------------+ | array ------\u003e|---------\u003e| (data in memory) | | len | +-------------------+ | cap | +-------------+ This makes slices efficient. Passing a slice by value doesn’t mean copying all its elements—only the small slice struct gets copied, while the data stays where it is. But this behavior is also the source of much confusion. The next sections cover some common pitfalls. Sliced slices share the underlying array Reslicing a slice doesn’t copy data. The newly created slices point to the same array. So modifying one slice will affect others. // Define the original slice original := []int{1, 2, 3, 4, 5} // -\u003e original: [1 2 3 4 5] // Create slice1 from index 1 to 4 slice1 := original[1:4] // -\u003e slice1: [2 3 4] // Create slice2 from index 2 to the end slice2 := original[2:] // -\u003e slice2: [3 4 5] // Modify the first element of slice1 (affects other slices) slice1[0] = 100 // -\u003e original: [1 100 3 4 5], slice1: [100 3 4], slice2: [3 4 5] Solution: To get independent slices, you need to explicitly copy the data. Use make to create a new slice and copy to transfer the elements. // Define the original slice original := []int{1, 2, 3, 4, 5} // -\u003e [1 2 3 4 5] // Create a new slice (slice1) from original[1:4] slice1 := make([]int, len(original[1:4])) // -\u003e [0 0 0] copy(slice1, original[1:4]) // -\u003e [2 3 4] // Create a new slice (slice2) from original[2:] slice2 := make([]int, len(original[2:])) // -\u003e [0 0 0] copy(slice2, original[2:]) // -\u003e [3 4 5] // Modify the first element of slice1 (doesn't affect others) slice1[0] = 100 // -\u003e original: [1 2 3 4 5], slice1: [100 3 4], slice2: [3 4 5] Append may reallocate append reallocates the underlying array if capacity is insufficient, changing the backing array pointer. When passing slices to functions, reallocation inside the function won’t update the original slice header in the caller unless the slice is returned and reassigned. Modifications within the capacity are visible. If you create a slice with a predefined capacity and start appending elements, everything looks fine until you exceed that capacity. Once that happens, Go reallocates memory and moves the slice to a new backing array. // Create a slice with length=0 and capacity=3 slice := make([]int, 0, 3) // Let's say the array pointer is p1 // Append 3 elements (1,2,3) to fill up capacity slice = append(slice, 1, 2, 3) // -\u003e still pointer p1, slice: [1 2 3] // Exceed capacity by appending 4 slice = append(slice, 4) // -\u003e new pointer p2, slice: [1 2 3 4] The same behavior applies when passing a slice to a function. If the function modifies elements within the allocated capacity, those changes persist and are visible from outside the function. But if append triggers a reallocation inside the function, the caller’s slice remains unchanged. // Demonstration function that modifies and appends func modifySlice(s []int) { s[0] = 99 // modification within capacity is visible s = append(s, 100) // may trigger reallocation // s pointer might change here, but the caller won't see that } // Example usage mySlice := make([]int, 1, 3) // -\u003e [0], capacity=3 mySlice[0] = 1 // -\u003e [1] modifySlice(mySlice) // -\u003e mySlice[0] becomes 99 (within capacity) // -\u003e the append inside function might reallocate, but that reallocated // version is lost // mySlice is effectively [99], capacity still = 3 // (the \"100\" appended is not in mySlice) Solution: If append inside a function reallocates memory, the caller won’t see the change. To make it explicit, return the modified slice and reassign it. // Correct approach: return the new slice func modifySliceCorrected(s []int) []int { s = append(s, 100) // may reallocate return s // return the updated slice } // Example usage mySlice := make([]int, 1, 3) // -\u003e [0], cap=3 mySlice[0] = 1 // -\u003e [1] mySlice = modifySliceCorrected(mySlice) // -\u003e now mySlice sees the appended element [1 100] Append returns new slice append returns a new slice. If you don’t reassign the result back to the original slice variable, the slice remains unchanged after the append operation. We already saw this in last section but I think it deserves a section of its own. slice := []int{1, 2, 3} // -\u003e [1 2 3] // Wrong usage (no reassign): append(slice, 4) // -\u003e appended result is discarded, slice remains [1 2 3] // Correct usage (assign back): slice = append(slice, 4) // -\u003e slice is now [1 2 3 4] Solution: Remember to always assign the return value of append back to the slice variable you are working with. slice := []int{1, 2, 3} // -\u003e [1 2 3] slice = append(slice, 4, 5, 6) // -\u003e [1 2 3 4 5 6] Nil and empty slices differ Nil slices have nil array pointers; empty slices have initialized, non-nil pointers and zero length. While often interchangeable for emptiness checks, the distinction matters in certain contexts like JSON encoding or API interactions. var nilSlice []int // -\u003e nil emptySliceMake := make([]int, 0) // -\u003e [] emptySliceLiteral := []int{} // -\u003e [] // nilSlice == nil -\u003e true // emptySliceMake == nil -\u003e false // emptySliceLiteral == nil -\u003e false Solution: When you need a truly empty slice (e.g., to represent an empty list in JSON), initialize it as an empty slice (e.g., []int{} or make([]int, 0)). For general emptiness checks, len(slice) == 0 works for both nil and empty slices. var nilSlice []int // nil slice (pointer is nil) emptySlice := []int{} // empty slice (pointer is non-nil) nilJSON, _ := json.Marshal(nilSlice) // -\u003e \"null\" emptyJSON, _ := json.Marshal(emptySlice) // -\u003e \"[]\" Slicing can leak memory Small slices created from large arrays can keep the entire large array in memory. // Suppose we have a function returning a large slice func getLargeSlice() []int { largeSlice := make([]int, 1_000_000) // large underlying array return largeSlice } // Usage example: largeData := getLargeSlice() // -\u003e slice of 1,000,000 ints smallSlice := largeData[10:20] // -\u003e slice with length=10, cap=999,990 // Setting largeData to nil does not free the large array, // because smallSlice still references it. largeData = nil // The memory for the big array won't be garbage collected // due to the reference from smallSlice. Solution: To avoid memory leaks, copy the data of the small slice into a new, independent slice. This allows the large underlying array to be garbage collected if no longer referenced elsewhere. func getLargeSlice() []int { largeSlice := make([]int, 1_000_000) return largeSlice } // Usage example: largeData := getLargeSlice() subset := largeData[10:20] // -\u003e references big array smallSlice := make([]int, len(subset)) // -\u003e new small array copy(smallSlice, subset) // -\u003e copies only 10 elements largeData = nil // Now only smallSlice references a small array (cap=10) // The large array is eligible for GC. Range copies values for...range on value types iterates over copies. Modifications to the loop variable don’t change the original slice. slice := []int{1, 2, 3} // -\u003e [1 2 3] // \"val\" is a copy of each element in the slice for _, val := range slice { val *= 2 // modifies only \"val,\" not slice } // slice remains [1 2 3] // Using an index-based loop: for i := range slice { slice[i] *= 2 // modifies the element in place } // slice is now [2 4 6] Solution: If you need to modify slice elements during iteration, use an index-based for loop. This provides direct access to each element via its index. slice := []int{1, 2, 3} // -\u003e [1 2 3] for i := range slice { slice[i] *= 2 // modifies the original slice } // slice is now [2 4 6] Make with length initializes make([]T, length, capacity) initializes the first length elements with the zero value of T. This can be a subtle point if you expect an uninitialized slice of a certain size. slice := make([]int, 3, 5) // -\u003e [0 0 0], cap=5 // The first 3 elements are zero-initialized slice[0] = 10 // -\u003e [10 0 0] slice = append(slice, 1, 2) // -\u003e [10 0 0 1 2], len=5, cap=5 emptySliceCap := make([]int, 0, 5) // -\u003e [], cap=5 // This one starts with length=0, so no initial elements Solution: If you want an empty slice with a specific capacity but without initial zero values, use make([]T, 0, capacity). Or use the slice literal []T{} syntax if you don’t care about the capacity. If you need a slice of a certain length initialized with zero values, make([]T, length, capacity) is the correct approach. emptySliceWithCap := make([]int, 0, 5) // -\u003e [], cap=5 initializedSlice := make([]int, 3, 5) // -\u003e [0 0 0], cap=5 Overlapping copy is tricky copy(dst, src) with overlapping slices can corrupt data when dst starts inside src. data := []int{1, 2, 3, 4, 5} // -\u003e [1 2 3 4 5] src := data[:] // -\u003e [1 2 3 4 5] dst := data[2:] // -\u003e overlap (dst starts at index 2): [3 4 5] // Copy from src to dst copy(dst, src) // Expected output: data -\u003e [1 2 3 4 5] (if copied correctly) // Actual output: data -\u003e [1 2 1 2 3] (corrupted) Solution: To avoid corruption, just don’t do it. If you have to, then one way to fix it is by using a temporary buffer. Even then it’s messy. data := []int{1, 2, 3, 4, 5} src := data[:] dst := make([]int, len(src)-2) // Create dst as a NEW slice, shorter than src // Use a temporary buffer temp := make([]int, len(src)) // Copy from src to temp copy(temp, src) // Copy from temp to src copy(dst, temp[2:]) // Expected output: data -\u003e [1 2 3 4 5] (data remains unchanged) // Actual output: data -\u003e [1 2 3 4 5] // dst -\u003e [3 4 5] (dst is a copy of the last part of src) Copy truncates silently copy also returns the number of elements copied, which is the smaller of len(dst) and len(src). If dst is shorter, data gets truncated. src := []int{1, 2, 3, 4, 5} // -\u003e [1 2 3 4 5] dst := make([]int, 3) // -\u003e [0 0 0] (length 3) copied := copy(dst, src) // Expected output: dst -\u003e [1 2 3 4 5], copied -\u003e 5 // Real output: dst -\u003e [1 2 3], copied -\u003e 3 Solution: On dst, always set the length from the src while copying. src := []int{1, 2, 3, 4, 5} // -\u003e [1 2 3 4 5] dst := make([]int, len(src)) // -\u003e [0 0 0 0 0] (length 5) copied := copy(dst, src) // Expected output: dst -\u003e [1 2 3 4 5], copied -\u003e 5 // Real output: dst -\u003e [1 2 3 4 5], copied -\u003e 5 I may have missed, forgotten, or not yet encountered a few other gotchas. If you’ve run into any that aren’t listed here, I’d love to hear about them. ","permalink":"http://rednafi.com/go/slice_gotchas/","publishDate":"2025-02-06","summary":"Just like any other dynamically growable container structure, Go slices come with a few gotchas. I don’t always remember all the rules I need to be aware of. So this is an attempt to list some of the most common mistakes I’ve made at least once.\nSlices are views over arrays In Go, a slice is a lightweight wrapper around an array. Instead of storing data itself, it keeps track of three things: a pointer to an underlying array where the data is stored, the number of elements it currently holds, and the total capacity before it needs more space. The Go runtime defines it like this:\n","tags":["Go"],"title":"Go slice gotchas"},{"content":"Seven years isn’t an awfully long time to work as an IC in the industry, but it’s enough to see a few cycles of change. One thing I’ve learned during this period is that, to be a key player in a business as an engineer, one of the biggest moats you can build for yourself is domain knowledge.\nWhen you know the domain well, it becomes a lot easier to weather waves of technological and managerial change. This is especially true in businesses where the tech is mostly a fleet of services communicating over some form of RPC. Doing something novel in setups like that is often hard. In situations like these, picking up the domain quickly and being able to apply a template solution is probably one of the few edges we still have over LLMs.\nTelling someone to acquire domain knowledge in a business is kind of like telling a CS grad to focus more on protocols and less on mechanisms. Protocol changes are way harder, and mechanisms shift under your feet constantly—grappling with that change is just part of the job.\nThat said, not all domain knowledge yields the same result, and it’s often not obvious which ones deserve your attention or when diving too deep into a domain can actually backfire. For example, large companies often build their own internal tooling, and knowing its quirks can be a huge advantage while you’re there. But if you switch jobs, reusing that domain knowledge can be tricky.\nOn the other hand, if you’ve worked on/with something like Bazel at Google or Cassandra/HBase at Facebook, that’s a different story. The names alone are much more marketable, and you won’t have any shortage of opportunities, regardless of whether the knowledge is reusable.\nKnowing the ins and outs of an internal feature-flagging system at your company isn’t the same as understanding the machinations of a database abstraction layer at Netflix. Not all of us work at Netflix, and finding that balance is hard. Sometimes you just have to learn enough to get by, and that’s fine—learning is part of why we get paid. But domain lock-in is real. I’ve seen extremely good engineers get stuck in super niche areas and then struggle to pivot away.\nThis is probably obvious to veterans, but it wasn’t to me when I started. I’ve seen some people get burned by over-specializing, while others pulled off moonshots by spotting opportunities that let them do fantastic, novel work. There’s a line between specialization and hyper-specialization, and most of the time, being more of a jack-of-all-trades isn’t a bad thing. At the same time, it’s neat to be able to identify those rare opportunities where getting involved early can yield outsized returns.\nP.S. Domain knowledge around a business and knowledge related to specialized tools are two different concepts. I realize this post might’ve blurred the lines, mostly for lack of a better term.\n","permalink":"http://rednafi.com/zephyr/domain_knowledge_dilemma/","publishDate":"2025-01-19","summary":"Seven years isn’t an awfully long time to work as an IC in the industry, but it’s enough to see a few cycles of change. One thing I’ve learned during this period is that, to be a key player in a business as an engineer, one of the biggest moats you can build for yourself is domain knowledge.\nWhen you know the domain well, it becomes a lot easier to weather waves of technological and managerial change. This is especially true in businesses where the tech is mostly a fleet of services communicating over some form of RPC. Doing something novel in setups like that is often hard. In situations like these, picking up the domain quickly and being able to apply a template solution is probably one of the few edges we still have over LLMs.\n","tags":["Essay"],"title":"The domain knowledge dilemma"},{"content":"Recently at work, we ran into this problem: We needed to send Slack notifications for specific events but had to enforce rate limits to avoid overwhelming the channel. Here’s how the limits worked: Global limit: Max 100 requests every 30 minutes. Category limit: Each event type (e.g., errors, warnings) capped at 10 requests per 30 minutes. Now, imagine this: There are 20 event types. Each type hits its 10-notification limit in 30 minutes. That’s 200 requests total, but the global limit only allows 100. So, 100 requests must be dropped—even if some event types still have room under their individual caps. This created a hierarchy of limits: Category limits keep any event type from exceeding 10 requests. The global limit ensures the combined total stays under 100. Every 30 minutes, the system resets. Here are two issues that could arise: If some event types are busier, the global limit could block quieter ones. Even with room under the global limit, some event types might still hit their category caps. In our case, the event types are limited, and the category limits are both uniform and significantly smaller than the global limit, so this isn’t a concern. Redis sorted sets The notification sender service runs on multiple instances, each processing events and sending notifications independently. Without a shared system to enforce rate limits, these instances would maintain separate counters for global and category-specific limits. This would create inconsistencies because no instance would have a complete view of the overall activity, leading to conflicts and potential exceedance of limits. Redis provides a centralized state that all instances can access, ensuring they share the same counters for rate limits. This removes inconsistencies and makes rate limiting reliable, even when the notification sender scales to multiple instances. Sorted sets in Redis track notifications within a rolling time window by using timestamps as scores, which keeps entries ordered by time. The implementation: Maintains a global sorted set to enforce the overall limit (e.g., 100 notifications per 30 minutes). Uses category-specific sorted sets to enforce category limits for each event type (e.g., 10 notifications per 30 minutes for errors, warnings, etc.). The limits are enforced with two Redis commands: ZREMRANGEBYSCORE removes entries with timestamps outside the rolling time window, keeping only recent notifications. ZCARD counts the remaining entries in a set to check whether the global or category-specific limits have been reached. Lua script Instead of embedding the rate-limiting logic directly into the notification sender, we chose to implement it as a Lua script in Redis. While we could write the logic in the code and run it in a Redis pipeline, we opted not to, for the following reasons: A dedicated script keeps the rate-limiting logic separate and independently auditable. It saves a few TCP calls, as the entire logic runs within Redis itself. And most importantly, I wanted to write some Lua. The script is as follows: -- rate_limiter.lua local function check_rate_limit( global_key, category_key, global_limit, category_limit, window ) -- Get the current timestamp in seconds (including microseconds) local current_time_raw = redis.call('TIME') -- Returns {seconds, microseconds} local current_time = current_time_raw[1] + (current_time_raw[2] / 1000000) -- Step 1: Remove expired entries redis.call('ZREMRANGEBYSCORE', global_key, 0, current_time - window) redis.call('ZREMRANGEBYSCORE', category_key, 0, current_time - window) -- Step 2: Check the global limit local global_count = redis.call('ZCARD', global_key) if global_count \u003e= global_limit then return 0 -- Reject the request if the global limit is reached end -- Step 3: Check the category-specific limit local category_count = redis.call('ZCARD', category_key) if category_count \u003e= category_limit then return 0 -- Reject the request if the category limit is reached end -- Step 4: Add the current notification to the sorted sets redis.call('ZADD', global_key, current_time, current_time) redis.call('ZADD', category_key, current_time, current_time) return 1 -- Allow the request end -- Parameters passed to the script: -- KEYS[1]: The Redis key for the global sorted set -- KEYS[2]: The Redis key for the category-specific sorted set -- ARGV[1]: Global limit (e.g., 100) -- ARGV[2]: Category limit (e.g., 10) -- ARGV[3]: Time window in seconds (e.g., 1800 for 30 minutes) local global_key = KEYS[1] local category_key = KEYS[2] local global_limit = tonumber(ARGV[1]) local category_limit = tonumber(ARGV[2]) local window = tonumber(ARGV[3]) -- Execute the rate-limiting function and return the result return check_rate_limit( global_key, category_key, global_limit, category_limit, window ) The script performs the following operations in order: Remove expired entries: It uses ZREMRANGEBYSCORE to remove notifications older than the time window (current_time - window). This ensures that only active notifications are considered for the limits. This eliminates the need for additional bookkeeping to remove expired keys. ZREMRANGEBYSCORE is fast enough to handle the removal of a small number of keys during each invocation. Check the global limit: ZCARD counts the number of active notifications in the global sorted set. If this count equals or exceeds the global limit (e.g., 100), the request is rejected (return 0). Check the category-specific limit: ZCARD is used again to count the active notifications for the specific category. If this count equals or exceeds the category limit (e.g., 10), the request is rejected (return 0). Add the notification: If both limits are within bounds, the script uses ZADD to insert the current notification into both the global and category-specific sorted sets, using a timestamp as the score for accurate tracking. Using the script You can load the Lua script from disk, register it with Redis, and call it before invoking the notification service. If the script returns 0, drop the notification request. If it returns 1, send the notification. Here’s how to do it in Python: from redis import Redis from redis.commands.core import Script def load_lua_script(redis_client: Redis, script_path: str) -\u003e Script: with open(script_path, \"r\") as file: lua_script = file.read() return redis_client.register_script(lua_script) def send_notification( script: Script, global_key: str, category_key: str, global_limit: int, category_limit: int, window: int, message: str, ) -\u003e None: # Check the rate limiter result: int = script( keys=[global_key, category_key], args=[global_limit, category_limit, window], ) if result == 1: # Allowed: send the notification print(f\"Notification sent: {message}\") # Add actual notification-sending logic here else: # Blocked: drop the notification print(f\"Notification dropped (rate limit exceeded): {message}\") if __name__ == \"__main__\": # Connect to Redis redis_client = Redis(host=\"localhost\", port=6379, decode_responses=True) # Load and register the Lua script script_path = \"rate_limiter.lua\" script = load_lua_script(redis_client, script_path) # Define rate limiting parameters global_key = \"rate_limit:global\" category_key = \"rate_limit:category:errors\" global_limit = 100 # Max 100 requests globally category_limit = 10 # Max 10 requests per category window = 1800 # 30-minute window # Send a single notification send_notification( script, global_key, category_key, global_limit, category_limit, window, \"This is a single notification message\", ) Registering the Lua script loads it from disk once and reuses it, which is faster than repeatedly loading and evaluating it for each invocation. To test this, you’ll need a running Redis instance. You can run one with Docker: docker run --name redis-server -d -p 6379:6379 redis Now, running the script will print: Notification sent: This is a single notification message Since this sends a notification only once, the rate limiting isn’t apparent yet, but it’s working under the hood and will kick in if any limit is exceeded. To see it in action, you can attempt to send multiple notifications in a tight loop. Testing the rate limiter You can call the send_notification function multiple times to test the rate limiter. Below is an example that simulates several notification requests in a short loop, giving you a sense of how many will be allowed versus blocked: from redis import Redis import time def main() -\u003e None: # Connect to Redis redis_client = Redis(host=\"localhost\", port=6379, decode_responses=True) # Load the Lua script with open(\"rate_limiter.lua\", \"r\") as file: lua_script = file.read() # Register the Lua script script = redis_client.register_script(lua_script) # Example keys and arguments global_key = \"rate_limit:global\" category_key = \"rate_limit:category:errors\" global_limit = 10 category_limit = 3 window = 60 # 1 minute in seconds for this test # Run the script in a loop for i in range(10): time.sleep(0.1) result = script( keys=[global_key, category_key], args=[global_limit, category_limit, window], ) message = \"Some notification message\" if result == 1: # Allowed: send the notification print(f\"{i}. Notification sent: {message}\") # Add actual notification-sending logic here else: # Blocked: drop the notification print( f\"{i}. Notification dropped (rate limit exceeded): {message}\" ) if __name__ == \"__main__\": main() This code demonstrates how to test the rate limiter by simulating multiple notification requests. The Lua script is loaded, registered with Redis, and executed in a loop to evaluate whether each request is allowed or blocked based on the defined rate limits. Running this will produce output similar to: 0. Notification sent: Some notification message 1. Notification sent: Some notification message 2. Notification sent: Some notification message 3. Notification dropped (rate limit exceeded): Some notification message 4. Notification dropped (rate limit exceeded): Some notification message 5. Notification dropped (rate limit exceeded): Some notification message 6. Notification dropped (rate limit exceeded): Some notification message 7. Notification dropped (rate limit exceeded): Some notification message 8. Notification dropped (rate limit exceeded): Some notification message 9. Notification dropped (rate limit exceeded): Some notification message Here, for demonstration, we set the global rate limit to 10 and the category limit to 3 with a 60-second rolling window. After three successful category notifications (and a total of three global notifications), the rate limiter rejects additional requests in the same window, illustrating how both the global and category limits work together. ","permalink":"http://rednafi.com/misc/hierarchical_rate_limiting/","publishDate":"2025-01-12","summary":"Recently at work, we ran into this problem:\nWe needed to send Slack notifications for specific events but had to enforce rate limits to avoid overwhelming the channel. Here’s how the limits worked:\nGlobal limit: Max 100 requests every 30 minutes. Category limit: Each event type (e.g., errors, warnings) capped at 10 requests per 30 minutes. Now, imagine this:\nThere are 20 event types. Each type hits its 10-notification limit in 30 minutes. That’s 200 requests total, but the global limit only allows 100. So, 100 requests must be dropped—even if some event types still have room under their individual caps. This created a hierarchy of limits:\n","tags":["Database","Python","System"],"title":"Hierarchical rate limiting with Redis sorted sets"},{"content":"I came across a weird shell syntax today—dynamic shell variables. It lets you dynamically construct and access variable names in Bash scripts, which I haven’t encountered in any of the mainstream languages I juggle for work. In an actual programming language, you’d usually use a hashmap to achieve the same effect, but directly templating variable names is a quirky shell feature that sometimes comes in handy. A primer Dynamic shell variables allow shell scripts to define and access variables based on runtime conditions. Variable indirection (${!var} syntax) lets you reference the value of a variable through another variable. This can be useful for managing environment-specific configurations and function dispatch mechanisms. Here’s an example: #!/usr/bin/env bash # script.sh config_path=\"/etc/config\" var=\"config_path\" echo \"The value of \\$config_path is: ${!var}\" The value of $config_path is: /etc/config Here, ${!var} resolves to the value of the variable config_path because var contains its name. This allows you to dynamically decide which variable to reference at runtime. Context-aware environment management A more practical use of dynamic shell variables is managing environment-specific configurations. This is particularly handy in scenarios where you have multiple environments like staging and prod, each with its own unique configuration settings. #!/usr/bin/env bash # script.sh # Define environment-specific configurations dynamically declare staging_URL=\"https://staging.example.com\" declare staging_PORT=8081 declare prod_URL=\"https://example.com\" declare prod_PORT=80 # Set the current environment env=$1 # Validate input if [[ \"$env\" != \"staging\" \u0026\u0026 \"$env\" != \"prod\" ]]; then echo \"Invalid environment. Please specify 'staging' or 'prod'.\" exit 1 fi # Dynamically access the environment-specific variables URL=\"${env}_URL\" PORT=\"${env}_PORT\" echo \"URL: ${!URL}\" echo \"Port: ${!PORT}\" Run the script with an environment as the argument: ./script.sh staging Output for env=\"staging\": URL: https://staging.example.com Port: 8081 By passing the environment as an argument, you can switch between environments without duplicating configuration logic. One gotcha to be aware of is that appending text directly to the ${!VAR} syntax (e.g., ${!env}_URL) doesn’t produce the intended results. Instead of resolving staging_URL, this line will print only _URL: echo \"${!env}_URL\" Output: _URL This happens because ${!VAR} only resolves the value of VAR and doesn’t support direct concatenation. To avoid this, construct the full variable name (URL=\"${env}_URL\") before using ${!VAR} for indirect expansion. This ensures the correct variable is accessed. Function dispatch Another neat use case for dynamic variables is function dispatch—calling the appropriate function based on runtime conditions. This technique can be used to simplify scripts that need to handle multiple services or operations. #!/usr/bin/env bash # script.sh # Define functions for operations on different services web_start() { echo \"Starting web service...\" } web_stop() { echo \"Stopping web service...\" } db_status() { echo \"Checking database status...\" } # Dynamically bind operation to function declare web_start_function=\"web_start\" declare web_stop_function=\"web_stop\" declare db_status_function=\"db_status\" # Input variables for service and operation service=$1 operation=$2 # Build dynamic function name func=\"${service}_${operation}_function\" # Dispatch function dynamically if [[ $(type -t ${!func}) == \"function\" ]]; then ${!func} # Call the dynamically resolved function else echo \"Unknown operation: $service $operation\" fi Run the script with service and operation as arguments: ./script.sh web start This returns: Starting web service... Similarly, running ./script.sh db status prints: Checking database status... Temporary file handling Dynamic variables can also help manage temporary files or logs in scripts that process multiple datasets. By dynamically generating variable names, you can track temporary file paths for each dataset without conflicts. #!/usr/bin/env bash # script.sh # Process multiple datasets with temporary files for dataset in data1 data2 data3; do # Dynamically declare a temporary file variable temp_file_var=\"${dataset}_temp_file\" declare $temp_file_var=\"/tmp/${dataset}_processing.tmp\" # Simulate processing and logging echo \"Processing $dataset...\" \u003e ${!temp_file_var} cat ${!temp_file_var} # Clean up (or add a trap to make this more robust) rm -f ${!temp_file_var} done Running this prints the following: Processing data1... Processing data2... Processing data3... Here, each dataset gets a unique temporary file, managed dynamically by the script. It eliminates the need for manually creating and tracking file names. This works, but like everything else in shell scripts, it can quickly turn into a hairball if we’re not careful. While the syntax is nifty, I find it a bit hard to read at times! ","permalink":"http://rednafi.com/misc/dynamic_shell_variables/","publishDate":"2025-01-11","summary":"I came across a weird shell syntax today—dynamic shell variables. It lets you dynamically construct and access variable names in Bash scripts, which I haven’t encountered in any of the mainstream languages I juggle for work.\nIn an actual programming language, you’d usually use a hashmap to achieve the same effect, but directly templating variable names is a quirky shell feature that sometimes comes in handy.\nA primer Dynamic shell variables allow shell scripts to define and access variables based on runtime conditions. Variable indirection (${!var} syntax) lets you reference the value of a variable through another variable. This can be useful for managing environment-specific configurations and function dispatch mechanisms.\n","tags":["Shell","TIL"],"title":"Dynamic shell variables"},{"content":"One of my 2025 resolutions is doing things that don’t scale and doing them faster without overthinking. The idea is to focus on doing more while worrying less about scalability and sustainability in the things I do outside of work. With that in mind, I’ve been thinking for a while about tracking some of my out-of-band activities on this blog. The goal is to:\nList the things I do, books and articles I read, and talks I grok. Add some commentary or quote something I liked from the content, verbatim, for posterity. Not spam people who just want to read the regular articles. Not turn into a content junkie, churning out slop I wouldn’t want to read myself. This isn’t about getting more eyeballs on what I publish. It’s about tracking what I do so I can look back at the end of the year and enjoy a nice little lull of accomplishment. Plus, having a place to post stuff regularly nudges me to read more, explore more, and do more of the things I actually want to do.\nSocial media is usually where people do this, but digging up old posts and reviewing them later is a pain. Plus, platforms like Twitter tank your posts’ visibility if they have links to other sites. A simple link blog solves all that for me.\nHaving a dynamic site with a backend would make setting up a link blog trivial. But I’m too lazy to maintain a fancy dynamic site, even though this blog gets an okay-ish amount of traffic. This blog is just a static artifact1 generated by Hugo with a minimalistic theme and some custom CSS flair. Adding a link blog without a backend is tricky when I want to avoid dealing with a dedicated backend.\nIf you don’t mind maintaining a dynamic site, Simon Willison has a great piece2 on how he handles his link blog.\nHere’s what has worked for me I went with a dead simple approach that has worked for me for the past 5 months:\nDetail view: Have a single markdown page dedicated to each year and add the links there3. List view: Cluster the detail pages in a list view named “feed” 4. Detail view Each year gets its own detail page where I add links in reverse chronological order. The content has the following structure in markdown:\n--- title: \"2024\" layout: post --- ### December 26 #### [Reflecting on life – Armin Ronacher][32] ... [32]: https://lucumr.pocoo.org/2024/12/26/reflecting-on-life/ --- ### December 24 #### [How I write HTTP services in Go after 13 years – Mat Ryer][31] ... [31]: https://grafana.com/blog/2024/02/09/how-i-write-http-services/ --- Hugo renders the detail view like this:\nList view I use the Papermod theme for this site, and it automatically creates a list view from the yearly pages.\nTo avoid spamming people, the entries in the link blog are filtered out of the main RSS feed. Also, the client-side search functionality allows me to look for a particular link without having to maintain any extra infra.\nThis solution is simple and has been working well for me. I started it in August last year, and the 2024 page5 already has enough entries to make me feel good when I reviewed them at the end of the year.\nBehind the blog ↩︎\nMy approach to running a link blog - Simon Willison ↩︎\nFeed detail view ↩︎\nFeed list view ↩︎\nFeed 2024 ↩︎\n","permalink":"http://rednafi.com/misc/link_blog/","publishDate":"2025-01-06","summary":"One of my 2025 resolutions is doing things that don’t scale and doing them faster without overthinking. The idea is to focus on doing more while worrying less about scalability and sustainability in the things I do outside of work. With that in mind, I’ve been thinking for a while about tracking some of my out-of-band activities on this blog. The goal is to:\nList the things I do, books and articles I read, and talks I grok. Add some commentary or quote something I liked from the content, verbatim, for posterity. Not spam people who just want to read the regular articles. Not turn into a content junkie, churning out slop I wouldn’t want to read myself. This isn’t about getting more eyeballs on what I publish. It’s about tracking what I do so I can look back at the end of the year and enjoy a nice little lull of accomplishment. Plus, having a place to post stuff regularly nudges me to read more, explore more, and do more of the things I actually want to do.\n","tags":["Essay"],"title":"Link blog in a static site"},{"content":"I’ve been having a ton of fun fiddling with Tailscale1 over the past few days. While setting it up on a server, I came across this shell script2 that configures the ufw firewall on Linux to ensure direct communication across different nodes in my tailnet. It has the following block of code that I found interesting (added comments for clarity): #!/usr/bin/env bash # Define the path for the PID file, using the script's name to ensure uniqueness PIDFILE=\"/tmp/$(basename \"${BASH_SOURCE[0]%.*}.pid\")\" # Open file descriptor 200 for the PID file exec 200\u003e\"${PIDFILE}\" # Try to acquire a non-blocking lock; exit if the script is already running flock -n 200 \\ || { echo \"${BASH_SOURCE[0]} script is already running. Aborting...\"; exit 1; } # Store the current process ID (PID) in the lock file for reference PID=$$ echo \"${PID}\" 1\u003e\u0026200 # Do work (in the original script, real work happens here) sleep 999 Here, flock is a Linux command that ensures only one instance of the script runs at a time by locking a specified file (e.g., PIDFILE) through a file descriptor (e.g., 200). If another process already holds the lock, the script either waits or exits immediately. Above, it bails with an error message and exit code 1. If you try running two instances of this script, the second one will exit with this message: script is already running. Aborting... On most Linux distros, flock comes along with the coreutils. If not, it’s easy to install with your preferred package manager. A more portable version On macOS, the file locking mechanism is different, and flock doesn’t work there. To make your script portable, you can use mkdir in the following manner to achieve a similar result: #!/usr/bin/env bash LOCKDIR=\"/tmp/$(basename \"${BASH_SOURCE[0]%.*}.lock\")\" # Try to create the lock directory mkdir \"${LOCKDIR}\" 2\u003e/dev/null || { echo \"Another instance is running. Aborting...\" exit 1 } # Set up cleanup for the lock directory trap \"rmdir \\\"${LOCKDIR}\\\"\" EXIT # Main script logic echo \"Acquired lock, doing important stuff...\" # ... your script logic ... sleep 999 This works because mkdir is atomic. It creates the lock directory (LOCKDIR) in /tmp or fails if the directory already exists. This acts as a marker for the running instance. If successful, the script sets up a trap to remove the directory on exit and continues to the main logic. If mkdir fails, it means another instance of the process is running, and the script exits with a message. This is almost as effective as the flock version. Since I rarely write scripts for non-Linux environments, either option is fine! With Python Oftentimes, I opt for Python when I need to write larger scripts. The same can be achieved in Python like this: import fcntl import os import sys import time # Use the script name to generate a unique lock file LOCKFILE = f\"/tmp/{os.path.basename(__file__)}.lock\" def work() -\u003e None: time.sleep(999) if __name__ == \"__main__\": try: # Open a file and acquire an exclusive lock with open(LOCKFILE, \"w\") as lockfile: fcntl.flock(lockfile, fcntl.LOCK_EX | fcntl.LOCK_NB) print(\"Acquired lock, running script...\") # Main script logic here work() except BlockingIOError: print(\"Another instance is running. Exiting.\") sys.exit(1) The script uses fcntl.flock to prevent multiple instances from running. It creates a lock file (LOCKFILE) in the /tmp directory, named after the scripts filename. When the script starts, it opens the file in write mode and tries to lock it with fcntl.flock using an exclusive lock (LOCK_EX). The LOCK_NB flag makes the operation non-blocking. If another process holds the lock, the script exits with a message. This approach works on both Linux and macOS, as both support fcntl for file-based locks. The lock is automatically released when the file is closed, either at the end of the script or the with block. With Go I was curious about doing it in Go. It’s quite similar to Python: package main import ( \"fmt\" \"os\" \"path/filepath\" \"syscall\" \"time\" ) // Use the script name (basename) to generate a unique lock file var lockfile = fmt.Sprintf(\"/tmp/%s.lock\", filepath.Base(os.Args[0])) func work() { time.Sleep(999 * time.Second) } func main() { // Open the lock file file, err := os.OpenFile(lockfile, os.O_CREATE|os.O_RDWR, 0644) if err != nil { fmt.Println(\"Failed to open lock file:\", err) os.Exit(1) } defer file.Close() // Try to acquire an exclusive lock err = syscall.Flock(int(file.Fd()), syscall.LOCK_EX|syscall.LOCK_NB) if err != nil { fmt.Println(\"Another instance is running. Exiting.\") os.Exit(1) } // Release the lock on exit defer syscall.Flock(int(file.Fd()), syscall.LOCK_UN) fmt.Println(\"Acquired lock, running script...\") // Main script logic work() } Like the Python example, this uses syscall.Flock to prevent multiple script instances. It creates a lock file based on the script’s name using filepath.Base(os.Args[0]) and stores it in /tmp. The script tries to acquire an exclusive, non-blocking lock (LOCK_EX | LOCK_NB). If unavailable, it exits with a message. The lock is automatically released when the file is closed in the defer block. Underneath, Go makes sure that syscall.Flock works on both macOS and Linux. Tailscale ↩︎ Update tailscale ufw rules ↩︎ ","permalink":"http://rednafi.com/misc/run_single_instance/","publishDate":"2024-12-31","summary":"I’ve been having a ton of fun fiddling with Tailscale1 over the past few days. While setting it up on a server, I came across this shell script2 that configures the ufw firewall on Linux to ensure direct communication across different nodes in my tailnet. It has the following block of code that I found interesting (added comments for clarity):\n#!/usr/bin/env bash # Define the path for the PID file, using the script's name to ensure uniqueness PIDFILE=\"/tmp/$(basename \"${BASH_SOURCE[0]%.*}.pid\")\" # Open file descriptor 200 for the PID file exec 200\u003e\"${PIDFILE}\" # Try to acquire a non-blocking lock; exit if the script is already running flock -n 200 \\ || { echo \"${BASH_SOURCE[0]} script is already running. Aborting...\"; exit 1; } # Store the current process ID (PID) in the lock file for reference PID=$$ echo \"${PID}\" 1\u003e\u0026200 # Do work (in the original script, real work happens here) sleep 999 Here, flock is a Linux command that ensures only one instance of the script runs at a time by locking a specified file (e.g., PIDFILE) through a file descriptor (e.g., 200). If another process already holds the lock, the script either waits or exits immediately. Above, it bails with an error message and exit code 1.\n","tags":["Shell","Python","Go"],"title":"Running only a single instance of a process"},{"content":"People love single-method interfaces (SMIs) in Go. They’re simple to implement and easy to reason about. The standard library is packed with SMIs like io.Reader, io.Writer, io.Closer, io.Seeker, and more. One cool thing about SMIs is that you don’t always need to create a full-blown struct with a method to satisfy the interface. You can define a function type, attach the interface method to it, and use it right away. This approach works well when there’s no state to maintain, so the extra struct becomes unnecessary. However, I find the syntax for this a bit abstruce. So, I’m jotting down a few examples here to reference later. Using a struct to implement an interface This is how interfaces are typically implemented. Here, we’ll satisfy the io.Writer interface to create a writer that logs some stats before saving data to an in-memory buffer. The standard library defines io.Writer like this: type Writer interface { Write(p []byte) (n int, err error) } We can implement io.Writer by defining a struct type, LoggingWriter, and attaching a Write method with the required signature: // LoggingWriter writes data to an underlying writer and logs stats. type LoggingWriter struct { w io.Writer } func (lw *LoggingWriter) Write(data []byte) (int, error) { fmt.Printf(\"LoggingWriter: Writing %d bytes\\n\", len(data)) return lw.w.Write(data) } Here’s how to use it: func main() { var buf bytes.Buffer logWriter := \u0026LoggingWriter{w: \u0026buf} _, err := logWriter.Write([]byte(\"Hello, world!\")) if err != nil { fmt.Println(\"Error writing data:\", err) return } fmt.Println(\"Buffer content:\", buf.String()) } Running this will log the stats before writing to the buffer: LoggingWriter: Writing 13 bytes Buffer content: Hello, world! Using a function type instead Instead of defining the LoggingWriter struct, you can use a function type to satisfy io.Writer. This works well for SMIs but doesn’t make sense for interfaces with multiple methods. In those cases, we need to resort back to the methods-on-struct approach. Here’s how it looks: // WriteFunc is a function type that implements io.Writer. type WriteFunc func(data []byte) (int, error) // Write makes WriteFunc satisfy io.Writer. func (wf WriteFunc) Write(data []byte) (int, error) { return wf(data) } You can use WriteFunc like this: func main() { var buf bytes.Buffer // Define a WriteFunc to log stats and write data. logWriter := WriteFunc(func(data []byte) (int, error) { fmt.Printf(\"WriteFunc: Writing %d bytes\\n\", len(data)) return buf.Write(data) }) _, err := logWriter.Write([]byte(\"Hello, world!\")) if err != nil { fmt.Println(\"Error writing data:\", err) return } fmt.Println(\"Buffer content:\", buf.String()) } WriteFunc satisfies io.Writer by defining a Write method with the expected signature. You can adapt any function to match the signature (data []byte) (int, error) using WriteFunc, so there’s no need for a struct when no state is involved. In main, an anonymous function logs the number of bytes and writes the data to a buffer. Wrapping this function with WriteFunc lets it implement the io.Writer interface. The .Write method is called on the wrapped function to log stats and write data to the buffer. Finally, the buffer’s content is printed to verify everything worked. For a simple example like this, using a function type to implement an interface might feel like overkill. But there are cases where it simplifies things. The next sections explore real-world examples where function types make interface implementation a bit more ergonomic. Mocking interfaces for testing Function types let you mock interfaces without creating dedicated structs. Here’s how it works with an Authenticator interface: type Authenticator interface { Authenticate(username, password string) (bool, error) } type AuthFunc func(username, password string) (bool, error) func (af AuthFunc) Authenticate(username, password string) (bool, error) { return af(username, password) } The AuthFunc type implements the Authenticate method by calling itself with the provided arguments. This lets you create mock implementations inline in your tests. Here’s how to use it in a test: func TestLogin(t *testing.T) { mockAuth := AuthFunc(func(u, p string) (bool, error) { fmt.Printf(\"MockAuth called with username=%s, password=%s\\n\", u, p) return true, nil }) success, err := PerformLogin(\"john_doe\", \"secret\", mockAuth) if err != nil || !success { t.Fatalf(\"Authentication failed\") } } And in application code: func main() { auth := AuthFunc(func(u, p string) (bool, error) { return u == \"admin\" \u0026\u0026 p == \"password123\", nil }) if success, _ := auth.Authenticate(\"admin\", \"password123\"); success { fmt.Println(\"Authentication successful!\") } } Building HTTP middlewares The standard library’s http.HandlerFunc demonstrates function types in action. Here’s how to build a logging middleware that times requests: type Handler interface { ServeHTTP(http.ResponseWriter, *http.Request) } func LoggingMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { start := time.Now() fmt.Printf(\"Started %s %s\\n\", r.Method, r.URL.Path) next.ServeHTTP(w, r) fmt.Printf(\"Completed %s in %v\\n\", r.URL.Path, time.Since(start)) }) } http.HandlerFunc converts functions into HTTP handlers. The logging middleware wraps the next handler and adds timing and logging. We use it as follows: func main() { handler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \"Hello, World!\") }) http.Handle(\"/\", LoggingMiddleware(handler)) http.ListenAndServe(\":8080\", nil) } Adapting function types for database queries Function types can abstract database query execution for testing or supporting different database implementations: type QueryExecutor interface { Execute(query string, args ...any) (Result, error) } type QueryFunc func(query string, args ...any) (Result, error) func (qf QueryFunc) Execute(query string, args ...any) (Result, error) { return qf(query, args...) } QueryFunc turns regular functions into QueryExecutor implementations, making it easy to swap implementations or create mocks. This is how to use it: func main() { executor := QueryFunc(func(query string, args ...any) (Result, error) { fmt.Printf(\"Executing query: %s with args: %v\\n\", query, args) return Result{RowsAffected: 1}, nil }) result, _ := executor.Execute(\"SELECT * FROM users WHERE id = ?\", 1) fmt.Printf(\"Rows affected: %d\\n\", result.RowsAffected) } Implementing retry logic Function types can encapsulate retry behavior without creating configuration structs: type Retryer interface { Retry(fn func() error) error } type RetryFunc func(fn func() error) error func (rf RetryFunc) Retry(fn func() error) error { return rf(fn) } RetryFunc converts functions with the matching signature into a Retryer, letting you swap retry strategies or create test versions. We use it as such: func main() { retry := RetryFunc(func(fn func() error) error { for i := 0; i \u003c 3; i++ { if err := fn(); err == nil { return nil } time.Sleep(time.Second * time.Duration(i+1)) } return fmt.Errorf(\"operation failed after 3 retries\") }) err := retry.Retry(func() error { return nil // Your operation here }) if err != nil { fmt.Printf(\"Failed to execute operation: %v\\n\", err) } } Go lets us define methods on custom types, including function types. While this can be handy for adapting a function type to an interface, it can make the code hard to read at times. So I don’t always reach for it. It’s perfectly fine to define an empty struct with a single method if that makes the code more readable. Nonetheless, it’s a neat trick to keep in your repertoire. ","permalink":"http://rednafi.com/go/func_types_and_smis/","publishDate":"2024-12-22","summary":"People love single-method interfaces (SMIs) in Go. They’re simple to implement and easy to reason about. The standard library is packed with SMIs like io.Reader, io.Writer, io.Closer, io.Seeker, and more.\nOne cool thing about SMIs is that you don’t always need to create a full-blown struct with a method to satisfy the interface. You can define a function type, attach the interface method to it, and use it right away. This approach works well when there’s no state to maintain, so the extra struct becomes unnecessary. However, I find the syntax for this a bit abstruce. So, I’m jotting down a few examples here to reference later.\n","tags":["Go"],"title":"Function types and single-method interfaces in Go"},{"content":"Setting up SSH access to a new VM usually follows the same routine: generate a key pair, copy it to the VM, tweak some configs, confirm the host’s identity, and maybe set up an agent to avoid typing passphrases all day. Tools like cloud-init and Ansible handle most of the setup for me now, so I rarely think about it. But I realized I don’t fully understand how all the parts work together. This post attempts to give an overview of what happens when you type ssh user@host. It covers key pairs, authorized_keys, sshd_config, ~/.ssh/config, known_hosts, and how they all fit together. A new VM in the void You’ve provisioned a new VM and need key-based SSH access. This involves generating a key pair on your local machine, installing the public key on the remote VM, and ensuring the SSH daemon (sshd) on the VM trusts it. Done right, ssh user@host drops you into a shell without a password prompt. First, generate a key pair on your local machine: ssh-keygen -t ed25519 -C \"your_email@example.com\" This creates two files: a private key (~/.ssh/id_ed25519) and a public key (~/.ssh/id_ed25519.pub). The private key stays local. The public key is shared. Your local fortress Your local private key proves your identity and must be protected (~/.ssh/id_ed25519). The public key (~/.ssh/id_ed25519.pub) gets copied to the VM. To view the public key locally: cat ~/.ssh/id_ed25519.pub # Output: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIG... user@local Copy this public key to the VM. The remote gatekeeper On the VM, sshd listens for connections and authenticates users. Its configuration file, /etc/ssh/sshd_config, defines policies: whether password logins are allowed, which keys are trusted, and which crypto settings to use. A hardened snippet might look like this: # /etc/ssh/sshd_config (on the VM) PermitRootLogin no PasswordAuthentication no PubkeyAuthentication yes AuthorizedKeysFile .ssh/authorized_keys With PasswordAuthentication no, only keys can unlock access. Authorized keys and the handshake The ~/.ssh/authorized_keys file on the VM decides who gets access. Add your public key there to tell sshd that anyone holding the matching private key (you) can connect. On the VM, under the user’s home directory: mkdir -p ~/.ssh chmod 700 ~/.ssh echo \"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIG... user@local\" \\ \u003e\u003e ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys Now when you run ssh user@host, the server matches your key to one in authorized_keys and lets you in. The client and its configs Your local SSH client can be configured via ~/.ssh/config to simplify hostnames, ports, and key paths: # ~/.ssh/config (on your local machine) Host myvm HostName 203.0.113.10 User ubuntu IdentityFile ~/.ssh/id_ed25519 Port 22 Now you can connect with: ssh myvm Known hosts and server identity When you connect to the VM for the first time, SSH prompts you to confirm its identity. Accepting it adds the server’s host key to ~/.ssh/known_hosts. SSH will detect any identity changes during subsequent connections: 203.0.113.10 ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAI... This prevents man-in-the-middle attacks. Agent and forwarding If your private key has a passphrase, typing it constantly is annoying. ssh-agent caches the key in memory, so you only unlock it once: eval \"$(ssh-agent -s)\" ssh-add ~/.ssh/id_ed25519 # Enter passphrase once For hopping through multiple servers (local → bastion → internal server), enable agent forwarding: # ~/.ssh/config (local) Host myvm HostName 203.0.113.10 User ubuntu IdentityFile ~/.ssh/id_ed25519 ForwardAgent yes Now, you can connect through intermediary hosts without copying private keys around. Configuring and hardening the daemon On the VM, refine /etc/ssh/sshd_config to enforce stricter security: # /etc/ssh/sshd_config (on the VM) PasswordAuthentication no PermitRootLogin prohibit-password AllowUsers ubuntu PubkeyAuthentication yes KexAlgorithms curve25519-sha256@libssh.org Ciphers aes256-gcm@openssh.com,chacha20-poly1305@openssh.com MACs hmac-sha2-512-etm@openssh.com These settings ensure that only trusted keys with modern crypto algorithms can connect. Bringing it all together Here’s a quick summary of setting up SSH connection to a new machine: Generate a key pair on your local machine. Copy the public key to the authorized_keys file on the VM. Configure sshd_config on the VM to allow key-based authentication. Set up ~/.ssh/config on your local machine to simplify SSH commands (e.g., ssh myvm instead of ssh ). Confirm the server’s identity and save it to known_hosts on your local machine. Use ssh-agent on your local machine to cache your private key and avoid typing the passphrase repeatedly. Enable agent forwarding in your SSH config to connect through intermediary servers without copying keys. Harden sshd_config on the VM to enforce modern crypto algorithms and stricter security policies. ┌──────────────────────┐ │ LOCAL │ │ ~/.ssh/config │ │ ~/.ssh/id_* │ │ ~/.ssh/known_hosts │ │ ssh-agent │ └───▲───────┬──────────┘ │ │ │ │ SSH Connection (Port 22) │ │ │ │ │ ▼ ┌────────────────────────┐ │ REMOTE │ │ /etc/ssh/sshd_config │ │ ~/.ssh/authorized_keys │ │ sshd daemon │ └────────────────────────┘ ","permalink":"http://rednafi.com/misc/ssh_saga/","publishDate":"2024-12-17","summary":"Setting up SSH access to a new VM usually follows the same routine: generate a key pair, copy it to the VM, tweak some configs, confirm the host’s identity, and maybe set up an agent to avoid typing passphrases all day. Tools like cloud-init and Ansible handle most of the setup for me now, so I rarely think about it. But I realized I don’t fully understand how all the parts work together.\n","tags":["Shell"],"title":"SSH saga"},{"content":"Sometimes, when writing tests in Pytest, I find myself using fixtures that the test function/method doesn’t directly reference. Instead, Pytest runs the fixture, and the test function implicitly leverages its side effects. For example: import os from collections.abc import Iterator from unittest.mock import Mock, patch import pytest # Define an implicit environment mock fixture that patches os.environ @pytest.fixture def mock_env() -\u003e Iterator[None]: with patch.dict(\"os.environ\", {\"IMPLICIT_KEY\": \"IMPLICIT_VALUE\"}): yield # Define an explicit service mock fixture @pytest.fixture def mock_svc() -\u003e Mock: service = Mock() service.process.return_value = \"Explicit Mocked Response\" return service # IDEs tend to dim out unused parameters like mock_env def test_stuff(mock_svc: Mock, mock_env: Mock) -\u003e None: # Use the explicit mock response = mock_svc.process() assert response == \"Explicit Mocked Response\" mock_svc.process.assert_called_once() # Assert the environment variable patched by mock_env assert os.environ[\"IMPLICIT_KEY\"] == \"IMPLICIT_VALUE\" In the test_stuff function above, we directly use the mock_svc fixture but not mock_env. Instead, we expect Pytest to run mock_env, which modifies the environment variables. This works, but IDEs often mark mock_env as an unused parameter and dims it out. One way to avoid this is by marking the mock_env fixture with @pytest.fixture(autouse=True) and omitting it from the test function’s parameters. However, I prefer not to use autouse=True because it can make reasoning about tests harder. TIL that you can use @pytest.mark.usefixtures1 to inject these implicit fixtures without cluttering the test function signature or using autouse. Here’s the same test marked with usefixtures: # ... same as above @pytest.mark.usefixtures(\"mock_env\") def test_stuff(mock_svc: Mock) -\u003e None: # Use the explicit mock response = mock_svc.process() assert response == \"Explicit Mocked Response\" mock_svc.process.assert_called_once() # Assert the environment variable patched by mock_env assert os.environ[\"IMPLICIT_KEY\"] == \"IMPLICIT_VALUE\" Now, the mock_env fixture is applied without cluttering the test function’s signature, and no more greyed-out unused parameter warnings! The usefixtures marker also accepts multiple fixtures as variadic arguments: @pytest.mark.usefixtures(\"fixture_a\", \"fixture_b\"). One thing to keep in mind is that it won’t work if you try to mark another fixture with the usefixtures decorator. The pytest documentation includes a warning2 about this. Fin! Use fixtures in classes and modules with usefixtures - Pytest docs ↩︎ The usefixutre mark has no effect on fixtures - Pytest docs ↩︎ ","permalink":"http://rednafi.com/python/inject_pytest_fixture/","publishDate":"2024-12-02","summary":"Sometimes, when writing tests in Pytest, I find myself using fixtures that the test function/method doesn’t directly reference. Instead, Pytest runs the fixture, and the test function implicitly leverages its side effects. For example:\nimport os from collections.abc import Iterator from unittest.mock import Mock, patch import pytest # Define an implicit environment mock fixture that patches os.environ @pytest.fixture def mock_env() -\u003e Iterator[None]: with patch.dict(\"os.environ\", {\"IMPLICIT_KEY\": \"IMPLICIT_VALUE\"}): yield # Define an explicit service mock fixture @pytest.fixture def mock_svc() -\u003e Mock: service = Mock() service.process.return_value = \"Explicit Mocked Response\" return service # IDEs tend to dim out unused parameters like mock_env def test_stuff(mock_svc: Mock, mock_env: Mock) -\u003e None: # Use the explicit mock response = mock_svc.process() assert response == \"Explicit Mocked Response\" mock_svc.process.assert_called_once() # Assert the environment variable patched by mock_env assert os.environ[\"IMPLICIT_KEY\"] == \"IMPLICIT_VALUE\" In the test_stuff function above, we directly use the mock_svc fixture but not mock_env. Instead, we expect Pytest to run mock_env, which modifies the environment variables. This works, but IDEs often mark mock_env as an unused parameter and dims it out.\n","tags":["Python","TIL"],"title":"Injecting Pytest fixtures without cluttering test signatures"},{"content":"Although I’ve been using Python 3.12 in production for nearly a year, one neat feature in the typing module that escaped me was the @override decorator. Proposed in PEP-6981, it’s been hanging out in typing_extensions for a while. This is one of those small features you either don’t care about or get totally psyched over. I’m definitely in the latter camp. In languages like C#, Java, and Kotlin, explicit overriding is required. For instance, in Java, you use @Override to make it clear you’re overriding a method in a sub class. If you mess up the method name or if the method doesn’t exist in the superclass, the compiler throws an error. Now, with Python’s @override decorator, we get similar benefits—though only if you’re using a static type checker. Here’s an example: from typing import override class Animal: def sound(self) -\u003e str: return \"Unknown\" class Cat(Animal): @override def soud(self) -\u003e str: # Notice the typo: sound -\u003e soud # Your implementation here return \"Meow\" In this example, Cat inherits from Animal, and we intended to override the sound method. But there’s a typo in the subclass method name. Running mypy will flag it: error: Method \"soud\" is marked as an override, but no base method was found with this name [misc] Found 1 error in 1 file (checked 1 source file) This decorator also works with class, property, or any other methods. Observe: from typing import override class Animal: @property def species(self) -\u003e str: return \"Unknown\" class Cat(Animal): @override @property def species(self) -\u003e str: return \"Catus\" If the overriding method isn’t marked with @property, mypy will raise an error: error: Signature of \"species\" incompatible with supertype \"Animal\" [override] note: Superclass: note: str note: Subclass: note: def species(self) -\u003e str Found 1 error in 1 file (checked 1 source file) The error message could be clearer here, though. You can use @override with class methods too: from typing import override class Animal: @classmethod def category(cls) -\u003e str: return \"Unknown\" class Cat(Animal): @override @classmethod def category(cls) -\u003e str: return \"Mammal\" In these cases, the order of @override doesn’t matter; you can put it before or after the property decorator, and it’ll still work. I personally prefer keeping it as the outermost decorator. I’ve been gradually adding the @override decorator to my code, as it not only prevents typos but also alerts me if an upstream method name changes. PEP 698 – Override decorator for static typing ↩︎ ","permalink":"http://rednafi.com/python/typing_override/","publishDate":"2024-11-06","summary":"Although I’ve been using Python 3.12 in production for nearly a year, one neat feature in the typing module that escaped me was the @override decorator. Proposed in PEP-6981, it’s been hanging out in typing_extensions for a while. This is one of those small features you either don’t care about or get totally psyched over. I’m definitely in the latter camp.\nIn languages like C#, Java, and Kotlin, explicit overriding is required. For instance, in Java, you use @Override to make it clear you’re overriding a method in a sub class. If you mess up the method name or if the method doesn’t exist in the superclass, the compiler throws an error. Now, with Python’s @override decorator, we get similar benefits—though only if you’re using a static type checker.\n","tags":["Python","TIL"],"title":"Explicit method overriding with @typing.override"},{"content":"This morning, someone on Twitter pointed me to PEP 5621, which introduces __getattr__ and __dir__ at the module level. While __dir__ helps control which attributes are printed when calling dir(module), __getattr__ is the more interesting addition. The __getattr__ method in a module works similarly to how it does in a Python class. For example: class Cat: def __getattr__(self, name: str) -\u003e str: if name == \"voice\": return \"meow!!\" raise AttributeError(f\"Attribute {name} does not exist\") # Try to access 'voice' on Cat cat = Cat() cat.voice # Prints \"meow!!\" # Raises AttributeError: Attribute something_else does not exist cat.something_else In this class, __getattr__ defines what happens when specific attributes are accessed, allowing you to manage how missing attributes behave. Since Python 3.7, you can also define __getattr__ at the module level to handle attribute access on the module itself. For instance, if you have a module my_module.py: # my_module.py def existing_function() -\u003e str: return \"I exist!\" def __getattr__(name: str) -\u003e str: if name == \"dynamic_attribute\": return \"I was generated dynamically!\" raise AttributeError(f\"Module {__name__} has no attribute {name}\") Using this module: # another_module.py import my_module print(my_module.existing_function()) # Prints \"I exist!\" print(my_module.dynamic_attribute) # Prints \"I was generated dynamically!\" print(my_module.non_existent) # Raises AttributeError If an attribute isn’t found through the regular lookup (using object.__getattribute__), Python will look for __getattr__ in the module’s __dict__. If found, it calls __getattr__ with the attribute name and returns the result. But if you’re looking up a name directly as a module global, it bypasses __getattr__. This prevents performance issues that would arise from repeatedly invoking __getattr__ for built-in or common attributes. One practical use for module-level __getattr__ is lazy-loading heavy dependencies to improve startup performance. Imagine you have a module that relies on a large library but don’t need it immediately at import. # heavy_module.py from typing import Any def __getattr__(name: str) -\u003e Any: if name == \"np\": import numpy as np globals()[\"np\"] = np # Cache it in the module's namespace return np raise AttributeError(f\"Module {__name__} has no attribute {name}\") With this setup, importing heavy_module doesn’t immediately import NumPy. Only when you access heavy_module.np does it trigger the import: # main.py import heavy_module # NumPy hasn't been imported yet. # Code that doesn't need NumPy... # Now we need NumPy arr = heavy_module.np.array([1, 2, 3]) print(arr) # NumPy is now imported and used The first access to heavy_module.np imports NumPy (adding ~150ns), but since we cache np with globals()['np'] = np, subsequent accesses are fast, as the module now holds the reference to NumPy. This approach is handy in scenarios like CLIs where you want to keep startup quick. For example, if you need to initialize a database connection but only for specific commands, you can defer the setup until needed. Here’s an example with SQLite (though SQLite connections are quick, imagine a slower connection here): # db_module.py import sqlite3 # Caching initialized connection in the global namespace _connection: sqlite3.Connection | None = None def __getattr__(name: str) -\u003e sqlite3.Connection: if name == \"connection\": global _connection if _connection is None: print(\"Initializing database connection...\") _connection = sqlite3.connect(\"my_database.db\") return _connection raise AttributeError(f\"Module {__name__} has no attribute {name}\") In this setup, nothing is instantiated when you import db_module. The connection is only initialized on the first access of db_module.connection. Later calls use the cached _connection, making subsequent access fast. Here’s how you might use it in a CLI: # cli.py import click import db_module @click.group() def cli() -\u003e None: pass @cli.command() def greet() -\u003e None: click.echo(\"Hello!\") @cli.command() def show_data() -\u003e None: conn = ( db_module.connection ) # Initializes the database connection if needed cursor = conn.cursor() cursor.execute(\"SELECT * FROM my_table\") results = cursor.fetchall() click.echo(f\"Data: {results}\") if __name__ == \"__main__\": cli() When you run python cli.py greet, the CLI starts quickly since it doesn’t initialize the database connection. But running python cli.py show_data accesses db_module.connection, which triggers the connection setup. This could also be achieved by defining a function that initializes the database connection and caches it for subsequent calls. However, using module-level __getattr__ can be more convenient if you have multiple global variables that require expensive calculations or initializations. Instead of writing separate functions for each variable, you can handle them all within the __getattr__ method. Here’s one example of using it for a non-trivial case in the wild2. PEP 562 – Module __getattr__ and __dir__ ↩︎ Prefect - __getattr__ ↩︎ ","permalink":"http://rednafi.com/python/module_getattr/","publishDate":"2024-11-03","summary":"This morning, someone on Twitter pointed me to PEP 5621, which introduces __getattr__ and __dir__ at the module level. While __dir__ helps control which attributes are printed when calling dir(module), __getattr__ is the more interesting addition.\nThe __getattr__ method in a module works similarly to how it does in a Python class. For example:\nclass Cat: def __getattr__(self, name: str) -\u003e str: if name == \"voice\": return \"meow!!\" raise AttributeError(f\"Attribute {name} does not exist\") # Try to access 'voice' on Cat cat = Cat() cat.voice # Prints \"meow!!\" # Raises AttributeError: Attribute something_else does not exist cat.something_else In this class, __getattr__ defines what happens when specific attributes are accessed, allowing you to manage how missing attributes behave. Since Python 3.7, you can also define __getattr__ at the module level to handle attribute access on the module itself.\n","tags":["Python","TIL"],"title":"Quicker startup with module-level __getattr__"},{"content":"I always get tripped up by Docker’s different mount types and their syntax, whether I’m stringing together some CLI commands or writing a docker-compose file. Docker’s docs cover these, but for me, the confusion often comes from how “bind” is used in various contexts and how “volume” and “bind” sometimes get mixed up in the documentation.\nHere’s my attempt to disentangle some of my most-used mount commands.\nVolume mounts Volume mounts1 let you store data outside the container in a location managed by Docker. The data persists even after the container stops. On non-Linux systems, volume mounts are faster than bind mounts because data doesn’t need to cross the virtualization boundary.\nThe -v option The -v flag is the older and more common way to define volume mounts in the Docker CLI. For example:\ndocker run -v myvolume:/usr/share/nginx/html:ro nginx Here’s what each part means:\nmyvolume: The name of the Docker-managed volume on the host. /usr/share/nginx/html: The mount point inside the container. :ro: Mounts the volume as read-only inside the container. The host can still write to the volume, but the container cannot. The general syntax is:\n-v [SOURCE]:[TARGET]:[OPTIONS] It can be tricky to remember which part is the host and which is the container, especially since with volumes, the SOURCE is a volume name, not a host path.\nThe --mount option To make things clearer, Docker introduced the --mount option, which uses key-value pairs. The same volume mount using --mount looks like this:\ndocker run \\ --mount \\ type=volume,source=myvolume,target=/usr/share/nginx/html,readonly nginx Or using shorthands:\ndocker run --mount type=volume,src=myvolume,dst=/usr/share/nginx/html,ro nginx I find this syntax more explicit and less error-prone, even if it’s a bit more verbose.\nIn docker-compose.yml In docker-compose, volumes can be defined using both the old and new syntax. Here’s how they compare:\nOld style\nservices: app: image: nginx volumes: - myvolume:/usr/share/nginx/html:ro volumes: myvolume: New style\nservices: app: image: nginx volumes: - type: volume source: myvolume target: /usr/share/nginx/html read_only: true volumes: myvolume: I prefer the new style because it reduces ambiguity and makes the configuration clearer.\nBind mounts Bind mounts2 let you directly mount a file or directory from the host into the container. This is especially useful in development when you want the container to have access to your code or data.\nThe key difference between volume mounts and bind mounts is that volumes are fully managed by Docker and stored in a special location, while bind mounts rely on specific host paths. Volumes are more portable and isolated from the host, whereas bind mounts give you direct access to host files but can introduce permission issues and depend on the exact file structure of the host.\nThe -v option Using the -v syntax for bind mounts:\ndocker run -v /path/on/host:/usr/share/nginx/html:ro nginx Here:\n/path/on/host: The path on the host machine. This must be an absolute path. /usr/share/nginx/html: The mount point inside the container. :ro: Mounts the directory as read-only inside the container. The --mount option Using --mount for a bind mount:\ndocker run \\ --mount \\ type=bind,source=/path/on/host,target=/usr/share/nginx/html,readonly nginx This syntax makes it clear that you’re using a bind mount and specifies exactly which paths are involved.\nIn docker-compose.yml In docker-compose, bind mounts can be specified like this:\nOld style\nservices: app: image: nginx volumes: - ./path/on/host:/usr/share/nginx/html:ro New style\nservices: app: image: nginx volumes: - type: bind source: ./path/on/host target: /usr/share/nginx/html read_only: true Note: In docker-compose, if you specify a source that doesn’t start with / (an absolute path) or ./ (a relative path), Docker might think you’re referring to a volume. To ensure it’s interpreted as a bind mount, start the path with ./ or /.\nTmpfs mounts Tmpfs mounts3 store data in the host’s memory, not on disk. This makes them ideal for temporary storage that doesn’t need to persist after the container stops. They’re great for things like caches or scratch space.\nThe --tmpfs option Docker provides a --tmpfs option to create a tmpfs mount more concisely:\ndocker run --tmpfs /app/tmp:rw,size=64m nginx /app/tmp: The target directory inside the container. rw: This option allows read and write access to the tmpfs mount. size=64m: Sets the size of the tmpfs mount to 64 MB. The --mount option Alternatively, using the more flexible --mount option:\ndocker run \\ --mount type=tmpfs,target=/app/tmp,tmpfs-size=64m,tmpfs-mode=1777 nginx Here’s what each part means:\ntype=tmpfs: Specifies that this is a tmpfs mount, using the host’s memory. target=/app/tmp: The directory inside the container where the tmpfs mount is mounted. tmpfs-size=64m: Limits the size of the tmpfs mount to 64 MB. tmpfs-mode=1777: Sets permissions for the tmpfs mount (1777 grants read, write, and execute permissions to everyone). In docker-compose.yml In docker-compose, tmpfs mounts can be defined using both the old and new syntax.\nOld style\nservices: app: image: nginx tmpfs: - /app/tmp:size=64m New style\nservices: app: image: nginx volumes: - type: tmpfs target: /app/tmp tmpfs: size: 64m mode: 1777 Build cache mounts Build cache mounts4 help speed up Docker image builds by caching intermediate files like package downloads or compiled artifacts. They’re used during the build process and aren’t part of the final container image.\nIn a Dockerfile, you might use a build cache like this:\nRUN --mount=type=cache,target=/var/cache/apt \\ apt-get update \u0026\u0026 apt-get install -y curl Here’s what each option does:\n--mount=type=cache: Defines a cache mount that stores the files from the apt-get commands to speed up future builds by reusing the downloaded packages. target=/var/cache/apt: Specifies the location inside the container where the cache will be stored during the build process. In my Python projects, I cache my dependencies and install them with uv like this:\nRUN --mount=type=cache,target=/root/.cache/uv \\ --mount=type=bind,source=uv.lock,target=uv.lock \\ --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\ uv sync --no-install-project --locked --no-dev Above, the cache mount at /root/.cache/uv saves dependencies so they don’t need to be re-downloaded in future builds if nothing changes. The bind mounts provide access to uv.lock and pyproject.toml from the host, allowing the container to read these config files during the build. Any changes to the files are picked up, while cached dependencies are reused unless the configurations have been updated.\nVolume mounts ↩︎\nBind mounts ↩︎\nTmpfs mounts ↩︎\nBuild cache mounts ↩︎\n","permalink":"http://rednafi.com/misc/docker_mount/","publishDate":"2024-10-22","summary":"I always get tripped up by Docker’s different mount types and their syntax, whether I’m stringing together some CLI commands or writing a docker-compose file. Docker’s docs cover these, but for me, the confusion often comes from how “bind” is used in various contexts and how “volume” and “bind” sometimes get mixed up in the documentation.\nHere’s my attempt to disentangle some of my most-used mount commands.\nVolume mounts Volume mounts1 let you store data outside the container in a location managed by Docker. The data persists even after the container stops. On non-Linux systems, volume mounts are faster than bind mounts because data doesn’t need to cross the virtualization boundary.\n","tags":["TIL","Docker"],"title":"Docker mount revisited"},{"content":"I was fiddling with graphlib in the Python stdlib and found it quite nifty. It processes a Directed Acyclic Graph (DAG), where tasks (nodes) are connected by directed edges (dependencies), and returns the correct execution order. The “acyclic” part ensures no circular dependencies. Topological sorting is useful for arranging tasks so that each one follows its dependencies. It’s widely used in scheduling, build systems, dependency resolution, and database migrations. For example, consider these tasks: Task A must be completed before Tasks B and C. Tasks B and C must be completed before Task D. This can be represented as: graph TD A --\u003e B A --\u003e C B --\u003e D C --\u003e D Here, A can start right away, B and C follow after A, and D is last, depending on both B and C. The task order can be determined as: A B and C (in parallel, since both depend only on A) D (which depends on both B and C) This method ensures tasks are executed in the right sequence while respecting all dependencies. To resolve the above-mentioned case with graphlib, you’d do the following: from graphlib import TopologicalSorter # Define the graph graph = { \"A\": [], # A has no dependency \"B\": [\"A\"], # B depends on A \"C\": [\"A\"], # C depends on A \"D\": [\"B\", \"C\"], # D depends on B and C } # Create a TopologicalSorter instance sorter = TopologicalSorter(graph) # Get the tasks in the correct order sorter.prepare() # Resolve the tasks in batch mode while sorter.is_active(): batch = tuple(sorter.get_ready()) print(\"Executing:\", batch) sorter.done(*batch) Running this will print the following: Executing: ('A',) Executing: ('B', 'C') Executing: ('D',) Since Python’s stdlib already has graphlib, I thought I’d write a sloppy one in Go to learn the mechanics of how it works. Writing a topological sorter in Go The API will be similar to what we’ve seen in the graphlib example. Defining the graph structure First, we need a graph structure to hold the tasks and their dependencies. We’ll use an adjacency list to represent the graph, and a map to track the in-degree of each node (how many tasks it depends on). type Graph struct { vertices map[string][]string // Adjacency list for dependencies inDegree map[string]int // Tracks the number of incoming edges queue []string // Queue of nodes ready to process active int // Number of active tasks to process } Here: vertices: a list of tasks that each node points to (i.e., its dependents). inDegree: how many tasks must finish before each task can be processed. queue: tasks that can be processed because they have no unmet dependencies. active: how many tasks are currently ready for processing. Adding dependencies Next, we’ll define how one task depends on another. The AddEdge function sets up this relationship, ensuring the source task knows it must finish before the destination task can proceed. func (g *Graph) AddEdge(source, destination string) { g.vertices[source] = append(g.vertices[source], destination) g.inDegree[destination]++ // Increase destination's in-degree if _, exists := g.inDegree[source]; !exists { g.inDegree[source] = 0 // Ensure the source node is tracked } } The destination task is added to the list of tasks that the source task points to, marking the dependency. The in-degree of the destination task is increased by 1 because it depends on the source task. If the source task is new, we initialize its in-degree to 0. Initializing and processing tasks in batches Now we’ll initialize the graph by identifying tasks that can be processed immediately—those with an in-degree of 0 (i.e., they have no dependencies). We then process tasks batch by batch. func (g *Graph) Prepare() { // Start by adding tasks with in-degree 0 to the queue for task, degree := range g.inDegree { if degree == 0 { g.queue = append(g.queue, task) // Ready to process } } g.active = len(g.queue) // Count how many are active } This function finds tasks with an in-degree of 0 (no dependencies) and adds them to the processing queue. The active count keeps track of how many tasks are ready to run. Processing each batch of tasks We use GetReady to retrieve the next batch of tasks that are ready for processing. These are tasks with no unmet dependencies. func (g *Graph) GetReady() []string { batch := make([]string, len(g.queue)) // Create a batch from the queue copy(batch, g.queue) // Copy tasks to the batch g.queue = []string{} // Clear the queue after processing return batch // Return the ready batch } GetReady pulls the current batch of tasks from the queue and clears it for the next batch. Tasks are returned in the order they are ready to be processed. Marking the processed tasks as done Once a batch of tasks is completed, we mark them as done and reduce the in-degree of any tasks that depend on them. func (g *Graph) Done(tasks ...string) { for _, task := range tasks { // For each completed task for _, dependent := range g.vertices[task] { g.inDegree[dependent]-- // Decrement dependent's in-degree if g.inDegree[dependent] == 0 { // If ready, add to the queue g.queue = append(g.queue, dependent) } } } g.active = len(g.queue) // Update the active count } For each completed task, we reduce the in-degree of any dependent tasks. If a dependent task’s in-degree reaches 0, it’s added to the queue and is now ready to be processed in the next batch. Running the full topological sort Finally, we’ll implement the TopologicalSortBatch function, which processes all tasks in batches until none are left. func TopologicalSortBatch(graph *Graph) { graph.Prepare() // Prepare the graph by loading the initial batch for graph.IsActive() { // While tasks remain to be processed batch := graph.GetReady() // Get the next batch fmt.Println(\"Next batch:\", batch) // Process the batch graph.Done(batch...) // Mark the batch as done } } Prepare loads the first set of tasks that can be processed. IsActive checks if there are any tasks left to process. GetReady retrieves the next batch of tasks to process. Done marks tasks as finished, allowing dependent tasks to be processed next. Using the sorter You can use the API as follows: g := NewGraph() // Define task dependencies g.AddEdge(\"A\", \"B\") // B depends on A g.AddEdge(\"A\", \"C\") // C depends on A g.AddEdge(\"B\", \"D\") // D depends on B g.AddEdge(\"C\", \"D\") // D depends on C // Perform topological sort in batches TopologicalSortBatch(g) This will return: Next batch: [A] Next batch: [B C] Next batch: [D] Here, A needs to run first. B and C can run in parallel after A finishes, and only then can D run. Complete example Here’s the full implementation, heavily annotated for clarity: package main import \"fmt\" type Graph struct { vertices map[string][]string // Task dependencies inDegree map[string]int // Number of unmet dependencies queue []string // Ready tasks active int // Active task count } func NewGraph() *Graph { return \u0026Graph{ vertices: make(map[string][]string), inDegree: make(map[string]int), queue: []string{}, active: 0, } } func (g *Graph) AddEdge(source, destination string) { // Add the destination task to the source's dependency list g.vertices[source] = append(g.vertices[source], destination) // Increment the in-degree of the destination task g.inDegree[destination]++ // Ensure the source task is tracked with in-degree 0 if new if _, exists := g.inDegree[source]; !exists { g.inDegree[source] = 0 } } func (g *Graph) Prepare() { // Load tasks with no unmet dependencies (in-degree 0) for task, degree := range g.inDegree { if degree == 0 { g.queue = append(g.queue, task) } } g.active = len(g.queue) // Set active task count } func (g *Graph) IsActive() bool { return g.active \u003e 0 // Check if there are active tasks left } func (g *Graph) GetReady() []string { batch := make([]string, len(g.queue)) // Create batch of ready tasks copy(batch, g.queue) // Copy tasks to the batch g.queue = []string{} // Clear queue after processing return batch // Return ready tasks } func (g *Graph) Done(tasks ...string) { // For each completed task, decrement in-degree of its dependents for _, task := range tasks { for _, dependent := range g.vertices[task] { g.inDegree[dependent]-- // If dependent has no unmet dependencies, add to queue if g.inDegree[dependent] == 0 { g.queue = append(g.queue, dependent) } } } g.active = len(g.queue) // Update active task count } func TopologicalSortBatch(graph *Graph) { graph.Prepare() // Prepare initial batch of tasks for graph.IsActive() { // Process tasks while there are active ones batch := graph.GetReady() // Get the next batch fmt.Println(\"Next batch:\", batch) // Output batch graph.Done(batch...) // Mark tasks in the batch as done } } // Usage func main() { g := NewGraph() // Define task dependencies g.AddEdge(\"A\", \"B\") g.AddEdge(\"A\", \"C\") g.AddEdge(\"B\", \"D\") g.AddEdge(\"C\", \"D\") // Perform topological sort in batches TopologicalSortBatch(g) } This can be used to make custom task orchestrator. Here’s an example1. Python graphlib | My favourite Python library ↩︎ ","permalink":"http://rednafi.com/go/topological_sort/","publishDate":"2024-10-13","summary":"I was fiddling with graphlib in the Python stdlib and found it quite nifty. It processes a Directed Acyclic Graph (DAG), where tasks (nodes) are connected by directed edges (dependencies), and returns the correct execution order. The “acyclic” part ensures no circular dependencies.\nTopological sorting is useful for arranging tasks so that each one follows its dependencies. It’s widely used in scheduling, build systems, dependency resolution, and database migrations.\nFor example, consider these tasks:\n","tags":["Go"],"title":"Topological sort"},{"content":"Besides retries, circuit breakers1 are probably one of the most commonly employed resilience patterns in distributed systems. While writing a retry routine is pretty simple, implementing a circuit breaker needs a little bit of work. I realized that I usually just go for off-the-shelf libraries for circuit breaking and haven’t written one from scratch before. So, this is an attempt to create a sloppy one in Go. I picked Go instead of Python because I didn’t want to deal with sync-async idiosyncrasies or abstract things away under a soup of decorators. Circuit breakers A circuit breaker acts like an automatic switch that prevents your application from repeatedly trying to execute an operation that’s likely to fail. In a distributed system, you don’t want to bombard a remote service when it’s already failing, and circuit breakers prevent that. It has three states: Closed, Open, and Half-Open. Here’s a diagram that shows the state transitions: stateDiagram-v2 [*] --\u003e Closed: Start Closed --\u003e Open: Failure threshold reached Open --\u003e HalfOpen: Recovery period expired HalfOpen --\u003e Closed: Success threshold reached HalfOpen --\u003e Open: Request failed note right of Closed: All requests are allowed note right of Open: Requests are blocked note right of HalfOpen: Limited requests allowed to check recovery Closed: This is the healthy operating state where all requests are allowed to pass through to the service. If a certain number of consecutive requests fail (reaching a failure threshold), the circuit breaker switches to the Open state. Open: In this state, all requests are immediately blocked, and an error is returned to the caller without attempting to contact the failing service. This prevents overwhelming the service and gives it time to recover. After a predefined recovery period, the circuit breaker transitions to the Half-Open state. Half-Open: The circuit breaker allows a limited number of test requests to see if the service has recovered. If these requests succeed, it transitions back to the Closed state. If any of them fail, it goes back to the Open state. Building one in Go Here’s a simple circuit breaker in Go. Defining states First, we’ll define the constants for our states and create the circuitBreaker struct, which holds all the configurable knobs. // The three possible states of a circuit breaker const ( Closed = \"closed\" Open = \"open\" HalfOpen = \"half-open\" ) // circuitBreaker manages the state and behavior of the circuit breaker type circuitBreaker struct { mu sync.Mutex // Guards the circuit breaker state state string // Current state of the circuit breaker failureCount int // Number of consecutive failures lastFailureTime time.Time // Time of the last failure halfOpenSuccessCount int // Successful requests in half-open state failureThreshold int // Failures to trigger open state recoveryTime time.Duration // Wait time before half-open halfOpenMaxRequests int // Requests allowed in half-open state timeout time.Duration // Timeout for requests } This struct includes: mu: A mutex to ensure thread-safe access to the circuit breaker. state: The current state of the circuit breaker (Closed, Open, or HalfOpen). failureCount: The current count of consecutive failures. lastFailureTime: The timestamp of the last failure. halfOpenSuccessCount: The number of successful requests in the HalfOpen state. failureThreshold: The number of consecutive failures allowed before opening the circuit. recoveryTime: The cool-down period before the circuit breaker transitions from Open to HalfOpen. halfOpenMaxRequests: The maximum number of successful requests needed to close the circuit. timeout: The maximum duration to wait for a request to complete. Initializing the breaker Next, we provide a constructor function to initialize a new circuitBreaker instance. // NewCircuitBreaker initializes a new CircuitBreaker func NewCircuitBreaker( failureThreshold int, recoveryTime time.Duration, halfOpenMaxRequests int, timeout time.Duration, ) *circuitBreaker { return \u0026circuitBreaker{ state: Closed, failureThreshold: failureThreshold, recoveryTime: recoveryTime, halfOpenMaxRequests: halfOpenMaxRequests, timeout: timeout, } } This function sets the initial state to Closed and initializes the thresholds and timeout. Implementing the Call method The Call method is the primary interface for executing functions through the circuit breaker. It dispatches the appropriate state handler based on the current state. // Call attempts to execute the provided function, managing state transitions func (cb *circuitBreaker) Call(fn func() (any, error)) (any, error) { cb.mu.Lock() defer cb.mu.Unlock() slog.Info(\"Making a request\", \"state\", cb.state) switch cb.state { case Closed: return cb.handleClosedState(fn) case Open: return cb.handleOpenState() case HalfOpen: return cb.handleHalfOpenState(fn) default: return nil, errors.New(\"unknown circuit state\") } } We use a mutex to protect against concurrent access since the circuit breaker might be used by multiple goroutines. The Call method uses a switch statement to delegate the function call to the appropriate handler based on the current state. Handling closed states In the Closed state, all requests are allowed to pass through. We monitor the requests for failures to decide when to trip the circuit breaker. // handleClosedState executes the function and monitors failures func (cb *circuitBreaker) handleClosedState(fn func() (any, error)) (any, error) { result, err := cb.runWithTimeout(fn) if err != nil { slog.Warn( \"Request failed in closed state\", \"failureCount\", cb.failureCount+1, ) cb.failureCount++ cb.lastFailureTime = time.Now() if cb.failureCount \u003e= cb.failureThreshold { cb.state = Open slog.Error(\"Failure threshold reached, transitioning to open\") } return nil, err } slog.Info(\"Request succeeded in closed state\") cb.resetCircuit() return result, nil } In this function: We attempt to execute the provided function fn using runWithTimeout to handle possible timeouts. If the function call fails, we increment the failureCount and update lastFailureTime. If the failureCount reaches the failureThreshold, we transition the circuit to the Open state. If the function call succeeds, we reset the circuit breaker to the Closed state by calling resetCircuit. Resetting the breaker When a request succeeds, we reset the failure count and keep the circuit in the Closed state. // resetCircuit resets the circuit breaker to closed state func (cb *circuitBreaker) resetCircuit() { cb.failureCount = 0 cb.state = Closed slog.Info(\"Circuit reset to closed state\") } Handling open states In the Open state, all requests are blocked to prevent further strain on the failing service. We check if the recovery period has expired before transitioning to the HalfOpen state. // handleOpenState blocks requests if recovery time hasn't passed func (cb *circuitBreaker) handleOpenState() (any, error) { if time.Since(cb.lastFailureTime) \u003e cb.recoveryTime { cb.state = HalfOpen cb.halfOpenSuccessCount = 0 cb.failureCount = 0 slog.Info(\"Recovery period over, transitioning to half-open\") return nil, nil } slog.Warn(\"Circuit is still open, blocking request\") return nil, errors.New(\"circuit open, request blocked\") } Here: We check if the recovery period (recoveryTime) has passed since the last failure. If it has, we transition to the HalfOpen state and reset the counters. If not, we block the request and return an error immediately. Handling half-open states In the HalfOpen state, we allow a limited number of requests to test if the service has recovered. // handleHalfOpenState executes the function and checks for recovery func (cb *circuitBreaker) handleHalfOpenState( fn func() (any, error)) (any, error) { result, err := cb.runWithTimeout(fn) if err != nil { slog.Error(\"Failed in half-open state, transitioning to open\") cb.state = Open cb.lastFailureTime = time.Now() return nil, err } cb.halfOpenSuccessCount++ slog.Info(\"Succeeded in half-open\", \"successCount\", cb.halfOpenSuccessCount) if cb.halfOpenSuccessCount \u003e= cb.halfOpenMaxRequests { slog.Info(\"Max success, transitioning to closed\") cb.resetCircuit() } return result, nil } In this function: We attempt to execute the provided function fn. If the function call fails, we transition back to the Open state. If the function call succeeds, we increment halfOpenSuccessCount. Once the success count reaches halfOpenMaxRequests, we reset the circuit breaker to the Closed state. Running functions with timeout To prevent the circuit breaker from hanging on slow or unresponsive functions, we implement a timeout mechanism. You probably noticed that inside each state handler we called the wrapped functions with runWithTimeout. // runWithTimeout executes the provided function with a timeout func (cb *circuitBreaker) runWithTimeout(fn func() (any, error)) (any, error) { ctx, cancel := context.WithTimeout(context.Background(), cb.timeout) defer cancel() resultChan := make(chan struct { result any err error }, 1) go func() { result, err := fn() resultChan \u003c- struct { result any err error }{result, err} }() select { case \u003c-ctx.Done(): return nil, errors.New(\"request timed out\") case res := \u003c-resultChan: return res.result, res.err } } This function: Creates a context with a timeout using context.WithTimeout. Executes the provided function fn in a separate goroutine. Waits for either the result or the timeout. Returns an error if the function takes longer than the specified timeout. Taking it for a spin Let’s test our circuit breaker with an unreliable service that sometimes fails. func unreliableService() (any, error) { if time.Now().Unix()%2 == 0 { return nil, errors.New(\"service failed\") } return \"Success!\", nil } In the main function, we’ll create a circuit breaker and make several calls to the unreliable service. func main() { cb := cb.NewCircuitBreaker( 2, // Failure threshold 2*time.Second, // Recovery time 2, // Half-open max requests 2*time.Second, // Half-open max time ) for i := 0; i \u003c 5; i++ { result, err := cb.Call(unreliableService) if err != nil { slog.Error(\"Service request failed\", \"error\", err) } else { slog.Info(\"Service request succeeded\", \"result\", result) } time.Sleep(1 * time.Second) log.Println(\"-------------------------------------------\") } } This loop simulates multiple service calls, using the circuit breaker to handle failures and transitions between states. This prints: 2024/10/06 17:24:27 INFO Making a request state=closed 2024/10/06 17:24:27 INFO Request succeeded in closed state 2024/10/06 17:24:27 INFO Circuit reset to closed state 2024/10/06 17:24:27 INFO Service request succeeded result=42 2024/10/06 17:24:28 ----------------------------------------------- 2024/10/06 17:24:28 INFO Making a request state=closed 2024/10/06 17:24:28 WARN Request failed in closed state failureCount=1 2024/10/06 17:24:28 ERROR Service request failed error=\"service failed\" 2024/10/06 17:24:29 ----------------------------------------------- 2024/10/06 17:24:29 INFO Making a request state=closed 2024/10/06 17:24:29 INFO Request succeeded in closed state 2024/10/06 17:24:29 INFO Circuit reset to closed state 2024/10/06 17:24:29 INFO Service request succeeded result=42 2024/10/06 17:24:30 ----------------------------------------------- 2024/10/06 17:24:30 INFO Making a request state=closed 2024/10/06 17:24:30 WARN Request failed in closed state failureCount=1 2024/10/06 17:24:30 ERROR Service request failed error=\"service failed\" 2024/10/06 17:24:31 ----------------------------------------------- 2024/10/06 17:24:31 INFO Making a request state=closed 2024/10/06 17:24:31 INFO Request succeeded in closed state 2024/10/06 17:24:31 INFO Circuit reset to closed state 2024/10/06 17:24:31 INFO Service request succeeded result=42 2024/10/06 17:24:32 ----------------------------------------------- The log messages will give you a sense of what’s happening when we retry an intermittently failing function wrapped in a circuit breaker. The API could be better One limitation of Go generics is that you can’t use type parameters with methods that have a receiver. This means you can’t define a method like func (cb *CircuitBreaker[T]) Call(fn func() (T, error)) (T, error). For this, we have to use workarounds such as using any (an alias for interface{}) as the return type in our function signatures. While this sacrifices some type safety, it allows us to create a flexible circuit breaker that can handle functions returning different types. Handling incompatible function signatures What if the function you want to wrap doesn’t match the func() (any, error) signature? You can easily adapt it by wrapping your function to fit the required signature. Suppose you have a function like this: func fetchData(id int) (Data, error) { // ... implementation ... } You can wrap it like this: wrappedFunc := func() (any, error) { return fetchData(42) // Replace 42 with your desired argument } Now, wrappedFunc matches the func() (any, error) signature and can be used with our circuit breaker. Here’s the complete implementation2 with tests. Circuit breaker — Martin Fowler ↩︎ Circuit breaker implementation in Go ↩︎ ","permalink":"http://rednafi.com/go/circuit_breaker/","publishDate":"2024-10-06","summary":"Besides retries, circuit breakers1 are probably one of the most commonly employed resilience patterns in distributed systems. While writing a retry routine is pretty simple, implementing a circuit breaker needs a little bit of work.\nI realized that I usually just go for off-the-shelf libraries for circuit breaking and haven’t written one from scratch before. So, this is an attempt to create a sloppy one in Go. I picked Go instead of Python because I didn’t want to deal with sync-async idiosyncrasies or abstract things away under a soup of decorators.\n","tags":["Networking","Go"],"title":"Writing a circuit breaker in Go"},{"content":"I’m not really a fan of shims—code that automatically performs actions as a side effect or intercepts commands when you use the shell or when a prompt runs. That’s why, other than the occasional dabbling, I’ve mostly stayed away from tools like asdf or pyenv and instead stick to apt or brew for managing my binary installs, depending on the OS. Recently, though, I’ve started seeing many people I admire extolling direnv: If you’re old-school like me, my .envrc looks like this: uv sync --frozen source .venv/bin/activate The sync ensures there’s always a .venv, so no memory-baking required. — Hynek Schlawack1 Or, This is embarrassing, but after using direnv for 10+ years, I only discovered the source_env directive yesterday. Game changer. I used it to improve our project’s dev configuration ergonomics so new environment variables are easily distributed via Git. —Brandur2 So I got curious and wanted to try the tool to see if it fits into my workflow, or if I’ll quickly abandon it when something goes wrong. When I first visited their landing page3, I was a bit confused by the tagline: direnv – unclutter your .profile But I don’t have anything custom in my .profile, or more specifically, my .zprofile. Here’s what’s in it currently: cat ~/.zprofile eval \"$(/opt/homebrew/bin/brew shellenv)\" # Added by OrbStack: command-line tools and integration source ~/.orbstack/shell/init.zsh 2\u003e/dev/null || : Then I realized that .profile is used here as a general term for various configuration files like .*profile, .*rc, and .*env. I have quite a bit set up in both my ~/.zshrc and ~/.zshenv—a mix of global and project-specific commands and environment variables. To explain: .*profile files (like .profile or .bash_profile) are used by login shells, which are started when you log into a system, such as through SSH or a terminal login. In contrast, files like .bashrc or .zshrc are for interactive shells, meaning they run when you open a new terminal window or tab. For Zsh, .zshenv is sourced by all types of shells—both login and interactive—making it useful for global environment settings. What problem it solves Direnv solves the hassle of managing environment variables across different projects by automatically loading them when you enter a directory and unloading them when you leave. It keeps your global environment clean and avoids cluttering up your shell configuration files. It checks for an .envrc (or .env) file in the current or parent directories before each prompt. If found and authorized, it loads the file into a bash sub-shell and applies the environment variables to the current shell. It supports hooks for common shells like Bash, Zsh, Tcsh, and Fish, allowing you to manage project-specific environment variables without cluttering your ~/.profile. Since it’s a fast, single static executable, direnv runs seamlessly and is language-agnostic, meaning you can easily use it alongside tools like rbenv, pyenv, and phpenv. You might argue that source .env works just fine, but it’s an extra step to remember. Also, being able to communicate the project-specific environment commands and variables, and having them sourced automatically, is a nice bonus. Why .envrc file and not just a plain .env file This was the first question that came to my mind: why not just use a .env file? Why introduce another configuration file? Grokking the docs clarified things. The .envrc file is treated like a shell script, where you can also list arbitrary shell commands that you want to be executed when you enter a project directory. You can’t do that with a plain .env file. However, direnv does support .env files too. It’s such a simple idea that opens up many possibilities. How I use it Here are a few things I’m using it for: Automatically loading environment variables from a .env file. Loading different sets of values for the same environment keys, e.g., local vs. staging values. Activating the virtual environment when I enter the directory of a Python project. Let’s say you want to load your environment variables automatically when you cd into a directory and have them removed from the shell environment when you leave it. Suppose the project directory looks like this: svc/ ├── .env ├── .env.staging └── .envrc The .env file contains environment variables for local development: FOO=\"foo\" BAR=\"bar\" And the .env.staging file contains the variables for staging: FOO=\"foo-staging\" BAR=\"bar-staging\" The .envrc file can have just one command to load the default .env file: dotenv Now, from the svc directory, you’ll need to allow direnv to load the environment variables into the current shell: direnv allow This prints: direnv: loading ~/canvas/rednafi.com/svc/.envrc direnv: export +BAR +FOO You can now print the values of the environment variables like this: echo \"${FOO-default}\"; echo \"${BAR-default}\" This returns: foo bar If you want to load different variables depending on the environment, you can add the following shell script to the .envrc file: case \"${ENVIRONMENT}\" in \"staging\") if [[ -f \".env.staging\" ]]; then dotenv .env.staging fi ;; *) if [[ -f \".env\" ]]; then dotenv fi ;; esac The script loads the .env.staging file if the value of $ENVIRONMENT is staging; otherwise, it loads the default .env file. From the svc root, run: direnv allow This will still load the variables from .env. To load variables from .env.staging, run: export ENVIRONMENT=staging \u0026\u0026 direnv allow This time, printing the variables returns the staging values: foo-staging bar-staging Oh, and when you leave the directory, the environment variables will be automatically unloaded from your working shell. cd .. direnv: unloading You can do a lot more with the idea, but going overboard with environment variables can be risky. You don’t want to accidentally load something into the environment you didn’t intend to. Keeping it simple with sane defaults is the way to go. Like Hynek, I’ve adopted uv4 in my Python workflow, and now my default .envrc has these two commands: uv sync --frozen source .venv/bin/activate The first command updates the project’s environment without changing the uv.lock file, and the second ensures I never need to remember to activate the virtual environment before running commands. Now, when I cd into a Python project and run: echo $VIRTUAL_ENV It shows that the local .venv is active: /Users/rednafi/canvas/rednafi.com/.venv No more worrying about mucking up my global Python installation while running some commands. Another neat directive is source_up, which lets you inherit environment variables from the parent directory. Normally, when you move into a child directory, direnv unloads the parent directory’s environment variables. But with the source_up directive in your .envrc, it’ll keep those variables around in the child directory. Then there’s the source_env directive, which lets you pull one .envrc file into another. So, if you’ve got some common, non-secret variables in an .envrc.local file, you can easily reuse them in your .envrc. Here’s an example .envrc.local file: export API_URL=\"http://localhost:5222\" export DATABASE_URL=\"postgres://localhost:5432/project-db\" You can import .env.local into the .envrc file like this: source_env .envrc.local # Other commands and variables go here I haven’t used source_env much yet, but I love the possibilities it unlocks. The biggest reason I’ve adopted it everywhere is that it lets me share my shell environment variables and the magic commands without having anything stashed away in my ~/.zshrc or ~/.zshenv, so there’s no need for out-of-band communication. Hynek on Twitter ↩︎ Brandur on Twitter ↩︎ direnv ↩︎ uv ↩︎ ","permalink":"http://rednafi.com/misc/direnv/","publishDate":"2024-10-02","summary":"I’m not really a fan of shims—code that automatically performs actions as a side effect or intercepts commands when you use the shell or when a prompt runs. That’s why, other than the occasional dabbling, I’ve mostly stayed away from tools like asdf or pyenv and instead stick to apt or brew for managing my binary installs, depending on the OS.\nRecently, though, I’ve started seeing many people I admire extolling direnv:\n","tags":["TIL","Shell"],"title":"Discovering direnv"},{"content":"I spent the evening watching this incredibly grokkable talk on event-driven services by James Eastham at NDC London 2024. Below is a cleaned-up version of my notes.\nI highly recommend watching the full talk if you’re interested before reading this distillation.\nThe curse of tightly coupled microservices Microservices often start with HTTP-based request-response communication, which seems straightforward but quickly becomes a pain as systems grow. Coupling—where one service depends on another—creates a few issues. Take the order processing service in a fictional Plant-Based Pizza company. It has to talk to the pickup service, delivery service, kitchen, and loyalty point service. They’re all tied together, so if one fails, the whole system could go down.\nThe system relies on all services being up at the same time, which causes issues when any service crashes. Even something like loyalty points can take the whole thing offline, making you wonder if the order processing service really needs to care about that.\nOn top of that, there’s semantic coupling—things like data formats. “How do you handle null values in strings? What casing is your JSON using—camelCase?” These details might seem minor, but in tightly coupled systems, they pile up, making the integrations fragile and complicated.\nWhat event-driven architecture solves Event-driven architecture offers a way to decouple services. Instead of one service needing to communicate directly with another, services react to events, giving more flexibility and scalability. “Event-driven architecture is about reversing dependencies—reversing the lines of integration.” Now, the order processing service doesn’t need to know where the downstream services are. It simply publishes an event, and the downstream services react to it.\nThis shift is powerful because it frees services from having to know about each other. In this model, the kitchen doesn’t wait for the order processing service to send a direct HTTP request. Instead, it listens for an event that triggers its response without direct integration. “You’ve removed that runtime coupling because producers and consumers no longer need to know each other exist.” By decoupling systems, event-driven architecture improves fault tolerance, scalability, and flexibility.\nThe nature of an event At the core of event-driven systems is the concept of an event. An event is “an immutable fact. It’s something that’s happened in the past. It cannot be changed.” When an event is published, it’s a record of something that has already occurred, like “order confirmed” or “pizza boxed.” Events are simple, factual, and unchangeable.\nThe analogy of a light switch brings this to life: “You hit the light switch, and that raises a light switched-on event. You can’t un-switch on a light.” To turn the light off, you generate a new event—“light switched-off\"—but you don’t undo the original. This principle of immutability ensures that events in the system are reliable and unambiguous, forming the foundation for how systems react.\nEvent-driven vs. event-based systems It’s easy to confuse event-driven systems with event-based systems, but the distinction is crucial. Event-driven systems are driven by business-specific events—things that reflect real-world actions and decisions, not just technical events like a button click. “An event-driven system uses events like these: ‘order confirmed,’ ‘pizza boxed,’ ‘staff member clocked in.’” These are business-level events that reflect the narrative of the company, not just low-level system changes.\nIn contrast, event-based systems simply react to any change, such as a file being added to S3 or a button being clicked in a UI. “We’ve been building event-based systems for years… that doesn’t make it event-driven.” The difference is significant because event-driven systems align technical architecture with business needs, creating a more meaningful, coherent system where the events reflect the organization’s core processes.\nFat vs. sparse events One of the critical design decisions in event-driven systems is choosing between fat events (which carry a lot of data) and sparse events (which carry minimal data). Fat events, also known as Event-Carried State Transfer (ECST), include all the information a consumer might need. For instance, “the kitchen can consume this event—it’s got the list of items on the order, so now it knows what it needs to cook.” This reduces the need for callbacks or additional requests for data back to the original system that publishes the event, making the system more robust in terms of runtime interaction.\nHowever, fat events come with risks. “The downside of that is that you get more coupling at the schema level.” Because fat events contain so much information, it becomes harder to change the event format without impacting multiple consumers. As more services depend on that data, the risk of breaking something grows.\nIn contrast, sparse events are lightweight but require callbacks to get additional information. Initially, this might seem more efficient, but as more services join the system, the number of callbacks increases exponentially. “Now you’ve got this potentially infinite number of downstream services that are all making calls back to get more information.” The result is a more tightly coupled system, albeit in a different form.\nSo which one of these is the right one to choose? The answer is: it depends. Sparse events reduce the need for frequent changes, but fat events reduce the need for constant back-and-forth communication. Often, a combination of both is necessary, depending on the use case.\nPublish-subscribe pattern and the role of the broker The core of event-driven architecture lies in the publish-subscribe pattern, facilitated by an event broker. “At its core, an event-driven architecture is made up of three parts: You have a producer, a consumer, and some kind of event broker in the middle.” The producer generates the event, the broker routes it, and the consumer processes it. The beauty of this system is that producers and consumers don’t need to know about each other’s existence.\n“The first thing you’ll notice is that the producer and the consumer here have no idea each other exists—the communication is managed by the broker.” This decoupling makes the system more flexible and scalable. A consumer can be added or removed without impacting the producer. The broker ensures that events are delivered, allowing the system to continue functioning smoothly even as it evolves.\nHowever, one responsibility remains: “The schema of your event—the format of that event—is the biggest part of the coupling that you will see in event-driven architecture.” While runtime coupling is removed, semantic coupling still exists. Producers must ensure that the event schema doesn’t change in ways that break existing consumers.\nHandling constraints and governance In event-driven systems, the responsibility for handling constraints shifts from the producer to the consumer. Producers generate events as quickly as they can, without worrying about the load on consumers. “As a producer, it’s not your responsibility to care about how your events are used… that’s the subscriber’s responsibility.” Consumers must handle their own ingestion rates and ensure they don’t get overloaded.\nGovernance plays a critical role in managing these systems, particularly as they evolve. When changes are made to event schemas, it’s essential to communicate those changes to all consumers. “Governance is really important with event-driven architecture because you’ve got these systems that just don’t care about each other.” One effective method for managing this is through Request for Comments (RFCs), which allow for collaborative discussion before any changes are implemented.\n“Rather than just publishing an event and hoping for the best, introducing governance ensures that events remain consistent and understandable across teams.” This helps prevent breaking changes that could take down systems you didn’t even know were relying on your events.\nMetadata-data pattern for evolvability To enhance the evolvability of an event-driven system, East recommends using the metadata-data pattern. This pattern separates the event’s core data from its metadata, allowing for greater flexibility. “Splitting your event down into a metadata section and the data section helps you to stay evolvable.” The data contains the specifics of the event, while the metadata includes information like “event type,” “event ID,” and “version.”\nThis separation allows consumers to understand and process events more easily while providing room for schema changes. For example, “event versioning allows you to introduce breaking changes in a controlled manner.” By publishing multiple versions of an event, you can ensure backward compatibility while encouraging consumers to upgrade to the latest schema.\nEventual consistency in event-driven systems One of the trade-offs in event-driven architecture is that systems must embrace eventual consistency. In a request-response system, actions happen immediately and are reflected in real-time. But in an event-driven system, updates propagate over time. “Eventually, over time, these systems will converge on the same view of the world.” This is a shift in mindset for many developers used to strong consistency.\nTo illustrate this, consider a card payment: “When you make a card transaction, all you’re doing is making a theoretical guarantee that, at some point in the future, that money is going to move from your bank account to theirs.” While the system is eventually consistent, the end result will be correct, just not immediately. Event-driven architecture functions similarly—updates happen asynchronously, and systems eventually reach a consistent state.\nHandling HTTP communication in an event-driven world Not every system can fully adopt event-driven architecture, and many still rely on HTTP-based communication. To integrate these systems into an event-driven world, you need a middle layer. For example, if your loyalty point service is being replaced by a third-party SaaS product that only supports an HTTP API, you’d still have a service managing that integration. This service listens for events and translates them into HTTP requests for systems that aren’t event-driven.\nTo handle differences in response times and reliability between HTTP-based and event-driven systems, introducing a queue or intermediary storage is crucial. “Introducing this queue means you can keep this amount of durability… you can process to the third-party API as and when you need to.” This queue adds resilience, allowing your system to continue functioning smoothly, even when interacting with external services that don’t follow event-driven principles.\nAsynchronous commands Commands in an event-driven system don’t always need to be synchronous. Instead of waiting for an immediate response, systems can issue commands asynchronously, allowing for greater flexibility and non-blocking workflows. “You want to send an email, but you might not necessarily want it to be completely request-response.”\nAn asynchronous command might still send a request to a service, but the response isn’t required to continue processing. This allows systems like the email notification service to handle requests at its own pace, rather than blocking the core order processing service. “Your email service can still expose an endpoint, but as opposed to that being an HTTP-based endpoint, that could just be a message channel.” This approach decouples the services further and ensures more efficient use of resources.\nCQRS for separating reads and writes Command Query Responsibility Segregation (CQRS) is a powerful pattern that pairs well with event-driven architecture. CQRS separates the system into two parts: one for handling commands (writes) and another for handling queries (reads). “In CQRS, you split your system into two completely independent services—one for processing commands, one for handling queries.” This allows each part of the system to be optimized for its specific workload.\nFor example, the command service focuses on writing data to the database and publishing events, while the query service listens for those events and updates a read-optimized view of the world. This separation enables more efficient scaling, as the query service can be tuned for fast reads, potentially storing data in caches like Redis or even keeping it in memory.\nI’m a bit skeptical about CQRS since I’ve worked on a system with a terrible implementation that went horribly wrong. But I intend to keep an open mind.\nHandling failure with the outbox pattern In event-driven systems, failure is inevitable, so you need strategies to handle situations where events fail to publish. The outbox pattern is one such approach. “At the point you write the data to the main database… you also write the data to a secondary table.” This outbox table ensures that if the event fails to publish initially, it can be retried later.\nThis creates consistency across the system by acting as a buffer between the database and the event bus. Alternatively, systems can use change data capture to respond directly to changes in the database. “As a record is written to the database, you can stream that—you can react to that—and you can publish events off the back of that.” Both methods ensure reliability, preventing events from being lost due to temporary failures.\nThe outbox pattern sounds great in theory, but in practice, if you have a large system with many services publishing to the broker, managing an extra process for each service to read from the outbox table and publish to the event bus becomes a hassle. Instead, on the publisher side, retrying with a circuit breaker has worked better for me. Also, fun fact: I was asked about the outbox pattern in 4 of the last 5 places I interviewed for a backend role.\n","permalink":"http://rednafi.com/misc/notes_on_event_driven_systems/","publishDate":"2024-09-21","summary":"I spent the evening watching this incredibly grokkable talk on event-driven services by James Eastham at NDC London 2024. Below is a cleaned-up version of my notes.\nI highly recommend watching the full talk if you’re interested before reading this distillation.\nThe curse of tightly coupled microservices Microservices often start with HTTP-based request-response communication, which seems straightforward but quickly becomes a pain as systems grow. Coupling—where one service depends on another—creates a few issues. Take the order processing service in a fictional Plant-Based Pizza company. It has to talk to the pickup service, delivery service, kitchen, and loyalty point service. They’re all tied together, so if one fails, the whole system could go down.\n","tags":["Networking"],"title":"Notes on building event-driven systems"},{"content":"While going through a script at work today, I came across Bash’s nameref feature. It uses declare -n ref=\"$1\" to set up a variable that allows you to reference another variable by name—kind of like pass-by-reference in C. I’m pretty sure I’ve seen it before, but I probably just skimmed over it. As I dug into the man page1, I realized there’s a gap in my understanding of how variable references actually work in Bash—probably because I never gave it proper attention and just got by cobbling together scripts. Namerefs By default, Bash variables are global unless declared as local within a function. However, when you pass variables as arguments to a function, they are accessed via positional parameters like $1, $2, etc., and any changes to these parameters inside the function do not affect the original variables outside the function. Namerefs allow you to essentially define a pointer to another variable. By creating a nameref, you can indirectly reference and manipulate the target variable without knowing its name beforehand. This is incredibly useful for writing generic functions that can operate on different variables based on input parameters. Basic usage Here’s an example: #!/usr/bin/env bash # Declare a variable original_var=\"Hello, World!\" # Function that creates a nameref to a variable create_ref() { local ref_name=$1 declare -n ref=\"$ref_name\" ref=\"Hello from nameref!\" } # Call the function with the name of the variable create_ref original_var # Print the updated variable echo \"$original_var\" Running this will print: Hello from nameref! By running the create_ref function, we can dynamically update the value of $original_var, which exists outside of it. Notice that the function doesn’t even need to know about $original_var; it works on any variable name provided, making it generic. In this script: We declare a variable original_var with the value \"Hello, World!\". The create_ref function takes the name of a variable as an argument. Inside the function, declare -n ref=\"$ref_name\" creates a nameref ref that points to the variable named by $ref_name. By setting ref=\"Hello from nameref!\", we indirectly update original_var. Finally, we print original_var to see the updated value. Without the nameref, you could achieve the same thing with this eval (read: evil) trick: #!/usr/bin/env bash # Declare a variable original_var=\"Hello, World!\" # Function that updates a variable dynamically using eval create_ref() { local var_name=$1 local new_value=\"Hello from eval!\" eval \"$var_name=\\\"$new_value\\\"\" # eval 😈 } # Call the function with the name of the variable create_ref original_var # Print the updated variable echo \"$original_var\" This achieves the same result. The eval \"$var_name=\\\"$new_value\\\"\" dynamically updates the $original_var variable through $var_name. However, eval can be risky for security, and the nameref approach looks much cleaner syntactically. Managing multiple arrays Namerefs shine when you need to manage multiple arrays dynamically. Consider a scenario where you have several datasets stored in different arrays, and you want to process them using a single function. #!/usr/bin/env bash # Declare multiple arrays declare -a dataset1=(1 2 3 4 5) declare -a dataset2=(10 20 30 40 50) declare -a dataset3=(100 200 300 400 500) # Function to calculate the sum of an array sum_array() { local array_name=$1 declare -n arr=\"$array_name\" local sum=0 for num in \"${arr[@]}\"; do sum=$((sum + num)) done echo \"Sum of $array_name: $sum\" } # Process each dataset sum_array dataset1 sum_array dataset2 sum_array dataset3 This returns: Sum of dataset1: 15 Sum of dataset2: 150 Sum of dataset3: 1500 Here: We declare three arrays: dataset1, dataset2, and dataset3. The sum_array function takes the name of an array as an argument. Using declare -n arr=\"$array_name\", we create a nameref arr that points to the specified array. We then iterate over the elements of arr to calculate the sum. Finally, we call sum_array for each dataset, and the function correctly processes each array based on the reference. Without the nameref, you could again use the eval trick to achieve the same thing, but this time it looks even uglier: #!/usr/bin/env bash # Declare multiple arrays declare -a dataset1=(1 2 3 4 5) declare -a dataset2=(10 20 30 40 50) declare -a dataset3=(100 200 300 400 500) # Function to calculate the sum of an array without namerefs sum_array() { local array_name=$1 local sum=0 local index=0 local array_length eval \"array_length=\\${#$array_name[@]}\" for (( index=0; index","permalink":"http://rednafi.com/misc/bash_namerefs/","publishDate":"2024-09-20","summary":"While going through a script at work today, I came across Bash’s nameref feature. It uses declare -n ref=\"$1\" to set up a variable that allows you to reference another variable by name—kind of like pass-by-reference in C. I’m pretty sure I’ve seen it before, but I probably just skimmed over it.\nAs I dug into the man page1, I realized there’s a gap in my understanding of how variable references actually work in Bash—probably because I never gave it proper attention and just got by cobbling together scripts.\n","tags":["Shell","TIL"],"title":"Bash namerefs for dynamic variable referencing"},{"content":"When I started writing here about five years ago, I made a promise to myself that I wouldn’t give in to the trend of starting a blog, adding one overly enthusiastic entry about the stack behind it, and then vanishing into the ether. I was somewhat successful at that and wanted to write something I can link to when people are curious about the machinery that drives this site. The good thing is that the tech stack is simple and has remained stable over the years since I’ve only made changes when absolutely necessary. Markdown I write plain Markdown files in my editor of choice, which has been VSCode since its launch. Once I’m finished, pre-commit1 runs a fleet of linters like Prettier2 and Blacken-docs3 to fix line length and code formatting. Hugo Hugo4 is the static site generator that turns the Markdown files into HTML. I chose it because I needed something that can build the site quickly, even with lots of content. It lets me hot reload the server and check my changes as I write. Plus, I don’t get to write Go at work, so messing with Hugo templates or its source code gives me a reason to play around with Go. I initially tried some JS-based SSGs but dropped them pretty quickly because I couldn’t keep up with the constant tooling changes in the JavaScript universe. I use the Papermod5 theme and have tweaked the CSS over time. Papermod handles the SEO stuff, which I like to pretend I don’t care about. GitHub Issues I use GitHub Issues6 to brainstorm ideas and keep track of my writing. I usually gather ideas throughout the week, log them in Issues, and then write something over the weekend. This workflow is heavily inspired by Simon Willison’s piece on his work process7. GitHub Actions and GitHub Pages Once I push content to the main branch, GitHub Actions8 automatically runs, checks the linter, builds the site, and deploys it to GitHub Pages9. There’s nothing to maintain, and I don’t have to worry about scaling, even if one of my posts hits the front page of Hacker News. Aside from the domain, this site costs me nothing to run, and I plan to keep it that way. Cloudflare Cache and R2 I’m a huge fan of Cloudflare and often try to shoehorn their offerings into my projects. Since my domain is registered with them, setting up their proxy with my domain’s DNS and turning on caching took just a few minutes. Their caching layer absorbs most of the traffic, and less than 10% of the requests hit the origin server. Plus, having the proxy layer gives me access to more accurate analytics. Static assets like images, CSS, JS, and other files are stored on Cloudflare R210. I used to host my images with GitHub Issues and serve CSS and JS from the origin, but I recently switched everything to R2. Now I can manage it all from one place without worrying about costs. Their free plan is super generous—there’s no egress bandwidth fee, and because of caching, I barely use any of the quota. It’s fantastic! Oxipng Oxipng11 is used to compress images before uploading them to the Cloudflare R2 bucket with the Wrangler12 CLI. The Makefile in the repo has a single command called upload-static that handles everything in one go. upload-static: oxipng -o 6 -r static/images/ find static -type f | while read filepath; do \\ key=$$(echo \"$$filepath\" | sed 's|^|blog/|'); \\ wrangler r2 object put $$key --file \"$$filepath\"; \\ done I just drop the screenshots and images into /static/images//*.png, update the references in the Markdown file, and run make upload-static before pushing the changes to the repo. Google Analytics I’m still using Google Analytics13, even though I’m not a huge fan. Cloudflare already gives me better traffic insights, but the free version doesn’t show how many hits each page gets. At some point, I might just pay for Cloudflare’s upgraded plan so I can get rid of the bulky, intrusive analytics scripts for good. The source code and content for this site are all publicly available14 on GitHub. Pre-commit ↩︎ Prettier ↩︎ Blacken-docs ↩︎ Hugo ↩︎ Hugo Papermod ↩︎ I usually use GitHub Issues like this ↩︎ Coping strategies for the serial project hoarder – Simon Willison’s ↩︎ GitHub Actions ↩︎ GitHub Pages ↩︎ Cloudflare R2 ↩︎ Oxipng ↩︎ Cloudflare Wrangler ↩︎ Google Analytics ↩︎ GitHub - rednafi.com ↩︎ ","permalink":"http://rednafi.com/misc/behind_the_blog/","publishDate":"2024-09-14","summary":"When I started writing here about five years ago, I made a promise to myself that I wouldn’t give in to the trend of starting a blog, adding one overly enthusiastic entry about the stack behind it, and then vanishing into the ether.\nI was somewhat successful at that and wanted to write something I can link to when people are curious about the machinery that drives this site. The good thing is that the tech stack is simple and has remained stable over the years since I’ve only made changes when absolutely necessary.\n","tags":["Essay"],"title":"Behind the blog"},{"content":"I always struggle with the syntax for redirecting multiple streams to another command or a file. LLMs do help, but beyond the most obvious cases, it takes a few prompts to get the syntax right. When I know exactly what I’m after, scanning a quick post is much faster than wrestling with a non-deterministic kraken. So, here’s a list of the redirection and piping syntax I use the most, with real examples. Redirecting stdout and stderr Redirect stdout to a file Standard way: command \u003e file This replaces the content of file with the stdout of command. For example: echo \"Hello, world!\" \u003e hello.txt Print and redirect to file: command | tee file Example: echo \"Hello, world!\" | tee hello.txt This prints “Hello, world!” to the terminal and also writes it to hello.txt. Redirect stderr to a file Standard way: command 2\u003e file Sends all errors (stderr) to file. For example: ls non_existing_file 2\u003e error.log Print and redirect stderr to file: command 2\u003e \u003e(tee file) Example: ls non_existing_file 2\u003e \u003e(tee error.log) Redirect both stdout and stderr to a file Common approach: command \u003e file 2\u003e\u00261 Combines stdout and stderr into one stream and saves them to file. For example: ls non_existing_file existing_file \u003e output.log 2\u003e\u00261 Print and redirect both to file: command 2\u003e\u00261 | tee file Example: ls non_existing_file existing_file 2\u003e\u00261 | tee output.log Convenient shorthand: command \u0026\u003e file Example: ls non_existing_file existing_file \u0026\u003e output.log Append instead of overwriting Append stdout to a file: command \u003e\u003e file Example: echo \"Appending line\" \u003e\u003e hello.txt Print and append stdout to file: command | tee -a file Example: echo \"Appending line\" | tee -a hello.txt Append both stdout and stderr (explicit): command \u003e\u003e file 2\u003e\u00261 Example: ls non_existing_file existing_file \u003e\u003e output.log 2\u003e\u00261 Print and append both stdout and stderr to file: command 2\u003e\u00261 | tee -a file Example: ls non_existing_file existing_file 2\u003e\u00261 | tee -a output.log Convenient shorthand for appending both: command \u0026\u003e\u003e file Example: ls non_existing_file existing_file \u0026\u003e\u003e output.log Piping output Pipe stdout to another command Basic usage: command1 | command2 This sends the stdout of command1 to the input of command2. For example: echo \"Hello, world!\" | grep \"Hello\" Print and redirect piped stdout to file: command1 | tee file | command2 Example: echo \"Hello, world!\" | tee output.txt | grep \"Hello\" Pipe both stdout and stderr Common way: command1 2\u003e\u00261 | command2 Combines stdout and stderr, then pipes the combined stream to command2. For example: ls non_existing_file existing_file 2\u003e\u00261 | grep \"No\" Print and redirect both stdout and stderr to file: command1 2\u003e\u00261 | tee file | command2 Example: ls non_existing_file existing_file 2\u003e\u00261 | tee output.txt | grep \"No\" Shorthand for piping both stdout and stderr (|\u0026) Shorthand syntax: command1 |\u0026 command2 This is equivalent to command1 2\u003e\u00261 | command2, combining stdout and stderr. For example: ls non_existing_file existing_file |\u0026 grep \"No\" Print and redirect both stdout and stderr using |\u0026: command1 |\u0026 tee file | command2 Example: ls non_existing_file existing_file |\u0026 tee output.txt | grep \"No\" Redirecting file descriptors Custom file descriptors Create a new file descriptor (e.g., 3) and redirect stdout to it: exec 3\u003e outputfile command \u003e\u00263 This sends the stdout of command to file descriptor 3, which points to outputfile. For example: exec 3\u003e custom_output.txt echo \"Using FD 3\" \u003e\u00263 Print and redirect stdout to custom file descriptor: exec 3\u003e custom_output.txt echo \"Using FD 3\" | tee /dev/tty \u003e /dev/fd/3 This prints “Using FD 3” to the terminal and simultaneously writes it to custom_output.txt. Redirect stderr to a file descriptor Common case: command 2\u003e\u00263 Redirects stderr to file descriptor 3. For example: exec 3\u003e error_output.txt ls non_existing_file 2\u003e\u00263 Print and redirect stderr to custom file descriptor: command 2\u003e \u003e(tee \u003e(cat \u003e /dev/fd/3)) Example: ls non_existing_file 2\u003e \u003e(tee \u003e(cat \u003e /dev/fd/3)) Redirect both stdout and stderr to a file descriptor Common way: command \u003e /dev/fd/3 2\u003e\u00261 Combines stdout and stderr, and redirects them to file descriptor 3. Note: There’s no shorthand equivalent for redirecting both stdout and stderr to a file descriptor. You need to use the full syntax. For example: exec 3\u003e combined_output.txt ls non_existing_file existing_file \u003e /dev/fd/3 2\u003e\u00261 Discarding output Send stdout and stderr to /dev/null Common: command \u003e /dev/null 2\u003e\u00261 Silences all output (stdout and stderr). For example: ls non_existing_file \u003e /dev/null 2\u003e\u00261 Print and discard stdout and stderr (not sure why you’d ever need this): command | tee /dev/null Example: ls non_existing_file | tee /dev/null Convenient shorthand: command \u0026\u003e/dev/null Example: ls non_existing_file \u0026\u003e/dev/null At a glance Redirect stdout: command \u003e file Redirect stderr: command 2\u003e file Redirect both stdout and stderr: Standard: command \u003e file 2\u003e\u00261 Shorthand: command \u0026\u003e file Append stdout: command \u003e\u003e file Append both stdout and stderr: Standard: command \u003e\u003e file 2\u003e\u00261 Shorthand: command \u0026\u003e\u003e file Pipe stdout: command1 | command2 Pipe both stdout and stderr: Standard: command1 2\u003e\u00261 | command2 Shorthand: command1 |\u0026 command2 Custom file descriptors: Create and redirect stdout: exec 3\u003e file; command \u003e\u00263 Redirect stderr: command 2\u003e\u00263 Redirect both stdout and stderr: command \u003e /dev/fd/3 2\u003e\u00261 (no shorthand available) Discard stdout and stderr: Standard: command \u003e /dev/null 2\u003e\u00261 Shorthand: command \u0026\u003e/dev/null ","permalink":"http://rednafi.com/misc/shell_redirection/","publishDate":"2024-09-12","summary":"I always struggle with the syntax for redirecting multiple streams to another command or a file. LLMs do help, but beyond the most obvious cases, it takes a few prompts to get the syntax right. When I know exactly what I’m after, scanning a quick post is much faster than wrestling with a non-deterministic kraken. So, here’s a list of the redirection and piping syntax I use the most, with real examples.\n","tags":["Shell"],"title":"Shell redirection syntax soup"},{"content":"Here’s a Python snippet that makes an HTTP POST request: # script.py import httpx from typing import Any async def make_request(url: str) -\u003e dict[str, Any]: headers = {\"Content-Type\": \"application/json\"} async with httpx.AsyncClient(headers=headers) as client: response = await client.post( url, json={\"key_1\": \"value_1\", \"key_2\": \"value_2\"}, ) return response.json() The function make_request makes an async HTTP request with the httpx1 library. Running this with asyncio.run(make_request(\"https://httpbin.org/post\")) gives us the following output: { \"args\": {}, \"data\": \"{\\\"key_1\\\": \\\"value_1\\\", \\\"key_2\\\": \\\"value_2\\\"}\", \"files\": {}, \"form\": {}, \"headers\": { \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Content-Length\": \"40\", \"Content-Type\": \"application/json\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-httpx/0.27.2\", \"X-Amzn-Trace-Id\": \"Root=1-66d5f7b0-2ed0ddc57241f0960f28bc91\" }, \"json\": { \"key_1\": \"value_1\", \"key_2\": \"value_2\" }, \"origin\": \"95.90.238.240\", \"url\": \"https://httpbin.org/post\" } We’re only interested in the json field and want to assert in our test that making the HTTP call returns the expected values. Testing the HTTP request Now, how would you test it? One approach is by patching the httpx.AsyncClient instance to return a canned response and asserting against that. The happy path might be tested as follows: # test_script.py from unittest.mock import AsyncMock, patch import pytest from script import make_request @pytest.mark.asyncio async def test_make_request_ok() -\u003e None: url = \"https://httpbin.org/post\" expected_json = {\"key_1\": \"value_1\", \"key_2\": \"value_2\"} # Create a mock response object mock_response = AsyncMock() mock_response.json.return_value = expected_json mock_response.status_code = 200 # Patch the httpx.AsyncClient.post method to return the mock_response with patch( \"script.httpx.AsyncClient.post\", # Don't mock what you don't own return_value=mock_response, ) as mock_post: response = await make_request(url) # Await the coroutine that was returned response = await response # Assertions mock_post.assert_called_once_with(url, json=expected_json) assert response == expected_json That’s quite a bit of work just to test a simple HTTP request. The mocking gets pretty hairy as the complexity of your HTTP calls increases. One way to cut down the mess is by using a library like respx2 that handles the patching for you. Simplifying mocks with respx For instance: # test_script.py import pytest import respx from script import make_request, httpx @pytest.mark.asyncio async def test_make_request_ok() -\u003e None: url = \"https://httpbin.org/post\" expected_json = {\"key_1\": \"value_1\", \"key_2\": \"value_2\"} # Mocking the HTTP POST request using respx with respx.mock: respx.post(url).mock( return_value=httpx.Response(200, json=expected_json) ) # Calling the function response = await make_request(url) # Assertions assert response == expected_json Much cleaner. During tests, respx intercepts HTTP requests made by httpx, allowing you to test against canned responses. The library provides a context manager that acts like an httpx client, so you can set the expected response. This removes the need to manually patch methods like post in httpx.AsyncClient. Testing with a stub client The previous strategy wouldn’t work if you want to change your HTTP client since respx is coupled with httpx. As an alternative, you could rewrite make_request to parametrize the HTTP client, pass a stub object during the test, and assert against it. This eliminates the need to write fragile mocking sludges or depend on an external mocking library. Here’s how you’d change the code: # script.py import httpx import asyncio from typing import Any async def make_request(url: str, client: httpx.AsyncClient) -\u003e dict[str, Any]: # We don't want to initiate the ctx manager in every request # AsyncClient.__enter__(...) will be called once and passed to this function response = await client.post( url, json={\"key_1\": \"value_1\", \"key_2\": \"value_2\"}, ) return response.json() async def main() -\u003e None: headers = {\"Content-Type\": \"application/json\"} url = \"https://httpbin.org/post\" # Enter into the context manager and pass the instance to make_request async with httpx.AsyncClient(headers=headers) as client: response = await make_request(url, client) print(response) Now the tests would look as follows: import pytest from typing import Any from httpx import Response, Request, AsyncClient from script import make_request class StubAsyncClient(AsyncClient): async def post( self, url: str, json: Any = None, **kwargs: Any ) -\u003e Response: request = Request(method=\"POST\", url=url, json=json, **kwargs) # Simulate the original response that matches the request response = Response( status_code=200, json={\"key_1\": \"value_1\", \"key_2\": \"value_2\"}, request=request, ) return response @pytest.mark.asyncio async def test_make_request_ok() -\u003e None: url = \"https://httpbin.org/post\" headers = {\"Content-Type\": \"application/json\"} async with StubAsyncClient(headers=headers) as client: response_data = await make_request(url, client) assert response_data == {\"key_1\": \"value_1\", \"key_2\": \"value_2\"} Much better! Integration testing with a test server One thing I’ve picked up from writing Go is that it’s often just easier to perform integration tests on these I/O-bound functions. That is, you can spin up a server that returns a canned response and then test your code against it to assert if it’s getting the expected output. The test could look as follows. This assumes make_request takes in an AsyncClient instance as a parameter, as shown in the last example. import pytest from starlette.applications import Starlette from starlette.responses import JSONResponse from starlette.routing import Route from starlette.requests import Request from httpx import AsyncClient from script import make_request async def test_endpoint(request: Request) -\u003e JSONResponse: return JSONResponse({\"key_1\": \"value_1\", \"key_2\": \"value_2\"}) app = Starlette(routes=[Route(\"/post\", test_endpoint, methods=[\"POST\"])]) @pytest.mark.asyncio async def test_make_request() -\u003e None: # Manually create the AsyncClient async with AsyncClient(app=app, base_url=\"http://testserver\") as client: url = \"http://testserver/post\" response = await make_request(url, client=client) assert response == {\"key_1\": \"value_1\", \"key_2\": \"value_2\"} In the above test, we’re using starlette3 to define a simple ASGI server that returns our expected response. Then we set up the httpx.AsyncClient so it makes the request against the test server instead of making an external network call. Finally, we call the make_request function and assert the expected payload. Sure, you could set up the server with the standard library’s http module, but that code doesn’t look half as pretty. httpx ↩︎ respx ↩︎ starlette ↩︎ ","permalink":"http://rednafi.com/python/testing_http_requests/","publishDate":"2024-09-02","summary":"Here’s a Python snippet that makes an HTTP POST request:\n# script.py import httpx from typing import Any async def make_request(url: str) -\u003e dict[str, Any]: headers = {\"Content-Type\": \"application/json\"} async with httpx.AsyncClient(headers=headers) as client: response = await client.post( url, json={\"key_1\": \"value_1\", \"key_2\": \"value_2\"}, ) return response.json() The function make_request makes an async HTTP request with the httpx1 library. Running this with asyncio.run(make_request(\"https://httpbin.org/post\")) gives us the following output:\n{ \"args\": {}, \"data\": \"{\\\"key_1\\\": \\\"value_1\\\", \\\"key_2\\\": \\\"value_2\\\"}\", \"files\": {}, \"form\": {}, \"headers\": { \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Content-Length\": \"40\", \"Content-Type\": \"application/json\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-httpx/0.27.2\", \"X-Amzn-Trace-Id\": \"Root=1-66d5f7b0-2ed0ddc57241f0960f28bc91\" }, \"json\": { \"key_1\": \"value_1\", \"key_2\": \"value_2\" }, \"origin\": \"95.90.238.240\", \"url\": \"https://httpbin.org/post\" } We’re only interested in the json field and want to assert in our test that making the HTTP call returns the expected values.\n","tags":["API","Testing","TIL"],"title":"Shades of testing HTTP requests in Python"},{"content":"I love @pytest.mark.parametrize1—so much so that I sometimes shoehorn my tests to fit into it. But the default style of writing tests with parametrize can quickly turn into an unreadable mess as the test complexity grows. For example: import pytest from math import atan2 def polarify(x: float, y: float) -\u003e tuple[float, float]: r = (x**2 + y**2) ** 0.5 theta = atan2(y, x) return r, theta @pytest.mark.parametrize( \"x, y, expected\", [ (0, 0, (0, 0)), (1, 0, (1, 0)), (0, 1, (1, 1.5707963267948966)), (1, 1, (2**0.5, 0.7853981633974483)), (-1, -1, (2**0.5, -2.356194490192345)), ], ) def test_polarify(x: float, y: float, expected: tuple[float, float]) -\u003e None: # pytest.approx helps us ignore floating point discrepancies assert polarify(x, y) == pytest.approx(expected) The polarify function converts Cartesian coordinates to polar coordinates. We’re using @pytest.mark.parametrize in its standard form to test different conditions. Here, the list of nested tuples with inputs and expected values becomes hard to read as the test suite grows larger. When the function under test has a more complex signature, I find myself needing to do more mental gymnastics to parse the positional input and expected values inside parametrize. Also, how do you run a specific test case within the suite? For instance, what if you want to run only the third case where x, y, expected = (0, 1, (1, 1.5707963267948966))? I used to set custom test IDs like below to be able to run individual test cases within parametrize: # ... polarify implementation hasn't changed. @pytest.mark.parametrize( \"x, y, expected\", [ (0, 0, (0, 0)), (1, 0, (1, 0)), (0, 1, (1, 1.5707963267948966)), (1, 1, (2**0.5, 0.7853981633974483)), (-1, -1, (2**0.5, -2.356194490192345)), ], ids=[ \"origin\", \"positive_x_axis\", \"positive_y_axis\", \"first_quadrant\", \"third_quadrant\", ], ) def test_polarify(x: float, y: float, expected: tuple[float, float]) -\u003e None: # pytest.approx helps us ignore floating point discrepancies assert polarify(x, y) == pytest.approx(expected) This works, but mentally associating the IDs with the examples is cumbersome, and it doesn’t make things any easier to read. TIL, pytest.param2 gives you a better syntax and more control to achieve the same. Observe: # ... polarify implementation hasn't changed. @pytest.mark.parametrize( \"x, y, expected\", [ pytest.param(0, 0, (0, 0), id=\"origin\"), pytest.param(1, 0, (1, 0), id=\"positive_x_axis\"), pytest.param(0, 1, (1, 1.5707963267948966), id=\"positive_y_axis\"), pytest.param(1, 1, (2**0.5, 0.7853981633974483), id=\"first_quadrant\"), pytest.param( -1, -1, (2**0.5, -2.356194490192345), id=\"third_quadrant\" ), ], ) def test_polarify(x: float, y: float, expected: tuple[float, float]) -\u003e None: # pytest.approx helps us ignore floating point discrepancies assert polarify(x, y) == pytest.approx(expected) We’re setting the unique IDs inside pytest.param. Now, any test can be targeted with pytest’s -k flag like this: pytest -k positive_x_axis This will only run the second test case on the list. Or, pytest -k 'first or third' This will run the last two tests. But the test is still somewhat hard to read. I usually refactor mine to take a kwargs argument so that I can neatly tuck all the input and expected values associated with a test case in a single dictionary. Notice: # ... polarify implementation hasn't changed. @pytest.mark.parametrize( \"kwargs\", [ pytest.param({\"x\": 0, \"y\": 0, \"expected\": (0, 0)}, id=\"origin\"), pytest.param( {\"x\": 1, \"y\": 0, \"expected\": (1, 0)}, id=\"positive_x_axis\" ), pytest.param( {\"x\": 0, \"y\": 1, \"expected\": (1, 1.5707963267948966)}, id=\"positive_y_axis\", ), pytest.param( {\"x\": 1, \"y\": 1, \"expected\": (2**0.5, 0.7853981633974483)}, id=\"first_quadrant\", ), pytest.param( {\"x\": -1, \"y\": -1, \"expected\": (2**0.5, -2.356194490192345)}, id=\"third_quadrant\", ), ], ) def test_polarify(kwargs: dict[str, Any]) -\u003e None: # Extract expected from kwargs expected = kwargs.pop(\"expected\") # Unpack the remaining kwargs to the polarify function assert polarify(**kwargs) == pytest.approx(expected) Everything associated with a single test case is passed to pytest.param in a single dictionary, eliminating the need to guess any positional arguments. Using pytest.param also allows you to set custom test execution conditionals, which I’ve started to take advantage of recently: # ... polarify implementation hasn't changed. @pytest.mark.parametrize( \"kwargs\", [ pytest.param( {\"x\": 0, \"y\": 1, \"expected\": (1, 1.5707963267948966)}, id=\"positive_y_axis\", marks=pytest.mark.xfail( reason=\"Known issue with atan2 in this quadrant\" ), ), pytest.param( {\"x\": 1, \"y\": 1, \"expected\": (2**0.5, 0.7853981633974483)}, id=\"first_quadrant\", ), pytest.param( { \"x\": 1e10, \"y\": 1e10, \"expected\": (2**0.5 * 1e10, 0.7853981633974483), }, id=\"too_large\", marks=pytest.mark.skipif( lambda kwargs: kwargs[\"x\"] \u003e 1e6 or kwargs[\"y\"] \u003e 1e6, reason=\"Input values are too large\", ), ), ], ) def test_polarify(kwargs: dict[str, Any]) -\u003e None: # Extract expected from kwargs expected = kwargs.pop(\"expected\") # Unpack the remaining kwargs to the polarify function assert polarify(**kwargs) == pytest.approx(expected) In the last block, pytest.param bundles test data with execution conditions. We’re using xfail to mark a test as expected to fail, while skipif skips tests based on conditions. This keeps all the logic for handling test cases, including failures and skips, directly alongside the test data. pytest.mark.parametrize ↩︎ pytest.param ↩︎ ","permalink":"http://rednafi.com/python/pytest_param/","publishDate":"2024-08-28","summary":"I love @pytest.mark.parametrize1—so much so that I sometimes shoehorn my tests to fit into it. But the default style of writing tests with parametrize can quickly turn into an unreadable mess as the test complexity grows. For example:\nimport pytest from math import atan2 def polarify(x: float, y: float) -\u003e tuple[float, float]: r = (x**2 + y**2) ** 0.5 theta = atan2(y, x) return r, theta @pytest.mark.parametrize( \"x, y, expected\", [ (0, 0, (0, 0)), (1, 0, (1, 0)), (0, 1, (1, 1.5707963267948966)), (1, 1, (2**0.5, 0.7853981633974483)), (-1, -1, (2**0.5, -2.356194490192345)), ], ) def test_polarify(x: float, y: float, expected: tuple[float, float]) -\u003e None: # pytest.approx helps us ignore floating point discrepancies assert polarify(x, y) == pytest.approx(expected) The polarify function converts Cartesian coordinates to polar coordinates. We’re using @pytest.mark.parametrize in its standard form to test different conditions.\n","tags":["Python","Testing","TIL"],"title":"Taming parametrize with pytest.param"},{"content":"I learned this neat Bash trick today where you can make a raw HTTP request using the /dev/tcp file descriptor without using tools like curl or wget. This came in handy while writing a health check script that needed to make a TCP request to a service. The following script opens a TCP connection and makes a simple GET request to example.com: #!/bin/bash # Open a TCP connection to example.com on port 80 and assign file descriptor 3 # The exec command keeps /dev/fd/3 open throughout the lifetime of the script # 3\u003c\u003e enables bidirectional read-write exec 3\u003c\u003e/dev/tcp/example.com/80 # Send the HTTP GET request to the server # \u003e\u0026 redirects stdout to /dev/fd/3 echo -e \"GET / HTTP/1.1\\r\\nHost: example.com\\r\\nConnection: close\\r\\n\\r\\n\" \u003e\u00263 # Read and print the server's response # \u003c\u0026 redirects the output of /dev/fd/3 to cat cat \u003c\u00263 # Close the file descriptor, terminating the TCP connection exec 3\u003e\u0026- Running this will print the response from the site to your console. The snippet first opens a TCP connection to example.com on port 80 and assigns file descriptor 3 to manage this connection. The exec ensures that the file descriptor 3 remains open for the duration of the script, allowing multiple read and write operations without needing to reopen the connection each time. Using a file descriptor makes the code cleaner. Without it, we’d need to redirect input and output directly to /dev/tcp/example.com/80 for each read and write operation, making the script more cumbersome and harder to read. Then we send an HTTP GET request to the server by echoing the request to file descriptor 3. The server’s response is read and printed using cat \u003c\u00263, which reads from the file descriptor and prints the output to the console. Finally, the script closes the connection by terminating file descriptor 3 with exec 3\u003e\u0026-. This is a Bash-specific trick and won’t work in other shells like Zsh or Fish. It also allows you to open UDP connections in the same manner. The Bash manpage explains the usage like this: /dev/tcp/host/port If host is a valid hostname or Internet address, and port is an integer port number or service name, bash attempts to open the corresponding TCP socket. /dev/udp/host/port If host is a valid hostname or Internet address, and port is an integer port number or service name, bash attempts to open the corresponding UDP socket. I used this to write the following health check script. I didn’t want to install curl in a sidecar container that just runs a single health check process, keeping things simpler. #!/bin/bash # Enable bash strict mode set -euo pipefail # Constants readonly HOST=\"example.com\" readonly PORT=80 readonly HEALTH_PATH=\"/\" # Open a TCP connection to the specified host and port exec 3\u003c\u003e\"/dev/tcp/${HOST}/${PORT}\" # Send the HTTP GET request to the server echo -e \\ \"GET ${HEALTH_PATH} HTTP/1.1\\r\\nHost: ${HOST}\\r\\nConnection: close\\r\\n\\r\\n\" \\ \u003e\u00263 # Read the HTTP status from the server's response read -r HTTP_RESPONSE \u003c\u00263 HTTP_STATUS=$( echo \"${HTTP_RESPONSE}\" \\ | grep -o \"HTTP/1.1 [0-9]*\" \\ | cut -d ' ' -f 2 ) if [[ \"${HTTP_STATUS}\" == \"200\" ]]; then echo \"Service is healthy.\" exit 0 else echo \"Service is not healthy. HTTP status: ${HTTP_STATUS}\" exit 1 fi # Close the file descriptor, terminating the TCP connection exec 3\u003e\u0026- The script makes a GET request to the service and checks that the HTTP status from the raw response is 200. If not, it exits with a non-zero status. Note that the script will fail if your service returns a 301 redirect code. Plus, you need to make raw textual HTTP requests, which can become cumbersome if you need to do anything beyond a simple GET call. At that point, you’re better off using curl. ","permalink":"http://rednafi.com/misc/http_requests_via_dev_tcp/","publishDate":"2024-08-08","summary":"I learned this neat Bash trick today where you can make a raw HTTP request using the /dev/tcp file descriptor without using tools like curl or wget. This came in handy while writing a health check script that needed to make a TCP request to a service.\nThe following script opens a TCP connection and makes a simple GET request to example.com:\n#!/bin/bash # Open a TCP connection to example.com on port 80 and assign file descriptor 3 # The exec command keeps /dev/fd/3 open throughout the lifetime of the script # 3\u003c\u003e enables bidirectional read-write exec 3\u003c\u003e/dev/tcp/example.com/80 # Send the HTTP GET request to the server # \u003e\u0026 redirects stdout to /dev/fd/3 echo -e \"GET / HTTP/1.1\\r\\nHost: example.com\\r\\nConnection: close\\r\\n\\r\\n\" \u003e\u00263 # Read and print the server's response # \u003c\u0026 redirects the output of /dev/fd/3 to cat cat \u003c\u00263 # Close the file descriptor, terminating the TCP connection exec 3\u003e\u0026- Running this will print the response from the site to your console.\n","tags":["TIL","Shell"],"title":"HTTP requests via /dev/tcp"},{"content":"Let’s say you have a web app that emits log messages from different layers. Your log shipper collects and sends these messages to a destination like Datadog where you can query them. One common requirement is to tag the log messages with some common attributes, which you can use later to query them. In distributed tracing, this tagging is usually known as context propagation1, where you’re attaching some contextual information to your log messages that you can use later for query purposes. However, if you have to collect the context at each layer of your application and pass it manually to the downstream ones, that’d make the whole process quite painful. Suppose you have a web view for an endpoint that calls another function to do something: async def view(request: Request) -\u003e JSONResponse: # Collect contextual info from the header user_id = request.headers.get(\"Svc-User-Id\") platform = request.headers.get(\"Svc-Platform\") # Log the request with context logger.info( \"Request started\", extra={\"user_id\": user_id, \"platform\": platform} ) await work() # Log the response too logger.info( \"Request ended\", extra={\"user_id\": user_id, \"platform\": platform} ) return JSONResponse({\"message\": \"Work, work work!\"}) async def work() -\u003e None: await asyncio.sleep(1) logger.info(\"Work done\") I’m using Starlette2 syntax for the above pseudocode, but this is valid for any generic ASGI web app. The view procedure collects contextual information like user_id and platform from the request headers. Then it tags the log statements before and after calling the work function using the extra fields in the logger calls. This way, the log messages have contextual info attached to them. However, the work procedure also generates a log message, and that won’t get tagged here. We may be tempted to pass the contextual information to the work subroutine and use them to tag the logs, but that’ll quickly get repetitive and cumbersome. Passing a bunch of arguments to a function just so it can tag some log messages also makes things unnecessarily verbose. Plus, it’s quite easy to forget to do so, which will leave you with orphan logs with no way to query them. It turns out we can write a simple middleware to tag log statements in a way where we won’t need to manually propagate the contextual information throughout the call chain. To demonstrate that, here’s a simple get endpoint server written in Starlette that’ll just return a canned response after logging a few events. The app structure looks as follows: svc ├── __init__.py ├── log.py ├── main.py ├── middleware.py └── view.py Configure the logger The first step is to configure the application logger so that it emits structured log statements in JSON where each message will look as follows: { \"message\": \"Some log message\", \"timestamp\": 1722794887376, \"tags\": { \"user_id\": \"1234\", \"platform\": \"ios\" } } Here’s the log configuration logic: # log.py import contextvars import json import logging import time # Set up the context variable with default values default_context = {\"user_id\": \"unknown\", \"platform\": \"unknown\"} log_context_var = contextvars.ContextVar( \"log_context\", default=default_context.copy(), ) # Custom log formatter class ContextAwareJsonFormatter(logging.Formatter): def format(self, record): log_data = { \"message\": record.getMessage(), # Add millisecond precision timestamp \"timestamp\": int(time.time() * 1000), # Get the context from the context variable in a concurreny-safe way # The context will be set in the middleware, so .get() will always # return the current context \"tags\": log_context_var.get(), } return json.dumps(log_data) # Set up the logger logger = logging.getLogger() logger.setLevel(logging.INFO) handler = logging.StreamHandler() formatter = ContextAwareJsonFormatter() handler.setFormatter(formatter) logger.addHandler(handler) The contextvars module manages context information across asynchronous tasks, preventing context leakage between requests. We use a log_context_var context variable to store user ID and platform information, ensuring each log entry includes relevant context for the request. The ContextAwareJsonFormatter formats log statements to include the message, timestamp in milliseconds, and context tags. The context is retrieved using log_context_var.get(), ensuring concurrency-safe access. The context variable is set in the middleware, so log_context_var.get() always returns the current context for each request. Next, we set up a StreamHandler, attach the ContextAwareJsonFormatter to it, and add the handler to the root logger. Write a middleware that tags the log statements automatically With log formatting out of the way, here’s how to write the middleware to update the logger so that all the log messages within a request-response cycle get automatically tagged: # middleware.py import logging from collections.abc import Awaitable, Callable from starlette.middleware.base import BaseHTTPMiddleware from starlette.requests import Request from starlette.responses import Response from svc.log import default_context, log_context_var # Middleware for setting log context class LogContextMiddleware(BaseHTTPMiddleware): async def dispatch( self, request: Request, call_next: Callable[[Request], Awaitable[Response]], ) -\u003e Response: # Copy the default context so that we're not sharing anything # between requests context = default_context.copy() # Collect the contextual information from request headers user_id = request.headers.get(\"Svc-User-Id\") platform = request.headers.get(\"Svc-Platform\") # Update the context if user_id: context[\"user_id\"] = user_id if platform: context[\"platform\"] = platform # Set the log_context_var context variable token = log_context_var.set(context) try: # Log before making request logging.info(\"From middleware: request started\") response = await call_next(request) # Log after making request logging.info(\"From middleware: request ended\") finally: # Reset the context after the request is processed log_context_var.reset(token) return response The LogContextMiddleware class inherits from starlette.BaseHTTPMiddleware and gets initialized with the application. The dispatch method is called automatically for each request. It extracts user_id and platform from the request headers and sets these values in the log_context_var to tag log messages. Then it logs the incoming request, processes it, logs the outgoing response, and then clears the context so that we don’t leak the context information across requests. This way, our view function won’t need to be peppered with repetitive log statements. Write the simplified view Setting up the logger and middleware drastically simplifies our endpoint view since we won’t need to tag the logs explicitly or add request-response logs in each view. It looks like this now: # view.py import asyncio import logging from starlette.requests import Request from starlette.responses import JSONResponse async def view(request: Request) -\u003e JSONResponse: await work() logging.info(\"From view function: work finished\") return JSONResponse({\"message\": f\"Work work work!!!\"}) async def work() -\u003e None: logging.info(\"From work function: work started\") await asyncio.sleep(1) Notice there’s no repetitive request-response log statements in the view function, and we’re not passing the log context anywhere explicitly. The middleware will ensure that the request and response logs are always emitted and all the logs, including the one coming out of the work function, are tagged with the contextual information. Wire everything together The logging configuration and middleware can be wired up like this: # main.py import uvicorn from starlette.applications import Starlette from starlette.middleware import Middleware from starlette.routing import Route from svc.middleware import LogContextMiddleware from svc.view import view middlewares = [Middleware(LogContextMiddleware)] app = Starlette(routes=[Route(\"/\", view)], middleware=middlewares) if __name__ == \"__main__\": uvicorn.run(app, host=\"0.0.0.0\", port=8000) To instantiate the logger config, we import log.py in the __init__.py module: # __init__.py from svc import log # noqa Now the application can be started with: python -m svc.main And then we can make a request to the server: curl http://localhost:8000/ -H 'Svc-User-Id: 1234' -H 'Svc-Platform: ios' On the server, the request will emit the following log messages: INFO: Started server process [41848] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit) { \"message\": \"From middleware: request started\", \"timestamp\": 1723166008113, \"tags\": { \"user_id\": \"1234\", \"platform\": \"ios\" } } { \"message\": \"From work function: work started\", \"timestamp\": 1723166008113, \"tags\": { \"user_id\": \"1234\", \"platform\": \"ios\" } } { \"message\": \"From view function: work finished\", \"timestamp\": 1723166009114, \"tags\": { \"user_id\": \"1234\", \"platform\": \"ios\" } } { \"message\": \"From middleware: request ended\", \"timestamp\": 1723166009115, \"tags\": { \"user_id\": \"1234\", \"platform\": \"ios\" } } INFO: 127.0.0.1:54780 - \"GET / HTTP/1.1\" 200 OK And we’re done. You can find the fully working code in this GitHub gist3. Note: The previous version4 of this example wasn’t concurrency safe and used a shared logger filter, leaking context information during concurrent requests. This was pointed out in this GitHub comment5. Context propagation ↩︎ Starlette ↩︎ Complete example ↩︎ Previous version of this example ↩︎ GitHub discussion on context leakage in log statements ↩︎ ","permalink":"http://rednafi.com/python/log_context_propagation/","publishDate":"2024-08-06","summary":"Let’s say you have a web app that emits log messages from different layers. Your log shipper collects and sends these messages to a destination like Datadog where you can query them. One common requirement is to tag the log messages with some common attributes, which you can use later to query them.\nIn distributed tracing, this tagging is usually known as context propagation1, where you’re attaching some contextual information to your log messages that you can use later for query purposes. However, if you have to collect the context at each layer of your application and pass it manually to the downstream ones, that’d make the whole process quite painful.\n","tags":["Python","API"],"title":"Log context propagation in Python ASGI apps"},{"content":"With the recent explosion of LLM tools, I often like to kill time fiddling with different LLM client libraries and SDKs in one-off scripts. Lately, I’ve noticed that some newer tools frequently mess up the logger settings, meddling with my application logs. While it’s less common in more seasoned libraries, I guess it’s worth rehashing why hijacking the root logger isn’t a good idea when writing libraries or other forms of reusable code. In Python, when I say root logger1, I mean the logger instance that logging.basicConfig acts on, or the one you get back when you don’t specify a name in logging.getLogger(). The root logger is for the application code to use and if you’re a library author, you should probably steer clear from it. If not, people using your code might get into situations as follows. Let’s say there’s a single file library named lib.py that decides to configure the root logger: # lib.py import logging # Configuring the root logger here. Not a great idea! logging.basicConfig(level=logging.DEBUG) def frobnicate() -\u003e None: # Using the root logger throughout the library code logging.debug(\"This is a debug message from the library.\") Now, let’s say the user of lib.py imports the frobnicate function and configures the root logger in the following manner: # main.py import logging from lib import frobnicate # Library user attempts to reconfigure the root logger logging.basicConfig(level=logging.INFO) def main() -\u003e None: # Use library code frobnicate() # Emit log message from the application code logging.info(\"This is an info message from the application.\") if __name__ == \"__main__\": main() Since the application code has set the log threshold to INFO, you might think that running the code snippet would only print the log message from the application. But instead, you’ll also get the DEBUG message from the library: DEBUG:root:This is a debug message from the library. INFO:root:This is an info message from the application. It happens because before the application code had the chance to set the log level to INFO, the library code hijacked the root logger and configured it during the import time of frobnicate. You can test it by placing the from lib import frobnicate statement after the logging.basicConfig(...) line in the main.py file. By doing so, the log configuration in the application code gets to run before the library has the chance to meddle with it. This makes things confusing for the library user, and the logging how-to2 doc advises against doing so: It is strongly advised that you do not log to the root logger in your library. Instead, use a logger with a unique and easily identifiable name, such as the name for your library’s top-level package or module. Logging to the root logger will make it difficult or impossible for the application developer to configure the logging verbosity or handlers of your library as they wish. Solving this is quite straightforward. Avoid using the root logger in your library code. Instead, instantiate your own logger instance and configure it with your heart’s content. This way, your users get to keep using the root logger as they like, and they can also tap into the library’s log messages whenever they need to. Here’s how to achieve that in the library: # lib.py import logging # Create a logger object for the library logger = logging.getLogger(\"lib\") def frobnicate() -\u003e None: # Only use this logger object throughout the library logger.debug(\"Debug message from the library\") Now the library logger no longer conflicts with the application log configuration. The application code in the main.py from the previous section can remain the same and running the snippet will only print out the INFO message this time: INFO:root:This is an info message from the application. This setup also lets the application code access and adjust the library’s logger to suit its needs. Here’s how it can be done in the main.py file: # main.py import logging from lib import frobnicate # Configure the root logger logging.basicConfig(level=logging.INFO) # Get the logger object for the library. This was already created in lib.py lib_logger = logging.getLogger(\"lib\") # Set the log level for the library logger to DEBUG lib_logger.setLevel(logging.DEBUG) def main() -\u003e None: frobnicate() logging.info(\"Info message from the main\") if __name__ == \"__main__\": main() Above, the library user sets up the root logger as usual while also reconfiguring the library’s logger. It’s the library author’s job to properly name and initialize the logger in the library code. The logger name and the default behavior should be well-documented as well. This allows the application code to retrieve and customize the logger as needed. Note that calling getLogger with the same name always retrieves the same logger instance. Also, you should avoid adding any handlers to your library’s logger. Doing so can complicate things for users who may want to attach their own handlers. The logging how-to guide strongly warns against this: It is strongly advised that you do not add any handlers other than NullHandler to your library’s loggers. This is because the configuration of handlers is the prerogative of the application developer who uses your library. The application developer knows their target audience and what handlers are most appropriate for their application: if you add handlers ‘under the hood’, you might well interfere with their ability to carry out unit tests and deliver logs which suit their requirements. If you’re looking for a real-life example of how to minimally configure your library’s logger, check out the httpx3 codebase. The logging behavior is well-documented here. You can easily reconfigure the httpx logger in your application code while making an HTTP request like this: # Your application code import httpx import logging # Get the library's logger instance httpx_logger = logging.getLogger(\"httpx\") # Set the logger's log level httpx_logger.setLevel(logging.DEBUG) # Define a handler console_handler = logging.StreamHandler() # Set the handler's log level console_handler.setLevel(logging.DEBUG) # Define a formatter console_formatter = logging.Formatter(\"%(name)s - %(levelname)s - %(message)s\") # Add the handler to the library's logger instance httpx_logger.addHandler(console_handler) # Set the formatter for the handler console_handler.setFormatter(console_formatter) # Make a request that'll emit the log messages httpx.get(\"https://httpbin.org/get\") Running the script will print the DEBUG messages as follows: httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False httpx - DEBUG - load_verify_locations cafile='.../site-packages/certifi/cacert.pem' httpx - INFO - HTTP Request: GET https://httpbin.org/get \"HTTP/1.1 200 OK\" Root logger ↩︎ Logging how-to ↩︎ Logging in httpx ↩︎ ","permalink":"http://rednafi.com/python/no_hijack_root_logger/","publishDate":"2024-08-03","summary":"With the recent explosion of LLM tools, I often like to kill time fiddling with different LLM client libraries and SDKs in one-off scripts. Lately, I’ve noticed that some newer tools frequently mess up the logger settings, meddling with my application logs. While it’s less common in more seasoned libraries, I guess it’s worth rehashing why hijacking the root logger isn’t a good idea when writing libraries or other forms of reusable code.\n","tags":["Python"],"title":"Please don't hijack my Python root logger"},{"content":"TIL about the install command on *nix systems. A quick GitHub search for the term brought up a ton of matches1. I’m surprised I just found out about it now.\nOften, in shell scripts I need to:\nCreate a directory hierarchy Copy a config or binary file to the new directory Set permissions on the file It usually looks like this:\n# Create the directory hierarchy. The -p flag creates the parent directories # if they don't exist mkdir -p ~/.config/app # Copy the current config to the newly created directory. Here, conf already # exists in the current folder cp conf ~/.config/app/conf # Set the file permission chmod 755 ~/.config/app/conf Turns out, the install command in GNU coreutils2 can do all that in one line:\ninstall -D -m 755 conf ~/.config/app/conf You can check the file status with:\nstat ~/.config/app/conf On my machine, this prints:\nFile: /Users/rednafi/.config/app Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: 1,16 Inode: 16439606 Links: 1 Access: (0755/-rwxr-xr-x) Uid: ( 501/ rednafi) Gid: ( 20/ staff) Access: 2024-07-28 20:51:42.793765043 +0200 Modify: 2024-07-28 20:51:42.793765043 +0200 Change: 2024-07-28 20:51:42.793907876 +0200 Birth: 2024-07-28 20:51:42.793765043 +0200 The -D flag directs install to create the destination directories if they don’t exist, and the -m flag sets file permissions. The result is the same as the three lines of commands before.\nIt’s common for Makefiles in C/C++ projects to install binaries like this:\ninstall -D -m 744 app_bin /usr/local/bin/app_bin It copies app_bin to /usr/local/bin, creates the parent directory hierarchy if necessary, and sets permissions on the binary so only the current user has read, write, and execute permissions, while others have read-only access.\nYou can also set directory permissions:\ninstall -d -m 600 foo/bar/bazz This creates the directory hierarchy first and then sets the permission. Here’s how they look:\ntree foo Output:\nfoo └── bar └── bazz 3 directories, 0 files Then you can copy a file to the destination and set file permissions with another install command if needed.\nYou can also set user or group ownership while copying a file:\ninstall -D -m 644 -o root -g root seed.db /var/lib/app/seed.db This command copies seed.db to the destination, creates the directory if needed, and gives access to the root user and group with the -o and -g flags, respectively.\nThere are a few other options you can read about in the man pages, but I haven’t needed anything beyond the above.\nSearch for “install -D” on GitHub ↩︎\nGNU install ↩︎\n","permalink":"http://rednafi.com/misc/install/","publishDate":"2024-07-28","summary":"TIL about the install command on *nix systems. A quick GitHub search for the term brought up a ton of matches1. I’m surprised I just found out about it now.\nOften, in shell scripts I need to:\nCreate a directory hierarchy Copy a config or binary file to the new directory Set permissions on the file It usually looks like this:\n# Create the directory hierarchy. The -p flag creates the parent directories # if they don't exist mkdir -p ~/.config/app # Copy the current config to the newly created directory. Here, conf already # exists in the current folder cp conf ~/.config/app/conf # Set the file permission chmod 755 ~/.config/app/conf Turns out, the install command in GNU coreutils2 can do all that in one line:\n","tags":["Shell","TIL"],"title":"The *nix install command"},{"content":"I was working on the deployment pipeline for a service that launches an app in a dedicated VM using GitHub Actions. In the last step of the workflow, the CI SSHs into the VM and runs several commands using a here document1 in bash. The simplified version looks like this: # SSH into the remote machine and run a bunch of commands to deploy the service ssh $SSH_USER@$SSH_HOST \u003c","permalink":"http://rednafi.com/misc/heredoc_headache/","publishDate":"2024-07-19","summary":"I was working on the deployment pipeline for a service that launches an app in a dedicated VM using GitHub Actions. In the last step of the workflow, the CI SSHs into the VM and runs several commands using a here document1 in bash. The simplified version looks like this:\n# SSH into the remote machine and run a bunch of commands to deploy the service ssh $SSH_USER@$SSH_HOST \u003c","tags":["Shell"],"title":"Here-doc headache"},{"content":"One of the reasons why I’m a big advocate of rebasing and cleaning up feature branches, even when the changes get squash-merged to the mainline, is that it makes the PR reviewer’s life a little easier. I’ve written about my rebase workflow before1 and learned a few new things from the Hacker News discussion2 around it.\nWhile there’s been no shortage of text on why and how to craft atomic commits3, I often find those discussions focus too much on VCS hygiene, and the main benefit gets lost in the minutiae. When working in a team setup, I’ve discovered that individual commits matter much less than the final change list.\nAlso, I find some of the prescriptive suggestions for easier review, like keeping the PR under ~150 lines, ensuring that the tests pass in each commit, and tidying the commits to be strictly independent, quite cumbersome. Stacked PRs4 sometimes help to make large changes a bit more tractable, but that comes with a whole set of review-conflict-feedback challenges. So this piece will mainly focus on making large PRs a wee bit easier to work with.\nHere’s a quick rundown of the things I find useful to make reviewing the grunt work of pull requests a bit more tractable. I don’t always strictly follow them while doing personal or OSS work, but these steps have been helpful while working on a large shared repo at work.\nAvoiding the temptation to lump tangentially related changes into a PR to speed things up.\nHaving a ton of fragmented commits makes filtering useless when navigating the PR diff in a platform like GitHub. I really like to filter diffs on GitHub, but it wouldn’t be useful if the commits are all over the place.\nTo make diff filtering better, I often rebase my feature branch after a messy development workflow and divide the changes into a few commits clustered around the core implementation, tests, documentation, dependency upgrades, and occasional refactoring.\nRebasing all the changes into a single commit is okay if the change is small, but for bigger changes, this does more harm than good.\nI’ve rarely spent the time to ensure that the individual commits are perfect5—in the sense that they’re complete with passing tests or documentation. As long as the complete change list makes sense as a whole, it’s good enough. YMMV. The main goal is to make sure the diff makes sense to the person reviewing the work.\nAnnotated comments from the author on the PR are great. I wish they’d take up less space and there was a way to collapse them individually.\nEach PR must be connected to either an Issue or a Jira ticket, depending on how the team works.\nAdding context, screenshots, gifs, and videos to the PR description makes things so much easier for me when I do the review. Being able to see that the changes work as intended without running the code has its benefits.\nKeeping the PR in draft state until it’s ready to be reviewed. I’m not a fan of getting a notification to review some work only to find that it’s not ready yet.\nI kind of like rebasing ↩︎\nHN discussion on my rebasing workflow ↩︎\nMake atomic git commits ↩︎\nIn praise of stacked PRs ↩︎\nThe perfect commit ↩︎\n","permalink":"http://rednafi.com/misc/sane_pull_request/","publishDate":"2024-07-14","summary":"One of the reasons why I’m a big advocate of rebasing and cleaning up feature branches, even when the changes get squash-merged to the mainline, is that it makes the PR reviewer’s life a little easier. I’ve written about my rebase workflow before1 and learned a few new things from the Hacker News discussion2 around it.\nWhile there’s been no shortage of text on why and how to craft atomic commits3, I often find those discussions focus too much on VCS hygiene, and the main benefit gets lost in the minutiae. When working in a team setup, I’ve discovered that individual commits matter much less than the final change list.\n","tags":["Git"],"title":"The sane pull request"},{"content":"People tend to get pretty passionate about Git workflows on different online forums. Some like to rebase, while others prefer to keep the disorganized records. Some dislike the extra merge commit, while others love to preserve all the historical artifacts. There’s merit to both sides of the discussion. That being said, I kind of like rebasing because I’m a messy committer who: Usually doesn’t care for keeping atomic commits1. Creates a lot of short commits with messages like “fix” or “wip”. Likes to clean up the untidy commits before sending the branch for peer review. Prefers a linear history over a forked one so that git log --oneline --graph tells a nice story. Git rebase allows me to squash my disordered commits into a neat little one, which bundles all the changes with passing tests and documentation. Sure, a similar result can be emulated using git merge --squash feat_branch or GitHub’s squash-merge feature, but to me, rebasing feels cleaner. Plus, over time, I’ve subconsciously picked up the tricks to work my way around rebase-related gotchas. Julia Evans explores the pros and cons of rebasing in detail here2. Also, squashing commits is just one of the many things that you can do with the rebase command. Here, I just wanted to document my daily rebasing workflow where I mostly rename, squash, or fixup commits. A few assumptions Broadly speaking, there are two common types of rebasing: rebasing a feature branch onto the main branch and interactive rebasing on the feature branch itself. The workflow assumes a usual web service development cadence where: You’ll be working on a feature branch that’s forked off of a main branch. The main branch is protected, and you can’t directly push your changes to it. Once you’re done with your feature work, you’ll need to create a pull request against the main branch. After your PR is reviewed and merged onto the main branch, CI automatically deploys it to some staging environment. I’m aware this approach doesn’t work for some niches in software development, but it’s the one I’m most familiar with, so I’ll go with it. Rebasing a feature branch onto the main branch Let’s say I want to start working on a new feature. Here’s how I usually go about it: Pull in the latest main with git pull. Fork off a new branch via git switch -c feat_branch. Do the work in feat_branch, and before sending the PR, do interactive rebasing if necessary, and then rebase the feat_branch onto the latest changes of main with: git pull --rebase origin main Push the changes to the remote repository with git push origin HEAD and send a PR against main for review. Here, ...origin HEAD instructs git to push the current branch that HEAD is pointing to. The 3rd step is where I often do interactive rebasing before sending the PR to make my work presentable. The next section will explain that in detail. Occasionally, the 4th step doesn’t go as expected, and merge conflicts occur when I run git rebase main from feat_branch. In those cases, I use my editor (VSCode) to fix the conflict, add the changes with git add ., and run git rebase --continue. This completes the rebase operation, and we’re ready to push it to the remote. Rebasing interactively on the feature branch This is an extension of the 3rd step of the previous section. Sometimes, while working on a feature, I quickly make many messy commits and push them to the remote branch. This happens quite frequently when I’m prototyping on a feature or updating something regarding GitHub Actions. In these cases, I tend to make quick changes, commit with a message like “fix” or “ci” and push to remote to see if the CI is passing. However, once I’m done, the commit log on that branch looks like this: git log @ ^main --oneline --graph This command instructs git to show only the commits that exist on feat_branch but not on main. I learned recently that in git’s context, @ indicates the current branch. Neat, this means I won’t need to remember the branch name or do a git branch and then copy the name of the current branch. Running the command returns: * 148934c (HEAD -\u003e feat_branch) ci * e0f6152 ci * 8f4dc4c ci * bf33bf7 ci * 2e3dce6 ci I’m not too proud of the state of this feat_branch and prefer to tidy things up before making a PR against main. One common thing I do is squash all these commits into one and then add a proper commit message. Interactive rebasing allows me to do that. Let’s say you want to interactively rebase the 5 commits listed above and squash them. To do so, you can run the following command from the feat_branch: git rebase -i HEAD~5 This will open a file named git-rebase-todo in your default git editor (set via git config) that looks like this: pick 763e178 ci # empty pick 4b10faf ci # empty pick 7f7ce20 ci # empty pick 88fc529 ci # empty pick 8bc19b6 ci # empty # Rebase a2e45d3..8bc19b6 onto a2e45d3 (5 commands) # # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup [-C | -c] = like \"squash\" but keep only the previous # commit's log message, unless -C is used, in which case # keep only this commit's message; -c is same as -C but # opens the editor # x, exec = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with 'git rebase --continue') # d, drop = remove commit # l, label = label current HEAD with a name # t, reset = reset HEAD to a label # m, merge [-C | -c ] [# ] # create a merge commit using the original merge commit's # message (or the oneline, if no original merge commit was # specified); use -c to reword the commit message # u, update-ref = track a placeholder for the to be updated # to this position in the new commits. The is # updated at the end of the rebase # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. # Notice that the file has quite a bit of instructions that are commented out. You can perform actions like pick, reword, edit, fixup, etc. I usually use squash and edit the git-rebase-todo file like this: pick 763e178 ci # empty s 4b10faf ci # empty # \u003c- s=squash means melding this commit into the previous one s 7f7ce20 ci # empty s 88fc529 ci # empty s 8bc19b6 ci # empty # ... rest of the file remains untouched Now, if you close the previous file, git will automatically open another file like the following: # This is a combination of 5 commits. # This is the 1st commit message: ci # This is the commit message #2: ci # This is the commit message #3: ci # This is the commit message #4: ci # This is the commit message #5: ci After the first comment, you can put in the message for all the combined commits: # This is a combination of 5 commits. Add pip caching to the CI # \u003c- message for the combined commits # ... you can remove rest of the content If you close this file, you’ll see a message on your console indicating that the rebase has been successful: [detached HEAD 28f5084] Add pip caching to the CI Date: Wed Jun 19 22:42:07 2024 +0200 Successfully rebased and updated refs/heads/feat_branch. Now running git log will show that the messy commit has been squashed into one. git log @ ^main --oneline --graph This displays: * 28f5084 (HEAD -\u003e feat_branch) Add pip caching to the CI This is just one of the many things you can do during interactive rebasing. While I do this most commonly, sometimes I also drop unnecessary commits to tidy up things and group multiple commits instead of just squashing everything into one commit. All of these actions can be done in a similar manner to squashing commits as mentioned above. Sometimes, I don’t know how many commits I’ll need to interactively rebase. In those cases, I can get the number of all the new commits on a feature branch by counting the entries in git log as follows: git log @ ^main --oneline | wc -l Then you can use the number from the output of the previous command to rebase n number of commits: git rebase -i HEAD~n Another thing you can do is split a single commit into multiple commits. This is quite a bit more involved and I rarely do it during interactive rebasing. One last thing I learned recently is that you can run your tests or any arbitrary command during interactive rebasing. To do so, start your rebase session with --exec cmd as follows: git rebase -i --exec \"echo hello\" HEAD~5 In the git-rebase-todo file this time, you’ll see that the command is run after each commit as follows: pick dffb3c1 ci # empty exec echo hello pick 4d2fa08 ci # empty exec echo hello pick 2b35e4f ci # empty exec echo hello pick 6de7a52 ci # empty exec echo hello # ... You can edit this file to run the exec command after any commit you want to. The commands will run once you save and close this file. This is a neat way to run your test suite and make sure they pass in the intermediate commits. Fin! Atomic commits ↩︎ Git rebase: what can go wrong? ↩︎ Hackernews discussion3 ↩︎ ","permalink":"http://rednafi.com/misc/on_rebasing/","publishDate":"2024-06-18","summary":"People tend to get pretty passionate about Git workflows on different online forums. Some like to rebase, while others prefer to keep the disorganized records. Some dislike the extra merge commit, while others love to preserve all the historical artifacts. There’s merit to both sides of the discussion. That being said, I kind of like rebasing because I’m a messy committer who:\nUsually doesn’t care for keeping atomic commits1. Creates a lot of short commits with messages like “fix” or “wip”. Likes to clean up the untidy commits before sending the branch for peer review. Prefers a linear history over a forked one so that git log --oneline --graph tells a nice story. Git rebase allows me to squash my disordered commits into a neat little one, which bundles all the changes with passing tests and documentation. Sure, a similar result can be emulated using git merge --squash feat_branch or GitHub’s squash-merge feature, but to me, rebasing feels cleaner. Plus, over time, I’ve subconsciously picked up the tricks to work my way around rebase-related gotchas.\n","tags":["git"],"title":"I kind of like rebasing"},{"content":"People typically associate Google’s Protocol Buffer1 with gRPC2 services, and rightfully so. But things often get confusing when discussing protobufs because the term can mean different things: A binary protocol for efficiently serializing structured data. A language used to specify how this data should be structured. In gRPC services, you usually use both: the protobuf language in proto files defines the service interface, and then the clients use the same proto files to communicate with the services. However, protobuf can be used in non-gRPC contexts for anything that requires a strict interface. You can optionally choose to use the more compact serialization format that gRPC tools offer, or just keep using JSON if you prefer. I’ve seen this use case in several organizations over the past few years, though I haven’t given it much thought. It definitely has its benefits! Defining your service contracts with protobuf: Allows you to generate message serializers and deserializers in almost any language of your choice. You can choose from a set of serialization formats. The service contracts are self-documented, and you can simply hand over the proto files to your service users. Different parts of a service or a fleet of services can be written in different languages, as long as their communication conforms to the defined contracts. For example, consider an event-driven application that sends messages to a message broker when an event occurs. A consumer then processes these messages asynchronously. Both the producer and consumer need to agree on a message format, which is defined by a contract. The workflow usually goes as follows: Define the message contract using the protobuf DSL. Generate the code for serializing/deserializing the messages in the language of your choice. On the publisher side, serialize the message using the generated code. On the consumer side, generate code from the same contract and deserialize the message with that. Define the contract You can define your service interface in a .proto file. Let’s say we want to emit some event in a search service when a user queries something. The query message structure can be defined as follows: // ./search/protos/message.proto syntax = \"proto3\"; message SearchRequest { string query = 1; int32 page_number = 2; int32 results_per_page = 3; } I’m using proto3 syntax, and you can find more about that on the official guide3. Next, you can install the gRPC tools for your preferred programming language to generate the interfacing code that’ll be used to serialize and deserialize the messages. Here’s how it looks in Python: Install grpcio-tools. Generate the interface. From the directory where your proto files live, run: python -m grpc_tools.protoc -I. \\ --python_out=contracts \\ --grpc_python_out=contracts protos/message.proto This will generate the following files in the root directory: search ├── contracts │ └── protos │ ├── message_pb2.py │ └── message_pb2_grpc.py └── protos └── message.proto Serialize and publish Once you have the contracts in place and have generated the interfacing code, here’s how you can serialize a message payload before publishing it to an event stream: # ./search/services/publish.py from contracts.protos.message_pb2 import SearchRequest def serialize(query: str, page_number: int, results_per_page: int) -\u003e str: search_request = SearchRequest( query=query, page_number=page_number, results_per_page=results_per_page ) # Serialize the search request to a compact binary string return search_request.SerializeToString() def publish(serialized_message: str) -\u003e None: # Publish the message to a message broker ... if __name__ == \"__main__\": serialized_message = serialize(\"foo bar\", 1, 5) publish(serialized_message) The code is structured in the following manner now: search ├── contracts │ ├── __init__.py │ └── protos │ ├── message_pb2.py │ └── message_pb2_grpc.py ├── protos │ └── message.proto └── services ├── __init__.py └── publish.py Deserialize and consume On the consumer side, if you have access to the proto files, you can generate the interface code again via the same commands as before and use it to deserialize the message as follows: # ./search/services/consume.py from contracts.protos.message_pb2 import SearchRequest def get_message() -\u003e str: # Let's say we get the message from a message broker and return it return b\"\\n\\x04test\\x10\\x01\\x18\\x02\" def deserialize(serialized_message: str) -\u003e SearchRequest: search_request = SearchRequest() search_request.ParseFromString(serialized_message) return search_request def consume(message: SearchRequest) -\u003e None: ... if __name__ == \"__main__\": serialized_message = get_message() search_request = deserialize(serialized_message) consume(search_request) You can even save the proto files in a common repo, generate the interfacing code automatically for multiple languages, and package them up via CI whenever some changes are merged into the main branch. Then the services can just update those protocol packages and use the serializers and deserializers as needed. Protobuf ↩︎ GRPC ↩︎ Proto3 syntax ↩︎ ","permalink":"http://rednafi.com/misc/protobuffed_contracts/","publishDate":"2024-05-10","summary":"People typically associate Google’s Protocol Buffer1 with gRPC2 services, and rightfully so. But things often get confusing when discussing protobufs because the term can mean different things:\nA binary protocol for efficiently serializing structured data. A language used to specify how this data should be structured. In gRPC services, you usually use both: the protobuf language in proto files defines the service interface, and then the clients use the same proto files to communicate with the services.\n","tags":["API","Networking"],"title":"Protobuffed contracts"},{"content":"The handful of times I’ve reached for typing.TypeGuard in Python, I’ve always been confused by its behavior and ended up ditching it with a # type: ignore comment. For the uninitiated, TypeGuard allows you to apply custom type narrowing1. For example, let’s say you have a function named pretty_print that accepts a few different types and prints them differently onto the console: from typing import assert_never def pretty_print(val: int | float | str) -\u003e None: if isinstance(val, int): # assert_type(val, int) print(f\"Integer: {val}\") elif isinstance(val, float): # assert_type(val, float) print(f\"Float: {val}\") elif isinstance(val, str): # assert_type(val, str) print(f\"String: {val}\") else: assert_never(val) If you run it through mypy, in each branch, the type checker automatically narrows the type and knows exactly what the type of val is. You can test the narrowed type in each branch with the typing.assert_type function. This works well for 99% of cases, but occasionally, you need to check an incoming value more thoroughly to determine its type and want to take action based on the narrowed type. In those cases, just using isinstance may not be sufficient. So, you need to factor out the complex type checking logic into a separate function and return a boolean depending on whether the inbound value satisfies all the criteria to be of the expected type. For example: from typing import TypedDict, TypeGuard class Person(TypedDict): name: str age: int def is_person(val: dict) -\u003e TypeGuard[Person]: try: name, age = val[\"name\"], val[\"age\"] except KeyError: return False return len(name) \u003e 1 and 0 \u003c age \u003c 150 def print_age(val: dict) -\u003e None: if is_person(val): # assert_type(val, Person) print(f\"Age: {val['age']}\") else: print(\"Not a person!\") Here, is_person first checks that the inbound dictionary conforms to the structure of the Person typeddict and then verifies that name is at least 1 character long and age is between 0 and 150. This is a bit more involved than just checking the type with isinstance and the type checker needs a little more help from the user. Although the return type of the is_person function is bool, typing it with TypeGuard[Person] signals the type checker that if the inbound value satisfies all the constraints and the checker function returns True, the underlying type of val is Person. You can see more examples of TypeGuard in PEP-6472. All good. However, I find the behavior of TypeGuard a bit unintuitive whenever I need to couple it with union types. For example: from typing import Any, TypeGuard def is_non_zero_number(val: Any) -\u003e TypeGuard[int | float]: return val != 0 def pretty_print(val: str | int | float) -\u003e None: if is_non_zero_number(val): # assert_type(val, int | float) print(f\"Non zero number: {val}\") else: # assert_type(val, str | int | float); wat?? print(f\"String: {val}\") In the if branch, TypeGuard signals the type checker correctly that the narrowed type of the inbound value is int | float but in the else branch, I was expecting it to be str because the truthy if condition has already filtered out the int | float. But instead, we get str | int | float as the narrowed type. While there might be a valid reason behind this design choice, the resulting behavior with union types made TypeGuard fairly useless for cases I wanted to use it for. TypeIs has been introduced via PEP-7423 to fix exactly that. The PEP agrees that people might find the current behavior of TypeGuard a bit unexpected and introducing another construct with a slightly different behavior doesn’t make things any less confusing. We acknowledge that this leads to an unfortunate situation where there are two constructs with a similar purpose and similar semantics. We believe that users are more likely to want the behavior of TypeIs, the new form proposed in this PEP, and therefore we recommend that documentation emphasize TypeIs over TypeGuard as a more commonly applicable tool. TypeGuard and TypeIs have similar semantics, except, the latter can narrow the type in both the if and else branches of a conditional. Here’s another example with a union type where TypeIs does what I expected TypeGuard to do: import sys if sys.version_info \u003e (3, 13): # TypeIs is available in Python 3.13+ from typing import TypeIs else: from typing_extensions import TypeIs def is_number(value: object) -\u003e TypeIs[int | float | complex]: return isinstance(value, (int, float, complex)) def pretty_print(val: str | int | float | complex) -\u003e None: if is_number(val): # assert_type(val, int | float | complex) print(val) else: # assert_type(val, str) print(\"Not a number!\") Notice that TypeIs has now correctly narrowed the type in the else branch as well. This would’ve also worked had we returned early from the pretty_print function in the if branch and skipped the else branch altogether. Exactly what I need! Here are a few typeshed stubs4 for the stdlib functions in the inspect module that are already taking advantage of the new TypeIs construct: # fmt: off def isgenerator(obj: object) -\u003e TypeIs[GeneratorType[Any, Any, Any]]: ... def iscoroutine(obj: object) -\u003e TypeIs[CoroutineType[Any, Any, Any]]: ... def isawaitable(obj: object) -\u003e TypeIs[Awaitable[Any]]: ... def isasyncgen(object: object) -\u003e TypeIs[AsyncGeneratorType[Any, Any]]: ... def istraceback(object: object) -\u003e TypeIs[TracebackType]: ... # and so on and so forth Type narrowing ↩︎ PEP 647 – User-Defined Type Guards ↩︎ PEP 742 – Narrowing types with TypeIs ↩︎ Typeshed stubs taking advantage of TypeIs ↩︎ ","permalink":"http://rednafi.com/python/typeguard_vs_typeis/","publishDate":"2024-04-27","summary":"The handful of times I’ve reached for typing.TypeGuard in Python, I’ve always been confused by its behavior and ended up ditching it with a # type: ignore comment.\nFor the uninitiated, TypeGuard allows you to apply custom type narrowing1. For example, let’s say you have a function named pretty_print that accepts a few different types and prints them differently onto the console:\nfrom typing import assert_never def pretty_print(val: int | float | str) -\u003e None: if isinstance(val, int): # assert_type(val, int) print(f\"Integer: {val}\") elif isinstance(val, float): # assert_type(val, float) print(f\"Float: {val}\") elif isinstance(val, str): # assert_type(val, str) print(f\"String: {val}\") else: assert_never(val) If you run it through mypy, in each branch, the type checker automatically narrows the type and knows exactly what the type of val is. You can test the narrowed type in each branch with the typing.assert_type function.\n","tags":["Python","Typing","TIL"],"title":"TypeIs does what I thought TypeGuard would do in Python"},{"content":"One neat use case for the HTTP ETag header is client-side HTTP caching for GET requests. Along with the ETag header, the caching workflow requires you to fiddle with other conditional HTTP headers like If-Match or If-None-Match. However, their interaction can feel a bit confusing at times. Every time I need to tackle this, I end up spending some time browsing through the relevant MDN docs123 to jog my memory. At this point, I’ve done it enough times to justify spending the time to write this. Caching the response of a GET endpoint The basic workflow goes as follows: The client makes a GET request to the server. The server responds with a 200 OK status, including the content requested and an ETag header. The client caches the response and the ETag value. For subsequent requests to the same resource, the client includes the If-None-Match header with the ETag value it has cached. The server regenerates the ETag independently and checks if the ETag value sent by the client matches the generated one. If they match, the server responds with a 304 Not Modified status, indicating that the client’s cached version is still valid, and the client serves the resource from the cache. If they don’t match, the server responds with a 200 OK status, including the new content and a new ETag header, prompting the client to update its cache. Client Server | | |----- GET Request --------------------\u003e| | | |\u003c---- Response 200 OK + ETag ----------| | (Cache response locally) | | | |----- GET Request + If-None-Match ----\u003e| (If-None-Match == previous ETag) | | | Does ETag match? | |\u003c---- Yes: 304 Not Modified -----------| (No body sent; Use local cache) | No: 200 OK + New ETag ----------| (Update cached response) | | We can test this workflow with GitHub’s REST API suite via the GitHub CLI4. If you’ve installed the CLI and authenticated yourself, you can make a request like this: gh api -i /users/rednafi This asks for the data associated with the user rednafi. The response looks as follows: HTTP/2.0 200 OK Etag: W/\"b8fdfabd59aed6e0e602dd140c0a0ff48a665cac791dede458c5109bf4bf9463\" { \"login\":\"rednafi\", \"id\":30027932, ... } I’ve truncated the response body and omitted the headers that aren’t relevant to this discussion. You can see that the HTTP status code is 200 OK and the server has included an ETag header. The W/ prefix indicates that a weak validator5 is used to validate the content of the cache. Using a weak validator means when the server compares the response payload to generate the hash, it doesn’t do it bit-by-bit. So, if your response is JSON, then changing the format of the JSON won’t change the value of the ETag header since two JSON payloads with the same content but with different formatting are semantically the same thing. Let’s see what happens if we make the same request again while passing the value of the ETag in the If-None-Match header. gh api -i -H \\ 'If-None-Match: W/\"b8fdfabd59aed6e0e602dd140c0a0ff48a665cac791dede458c5109bf4bf9463\"' \\ /users/rednafi This returns: HTTP/2.0 304 Not Modified Etag: \"b8fdfabd59aed6e0e602dd140c0a0ff48a665cac791dede458c5109bf4bf9463\" gh: HTTP 304 This means that the cached response in the client is still valid and it doesn’t need to refetch that from the server. So, the client can be coded to serve the previously cached data to the users when asked for. A few key points to keep in mind: Always wrap your ETag values in double quotes when sending them with the If-None-Match header, just as the spec says6. Using the If-None-Match header to pass the ETag value means that the client request is considered successful when the ETag value from the client doesn’t match that of the server. When the values match, the server will return 304 Not Modified with no body. If we’re writing a compliant server, when it comes to If-None-Match, the spec tells us7 to use a weak comparison for ETags. This means that the client will still be able to validate the cache with weak ETags, even if there have been slight changes to the representation of the data. If the client is a browser, it’ll automatically manage the cache and send conditional requests without any extra work. Writing a server that enables client-side caching If you’re serving static content, you can configure your load balancer to enable this caching workflow. But for dynamic GET requests, the server needs to do a bit more work to allow client-side caching. Here’s a simple server in Go that enables the above workflow for a dynamic GET request: package main import ( \"crypto/sha256\" \"encoding/hex\" \"fmt\" \"net/http\" \"strings\" ) // calculateETag generates a weak ETag by SHA-256-hashing the content // and prefixing it with W/ to indicate a weak comparison func calculateETag(content string) string { hasher := sha256.New() hasher.Write([]byte(content)) hash := hex.EncodeToString(hasher.Sum(nil)) return fmt.Sprintf(\"W/\\\"%s\\\"\", hash) } func main() { http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { // Define the content within the handler content := `{\"message\": \"Hello, world!\"}` eTag := calculateETag(content) // Remove quotes and W/ prefix for If-None-Match header comparison ifNoneMatch := strings.TrimPrefix( strings.Trim(r.Header.Get(\"If-None-Match\"), \"\\\"\"), \"W/\") // Generate a hash of the content without the W/ prefix for comparison contentHash := strings.TrimPrefix(eTag, \"W/\") // Check if the ETag matches; if so, return 304 Not Modified if ifNoneMatch == strings.Trim(contentHash, \"\\\"\") { w.WriteHeader(http.StatusNotModified) return } // If ETag does not match, return the content and the ETag w.Header().Set(\"ETag\", eTag) // Send weak ETag w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) fmt.Fprint(w, content) }) fmt.Println(\"Server is running on http://localhost:8080\") http.ListenAndServe(\":8080\", nil) } The server generates a weak ETag for its content by creating a SHA-256 hash and adding W/ to the front, indicating it’s meant for weak comparison. You could make the calculateETag function format-agnostic, so the hash stays the same if the JSON format changes but the content does not. The current calculateETag implementation is susceptible to format changes, and I kept it that way to keep the code shorter. When delivering content, the server includes this weak ETag in the response headers, allowing clients to cache the content along with the ETag. For subsequent requests, the server checks if the client has sent an ETag in the If-None-Match header and weakly compares it with the current content’s ETag by independently generating the hash. If the ETags match, indicating no significant content change, the server replies with a 304 Not Modified status. Otherwise, it sends the content again with a 200 OK status and updates the ETag. When this happens, the client knows that the existing cache is still warm and can be served without any changes to it. You can spin up the server by running go run main.go and from a different console, start making requests to it like this: curl -i http://localhost:8080/foo This will return the ETag header along with the JSON response: HTTP/1.1 200 OK Content-Type: application/json Etag: W/\"1d3b4242cc9039faa663d7ca51a25798e91fbf7675c9007c2b0470b72c2ed2f3\" Date: Wed, 10 Apr 2024 15:54:33 GMT Content-Length: 28 {\"message\": \"Hello, world!\"} Now, you can make another request with the value of the ETag in the If-None-Match header: curl -i -H \\ 'If-None-Match: \"1d3b4242cc9039faa663d7ca51a25798e91fbf7675c9007c2b0470b72c2ed2f3\"' \\ http://localhost:8080/foo This will return a 304 Not Modified response with no body: HTTP/1.1 304 Not Modified Date: Wed, 10 Apr 2024 15:57:25 GMT In a real-life scenario, you’ll probably factor out the caching part in middleware so that all of your HTTP GET requests can be cached from the client-side without repetition. One thing to look out for While writing a cache-enabled server, make sure the system is set up so that the server always sends back the same ETag for the same content, even when there are multiple servers working behind a load balancer. If these servers give out different ETags for the same content, it can mess up how clients cache that content. Clients use ETags to decide if content has changed. If the ETag value hasn’t changed, they know the content is the same and don’t download it again, saving bandwidth and speeding up access. But if ETags are inconsistent across servers, clients might download content they already have, wasting bandwidth and slowing things down. This inconsistency also means servers end up dealing with more requests for content that clients could have just used from their cache if ETags were consistent. Etag - MDN ↩︎ If-Match - MDN ↩︎ If-None-Match - MDN ↩︎ GitHub CLI ↩︎ Weak validation ↩︎ Double quote in conditional header values ↩︎ Use weak comparison for Etags while caching ↩︎ ","permalink":"http://rednafi.com/misc/etag_and_http_caching/","publishDate":"2024-04-10","summary":"One neat use case for the HTTP ETag header is client-side HTTP caching for GET requests. Along with the ETag header, the caching workflow requires you to fiddle with other conditional HTTP headers like If-Match or If-None-Match. However, their interaction can feel a bit confusing at times.\nEvery time I need to tackle this, I end up spending some time browsing through the relevant MDN docs123 to jog my memory. At this point, I’ve done it enough times to justify spending the time to write this.\n","tags":["API","Go"],"title":"ETag and HTTP caching"},{"content":"Every once in a while, I find myself skimming through the MDN docs to jog my memory on how CORS1 works and which HTTP headers are associated with it. This is particularly true when a frontend app can’t talk to a backend service I manage due to a CORS error2.\nMDN’s CORS documentation is excellent but can be a bit verbose for someone just looking for a way to quickly troubleshoot and fix the issue at hand.\nTypically, the CORS issue I encounter boils down to:\nA backend service that accepts requests only from a list of specified domains. A new frontend service or some other client trying to access it from a different domain that’s not on the server’s allowlist. Consequently, the server rejects it with an HTTP 4xx error. Here’s a list of some commonly found headers associated with CORS:\nRequest headers\nOrigin: indicates the origin of the request Access-Control-Request-Method: used in preflight3 to specify the method of the actual request Access-Control-Request-Headers: used in preflight to specify headers that will be used in the actual request Response headers\nAccess-Control-Allow-Origin: specifies the origins that are allowed to access the resource Access-Control-Allow-Methods: indicates the methods allowed when accessing the resource Access-Control-Allow-Headers: specifies the headers that can be included in the actual request Access-Control-Allow-Credentials: indicates whether or not the response can be exposed when the credentials flag is true Access-Control-Expose-Headers: specifies the headers that can be exposed as part of the response Access-Control-Max-Age: indicates how long the results of a preflight request can be cached In most cases, focusing on the Origin and Access-Control-Allow-Origin headers is enough to verify whether a service can be reached from a certain domain without running into a CORS error.\nTo recreate the canonical CORS issue, here’s a simple server written in Go that exposes a single POST endpoint /hello. You can make a POST request to it with the {\"name\": \"Something\"} payload, and it will echo back with a JSON message.\nClick here ... // main.go package main import ( \"encoding/json\" \"fmt\" \"net/http\" ) // Person struct to parse the input JSON. type Person struct { Name string `json:\"name\"` } // helloNameHandler responds with \"Hello {name}\". func helloNameHandler(w http.ResponseWriter, r *http.Request) { if r.Method != \"POST\" { http.Error(w, \"Only POST method is allowed\", http.StatusMethodNotAllowed) return } var p Person if err := json.NewDecoder(r.Body).Decode(\u0026p); err != nil { http.Error(w, err.Error(), http.StatusBadRequest) return } response := fmt.Sprintf(\"Hello %s\", p.Name) w.Header().Set(\"Content-Type\", \"application/json\") json.NewEncoder(w).Encode(map[string]string{\"message\": response}) } // corsMiddleware adds CORS headers to the response. func corsMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { allowedOrigins := map[string]bool{ \"http://allowed-origin-1.com\": true, \"http://allowed-origin-2.com\": true, } origin := r.Header.Get(\"Origin\") if _, ok := allowedOrigins[origin]; ok { w.Header().Set(\"Access-Control-Allow-Origin\", origin) } else { // Optional: Handle not allowed origin, e.g., by returning an error. http.Error(w, \"Origin not allowed\", http.StatusForbidden) return } w.Header().Set(\"Access-Control-Allow-Methods\", \"POST, OPTIONS\") w.Header().Set(\"Access-Control-Allow-Headers\", \"Content-Type\") // Handle preflight request. if r.Method == \"OPTIONS\" { w.WriteHeader(http.StatusOK) return } next.ServeHTTP(w, r) }) } func main() { mux := http.NewServeMux() mux.Handle(\"/hello\", corsMiddleware(http.HandlerFunc(helloNameHandler))) fmt.Println(\"Server is running on http://localhost:7676\") http.ListenAndServe(\":7676\", mux) } Here, the server only allows requests from two particular domains: http://allowed-origin-1.com and http://allowed-origin-2.com. A client on another host can make a preflight OPTIONS request to check if the server will permit the subsequent POST request.\nIf the client is on a domain that’s not on the allowlist, the server will reject the request.\nYou can run the server with go run main.go and then, from another console, try making a preflight request without specifying the Origin header:\ncurl -i -X OPTIONS http://localhost:7676/hello The server will reject this:\nHTTP/1.1 403 Forbidden Content-Type: text/plain; charset=utf-8 X-Content-Type-Options: nosniff Date: Tue, 12 Mar 2024 21:52:26 GMT Content-Length: 19 Origin not allowed You need to specify one of the domains expected by the server via the Origin header as follows:\ncurl -i -X OPTIONS http://localhost:7676/hello \\ -H 'Origin: http://allowed-origin-1.com' This time, the preflight request will succeed:\nHTTP/1.1 200 OK Access-Control-Allow-Headers: Content-Type Access-Control-Allow-Methods: POST, OPTIONS Access-Control-Allow-Origin: http://allowed-origin-1.com Date: Tue, 12 Mar 2024 21:54:57 GMT Content-Length: 0 Notice that the Access-Control-Allow-Methods header also specifies the methods allowed on this endpoint.\nIf you make a preflight request with an origin not on the server’s allowlist, you will encounter a 4xx error again.\ncurl -i -X OPTIONS http://localhost:7676/hello -H 'Origin: http://notallowed.com' The return message indicates that requests from http://notallowed.com are blocked by CORS control:\nHTTP/1.1 403 Forbidden Content-Type: text/plain; charset=utf-8 X-Content-Type-Options: nosniff Date: Tue, 12 Mar 2024 21:57:06 GMT Content-Length: 19 Origin not allowed Similarly, making a POST request without sending the expected origin in the header will result in a 4xx error.\ncurl -i -X POST http://localhost:7676/hello --data '{\"name\": \"Foo\"}' This returns:\nHTTP/1.1 403 Forbidden Content-Type: text/plain; charset=utf-8 X-Content-Type-Options: nosniff Date: Tue, 12 Mar 2024 21:59:31 GMT Content-Length: 19 Origin not allowed Like the preflight request, you need to pass the expected origin in the header.\nSo, if your frontend cannot access the backend and the browser console indicates that CORS control is blocking the request, you’ll likely need to add the new domain to your server’s allowlist. Then make sure that the client is passing the desired origin in the header. In the case of a browser, this should be automatically handled for you.\nUse the preflight request commands to test that the server is only allowing access from the whitelisted domain while blocking everything else.\nCORS - mdn web docs ↩︎\nCORS error ↩︎\nPreflight request ↩︎\n","permalink":"http://rednafi.com/misc/crossing_the_cors_crossroad/","publishDate":"2024-03-12","summary":"Every once in a while, I find myself skimming through the MDN docs to jog my memory on how CORS1 works and which HTTP headers are associated with it. This is particularly true when a frontend app can’t talk to a backend service I manage due to a CORS error2.\nMDN’s CORS documentation is excellent but can be a bit verbose for someone just looking for a way to quickly troubleshoot and fix the issue at hand.\n","tags":["Networking","Go"],"title":"Crossing the CORS crossroad"},{"content":"Ever since Rob Pike published the text on the functional options pattern1, there’s been no shortage of blogs, talks, or comments on how it improves or obfuscates configuration ergonomics.\nWhile the necessity of such a pattern is quite evident in a language that lacks default arguments in functions, more often than not, it needlessly complicates things. The situation gets worse if you have to maintain a public API where multiple configurations are controlled in this manner.\nHowever, the pattern solves a valid problem and definitely has its place. Otherwise, it wouldn’t have been picked up by pretty much every other library23.\nIf you have no idea what I’m talking about, you might want to give my previous write-up on configuring options4 a read.\nFunctional options pattern As a recap, here’s how the functional options pattern works. Let’s say, you need to allow the users of your API to configure something. You can expose a struct from your package that’ll be passed to some other function to tune its behavior. For example:\npackage src type Config struct { // Required Foo, Bar string // Optional Fizz, Bazz int } func Do(config *Config) { // Do something with the config values } Then the Config struct will be imported, initialized, and passed to the Do function by your API users:\npackage main import \".../src\" func main() { // Initialize the config and pass it to the Do function config := \u0026src.Config{ Foo: \"hello\", Bar: \"world\", Fizz: 0, Bazz: 42, } // Call Do with the initialized Config struct src.Do(config) } This is one way of doing that, but it’s generally discouraged since it requires you to expose the internals of your API to the users. So instead, a library usually exposes a factory function that’ll do the struct initialization while keeping the struct and fields private. For instance:\npackage src type config struct { // Notice that the struct and fields are now private // Required foo, bar string // Optional fizz, bazz int } // Public factory function func NewConfig(foo, bar string, fizz, bazz int) config { return config{foo, bar, fizz, bazz} } func Do(c *config){} The API consumers will now use NewConfig to produce a configuration and then pass the struct instance to the Do function as follows:\npackage main import \".../src\" func main() { // Initialize the config with the NewConfig factory c := \u0026src.NewConfig(\"hello\", \"world\", 0, 42) // Call Do with the initialized Config struct src.Do(c) } This approach is better as it keeps the internal machinery hidden from users. However, it doesn’t allow for setting default values for some configuration attributes; all must be set explicitly. What if your users want to override the values of multiple attributes? This leads your configuration struct to be overloaded with options, making the NewConfig function demands numerous positional arguments.\nThis setup isn’t user-friendly, as it forces API users to explicitly pass all these options to the NewConfig factory. Ideally, you’d initialize config with some default values, offering users a chance to override them. But, Go doesn’t support default values for function arguments, which compels us to be creative and come up with different workarounds. Functional options pattern is one of them.\nHere’s how you can build your API to leverage the pattern:\npackage src type config struct { // Required foo, bar string // Optional fizz, bazz int } type option func(*config) // The value of each optional configuration attribute can be overridden with // an associated function func WithFizz(fizz int) option { return func(c *config) { c.fizz = fizz } } func WithBazz(bazz int) option { return func(c *config) { c.bazz = bazz } } func NewConfig(foo, bar string, opts ...option) config { // First fill in the options with default values c := config{foo, bar, 10, 100} // Now allow users to override the optional configuration attributes for _, opt := range opts { opt(\u0026c) } return c } func Do(c *config) {} Then you’d use it as follows:\npackage main import \".../src\" func main() { c := \u0026src.NewConfig(\"hello\", \"world\", src.WithFizz(1), src.WithBazz(2)) src.Do(c) } Functional options pattern relies on functions that modify the configuration struct’s state. These modifier functions, or option functions, are defined to accept a pointer to the configuration struct *config and then directly alter its fields. This direct manipulation is possible because the option functions are closures, which means they capture and modify the variables from their enclosing scope, in this case, the config instance.\nIn the NewConfig factory, the variadic parameter opts ...option allows for an arbitrary number of option functions to be passed. Here, opts represents the optional configurations that the users can override if they want to.\nThe NewConfig function iterates over this slice of option functions, invoking each one with the \u0026c argument, which is a pointer to the newly created config instance. The config instance is created with default values, and the users can use the With* functions to override them.\nCurse of indirection That’s a fair bit of indirection just to allow API users to configure some options. I don’t know about you, but multi-layered higher-order functions hurt my brain. It’s quite slow as well.\nAll this complexity could have been avoided if Go allowed default arguments in functions. Your configuration factory could simply grab the default values from the keyword arguments and pass them to the underlying struct. The idea that supporting default arguments in functions would lead to a parameter explosion seems unfounded, especially when the alternative requires gymnastics like the functional option pattern.\nAlso, the multiple layers of indirection hinder API discoverability. Trying to discover modifier functions by hovering your cursor over the factory function’s return value in the IDE won’t be very helpful, as these functions are defined at the package level.\nSo, if you need to configure multiple structs in this manner, the explosion of their respective package-level modifiers make it even harder for the user to know which function they’ll need to use to update a certain configuration attribute.\nRecently, I’ve spontaneously stumbled upon a fluent-style API to manage configurations that don’t require so many layers of indirection and lets you expose optional configuration attributes. Let’s call it dysfunctional options pattern.\nDysfunctional options pattern The idea is quite similar to how the API with functional options pattern is constructed. Here’s the complete implementation:\npackage src type config struct { // Required foo, bar string // Optional fizz, bazz int } // Each optional configuration attribute will have its own public method func (c *config) WithFizz(fizz int) *config { c.fizz = fizz return c } func (c *config) WithBazz(bazz int) *config { c.bazz = bazz return c } // This only accepts the required options as params func NewConfig(foo, bar string) *config { // First fill in the options with default values return \u0026config{foo, bar, 10, 100} } func Do(c *config) {} You’d use the API as follows:\npackage main import \".../src\" func main() { // Initialize the struct with only the required options and then chain // the option methods to update the optional configuration attributes c := src.NewConfig(\"hello\", \"world\").WithFizz(0).WithBazz(42) src.Do(c) } Similar to the previous pattern, we have modifiers here too. However, instead of being higher order functions, the modifiers are methods on config and return a pointer to the struct.\nThe NewConfig factory function instantiates the config struct with some default values and returns the struct pointer like the modifiers. This enables us to chain the WithFizz and WithBazz modifiers on the returned value of NewConfig and update the values of the optional configuration attributes.\nApart from simplicity and the lack of magic, you can hover over the return type of the factory and immediately know about the supported modifier methods.\nI did a rudimentary benchmark5 of the two approaches and was surprised that the second one was roughly ~76x faster on Go 1.22!\nHere’s an example6 of the pattern in the wild.\nP.S. This is indeed a lightweight spin on what OO languages call the builder pattern. However, I didn’t call it that because there’s no mandatory .Build() method to be called at the end of the method chain.\nSelf-referential functions and the design of options ↩︎\nFunctional options pattern in ngrok ↩︎\nFunction options pattern in elastic search agent ↩︎\nConfiguring options in Go ↩︎\nBenchmarking functional vs dysfunctional options pattern ↩︎\nDysfunctional options pattern in the wild ↩︎\n","permalink":"http://rednafi.com/go/dysfunctional_options_pattern/","publishDate":"2024-03-06","summary":"Ever since Rob Pike published the text on the functional options pattern1, there’s been no shortage of blogs, talks, or comments on how it improves or obfuscates configuration ergonomics.\nWhile the necessity of such a pattern is quite evident in a language that lacks default arguments in functions, more often than not, it needlessly complicates things. The situation gets worse if you have to maintain a public API where multiple configurations are controlled in this manner.\n","tags":["Go"],"title":"Dysfunctional options pattern in Go"},{"content":"In 9th grade, when I first learned about Lenz’s Law1 in Physics class, I was fascinated by its implications. It states:\nThe direction of an induced current will always oppose the motion causing it.\nIn simpler terms, imagine you have a hoop and a magnet. If you move the magnet close to the hoop, the hoop generates a magnetic field that pushes the magnet away. Conversely, if you pull the magnet away, the hoop creates a magnetic field that pulls it back. This occurs because the hoop aims to prevent any change in the surrounding magnetic field. That’s Lenz’s Law: the hoop consistently acts to maintain the magnetic field’s status quo, reacting against the motion that’s the cause of the existence of the magnetic flux in the first place. Generators leverage this principle to convert mechanical motion into electrical energy.\nIn my experience, knowledge works like this as well. The very thing that made you good at what you do often prevents you from getting better.\nThe first decent program I ever wrote was a simulation in MATLAB during my sophomore year. Since then, I’ve switched career gears a few times and picked up a few programming languages along the way. The weird thing is, over the years, learning new paradigms hasn’t become any easier than I thought it’d be. I only got better at grokking technologies that are woefully similar to what I already know.\nSure, picking up a new tool or library in a familiar language has become second nature, but so has the tendency to keep doing the same thing I’m somewhat good at, slightly differently, and falling into the false lull of growth. This can be perilous in the sense that, unless you actively fight it, after a decade, you might find yourself in a situation where you actually have only a year’s worth of experience repeated ten times.\nI’ve been looking for a phrase to label the dilemma where the curse of knowledge, accumulated over time, prevents you from acquiring new knowledge and adopting new ways of thinking. Turns out, there’s one for that—the Einstellung effect2. Einstellung is a German word for “attitude” or “setting.” It describes a situation where we stick to a familiar way of thinking or solving problems, which can stop us from seeing or seeking easier or better solutions. This only gets worse as we start growing older or becoming more experienced in a narrow slice of a highly specialized domain.\nTo put it more concretely, when I learned my first proper programming language, Python, it was much easier for me to pick up the syntax, semantics, and culture around it since all that knowledge was basically getting dumped into an empty slate. When I had to pick up some JavaScript for work, it was still relatively easy since I started to compare the features of the language with the one I knew and worked my way up. However, the time it took for me to get comfortable in the JS world wasn’t that short since I was begrudgingly resisting learning about the warts of the language and complaining about how nice Python’s type system was compared to this mess!\nIt was only when I started to learn Go that I became aware of the warts of Python and when it just wasn’t the right tool to solve the problem at hand. Despite Go’s fame for being simple, I had to read a few books, solve a few koans, and build a few tools before I got confident in solving problems with it. One reason why it took longer is because I was trying to emulate Python idioms inside Go while at the same time bashing people who’d write Go like Java, remaining blissfully unaware of my own stance. In each of these instances, my accumulated experience and acquired predispositions actively resisted adopting different ways of solving problems.\nIt’s natural for us to acquire new skills by mapping them with what we already know, and that’s true for any skill—whether it’s learning to cook a new dish or writing in an unfamiliar programming language. But at the same time, as we grow older, we actively need to strain against the tide to learn to learn like kids. Because without it, we are nothing but simulacra acting as conscious exotica3, spellbound by the allure of mimicry!\nLenz’s law ↩︎\nEinstellung effect ↩︎\nSimulacra acting as conscious exotica ↩︎\n","permalink":"http://rednafi.com/zephyr/einstellung_effect/","publishDate":"2024-02-24","summary":"In 9th grade, when I first learned about Lenz’s Law1 in Physics class, I was fascinated by its implications. It states:\nThe direction of an induced current will always oppose the motion causing it.\nIn simpler terms, imagine you have a hoop and a magnet. If you move the magnet close to the hoop, the hoop generates a magnetic field that pushes the magnet away. Conversely, if you pull the magnet away, the hoop creates a magnetic field that pulls it back. This occurs because the hoop aims to prevent any change in the surrounding magnetic field. That’s Lenz’s Law: the hoop consistently acts to maintain the magnetic field’s status quo, reacting against the motion that’s the cause of the existence of the magnetic flux in the first place. Generators leverage this principle to convert mechanical motion into electrical energy.\n","tags":["Essay"],"title":"Einstellung effect"},{"content":"These days, I don’t build hierarchical types through inheritance even when writing languages that support it. Type composition has replaced almost all of my use cases where I would’ve reached for inheritance before. I’ve written1 about how to escape the template pattern2 hellscape and replace that with strategy pattern3 in Python before. While by default, Go saves you from shooting yourself in the foot by disallowing inheritance, it wasn’t obvious to me how I could apply the strategy pattern to make things more composable and testable. Also, often the Go community exhibits a knee-jerk reaction to the word “pattern,” even when it has nothing to do with OO. However, I feel it’s important to use a specific term while explaining a concept, and I’d rather not attempt to relabel a concept when an established term already exists for it. Just a quick recap: the strategy pattern is a design approach where you can choose from a set of methods to solve a problem, each method wrapped in its own class. This way, you can swap out these methods easily without messing with the rest of your code, making it simple to adjust behaviors on the fly. Let’s say you’re writing a display service that prints a message in either plain text or JSON formats. Imperatively you could do this: # main.rb require 'json' def display(message, format) if format == :text puts message elsif format == :json json_output = { message: message }.to_json puts json_output else puts \"Unknown format\" end end # Usage display(\"Hello, World!\", :text) # Prints \"Hello, World!\" display(\"Hello, World!\", :json) # Prints \"{\"message\":\"Hello, World!\"}\" While this is a trivial example, you can see that adding more formats means we’ll need to extend the conditionals in the display function, and this gets out of hand pretty quickly in many real-life situations where you might have a non-trivial amount of cases. However, the biggest reason why the imperative solution isn’t ideal is because of how difficult it is to test. Imagine each of the conditionals triggers some expensive side effects when the corresponding block runs. How’d you test display then in an isolated manner without mocking the whole universe? Strategy pattern tells us that each conditional can be converted into a class with one method. We call these classes strategies. Then, we initialize these strategy classes at runtime and explicitly pass the instances to the display function. The function knows how to use the strategy instances and executes a specific strategy to print a message in a particular format based on a certain condition. Here’s how you could rewrite the previous example. In the first phase, we’ll wrap each formatter in a separate class: # main.rb require 'json' # Formatter Interface class MessageFormatter def output(message) raise NotImplementedError, \"This method should be overridden\" end end # Concrete Formatter for Text class TextFormatter \u003c MessageFormatter def output(message) message end end # Concrete Formatter for JSON class JsonFormatter \u003c MessageFormatter def output(message) { message: message }.to_json end end Here, the TextFormatter and JsonFormatter classes implement the MessageFormatter interface. This interface requires the downstream classes to implement the output method. The output methods of the respective formatters know how to format and print the messages. The display function simply takes a message and a formatter, and calls formatter.output(message) without knowing anything about what the formatter does. # main.rb # Display Function with direct unknown format handling def display(message, formatter) unless formatter.is_a?(MessageFormatter) puts \"Unsupported format\" return end output = formatter.output(message) puts output end Finally, at runtime, you can instantiate the strategy classes and pass them explicitly to the display function as necessary: # main.rb require_relative 'formatter' text_formatter = TextFormatter.new json_formatter = JsonFormatter.new display(\"Hello, World!\", text_formatter) # Prints \"Hello, World!\" display(\"Hello, World!\", json_formatter) # Prints \"{\"message\":\"Hello, World!\"}\" Now whenever you need to test the display function, you can just create a fake formatter and pass that as an argument. The display function will happily accept any formatter as long as the strategy class satisfies the MessageFormatter interface. The same thing can be achieved in a more functional4 manner and we’ll see that in the Go example. But Ruby is still primarily an OO language and it has classes. How’d you model the same solution in a language like Go where there’s no concept of a class or explicit interface implementation? This wasn’t clear to me from the get-go until I started playing with the language a little more and digging through some OSS codebases. Turns out, in Go, you can do the same thing using interfaces and custom types, and with even fewer lines of code. Here’s how: // main.go // Formatter interface defines a method for outputting messages type Formatter interface { Output(message string) string } // OutputFunc is a function type that matches the signature of the Output // method in the Formatter interface type OutputFunc func(message string) string // Output method makes OutputFunc satisfy the Formatter interface func (f OutputFunc) Output(message string) string { return f(message) } Above, we’re defining a Formatter interface that contains only a single method Output. Then we define an OutputFunc type that implements the Output method on the function to satisfy the Formatter interface. We could opt in for a struct type here instead of defining a function type but since we don’t need to hold any state, a function type keeps things concise. The display function will look as follows: func Display(message string, format Formatter) { fmt.Println(format.Output(message)) } Similar to the Ruby example, Display intakes a string message and an object of any type that implements the Formatter interface. Next, it calls the Output method on format without having any knowledge of what that does, achieving polymorphism. Also, notice that we aren’t handling the “unknown formatter” case explicitly because now it’ll be a compile-time error if an unknown formatter is passed to the caller. Next, you’ll define your strategies and pass them to the Display function as follows: func main() { message := \"Hello, World!\" // Each strategy needs to be wrapped in OutputFunc so that the // underlying function satisfies the Formatter interface. TextFormatted := OutputFunc(func (message string) string { return message }) JSONFormatted := OutputFunc(func (message string) string { jsonData, _ := json.Marshal(map[string]string{\"message\": message}) return string(jsonData) }) Display(message, TextFormatted) // Prints \"Hello, World!\" Display(message, JSONFormatted) // Prints \"{\"message\":\"Hello, World!\"}\" } We’re defining each formatting strategy as a function and casting it to the OutputFunc so that it satisfies the Formatter interface. Then we just pass the message and the strategy instance to the Display function as before. Notice how your data and strategies are also decoupled in this case; one has no knowledge of the existence of the other. And voila, you’re done! Update: The original Go example used struct types rather than a function type to meet the Formatter interface requirements. In this particular case, the function type makes things simpler. However, if your strategy needs to do multiple things, then a struct with multiple methods is probably going to be better. Escaping the template pattern hellscape in Python ↩︎ Template method pattern in Ruby ↩︎ Strategy pattern in Ruby ↩︎ All of these festivities can be avoided in languages that support first-class functions. You could just define your strategies as functions and pass them to the caller during runtime; same idea, different implementation. ↩︎ ","permalink":"http://rednafi.com/go/strategy_pattern/","publishDate":"2024-02-17","summary":"These days, I don’t build hierarchical types through inheritance even when writing languages that support it. Type composition has replaced almost all of my use cases where I would’ve reached for inheritance before.\nI’ve written1 about how to escape the template pattern2 hellscape and replace that with strategy pattern3 in Python before. While by default, Go saves you from shooting yourself in the foot by disallowing inheritance, it wasn’t obvious to me how I could apply the strategy pattern to make things more composable and testable.\n","tags":["Go","TIL"],"title":"Strategy pattern in Go"},{"content":"While I like Go’s approach of treating errors as values as much as the next person, it inevitably leads to a situation where there isn’t a one-size-fits-all strategy for error handling like in Python or JavaScript.\nThe usual way of dealing with errors entails returning error values from the bottom of the call chain and then handling them at the top. But it’s not universal since there are cases where you might want to handle errors as early as possible and fail catastrophically. Yet, it’s common enough that we can use it as the base of our conversation.\nThis simple but verbose error handling works okay and makes us painfully aware of all the possible error paths. Yet, the model doesn’t hold up as your program grows in scope and complexity, forcing you to devise custom patterns to add context and build thin stack traces. There’s no avoiding that.\nBut the good thing is that building an emaciated stack trace is fairly straightforward, and some of the patterns are quite portable. After reading Rob Pike’s blog on error handling in the upspin1 project, I had some ideas on creating custom errors to emulate stack traces. I ended up spending a few hours this morning experimenting with some of the ideas in a more limited scope.\nLet’s say we’re building a file-copy service that will accept a src and dst path and copy the contents from source to destination.\nfunc copyFile(src, dst string) error { // Open the source file for reading srcFile, err := os.Open(src) if err != nil { return err } defer srcFile.Close() // Create the destination file for writing dstFile, err := os.Create(dst) if err != nil { return err } defer dstFile.Close() // Copy the contents from source to destination file _, err = io.Copy(dstFile, srcFile) if err != nil { return err } // Ensure that the destination file's content is successfully written err = dstFile.Sync() if err != nil { return err } return nil } This typical error handling pattern involves returning error values from lower-level functions and addressing them in top-level ones. Here, the main function manages the error:\nfunc main() { // Define the source and destination file paths src := \"path/to/source/file\" dst := \"path/to/destination/file\" // Call copyFile and handle any errors err := copyFile(src, dst) if err != nil { fmt.Fprintf(os.Stderr, \"Error copying file: %s\\n\", err) os.Exit(1) } fmt.Println(\"File copied successfully.\") } Running this function gives us the following output:\nError copying file: open path/to/source/file: no such file or directory exit status 1 This is usually enough if you’re building a CLI or a small program. Also, squinting at the error message gives us a hint that among the 4 error-return paths, the copyFile function bailed at the first one when it couldn’t find the source file.\nA proper way to handle this in larger applications is to wrap the errors and provide them with your own context. Then, in the top-level function, you can unwrap the error message or just log it verbatim as before. So, copyFile can be rewritten as follows:\nfunc copyFile(src, dst string) error { srcFile, err := os.Open(src) if err != nil { return fmt.Errorf(\"cannot open source file: %w\", err) } defer srcFile.Close() dstFile, err := os.Create(dst) if err != nil { return fmt.Errorf(\"cannot create destination file: %w\", err) } defer dstFile.Close() _, err = io.Copy(dstFile, srcFile) if err != nil { return fmt.Errorf(\"cannot copy file contents: %w\", err) } err = dstFile.Sync() if err != nil { return fmt.Errorf(\"cannot sync destination file: %w\", err) } return nil } Notice how we’re adding extra context to the error values with the %w verb in the fmt.Errorf function.\nIf you keep the previous main function unchanged and run it, you’ll get the following output:\nError copying file: cannot open source file: open path/to/source/file: no such file or directory exit status 1 This time, since you know where you added the context, you also know which error-path the copyFile function returned from. However, even in this case, the main function just relays whatever comes out of copyFile and logs the error message.\nHow would you make the error message prettier without losing context? Also, how would you attach file names and line numbers to make debugging easier?\nThe debugging part isn’t an issue in languages that support stack traces, this is usually taken care of automatically. Now, whether that’s a good thing or a bad thing is a discussion for another day.\nWe can define a custom error struct to represent a generic error in the package that houses copyFile.\ntype Error struct { Op string Path string LineNo int FileName string Err error Debug bool } func (e *Error) Error() string { if e.Debug { return fmt.Sprintf( \"%s: %s: %s\\n\\t%s:%d\", e.Op, e.Path, e.Err, e.FileName, e.LineNo, ) } msg := e.Err.Error() msgs := strings.Split(msg, \":\") msg = strings.TrimSpace(msgs[len(msgs)-1]) return fmt.Sprintf(\"%s: %s: \\n\\t%s\", e.Op, e.Path, msg) } Inside the Error struct, Op represents the name of the function that the error originates from, Path is the file path, LineNo and FileName denote the precise location of the error, Err is the original error we’re wrapping, and finally the debug boolean is be used to control the verbosity of error messages.\nThen the Error() method on the struct builds either a rudimentary stack trace or a prettier error message depending on the value of the Debug flag. The Error struct can be constructed with the following constructor function:\nvar Debug bool // Flag to control output verbosity func NewError(op string, path string, err error) *Error { _, file, line, ok := runtime.Caller(1) if !ok { file = \"???\" line = 0 } return \u0026Error{ Op: op, Path: path, LineNo: line, FileName: file, Err: err, Debug: Debug, // Populate from the global flag } } This uses the runtime package to add the location data of the caller. It’ll be called in the copyFile function as follows:\nfunc copyFile(src, dst string) error { // Open the source file for reading srcFile, err := os.Open(src) if err != nil { return NewError(\"os.Open\", src, err) } defer srcFile.Close() // Create the destination file for writing dstFile, err := os.Create(dst) if err != nil { return NewError(\"os.Create\", dst, err) } defer dstFile.Close() // Copy the contents from source to destination file _, err = io.Copy(dstFile, srcFile) if err != nil { return NewError(\"io.Copy\", dst, err) } // Ensure that the destination file's content is successfully written err = dstFile.Sync() if err != nil { return NewError(\"dstFile.Sync\", dst, err) } return nil } You can turn on the Debug flag to print the stack trace in the main function:\nfunc main() { src := \"/path/to/source/file\" dst := \"/path/to/destination/file\" Debug = true // Set the Debug flag err := copyFile(src, dst) if err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } fmt.Println(\"File copied successfully.\") } The output will be:\nos.Open: /path/to/source/file: open /path/to/source/file: no such file or directory /Users/rednafi/canvas/rednafi.com/main.go:54 exit status 1 Toggling Debug to false and running the snippet will return:\nos.Open: /path/to/source/file: no such file or directory exit status 1 You can add even more context to this error in different calling locations like this:\nsrcFile, err := os.Open(src) if err != nil { return fmt.Errorf( \"more context: %w\", NewError(\"os.Open\", src, err), ) } It’ll be pretty-printed like this when Debug is false:\nmore ctx: os.Open: /path/to/source/file: no such file or directory exit status 1 Now depending on your needs, you can customize the Error struct and NewError constructor to enable more elaborate error tracing.\nHowever, this isn’t a proper stack in the sense that it only unwinds errors one level deep. But it can be extended to recursively build the full error trace if needed. The Upspin2 repo demonstrates a few techniques on how to do so. But for this particular case, anything more than a level deep stack is borderline overkill.\nHere’s the complete working example3.\nFin!\nError handling in Upspin ↩︎\nUpspin’s error package ↩︎\nComplete example ↩︎\n","permalink":"http://rednafi.com/go/anemic_stack_traces/","publishDate":"2024-02-10","summary":"While I like Go’s approach of treating errors as values as much as the next person, it inevitably leads to a situation where there isn’t a one-size-fits-all strategy for error handling like in Python or JavaScript.\nThe usual way of dealing with errors entails returning error values from the bottom of the call chain and then handling them at the top. But it’s not universal since there are cases where you might want to handle errors as early as possible and fail catastrophically. Yet, it’s common enough that we can use it as the base of our conversation.\n","tags":["Go"],"title":"Anemic stack traces in Go"},{"content":"I used reach for reflection whenever I needed a Retry function in Go. It’s fun to write, but gets messy quite quickly. Here’s a rudimentary Retry function that does the following: It takes in another function that accepts arbitrary arguments. Then tries to execute the wrapped function. If the wrapped function returns an error after execution, Retry attempts to run the underlying function n times with some backoff. The following implementation leverages the reflect module to achieve the above goals. We’re intentionally avoiding complex retry logic for brevity: func Retry( fn interface{}, args []interface{}, maxRetry int, startBackoff, maxBackoff time.Duration) ([]reflect.Value, error) { fnVal := reflect.ValueOf(fn) if fnVal.Kind() != reflect.Func { return nil, errors.New(\"retry: function type required\") } argVals := make([]reflect.Value, len(args)) for i, arg := range args { argVals[i] = reflect.ValueOf(arg) } for attempt := 0; attempt \u003c maxRetry; attempt++ { result := fnVal.Call(argVals) errVal := result[len(result)-1] if errVal.IsNil() { return result, nil } if attempt == maxRetry-1 { return result, errVal.Interface().(error) } time.Sleep(startBackoff) if startBackoff \u003c maxBackoff { startBackoff *= 2 } fmt.Printf( \"Retrying function call, attempt: %d, error: %v\\n\", attempt+1, errVal, ) } return nil, fmt.Errorf(\"retry: max retries reached without success\") } The Retry function uses reflection to call a function passed as an interface{}. It handles the function’s arguments, which are given in an interface{} slice. This approach allows us to run functions with varied signatures. Reflection, using reflect.ValueOf(fn).Call(argVals), dynamically invokes the target function. It converts its arguments from interface{} to reflect.Value types. Within the retry logic, it tries up to maxRetry times, using exponential backoff to set the delay between retries. The delay begins at startBackoff, doubles after each failure, and is limited by maxBackoff to avoid long waits. The function looks for errors in the last return value of the called function. If it finds an error and there are retries left, it waits for the backoff period before trying again. Otherwise, it gives up with an error message. You can wrap a dummy function that always returns an error to see how Retry works: func main() { someFunc := func(a, b int) (int, error) { fmt.Printf(\"Function called with a: %d and b: %d\\n\", a, b) return 42, errors.New(\"some error\") } result, err := Retry( someFunc, []interface{}{42, 100}, 3, 1*time.Second, 4*time.Second, ) if err != nil { fmt.Println(\"Function execution failed:\", err) return } fmt.Println(\"Function executed successfully:\", result[0]) } Running it will give you the following output: Function called with a: 42 and b: 100 Retrying function call, attempt: 1, error: some error Function called with a: 42 and b: 100 Retrying function call, attempt: 2, error: some error Function called with a: 42 and b: 100 Function execution failed: some error This isn’t too terrible for a reflection-infested code snippet. However, now that Go has generics, I wanted to see if I could leverage that to avoid metaprogramming. While reflection is powerful, it’s quite easy to run buggy code that causes runtime panics. Plus, the compiler can’t do many of the type checks when the underlying code leverages the dynamic features. Turns out, there’s a way to write the same functionality with generics if you don’t mind trading off some flexibility for shorter and more type-safe code. Here’s how: // Define a generic function type that can return an error type Func[T any] func() (T, error) func Retry[T any]( fn Func[T], args []any, maxRetry int, startBackoff, maxBackoff time.Duration) (T, error) { var zero T // Zero value for the function's return type for attempt := 0; attempt \u003c maxRetry; attempt++ { result, err := fn(args...) if err == nil { return result, nil } if attempt == maxRetry-1 { return zero, err // Return with error after max retries } fmt.Printf( \"Retrying function call, attempt: %d, error: %v\\n\", attempt+1, err, ) time.Sleep(startBackoff) if startBackoff \u003c maxBackoff { startBackoff *= 2 } } return zero, fmt.Errorf(\"retry: max retries reached without success\") } Functionally, the generic implementation works the same way as the previous one. However, it has a few limitations: The generic Retry function assumes that the wrapped function will always return the result as the first value and error as the second. This works well since it’s a common Go idiom, but the reflection version could dynamically handle different return value patterns. The reflection-based Retry can directly wrap any function because it accepts an empty interface. The generic Retry needs the target function to match the expected signature. So you have to create a thin wrapper function to adapt the signatures. This wrapper function is necessary to make the process somewhat type-safe. Here’s how you’d use the generic Retry function: func main() { someFunc := func(a, b int) (int, error) { fmt.Printf(\"Function called with a: %d and b: %d\\n\", a, b) return 42, errors.New(\"some error\") } wrappedFunc := func(args ...any) (any, error) { return someFunc(args[0].(int), args[1].(int)) } result, err := Retry( wrappedFunc, []interface{}{42, 100}, 3, 1*time.Second, 4*time.Second, ) if err != nil { fmt.Println(\"Function execution failed:\", err) } else { fmt.Println(\"Function executed successfully:\", result) } } Running it will give you the same output as before. Notice how someFunc is wrapped in a wrappedFunc where wrappedFunc has the signature that Retry expects. Then inside, the someFunc function is called with the appropriate arguments. This type of adaptation gymnastics is necessary to make the process acceptably type-safe. Personally, I don’t mind it if it means I get to avoid reflections to achieve the same result. Also, the generic version is a tad bit more performant. After this entry went live, Anton Zhiyanov1 pointed out on Twitter2 that there’s a closure-based approach that’s even simpler and eliminates the need for generics. The implementation looks like this: func Retry( fn func() error, maxRetry int, startBackoff, maxBackoff time.Duration) { for attempt := 0; ; attempt++ { if err := fn(); err == nil { return } if attempt == maxRetry-1 { return } fmt.Printf(\"Retrying after %s\\n\", startBackoff) time.Sleep(startBackoff) if startBackoff \u003c maxBackoff { startBackoff *= 2 } } } Now, calling Retry is much easier since the signature of the closure function it accepts is static. So you won’t need to adapt your retry call whenever the signature of the wrapped function changes. You’d call it as follows: func main() { someFunc := func(a, b int) (int, error) { fmt.Printf(\"Function called with a: %d and b: %d\\n\", a, b) return 42, errors.New(\"some error\") } var res int var err error Retry( func() error { res, err = someFunc(42, 100) return err }, 3, 1*time.Second, 4*time.Second, ) fmt.Println(res, err) } The runtime behavior of this version is the same as the ones before. Fin! Anton Zhiyanov ↩︎ Discussions on Twitter ↩︎ ","permalink":"http://rednafi.com/go/retry_function/","publishDate":"2024-02-04","summary":"I used reach for reflection whenever I needed a Retry function in Go. It’s fun to write, but gets messy quite quickly.\nHere’s a rudimentary Retry function that does the following:\nIt takes in another function that accepts arbitrary arguments. Then tries to execute the wrapped function. If the wrapped function returns an error after execution, Retry attempts to run the underlying function n times with some backoff. The following implementation leverages the reflect module to achieve the above goals. We’re intentionally avoiding complex retry logic for brevity:\n","tags":["Go","TIL"],"title":"Retry function in Go"},{"content":"Despite moonlighting as a gopher for a while, the syntax for type assertion and type switches still trips me up every time I need to go for one of them.\nSo, to avoid digging through the docs or crafting stodgy LLM prompts multiple times, I decided to jot this down in a gobyexample1 style for the next run.\nType assertion Type assertion in Go allows you to access an interface variable’s underlying concrete type. After a successful assertion, the variable of interface type is converted to the concrete type to which it’s asserted.\nThe syntax is i.(T), where i is a variable of interface type and T is the type you are asserting.\nBasic usage var i interface{} = \"Hello\" // or use `any` as an alias for `interface{}` s := i.(string) fmt.Println(s) Here, s gets the type string, and the program outputs Hello.\nAsserting primitive types var i interface{} = 42 if v, ok := i.(int); ok { fmt.Println(\"integer:\", v) } This code checks if i is an int and prints its value if so. The value of ok will be false if i isn’t an integer and nothing will be printed to the console.\nAsserting composite types var i interface{} = []string{\"apple\", \"banana\", \"cherry\"} if v, ok := i.([]string); ok { fmt.Println(\"slice of strings:\", v) } This will print slice of strings: [apple banana cherry] to the console.\nSimilar to primitive types, you can also perform type assertions with composite types. In the example above, we check whether the variable i, which is of an interface type, holds a value of the type ‘slice of strings’.\nAsserting other interface types type fooer interface{ foo() } type barer interface{ bar() } type foobarer interface { fooer; barer } type thing struct{} func (t *thing) foo() {} func (t *thing) bar() {} var i foobarer = \u0026thing{} func main() { if v, ok := i.(fooer); ok { fmt.Println(\"i satiesfies fooer:\", v) } } Type assertion can also be used to convert the type of an interface variable to another interface type. Here struct i implements both foo() and bar() methods; satisfying the foobarer interface.\nThen in the main function, we check whether i satisfies fooer interface and print a message if it does. Running this snippet will print i satiesfies fooer: \u0026{}.\nDynamically checking for certain methods type fooer interface{ foo() } type thing struct{} func (t *thing) foo() {} func (t *thing) bar() {} func main() { var i fooer = \u0026thing{} if v, ok := i.(interface{ bar() }); ok { fmt.Println(\"thing implements bar method:\", v) } } Type assertion can be used to dynamically check if an interface variable implements a certain method. This can come in handy when you want to know if an interface variable has a certain method right before invoking it.\nHere, the thing struct implements both foo and bar but the fooer interface only needs the foo() method to be implemented. However, we can check dynamically whether i also implements the bar() method using anonymous interface definition. Running this prints thing implements bar method: \u0026{}\nHandling failures var i interface{} = \"Hello\" f := i.(float64) // This triggers a panic Wrong assertions, like attempting to convert a string to a float64, cause runtime panics.\nType switches Type switches let you compare an interface variable’s type against several types. It’s similar to a regular switch statement, but focuses on types.\nBasic usage var i interface{} = 7 switch i.(type) { case int: fmt.Println(\"i is an int\") case string: fmt.Println(\"i is a string\") default: fmt.Println(\"unknown type\") } This outputs i is an int.\nUsing a variable in a type switch case var i interface{} = []byte(\"hello\") switch v := i.(type) { case []byte: fmt.Println(string(v)) case string: fmt.Println(v) } Notice how we’re assinging variable v to i.(type) and then reusing the extracted value in the case statements. The snippet converts []byte to a string and prints hello.\nHandling multiple types var i interface{} = 2.5 switch i.(type) { case int, float64: fmt.Println(\"i is a number\") case string: fmt.Println(\"i is a string\") } The case T1, T2 syntax works like an OR relationship, outputting i is a number.\nAddressing composite types var i interface{} = map[string]bool{\"hello\": true, \"world\": false} switch i.(type) { case map[string]bool: fmt.Println(\"i is a map\") case []string: fmt.Println(\"i is a slice\") default: fmt.Println(\"unknown type\") } Similar to primitive types, you can check for composite types in the case statement of a type switch. Here, we’re checking whether i is a map[string]bool or not. Running this will output i is a map.\nComparing against other interface types type fooer interface{ foo() } type barer interface{ bar() } type foobarer interface { fooer; barer } type thing struct{} func (t *thing) foo() {} func (t *thing) bar() {} var i foobarer = \u0026thing{} func main() { switch v := i.(type) { case fooer: fmt.Println(\"fooer:\", v) case barer: fmt.Println(\"barer:\", v) case foobarer: fmt.Println(\"foobarer:\", v) default: panic(\"none of them\") } } Type switches can be also used to compare an interface variable with another interface type. This example is similar to the type assertion one where we’re checking whether i satisfies fooer, barer or foobarer interface. In this case, i satisfies all three of them but the case statement will stop after the first successful check. So it prints fooer: \u0026{} and bails.\nDynamically checking for certain methods type fooer interface{ foo() } type thing struct{} func (t *thing) foo() {} func (t *thing) bar() {} func main() { var i fooer = \u0026thing{} switch v := i.(type) { case interface{ bar() }: fmt.Println(\"thing implements bar method:\", v) default: panic(\"thing doesn't implement bar method\") } } Similar to type assertion, within a type switch, anonymous interface definition can be used to dynamically check if an interface variable implements some method.\nThe thing struct implements both foo() and bar() methods. However, the fooer interface only requires it to implement foo(). The type switch dynamically checks whether i also implements the method bar(). Running this will print thing implements bar method: \u0026{}.\nSimilarities and differences Similarities Both handle interfaces and extract their concrete types. They evaluate an interface’s dynamic type. Differences Type assertions check a single type, while type switches handle multiple types. Type assertion uses i.(T), type switch uses a switch statement with literal i.(type). Type assertions can panic or return a success boolean, type switches handle mismatches more gracefully. Type assertions are good when you’re sure of the type. Type switches are more versatile for handling various types. Type assertion can get the value and success boolean. Type switches let you access the value in each case block. Type switches can handle multiple types, including a default case, offering more flexibility for various types. Fin!\nGo by example ↩︎\n","permalink":"http://rednafi.com/go/type_assertion_vs_type_switches/","publishDate":"2024-01-31","summary":"Despite moonlighting as a gopher for a while, the syntax for type assertion and type switches still trips me up every time I need to go for one of them.\nSo, to avoid digging through the docs or crafting stodgy LLM prompts multiple times, I decided to jot this down in a gobyexample1 style for the next run.\nType assertion Type assertion in Go allows you to access an interface variable’s underlying concrete type. After a successful assertion, the variable of interface type is converted to the concrete type to which it’s asserted.\n","tags":["Go","TIL"],"title":"Type assertion vs type switches in Go"},{"content":"I’ve been a happy user of pydantic1 settings to manage all my app configurations since the 1.0 era. When pydantic 2.0 was released, the settings portion became a separate package called pydantic_settings2. It does two things that I love: it automatically reads the environment variables from the .env file and allows you to declaratively convert the string values to their desired types like integers, booleans, etc. Plus, it lets you override the variables defined in .env by exporting them in your shell. So if you have a variable called FOO in your .env file like this: FOO=\"some_value\" Then you can override it via: export FOO=\"other_value\" And pydantic settings will automatically pick up the overridden values without much fuss. This is neat but can make writing deterministic unit tests tricky. If the settings instance implicitly pulls config values from both the environment file and shell, testing functions using those values can easily become flaky. Also, it’s usually frowned upon if your unit tests depend on environment variables in general. Consider this common instantiation workflow of the settings class. Here, we have the following app structure: . ├── src │ ├── __init__.py │ ├── config.py │ └── main.py ├── tests │ ├── __init__.py │ └── test_main.py └── .env In the src/config.py file, we define our settings class as follows: from pydantic_settings import BaseSettings, SettingsConfigDict class Settings(BaseSettings): # Override defaults with .env file values. model_config = SettingsConfigDict(env_file=\".env\") env_var_1: str = \"default_value\" env_var_2: int = 123 env_var_3: bool = False Then the corresponding values of the environment variables are defined in the .env file. Pydantic will automatically convert the upper-cased definitions to lower-case. ENV_VAR_1=\"overridden_value\" ENV_VAR_2=\"42\" ENV_VAR_3=\"true\" Next, we instantiate the Settings class in the src/__init__.py file: from src.config import Settings settings = Settings() Finally, we use the config values in src/main.py: from src import settings def read_env() -\u003e tuple[str, int, bool]: return settings.env_var_1, settings.env_var_2, settings.env_var_3 if __name__ == \"__main__\": env_var_1, env_var_2, env_var_3 = read_env() print(f\"{env_var_1=}\") print(f\"{env_var_2=}\") print(f\"{env_var_3=}\") From the root directory, run the main.py file with this command: python -m src.main This reveals that pydantic settings is doing its magic–reading the .env file and overriding the default config values: env_var_1='overridden_value' env_var_2=42 env_var_3=True Fantastic! But now, testing the read_env function becomes tricky. Normally, you’d try to patch the environment variables in a pytest fixture and then test the values like this: # tests/test_main.py import os from collections.abc import Iterator from unittest.mock import patch import pytest from src.main import read_env @pytest.fixture def patch_env_vars() -\u003e Iterator[None]: with patch.dict( os.environ, { \"ENV_VAR_1\": \"test_env_var_1\", \"ENV_VAR_2\": \"456\", \"ENV_VAR_3\": \"True\", }, ): yield def test_read_env(patch_env_vars: None) -\u003e None: env_var_1, env_var_2, env_var_3 = read_env() assert env_var_1 == \"test_env_var_1\" assert env_var_2 == 456 assert env_var_3 is True But the test will fail because we’re initializing the Settings class in the src/__init__.py file and pydantic processes the environment file and variables before pytest can intervene. We want our unit tests to have no dependencies on the environment variables. You might say initializing a class in the __init__.py file like that is an anti-pattern and all this can be avoided through dependency injection. You’d be right but you’d also be surprised at how many apps with 7+ figure ARR initialize their config classes like that. So patching the environment variables doesn’t work, what does? The idea is to let pydantic do its magic and then reset the attributes of the Settings instance to their default values in a fixture. We also want the user of the fixture to be able to override the values of some or all of the environment variables if necessary. Here’s what has worked well for me: import pytest from src.main import read_env from src import settings, Settings import pytest from collections.abc import Iterator from pytest import FixtureRequest @pytest.fixture def patch_settings(request: FixtureRequest) -\u003e Iterator[Settings]: # Make a copy of the original settings original_settings = settings.model_copy() # Collect the env vars to patch env_vars_to_patch = getattr(request, \"param\", {}) # Patch the settings to use the default values for k, v in settings.model_fields.items(): setattr(settings, k, v.default) # Patch the settings with the parametrized env vars for key, val in env_vars_to_patch.items(): # Raise an error if the env var is not defined in the settings if not hasattr(settings, key): raise ValueError(f\"Unknown setting: {key}\") # Raise an error if the env var has an invalid type expected_type = getattr(settings, key).__class__ if not isinstance(val, expected_type): raise ValueError( f\"Invalid type for {key}: {val.__class__} instead \" \"of {expected_type}\" ) setattr(settings, key, val) yield settings # Restore the original settings settings.__dict__.update(original_settings.__dict__) Here, patch_settings is a parametrizable fixture where you can optionally pass values via pytest.mark.parametrize to override certain config attributes. If you don’t override anything, the fixture sets the attributes of the Setting instance to their default values defined in the class. Above, first we make a copy of the original settings instance. Then we reset the attributes of the Setting instance to their default values. Next, we move on to override any values passed via the @parametrize decorator. While doing this, we also check for the correct type of the incoming values and raise an error accordingly. Finally, we yield the patched instance and reset everything back to their original values after a test ends. You can use the fixture like this: def test_read_env(patch_settings: Settings) -\u003e None: env_var_1, env_var_2, env_var_3 = read_env() assert env_var_1 == \"default_value\" assert env_var_2 == 123 assert env_var_3 is False @pytest.mark.parametrize( \"patch_settings\", [ {\"env_var_1\": \"patched_value\", \"env_var_2\": 456}, {\"env_var_2\": 459}, ], indirect=True, ) def test_read_env_override(patch_settings: Settings) -\u003e None: env_var_1, env_var_2, env_var_3 = read_env() assert env_var_1 == patch_settings.env_var_1 assert env_var_2 == patch_settings.env_var_2 assert env_var_3 is patch_settings.env_var_3 In the first case, we’re not overriding anything. So the tests will use the Settings instance with all the default values. In the second test, we’re overriding a few values and the read_env function will use the overridden values. Either way, the tests don’t directly depend on the environment variables and it reduces the probability of spooky actions at a distance. Fin! pydantic ↩︎ pydantic_settings ↩︎ ","permalink":"http://rednafi.com/python/patch_pydantic_settings_in_pytest/","publishDate":"2024-01-27","summary":"I’ve been a happy user of pydantic1 settings to manage all my app configurations since the 1.0 era. When pydantic 2.0 was released, the settings portion became a separate package called pydantic_settings2.\nIt does two things that I love: it automatically reads the environment variables from the .env file and allows you to declaratively convert the string values to their desired types like integers, booleans, etc.\nPlus, it lets you override the variables defined in .env by exporting them in your shell.\n","tags":["Python","TIL"],"title":"Patching pydantic settings in pytest"},{"content":"As of now, unlike Python or NodeJS, Go doesn’t allow you to specify your development dependencies separately from those of the application. However, I like to specify the dev dependencies explicitly for better reproducibility.\nWhile working on a new CLI tool1 for checking dead URLs in Markdown files, I came across this neat convention: you can specify dev dependencies in a tools.go file and then exclude them while building the binary using a build tag.\nHere’s how it works. Let’s say our project foo currently has the following structure:\nfoo ├── go.mod ├── go.sum └── main.go The main.go file contains a simple hello-world function that uses a 3rd party dependency just to make a point:\npackage main import ( \"fmt\" // Cowsay is a 3rd party app dependency cowsay \"github.com/Code-Hex/Neo-cowsay\" ) func main() { fmt.Println(cowsay.Say(cowsay.Phrase(\"Hello, World!\"))) } Here, Neo-cowsay is our app dependency. To initialize the project, we run the following commands serially:\ngo mod init example.com/foo # creates the go.mod and go.sum files go mod tidy # installs the app dependencies Now, let’s say we want to add the following dev dependencies: golangci-lint2 to lint the project in the CI and gofumpt3 as a stricter gofmt. Since we don’t import these tools directly anywhere, they aren’t tracked by the build toolchain.\nBut we can leverage the following workflow:\nPlace a tools.go file in the root directory. Import the dev dependencies in that file. Run go mod tidy to track both app and dev dependencies via go.mod and go.sum. Specify a build tag in tools.go to exclude the dev dependencies from the binary. In this case, tools.go looks as follows:\n// go:build tools package tools import ( // Dev dependencies _ \"github.com/golangci/golangci-lint/cmd/golangci-lint\" _ \"mvdan.cc/gofumpt\" ) Above, we’re importing the dev dependencies and assigning them to underscores since we won’t be using them directly. However, now if you run go mod tidy, Go toolchain will track the dependencies via the go.mod and go.sum files. You can inspect the dependencies in go.mod:\n// go.mod module example.com/foo go 1.21.6 require ( github.com/Code-Hex/Neo-cowsay v1.0.4 // app dependency github.com/golangci/golangci-lint v1.55.2 // dev dependency mvdan.cc/gofumpt v0.5.0 // dev dependency ) // ... transient dependencies Although we’re tracking the dev dependencies along with the app ones, the build tag // go:build tools at the beginning of tools.go file will instruct the build toolchain to ignore them while creating the binary.\nFrom the root directory of foo, you can build the project by running:\ngo build main.go This will create a binary called main in the root directory. To ensure that the binary doesn’t contain the dev dependencies, run:\ngo tool nm main | grep -Ei 'golangci-lint|gofumpt' This won’t return anything if the dev dependencies aren’t packed into the binary.\nBut if you do that for the app dependency, it’ll print the artifacts:\ngo tool nm main | grep -Ei 'cowsay' This prints:\n1000b6d40 T github.com/Code-Hex/Neo-cowsay.(*Cow).Aurora 1000b6fb0 T github.com/Code-Hex/Neo-cowsay.(*Cow).Aurora.func1 1000b5610 T github.com/Code-Hex/Neo-cowsay.(*Cow).Balloon 1000b6020 T github.com/Code-Hex/Neo-cowsay.(*Cow).Balloon.func1 ... For some weird reason, if you want to include the dev dependencies in your binary, you can pass the tools tag while building the binary:\ngo build --tags tools main.go However, this will most likely fail if any of your dev dependencies aren’t importable.\nHere’s an example4 of this pattern in the wild from the Kubernetes repo.\nWhile it works, I’d still prefer to have a proper solution instead of a hack. Fin!\nlink-patrol ↩︎\ngolangci-lint ↩︎\ngofumpt ↩︎\ntools.go in the kubernetes repo ↩︎\n","permalink":"http://rednafi.com/go/omit_dev_dependencies_in_binaries/","publishDate":"2024-01-21","summary":"As of now, unlike Python or NodeJS, Go doesn’t allow you to specify your development dependencies separately from those of the application. However, I like to specify the dev dependencies explicitly for better reproducibility.\nWhile working on a new CLI tool1 for checking dead URLs in Markdown files, I came across this neat convention: you can specify dev dependencies in a tools.go file and then exclude them while building the binary using a build tag.\n","tags":["Go","TIL"],"title":"Omitting dev dependencies in Go binaries"},{"content":"I love dynamically typed languages as much as the next person. They let us make ergonomic API calls like this: import httpx # Sync call for simplicity r = httpx.get(\"https://dummyjson.com/products/1\").json() print(r[\"id\"], r[\"title\"], r[\"description\"]) or this: fetch(\"https://dummyjson.com/products/1\") .then((res) =\u003e res.json()) .then((json) =\u003e console.log(json.id, json.type, json.description)); In both cases, running the snippets will return: 1 'iPhone 9' 'An apple mobile which is nothing like apple' Unless you’ve worked with a statically typed language that enforces more constraints, it’s hard to appreciate how incredibly convenient it is to be able to call and use an API endpoint without having to deal with types or knowing anything about its payload structure. You can treat the API response as a black box and deal with everything in runtime. For example, Go wouldn’t even allow you to do so in such a loosey-goosey way. To consume the API, you’d need to create a struct in the essence of the return payload and then unmarshal the payload into it. Here’s the complete response payload that curl -s https://dummyjson.com/products/1 | jq returns: { \"id\": 1, \"title\": \"iPhone 9\", \"description\": \"An apple mobile which is nothing like apple\", \"price\": 549, \"discountPercentage\": 12.96, \"rating\": 4.69, \"stock\": 94, \"brand\": \"Apple\", \"category\": \"smartphones\", \"thumbnail\": \"https://cdn.dummyjson.com/product-images/1/thumbnail.jpg\", \"images\": [ \"https://cdn.dummyjson.com/product-images/1/1.jpg\", \"https://cdn.dummyjson.com/product-images/1/2.jpg\", \"https://cdn.dummyjson.com/product-images/1/3.jpg\", \"https://cdn.dummyjson.com/product-images/1/4.jpg\", \"https://cdn.dummyjson.com/product-images/1/thumbnail.jpg\" ] } This is how you’d call the API endpoint in Go. I’m using this json-to-go1 service to generate the Go struct instead of handwriting it: package main import ( \"encoding/json\" \"fmt\" \"io\" \"net/http\" ) // Define the struct that reflects the response payload type Product struct { ID int `json:\"id\"` Title string `json:\"title\"` Description string `json:\"description\"` Price int `json:\"price\"` DiscountPercentage float64 `json:\"discountPercentage\"` Rating float64 `json:\"rating\"` Stock int `json:\"stock\"` Brand string `json:\"brand\"` Category string `json:\"category\"` Thumbnail string `json:\"thumbnail\"` Images []string `json:\"images\"` } func main() { // Ignore error handling for brevity var product Product response, _ := http.Get(\"https://dummyjson.com/products/1\") defer response.Body.Close() body, _ := io.ReadAll(response.Body) _ = json.Unmarshal(body, \u0026product) // project the response into the struct // Do processing with product instance fmt.Println(product.ID, product.Title, product.Description) } This will give us the same output as the Python and JS code snippets: 1 iPhone 9 An apple mobile which is nothing like apple Above, we had to create a new struct type to represent the response payload, instantiate it, and unmarshal the JSON payload into the struct before we were able to process it. Notice that we’re only using 3 fields and ignoring the rest. In this case, you can get away with only including those 3 fields in the struct type, and Go will do the right thing: // ... same as before type Product struct { ID int `json:\"id\"` Title string `json:\"title\"` Description string `json:\"description\"` } // ... same as before While this is less work than having to emulate the whole structure of the JSON output in the struct definition, it’s still not winning any medals for brevity against the Python and JS snippets. Dynamically processing a JSON payload is nice as long as you’re working on a throwaway script. Anything more, it becomes a headache since the readers won’t have any idea about what the API response looks like without looking at the documentation or traces. Also, type safety is an issue. Since the imperative workflow doesn’t assume the structure of the response, you’ll be surprised with a runtime error if you make an incorrect assumption about the response structure. Sure, having to write a struct is a chore, but the free documentation and the type safety are things that you don’t get with the black box API calls. Statically typed languages force you to maintain good hygiene while working with JSON payloads. Declaratively embedding the payload structure directly into the codebase is immensely beneficial; it reduces the out-of-band knowledge required to understand the code and adds type safety as a cherry on top. But how do you do that in a language like Python? If you want to go with what’s in the standard library, you can handroll a dataclass like this and project the return payload onto it: # ... from dataclasses import dataclass from typing import Self @dataclass(slots=True) class Product: id: int title: str description: str price: int discount_percentage: float rating: float stock: int brand: str category: str thumbnail: str images: list[str] @classmethod def from_dict(cls, d: dict) -\u003e Self: # This is needed to reconcile the snake_case and camelCase variables discount_percentage = d.pop(\"discountPercentage\", None) return cls(discount_percentage=discount_percentage, **d) # ... Then just call Product.from_dict and pass the output of response.json() as before. This way, the API response is documented in the code, and the reader won’t have to depend on out-of-band information while reading the code. However, you can see that hand rolling data classes can quickly become hairy when you have a large JSON payload and need to reconcile the discrepancies between snake case and camel case variables. We had to add a custom from_dict class method to convert the camel case variables to their snake case counterparts in Python. Also, unlike Go, you can’t define a structure to represent only a portion of the whole payload in Python without adding extra code to ignore the rest of the fields that aren’t relevant to you. Pydantic2 shines here. It not only allows you to define a class to represent a partial payload structure, but also applies runtime validation to guarantee operational type safety. As a bonus, you can use a tool like this3 to generate pydantic classes from JSON: from pydantic import BaseModel, Field class Product(BaseModel): id: int title: str description: str price: int discount_percentage: float = Field(alias=\"discountPercentage\") rating: float stock: int brand: str category: str thumbnail: str images: list[str] You can project your response onto the data class with Product(**response.json()) and get a rich object that also validates the incoming values. This will work the same way with partially defined classes: # ... from pydantic import BaseModel class Product(BaseModel): id: int title: str description: str # ... Here’s a complete example: import httpx from pydantic import BaseModel import asyncio # Partially defined Pydantic model to represent the response class Product(BaseModel): id: int title: str description: str async def main() -\u003e None: async with httpx.AsyncClient() as client: response = await client.get(\"https://dummyjson.com/products/1\") response.raise_for_status() data = response.json() product = Product(**data) print(product.id, product.title, product.description) if __name__ == \"__main__\": asyncio.run(main()) In the JS land, you can adopt TypeScript and zod4 to achieve a similar result: // index.ts import { z } from \"zod\"; const ProductSchema = z.object({ id: z.number(), title: z.string(), description: z.string(), // Other fields can be added if needed }); type Product = z.infer; fetch(\"https://dummyjson.com/products/1\") .then((response) =\u003e response.json()) .then((data) =\u003e ProductSchema.parse(data)) .then((product: Product) =\u003e { console.log(product.id, product.title, product.description); }); I don’t mind the added verbosity if it leads to better readability and type safety. Fin! json-to-go ↩︎ pydantic ↩︎ json-to-pydantic ↩︎ zod ↩︎ ","permalink":"http://rednafi.com/misc/eschewing_black_box_api_calls/","publishDate":"2024-01-15","summary":"I love dynamically typed languages as much as the next person. They let us make ergonomic API calls like this:\nimport httpx # Sync call for simplicity r = httpx.get(\"https://dummyjson.com/products/1\").json() print(r[\"id\"], r[\"title\"], r[\"description\"]) or this:\nfetch(\"https://dummyjson.com/products/1\") .then((res) =\u003e res.json()) .then((json) =\u003e console.log(json.id, json.type, json.description)); In both cases, running the snippets will return:\n1 'iPhone 9' 'An apple mobile which is nothing like apple' Unless you’ve worked with a statically typed language that enforces more constraints, it’s hard to appreciate how incredibly convenient it is to be able to call and use an API endpoint without having to deal with types or knowing anything about its payload structure. You can treat the API response as a black box and deal with everything in runtime.\n","tags":["Python","JavaScript","Go"],"title":"Eschewing black box API calls"},{"content":"While I tend to avoid *args and **kwargs in my function signatures, it’s not always possible to do so without hurting API ergonomics. Especially when you need to write functions that call other helper functions with the same signature. Typing *args and **kwargs has always been a pain since you couldn’t annotate them precisely before. For example, if all the positional and keyword arguments of a function had the same type, you could do this: def foo(*args: int, **kwargs: bool) -\u003e None: ... This implies that args is a tuple where all the elements are integers, and kwargs is a dictionary where the keys are strings and the values are booleans. On the flip side, you couldn’t annotate *args and **kwargs properly if the values of the positional and keyword arguments had different types. In those cases, you’d have to fall back to Any, which defeats the purpose. Consider this example: def foo(*args: tuple[int, str], **kwargs: dict[str, bool | None]) -\u003e None: ... Here, the type checker sees each positional argument as a tuple of an integer and a string. Plus, it considers each keyword argument as a dictionary where the keys are strings and the values are either booleans or None. With the previous annotation, mypy will reject this: foo(*(1, \"hello\"), **{\"key1\": 1, \"key2\": False}) error: Argument 1 to \"foo\" has incompatible type \"*tuple[int, str]\"; expected \"tuple[int, str]\" [arg-type] error: Argument 2 to \"foo\" has incompatible type \"**dict[str, int]\"; expected \"dict[str, bool | None]\" [arg-type] Instead, it’ll accept the following: foo((1, \"hello\"), kw1={\"key1\": 1, \"key2\": False}) You probably wanted to represent the former while the type checker wants the latter. To annotate the second instance correctly, you’ll need to leverage bits of PEP-5891, PEP-6462, PEP-6553, and PEP-6924. We’ll use Unpack and TypedDict from the typing module to achieve this. Here’s how: from typing import TypedDict, Unpack # Python 3.12+ # from typing_extensions import TypedDict, Unpack # \u003c Python 3.12 class Kw(TypedDict): key1: int key2: bool def foo(*args: Unpack[tuple[int, str]], **kwargs: Unpack[Kw]) -\u003e None: ... args = (1, \"hello\") kwargs: Kw = {\"key1\": 1, \"key2\": False} foo(*args, **kwargs) # Ok TypedDict was introduced in Python 3.8 to allow you to annotate heterogeneous dictionaries. If all the values of a dictionary have the same type, you can simply use dict[str, T] to annotate it. However, TypedDict covers the case where all the keys of a dictionary are strings but the type of the values varies. The following example shows how you might annotate a heterogeneous dictionary: from typing import TypedDict class Movie(TypedDict): name: str year: int movies: Movie = {\"name\": \"Mad Max\", \"year\": 2015} Unpack marks an object as having been unpacked. Using TypedDict with Unpack allows us to communicate with the type checker so that each positional and keyword argument isn’t mistakenly assumed as a tuple and a dictionary respectively. While the type checker is satisfied when you pass the *args and **kwargs as foo(*args, **kwargs) it’ll complain if you don’t pass all the keyword arguments: foo(*args, key1=1) # error: Missing named argument \"key2\" for \"foo\" To make all of the keywords optional, you could turn off the total flag in the typed-dict definition: # ... class Kw(TypedDict, total=False): key1: int key2: str # ... Or you could mark specific keywords as optional with typing.NotRequired: # ... class Kw(TypedDict): key1: int key2: NotRequired[str] # ... This will let you pass an incomplete set of optional keyword arguments without the type checker yelling at you. Fin! PEP 589 – TypedDict: Type Hints for Dictionaries with a Fixed Set of Keys ↩︎ PEP 646 – Variadic Generics ↩︎ PEP 655 – Marking individual TypedDict items as required or potentially-missing ↩︎ PEP 692 – Using TypedDict for more precise **kwargs typing ↩︎ ","permalink":"http://rednafi.com/python/annotate_args_and_kwargs/","publishDate":"2024-01-08","summary":"While I tend to avoid *args and **kwargs in my function signatures, it’s not always possible to do so without hurting API ergonomics. Especially when you need to write functions that call other helper functions with the same signature.\nTyping *args and **kwargs has always been a pain since you couldn’t annotate them precisely before. For example, if all the positional and keyword arguments of a function had the same type, you could do this:\n","tags":["TIL"],"title":"Annotating args and kwargs in Python"},{"content":"I needed to integrate rate limiting into a relatively small service that complements a monolith I was working on. My initial thought was to apply it at the application layer, as it seemed to be the simplest route.\nPlus, I didn’t want to muck around with load balancer configurations, and there’s no shortage of libraries that allow me to do this quickly in the app. However, this turned out to be a bad idea. In the event of a DDoS1 or thundering herd2 incident, even if the app rejects the influx of inbound requests, the app server workers still have to do a minimal amount of work.\nAlso, ideally, rate limiting is an infrastructure concern; your app should be oblivious to it. Implementing rate limiting in a layer in front of your app prevents rogue requests from even reaching the app server in the event of an incident. So, I decided to spend some time investigating how to do it at the load balancer layer. Nginx3 makes it quite manageable4 without much fuss and the system was already using it as a reverse proxy.\nFor the initial pass, I chose to go with the default Nginx settings, avoiding any additional components like a Redis layer for centralized rate limiting.\nApp structure For this demo, I’ll proceed with a simple hello-world server written in Go. Here’s the app directory:\napp ├── Dockerfile ├── docker-compose.yml ├── go.mod ├── main.go ├── main_test.go └── nginx ├── default.conf └── nginx.conf The main.go file exposes the server at the /greetings endpoint on port 8080:\npackage main import ( \"encoding/json\" \"net/http\" ) type HelloWorldResponse struct { Message string `json:\"message\"` } func helloWorldHandler(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") response := HelloWorldResponse{Message: \"Hello World\"} json.NewEncoder(w).Encode(response) } func main() { http.HandleFunc(\"/greetings\", helloWorldHandler) http.ListenAndServe(\":8080\", nil) } If you run the server with the go run main.go command and make a curl request to it, it’ll give you the following JSON output:\ncurl localhost:8080/greetings | jq { \"message\": \"Hello World\" } Now, we want to set up the rate limiter in the reverse proxy layer so that it will reject requests when the inbound request rate exceeds 50 req/sec.\nNginx config The Nginx config lives in the nginx directory and consists of two config files:\napp/nginx ├── default.conf └── nginx.conf The nginx.conf file is the core configuration file. It’s where you define the server’s global settings, like how many worker processes to run, where to store log files, rate limiting policies, and overarching security protocols.\nThen there’s the default.conf file, which is typically more focused on the configuration of individual server blocks or virtual hosts. This is where you get into the specifics of each website or service you’re hosting on the server. Settings like server names, SSL certificates, and specific location directives are defined here. It’s tailored to manage the nitty-gritty of how each site or application behaves under the umbrella of the global settings set in nginx.conf.\nYou can have multiple *.conf files like default.conf and all of them are included in the nginx.conf file.\nnginx.conf Here’s how the nginx.conf looks:\nevents { worker_connections 1024; } http { # Define the rate limiting zone limit_req_zone $binary_remote_addr zone=mylimit:10m rate=50r/s; # Custom error pages should be defined within a server block # It's better to define this in the specific server configuration files. # Include server configurations from conf.d directory include /etc/nginx/conf.d/*.conf; } In the nginx.conf file, you’ll find two main sections: events and http. Each of these serves different purposes in the setup.\nEvents block events { worker_connections 1024; } This section defines settings for the events block, specifically the worker_connections directive. It sets the maximum number of connections that each worker process can handle concurrently to 1024.\nHTTP block http { limit_req_zone $binary_remote_addr zone=mylimit:10m rate=50r/s; include /etc/nginx/conf.d/*.conf; } The http block contains directives that apply to HTTP/S traffic.\nSet the rate limiting policy (limit_req_zone directive)\nlimit_req_zone $binary_remote_addr zone=mylimit:10m rate=50r/s; This line sets up rate limiting policy using three parameters:\nKey ($binary_remote_addr): This is the client’s IP address in a binary format. It’s used as a key to apply the rate limit, meaning each unique IP address is subjected to the rate limit specified.\nZone (zone=mylimit:10m): This defines a shared memory zone named mylimit with a size of 10 megabytes. The zone stores the state of each IP address, including how often it has accessed the server. Approximately 160,000 IP addresses can be tracked with this size. If the zone is full, Nginx will start removing the oldest entries to free up space.\nRate (rate=50r/s): This parameter sets the maximum request rate to 50 requests per second for each IP address. If the rate is exceeded, additional requests may be delayed or rejected.\nInclude the default.conf file\ninclude /etc/nginx/conf.d/*.conf; This directive instructs Nginx to include additional server configurations—like default.conf—from the /etc/nginx/conf.d/ directory. This modular approach allows for better organization and management of server configurations.\ndefault.conf The default.conf file, included in the previously discussed nginx.conf, mainly configures a server block in Nginx. We’ll use the rate limiting policy defined there in the default.conf file. Here’s the content:\nserver { listen 80 default_server; # Custom JSON response for 429 errors error_page 429 = @429; location @429 { default_type application/json; return 429 '{\"status\": 429, \"message\": \"Too Many Requests\"}'; } location / { # Apply rate limiting limit_req zone=mylimit burst=10 nodelay; limit_req_status 429; # Set the status code for rate-limited requests # Proxy settings - adjust as necessary for your application proxy_pass http://app:8080; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } } This file currently contains a server block where we employ the rate limiting policy and set up the reverse proxy.\nServer Block server { listen 80 default_server; ... } This section defines the server block, with Nginx listening on port 80, the default port for HTTP traffic. The default_server parameter indicates that this server block should be used if no other matches are found.\nCustom error handling error_page 429 = @429; location @429 { default_type application/json; return 429 '{\"status\": 429, \"message\": \"Too Many Requests\"}'; } By default, when a client experiences rate limiting, the server returns an HTTP 503 error with an HTML page. But we want to return 429 (Too many requests) error code with an error message in a JSON payload. This section does that.\nLocation block location / { limit_req zone=mylimit burst=10 nodelay; limit_req_status 429; ... } The location / block applies to all requests to the root URL and its subdirectories.\nApply the rate limiting policy\nlimit_req zone=mylimit burst=10 nodelay; limit_req_status 429; These directives enforce the rate limiting policy set in nginx.conf. The limit_req directive uses the previously defined mylimit zone. The burst parameter allows a burst of 10 requests above the set rate before enforcing the limit. The nodelay option ensures that excess requests within the burst limit are processed immediately without delay. limit_req_status sets the HTTP status code for rate-limited requests to 429.\nConfigure the proxy\nproxy_pass http://app:8080; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; These lines configure Nginx to act as a reverse proxy. Requests to this server are forwarded to an application server running on http://app:8080. The directives also handle HTTP headers to properly manage the connection and caching between the client, reverse proxy, and backend application server.\nContainerize everything The Dockerfile builds the hello-world service:\nFROM golang:1.21 as build WORKDIR /go/src/app COPY . . RUN go mod download RUN CGO_ENABLED=0 go build -o /go/bin/app FROM gcr.io/distroless/static-debian12 COPY --from=build /go/bin/app / CMD [\"/app\"] Then we orchestrate the app with reverse proxy in the docker-compose.yml file:\nversion: \"3.8\" services: app: build: . ports: - \"8080:8080\" nginx: image: nginx:alpine ports: - \"80:80\" volumes: - ./nginx/nginx.conf:/etc/nginx/nginx.conf - ./nginx/default.conf:/etc/nginx/conf.d/default.conf depends_on: - app The docker-compose file defines two services: app and nginx. The app service exposes port 8080, meaning the app will be accessible on this port from outside the Docker environment.\nThe nginx service sits in front of the app and is configured to expose port 80. All the external requests will hit the default port 80 where the reverse proxy will relay the request to the backend app. The custom Nginx configuration volumes are mounted in the volumes section.\nTake it for a spin Navigate to the app directory and start the system with the following command:\ndocker compose up -d Now make 200 concurrent curl requests to see the rate limiter in action:\nseq 200 | xargs -n 1 -P 100 bash -c 'curl -s location/greetings | jq' This returns:\n{ \"message\": \"Hello World\" } { \"message\": \"Hello World\" } ... { \"status\": 429, \"message\": \"Too Many Requests\" } { \"status\": 429, \"message\": \"Too Many Requests\" } { \"message\": \"Hello World\" } { \"status\": 429, \"message\": \"Too Many Requests\" } See the deployed service in action (might not be available later):\nseq 200 | xargs -n 1 -P 100 bash -c 'curl -s 34.138.11.32/greetings | jq' This will print the same output as the local service.\nNginx uses the leaky bucket algorithm to enforce the rate limiting, where requests arrive at the bucket at various rates and leave the bucket at fixed rate. I had fun reading about it here5.\nFind the complete implemention6 on GitHub.\nFin!\nWhat is a DDoS attack? ↩︎\nThe “thundering herd” problem - Nick Groenen ↩︎\nNginx ↩︎\nRate limiting with Nginx and Nginx plus ↩︎\nLeaky bucket ↩︎\nComplete implementation ↩︎\n","permalink":"http://rednafi.com/go/rate_limiting_via_nginx/","publishDate":"2024-01-06","summary":"I needed to integrate rate limiting into a relatively small service that complements a monolith I was working on. My initial thought was to apply it at the application layer, as it seemed to be the simplest route.\nPlus, I didn’t want to muck around with load balancer configurations, and there’s no shortage of libraries that allow me to do this quickly in the app. However, this turned out to be a bad idea. In the event of a DDoS1 or thundering herd2 incident, even if the app rejects the influx of inbound requests, the app server workers still have to do a minimal amount of work.\n","tags":["Go","Networking"],"title":"Rate limiting via Nginx"},{"content":"You can use @dataclass(frozen=True) to make instances of a data class immutable during runtime. However, there’s a small caveat—instantiating a frozen data class is slightly slower than a non-frozen one. This is because, when you enable frozen=True, Python has to generate __setattr__ and __delattr__ methods during class definition time and invoke them for each instantiation. Below is a quick benchmark comparing the instantiation times of a mutable dataclass and a frozen one (in Python 3.12): from dataclasses import dataclass import timeit @dataclass class NormalData: a: int b: int c: int @dataclass(frozen=True) class FrozenData: a: int b: int c: int # Measure instantiation time for NormalData normal_time = timeit.timeit(lambda: NormalData(1, 2, 3), number=1_000_000) # Measure instantiation time for FrozenData frozen_time = timeit.timeit(lambda: FrozenData(1, 2, 3), number=1_000_000) print(f\"Normal data class: {normal_time}\") print(f\"Frozen data class: {frozen_time}\") print(f\"Frozen data class is {frozen_time / normal_time}x slower\") Running this prints: Normal data class: 0.13145725009962916 Frozen data class: 0.3248348340857774 Frozen data class is 2.4710301930064014x slower So, frozen data classes are approximately 2.4 times slower to instantiate than their non-frozen counterparts. This gap can widen further if you compare slotted data classes (via @dataclass(slots=True)) with frozen ones. While the cost for immutability is small, it can add up if you need to create many frozen instances. I was reading Tin Tvrtković’s article1 on making attr2 instances frozen at compile time. He mentions how to leverage mypy to enforce instance immutability statically and use mutable attr classes at runtime to avoid any instantiation cost. I wanted to see if I could do the same with standard data classes. Here’s how to do it: from dataclasses import dataclass from typing import TYPE_CHECKING, TypeVar, dataclass_transform if TYPE_CHECKING: T = TypeVar(\"T\") @dataclass_transform(frozen_default=True) def frozen(cls: type[T]) -\u003e type[T]: ... else: frozen = dataclass # or dataclass(slots=True) for even faster performance @frozen class Foo: x: int y: int # Instantiate the class foo = Foo(1, 2) # Mypy will raise an error here since foo is frozen during type checking foo.x = 3 print(foo) It involves: Using the type checker to ensure the data class instance is immutable. Replacing the immutable data class with a more performant mutable one at runtime. The if TYPE_CHECKING condition only executes during type-checking. In that block, we use typing.dataclass_transform, introduced in PEP-6813, to create a construct similar to the dataclass function that type checkers recognize. The frozen_default flag, added in Python 3.12, makes this work seamlessly, but the code should also function in Python 3.11 without changes, as dataclass_transform accepts any keyword arguments. In Python 3.10 and earlier, you can import dataclass_transform from typing_extensions and leave the rest of the code as is. The else ... block is what runs when you actually execute the code. There, we’re just aliasing the vanilla dataclass function as frozen. Running this code snippet results in: Foo(x=3, y=2) However, mypy will flag an error since we’re trying to mutate foo.x: foo.py:24: error: Property \"x\" defined in \"Foo\" is read-only [misc] Voilà! I struggled to figure this one out myself, and LLMs were of no help. So, I ended up posting a question4 on Stack Overflow, where someone pointed out how to use dataclass_transform to achieve this. Fin! Zero-overhead frozen attrs classes - Tin Tvrtković ↩︎ attrs ↩︎ PEP 681 – Data Class Transforms ↩︎ How to statically enforce frozen data classes in Python? ↩︎ ","permalink":"http://rednafi.com/python/statically_enforcing_frozen_dataclasses/","publishDate":"2024-01-04","summary":"You can use @dataclass(frozen=True) to make instances of a data class immutable during runtime. However, there’s a small caveat—instantiating a frozen data class is slightly slower than a non-frozen one. This is because, when you enable frozen=True, Python has to generate __setattr__ and __delattr__ methods during class definition time and invoke them for each instantiation.\nBelow is a quick benchmark comparing the instantiation times of a mutable dataclass and a frozen one (in Python 3.12):\n","tags":["Python","TIL"],"title":"Statically enforcing frozen data classes in Python"},{"content":"When I started my career in a tightly-knit team of six engineers at a small e-commerce startup, I was struck by the remarkable efficiency of having a centralized hub for all the documents used for planning. We used a single Trello board with four columns—To-do, Doing, Q/A, Done—where the tickets were grouped by feature tags. We’d leverage a dummy ticket as an epic to sketch out the full plan for a feature and link related tickets to it. The rest of the discussions took place in Slack or GitHub Issues.\nThe setup was rudimentary but stood the test of time. As we expanded into multiple teams, each unit had its own board mirroring the original structure. Managers had a clear picture of where to find stuff, everything was searchable from one spot, and communication impedance was surprisingly low.\nA few years down the line, I was fortunate enough to land gigs at larger companies with bigger teams and more corporate structures. What caught me off guard was the chaotic state of planning documents. They were scattered everywhere—RFCs1, ADRs2, Epics, Jira Issues, Subtasks, Design Docs3, you name it. Often, a single team would juggle all these formats to plan and record their work. I’m not claiming every workplace was like this, but it’s more common than I’d like to admit.\nThe fallout? Discussing a feature or onboarding new people became a pain since explaining any part of the system required going down the rabbit hole of finding the concomitant entry point and traversing its branches. More often than not, the documents weren’t even linked properly, so figuring out which RFCs, ADRs, Epics, or Jira Issues were associated with what feature was a frustrating exercise. Also, they’d quickly go outdated since keeping all of them up to date was a full-time job itself!\nThis cultural shift doesn’t happen in a day. People, in general, love reading books, blogs, or Hacker News discussions about the engineering practices in FAANG companies and mean well when they try to slowly incorporate these insights into their current teams. But let this osmosis continue for a few years without any oversight, and you’ll end up in a labyrinth of documents, encumbered by stiff structures and other enterprise-y fluff.\nSometimes I wonder if all these theatrics are actually necessary to do the job. Hundreds of people work on OSS projects where GitHub Issues and Projects are used to coordinate work. Just pressing cmd + k lets you find anything, anywhere, allowing immediate access to feature designs without having to sift through a quagmire of documents in disparate locations. The ability to access all documentation from a single place is not just efficient; it’s empowering, especially if it’s housed alongside your code.\nAnother approach that works well in practice is having a single Jira board per team where an Epic contains all the design decisions of a feature, and individual Task tickets under that are linked to GitHub Issues. This ensures that project managers can have a bird’s eye view of everything without having to log into the code repository, and developers can easily navigate back to the corresponding Task and Epic from the GitHub Issue with a single click.\nWhatever the strategy may be, I find it incredibly hard to justify the necessity to fragment these documents behind obscure names and waste time endlessly bikeshedding about whether ADRs should be written before RFCs or vice versa.\nFin!\nCompanies Using RFCs or Design Docs and Examples of These - Gergely Orosz ↩︎\nArchitectural Decision Records (ADRs) ↩︎\nDesign docs at Google ↩︎\n","permalink":"http://rednafi.com/zephyr/planning_palooza/","publishDate":"2024-01-01","summary":"When I started my career in a tightly-knit team of six engineers at a small e-commerce startup, I was struck by the remarkable efficiency of having a centralized hub for all the documents used for planning. We used a single Trello board with four columns—To-do, Doing, Q/A, Done—where the tickets were grouped by feature tags. We’d leverage a dummy ticket as an epic to sketch out the full plan for a feature and link related tickets to it. The rest of the discussions took place in Slack or GitHub Issues.\n","tags":["Essay"],"title":"Planning palooza"},{"content":"I’ve always had a thing for old-school web tech. By the time I joined the digital fray, CGI scripts were pretty much relics, but the term kept popping up in tech forums and discussions like ghosts from the past. So, I got curious, started reading about them, and wanted to see if I could reason about them from the first principles. Writing one from the ground up with nothing but Go’s standard library seemed like a good idea. Turns out, the basis of the technology is deceptively simple, but CGI scripts mostly went out of fashion because of their limitations around performance. What are those CGI scripts, or Common Gateway Interface scripts, emerged in the early 1990s as a solution for creating dynamic web content. They acted as intermediaries between the web server and external applications, allowing servers to process user input and return personalized content. This made them essential for adding interactivity to websites, such as form submissions and dynamic page updates. The key function of CGI scripts was to handle data from web forms, process it, and then generate an appropriate response. The server then takes this response and displays it on a new web page. Here’s how the process might look: sequenceDiagram participant U as Client participant S as Server participant C as CGI Script U-\u003e\u003eS: Post request with a dynamic field value S-\u003e\u003eC: Execute the CGI script in a new process Note right of C: CGI script receives the value C--\u003e\u003eS: Process and return result S--\u003e\u003eU: Respond with result How to write one CGI scripts are usually written in dynamic scripting languages like Perl, Ruby, Python, or even Bash. However, they can also be written in a static language where the server will need execute the compiled binary. For this demo, we’re going to write the server in Go using the cgi stdlib, but the CGI script itself will be written in Bash. Here’s the plan: Set up a basic HTTP server in Go. The server will await an HTTP POST request containing a form field called name. Upon receiving the request, the server will extract the value of name. Next, it’ll set the $name environment variable for the current process. A Bash CGI script is invoked, which uses the $name environment variable to echo an HTML-formatted dynamic message. Finally, the server will then return this HTML response to the client. The server lives in a single main.go script. I’m leaving out Go’s verbose error handling for clarity. package main import ( \"net/http\" \"net/http/cgi\" \"os/exec\" ) // Leaves out error handling for clarity func cgiHandler(w http.ResponseWriter, r *http.Request) { // Parse name from post request r.ParseForm() name := r.FormValue(\"name\") // Execute the CGI script with the name as an environment variable cmd := exec.Command(\"cgi-script.sh\") cmd.Env = append(cmd.Env, \"name=\"+name) // Serve the CGI script handler := cgi.Handler{Path: cmd.Path, Dir: cmd.Dir, Env: cmd.Env} handler.ServeHTTP(w, r) } func main() { http.HandleFunc(\"/\", cgiHandler) http.ListenAndServe(\"localhost:8080\", nil) } Upon every new request, the server above will execute a CGI script written in Bash. Name the shell script as cgi-script.sh and place it in the same directory as the server’s main.go file. Here’s how it looks: #!/bin/bash set -euo pipefail name=$name echo \"Content-type: text/html\" echo \"\" echo '' echo \"Hello $name, greetings from bash!\" echo '' The script just reads name from the environment variable, sets the Content-Type header, injects the value of name into the message, and echos the out the final HTML response. The server then just relays it back to the client. To test this: Run the server with go run main.go. Set the permission of the CGI script: sudo chmod +x cgi-script.sh Make a cURL request: curl -X POST http://localhost:8080 -d \"name=Redowan\" This returns the following response: Hello Redowan, greetings from bash! Why they didn’t catch on CGI scripts have fallen out of favor primarily due to concerns related to performance and security. When a CGI script is executed, it initiates a new process for each request. While this approach is straightforward, it becomes increasingly inefficient as web traffic volume grows. However, it’s worth noting that modern Linux kernels have made improvements in process spawning, and solutions like FastCGI utilize persistent process pools to reduce the overhead of creating new processes. Nevertheless, you still incur the VM startup cost for each request when using interpreted languages like Python or Ruby. Modern application servers like Uvicorn, Gunicorn, Puma, Unicorn or even Go’s standard server have addressed these inefficiencies by maintaining persistent server processes. This, along with the advantage of not having to bear the VM startup cost, has led people to opt for these alternatives. Another concern worth considering is the evident security issues associated with CGI scripts. Even in our simple example, the Bash script accepts any value for the name parameter and passes it directly to the response. This exposes a significant vulnerability to injection attacks. While it’s possible to manually sanitize the input before passing it to the next step, many of these security steps are automatically handled for you by almost any modern web framework. Fin! Apache Tutorial: Dynamic Content with CGI1 ↩︎ ","permalink":"http://rednafi.com/go/reminiscing_cgi_scripts/","publishDate":"2023-12-25","summary":"I’ve always had a thing for old-school web tech. By the time I joined the digital fray, CGI scripts were pretty much relics, but the term kept popping up in tech forums and discussions like ghosts from the past. So, I got curious, started reading about them, and wanted to see if I could reason about them from the first principles. Writing one from the ground up with nothing but Go’s standard library seemed like a good idea.\n","tags":[],"title":"Reminiscing CGI scripts"},{"content":"Despite using VSCode as my primary editor, I never really bothered to set up the native debugger to step through application code running inside Docker containers. Configuring the debugger to work with individual files, libraries, or natively running servers is trivial1. So, I use it in those cases and just resort back to my terminal for debugging containerized apps running locally. However, after seeing a colleague’s workflow in a pair-programming session, I wanted to configure the debugger to cover this scenario too. I’m documenting this to save my future self from banging his head against the wall trying to figure it out again. Desiderata I want to start a web app with docker compose up and connect the VSCode debugger to it from the UI. For this to work, along with the webserver, we’ll need to expose a debug server from the app container which the debugger can connect to. App layout For demonstration, I’ll go with a simple containerized starlette2 app served with uvicorn3. However, the strategy will be similar for any web app. Here’s the app’s directory structure: src ├── __init__.py ├── main.py ├── requirements.txt ├── Dockerfile └── docker-compose.yml In main.py, we’re exposing an endpoint as follows: from starlette.applications import Starlette from starlette.responses import JSONResponse from starlette.routing import Route async def homepage(request) -\u003e JSONResponse: return JSONResponse({\"hello\": \"world\"}) app = Starlette(debug=True, routes=[Route(\"/\", homepage)]) The requirement.txt lists out the runtime dependencies: starlette uvicorn Then the Dockerfile builds the application: FROM python:3.12-slim-bookworm WORKDIR /usr/src/app COPY . /usr/src/app RUN pip install --no-cache-dir -r requirements.txt EXPOSE 8000 CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] Finally, we orchestrate the app in a docker-compose.yml file: # docker-compose.yml version: \"3.9\" services: web: build: context: . dockerfile: ./Dockerfile # This overrides the CMD in the Dockerfile command: uvicorn main:app --host 0.0.0.0 --port 8000 ports: - 8000:8000 Add launch.json Now, in the .vscode folder of the project’s root directory, add a file named launch.json. Create the folder if it doesn’t exist. You can also do this part manually; to do so: Click on the debugger button and then click on create a launch.json file. Select the Python debugger. Finally, select the Remote Attach debug config. However, if you dislike clicking around, here’s the full content of launch.json for you to copy and paste (into $PWD/.vscode/launch.json): { \"version\":\"0.2.0\", \"configurations\":[ { \"name\":\"Python: Remote Attach\", \"type\":\"python\", \"request\":\"attach\", \"connect\":{ \"host\":\"localhost\", \"port\":5678 }, \"pathMappings\":[ { \"localRoot\":\"${workspaceFolder}\", \"remoteRoot\":\".\" } ], \"justMyCode\":true } ] } This instructs the VSCode debugger to attach to a debug server running on localhost through port 5678. The next section will elaborate on how to run the debug server in a container. Launch configuration will vary depending on your project and each project needs to be set up individually. The official doc4 lists out all the supported application types with example configurations. To avoid having to reconfigure the same app repetitively, tracking the entire .vscode directory via source control is probably a good idea. Add docker-compose.debug.yml Next up, we’ll need to update the command section of services.web in the docker-compose.yml to expose the debug server. The debugpy5 tool from Microsoft does that for us. However, instead of changing the docker-compose.yml file for debugging, we can add a separate file for it named docker-compose.debug.yml. Here’s the content of it: # docker-compose.debug.yml version: \"3.9\" services: web: build: context: . dockerfile: ./Dockerfile command: - \"sh\" - \"-c\" - | pip install debugpy -t /tmp \\ \u0026\u0026 python /tmp/debugpy --wait-for-client --listen 0.0.0.0:5678 \\ -m uvicorn main:app --host 0.0.0.0 --port 8000 ports: - 8000:8000 - 5678:5678 Here: sh -c: Selects the shell inside the Docker container. pip install debugpy -t /tmp: Installs the debugpy tool into the /tmp directory of the container. python /tmp/debugpy --wait-for-client --listen 0.0.0.0:5678: Runs debugpy, sets it to wait for a client connection and listen on all network interfaces at port 5678. -m uvicorn main:app --host 0.0.0.0 --port 8000: Starts an uvicorn server hosting the application defined in main:app, making it accessible on all network interfaces at port 8000. Start the debugger Before starting the VScode debugger, go to the project root and run: docker compose -f docker-compose.debug.yml up Now click on the debugger button and select the Python: Remote attach profile to start debugging. Hack away! P.S.: An altruist on Reddit brought to my attention that a more elaborate version of this, with prettier screenshots, can be found in the official documentation6. Python debugging in VS Code ↩︎ starlette ↩︎ uvicorn ↩︎ Debug Python within a container ↩︎ debugpy ↩︎ Python in a container ↩︎ ","permalink":"http://rednafi.com/python/debug_dockerized_apps_in_vscode/","publishDate":"2023-12-22","summary":"Despite using VSCode as my primary editor, I never really bothered to set up the native debugger to step through application code running inside Docker containers. Configuring the debugger to work with individual files, libraries, or natively running servers is trivial1. So, I use it in those cases and just resort back to my terminal for debugging containerized apps running locally. However, after seeing a colleague’s workflow in a pair-programming session, I wanted to configure the debugger to cover this scenario too.\n","tags":["Python","TIL"],"title":"Debugging dockerized Python apps in VSCode"},{"content":"Data classes are containers for your data—not behavior. The delineation is right there in the name. Yet, I see state-mutating methods getting crammed into data classes and polluting their semantics all the time. While this text will primarily talk about data classes in Python, the message remains valid for any language that supports data classes and allows you to add state-mutating methods to them, e.g., Kotlin, Swift, etc. By state-mutating method, I mean methods that change attribute values during runtime. For instance: from dataclasses import dataclass @dataclass class Person: name: str age: int def make_older(by: int = 1) -\u003e None: self.age += by In this case, calling the make_older method will change the value of age in-place. Every time I spot a data class decked out with such methods, I feel like I’m looking at the penguin with an elephant head1 from the Family Guy. Whenever I traverse down to see how the instances of the class are being used, more often than not, I find them being treated just like regular mutable class instances with fancy reprs. But if you only need a nice repr for your large OO class, adding a __repr__ to the class definition is not that difficult. Why pay the price for building heavier data class instances only for that? In Python, data classes are considerably slower2 to define and import compared to vanilla classes. However, they serve a different purpose than your typical run-of-the-mill classes. When you decorate a class with the @dataclass decorator without changing any of the default parameters, Python automatically generates __init__, __eq__, and __repr__ methods. If you set @dataclass(order=True), it’ll also generate __lt__, __le__, __gt__, and __ge__ special methods that enable you to compare and sort the data class instances. All of this implicates that the construct was specifically designed to contain rich data that provides the means for you to create nice abstractions around lower-level primitives. My gripe isn’t against using data classes because of their heavier size. If it were, Python probably wouldn’t be one of my favorite languages. I use data classes all the time and love how they often allow me to craft nicer APIs with little effort. My issue is when people add state-mutating methods to data classes. The moment you’re doing that, you’re breaking the semantics of the data structure. You probably wouldn’t use hashmaps to represent sequential data even though Python currently maintains3 the insertion order of the keys in dicts. In Kotlin, I almost always define immutable data classes and pass them around in different functions that perform transformations and calculations. In Python, however, instantiating frozen data classes (@dataclass(frozen=True)) is almost twice as slow4 compared to mutable data classes. So I just set slots=True to make the instantiation quicker and call it a day. But in either case, if I need to add a method that mutates the attributes of the class instance, I reconsider whether a data class is the right abstraction for the problem at hand. The necessity to add a state-mutating method is an indicator that you need a regular OO class. You’ll signal incorrect intent to the reader if you keep using data classes in this context. Dataclasses are also great candidates for domain modeling with types. With the help of mypy, you can leverage sum types5 to emulate ADTs6 as follows (using PEP-6957 generic syntax): from dataclasses import dataclass @dataclass(slots=True) class Barcode[T: str | int]: code: T @dataclass(slots=True) class Sku[T: str | int]: # Stock Keeping Unit code: T type ProductId = Barcode | Sku | None But it only works if your data containers don’t exhibit any behavior. Here the data classes are just labels for values in a set that can contain the instances of the classes. Adding state-mutating methods to either Barcode or Sku would break the semantics of how these types can be composed. I still think it’s okay if you need to validate the data class attributes in a __post_init__ method or override the __eq__ or __hash__ for some reason. Read-only methods are also acceptable since they don’t do in-place state modification. Comparing two data class instances that have read-only methods is not as awkward as comparing data class instances with methods that mutate attributes. So if you need to slap a method on a data class, write a function and pass the instance as a parameter or write a normal class with a repr and add the method there. This way, the reader won’t have to wonder whether your data containers have some hidden behavior attached to them or not. Penguin with an elephant head – Family Guy ↩︎ Improving data classes startup performance ↩︎ Dicts are now ordered, get used to it ↩︎ Frozen data classes are slower ↩︎ Sum types ↩︎ Algebraic Data Types in (typed) Python ↩︎ PEP 695 – Type Parameter Syntax ↩︎ ","permalink":"http://rednafi.com/python/dataclasses_and_methods/","publishDate":"2023-12-16","summary":"Data classes are containers for your data—not behavior. The delineation is right there in the name. Yet, I see state-mutating methods getting crammed into data classes and polluting their semantics all the time. While this text will primarily talk about data classes in Python, the message remains valid for any language that supports data classes and allows you to add state-mutating methods to them, e.g., Kotlin, Swift, etc. By state-mutating method, I mean methods that change attribute values during runtime. For instance:\n","tags":["Python"],"title":"Banish state-mutating methods from data classes"},{"content":"Despite being an IC for the bulk of my career, finding my groove amidst the daily torrent of meetings from the early hours has always felt like balancing on a seesaw during a never-ending earthquake. Now, pair that with the onslaught of Slack inquiries and the incessant chiming of email notifications, and you have a front-row ticket to the anxiety circus. There are days when carving out a single hour of focus time is a wild goose chase, pushing me to work after hours to get stuff done, followed by a guilt trip about screen-gazing my life away.\nMeetings in the corporate software world come in two flavors: one where you’re charting the work terrain or dissecting issues, and the other is a procession of process meetings. Daily standups, one-on-ones with the manager, monthly all hands, and a confetti of other mandatory rendezvous that often makes you question your career choice at the tail of a chaotic but unproductive day. Although the meeting marathon for an IC isn’t as grueling as what a product manager or team lead endures, it’s still a hefty list that often sidelines the real deal—doing the actual work you were hired for.\nNow, railing against the corporate sky is useless. I’m well aware that far more astute minds have written about this many times before. Also, some of these meetings are paramount in a larger organization for coordinating work among many different teams and keeping the stakeholders in the loop. But that doesn’t mean things couldn’t improve. One thing that I’ve experienced is that I work better when all the meetings are clustered together in the morning, so there’s room for a couple of hours of deep work after 1-2 PM every day. That way, I can do all the meetings, write all the emails and Slack messages, do all the busy work before lunch, and then tune into focused work until the end of the day.\nOf course, achieving this nirvana is easier said than done, especially if your manager or coworkers don’t work in the same cadence. Neither should a team adopt an immutable work style that isn’t flexible enough to cater to changes. Plus, chaos induced by the messages and meetings between different teams is often beyond your control. The best you can probably do is talk to your manager, propose a potential workflow, and see whether it works for them and your coworkers.\nI’m yet to discover the unicorn’s horn that’ll allow me to deftly toggle between meetings and in-zone work without having to dodge a few anxiety attacks during work hours. Whether you’re a manager or an IC, if you have a story of how your current or previous team tackled this issue or have a better idea, I’d love to hear about it!\n","permalink":"http://rednafi.com/zephyr/finding_flow_amid_chaos/","publishDate":"2023-11-25","summary":"Despite being an IC for the bulk of my career, finding my groove amidst the daily torrent of meetings from the early hours has always felt like balancing on a seesaw during a never-ending earthquake. Now, pair that with the onslaught of Slack inquiries and the incessant chiming of email notifications, and you have a front-row ticket to the anxiety circus. There are days when carving out a single hour of focus time is a wild goose chase, pushing me to work after hours to get stuff done, followed by a guilt trip about screen-gazing my life away.\n","tags":["Essay"],"title":"Finding flow amid chaos"},{"content":"Ever been in a situation where you landed a software engineering job with a particular tech stack, mastered it, switched to another company with a different stack, nailed that too, and then found yourself in a third company that used the original stack? Now, you suddenly sense that your hard-earned acumen in that initial stack has not only atrophied over the years but also a portion, or all of it, has become irrelevant, making it a bit of a struggle to catch up with the latest changes.\nAfter graduation, I switched gears from electrical to software engineering. I started out as a junior data scientist at an e-commerce startup. There, I juggled tasks like training small-scale machine learning models, analyzing tabular data, and building visualizations with the Python data stack. When COVID hit, I jumped ship to another company using a different tech stack, shifting my attention to distributed system and backend engineering.\nIn my current role, I’m still mostly doing backend work, just with a different set of tools than before. Throughout this journey, I’ve been fortunate enough to dart around three different continents. While hopping between positions and tech stacks has definitely widened my horizon, I’ve been grappling with the observation that as I pick up new skills, some of the older ones depreciate at a faster pace. It’s quite difficult to keep yourself sharp with tools that you don’t get to use regularly at work.\nWith all the hype around LLMs lately, I’m feeling drawn back to my original data science roots. To rekindle that part of my brain, I’ve started dabbling in some of my old OSS work, and to my great surprise, I’m struggling quite a bit to pick up the fundamentals and the required mathematics since I’ve been out of the game for so long. While I’ve kept in touch with some part of the open-source data world to stay relevant, apparently that wasn’t enough. On top of that, the world keeps piling on new concepts and skills that I’ll need to pick up if I ever intend to get past the interview barrier and professionally work in this arena again.\nTurns out this manifestation of stochastic knowledge decay is a well-studied phenomenon. The term half-life of knowledge1 was coined in 1962 by economist Fritz Machlup2 to represent the amount of time that has to elapse before half of the knowledge or facts in a particular area is superseded or shown to be untrue. It’s named after the half-life of decay in radioactive materials. This article3 on the IEEE Spectrum dives deep into the concept and reflects upon its effect on the industry. It postulates that the half-life of engineering knowledge is shrinking faster than ever before and the only way to tackle this is through continuous learning and getting better at managing the onslaught of information.\nI don’t have a prescriptive solution for this. I wrote this text to start a discussion around a feeling I previously struggled with but didn’t know how to label. So far, engaging with the OSS community on topics I find exciting, taking meticulous notes, tracking my learning progress, adopting boring technology, and writing about them have helped me stay relevant. However, this approach isn’t bulletproof and is quite susceptible to lack of motivation at the tail end of a 40-hour workweek. If you’ve experienced something similar and found a solution that worked for you to some extent, I’d love to hear about it!\nHalf-life of knowledge ↩︎\nFritz Machlup ↩︎\nHalf-life of knowledge pressures employers to seek out young engineers ↩︎\n","permalink":"http://rednafi.com/zephyr/diminishing_half_life_of_knowledge/","publishDate":"2023-11-12","summary":"Ever been in a situation where you landed a software engineering job with a particular tech stack, mastered it, switched to another company with a different stack, nailed that too, and then found yourself in a third company that used the original stack? Now, you suddenly sense that your hard-earned acumen in that initial stack has not only atrophied over the years but also a portion, or all of it, has become irrelevant, making it a bit of a struggle to catch up with the latest changes.\n","tags":["Essay"],"title":"The diminishing half-life of knowledge"},{"content":"Adopting existing tools that work, applying them to the business problems at hand, and quickly iterating in the business domain rather than endlessly swirling in the vortex of technobabble is woefully underrated. I’ve worked at two kinds of companies before:\nOne that only cares about measurable business outcomes, accruing technical debt and blaming engineers when no one wants to work with their codebase, ultimately hurting the product. Another that has staff engineers spending all day on linter configurations and writing seldom-read RFCs while juniors want to ditch Celery for Kafka because the latter is hip.\nWhile both are equally bad, technical people love to lambaste the former while remaining blissfully ignorant about the second type. Maybe because there’s no incentive for doing that and resume-driven development1 genuinely pays better. As long as companies keep making people solve obscure puzzles that has nothing to do with the job or hiring managers keep employing automated systems to look for keywords in resumes, a group of smart people will always engage in techno-maximalism to prepare for the next big opportunity; setting the underlying product up for failure.\nThe bigger and more established the company is, the more the properties of the second type start manifesting. Add middle managers with zero ideas of what the worker bees are cooking underneath, and you have the perfect recipe for disaster. Raking in absurd sums to tweak linters or buttons may not be the worst thing in the world, if it also didn’t lead these bored people to dream of becoming architecture astronauts by introducing absurdly complex tools to solve imaginary problems.\nThe situation exacerbates when companies start introducing useless metrics like LOCs, PR counts, or the number of feature tickets to quantify developer productivity. This often leads to the creation of needless tickets and kickstarts the vicious PR cycle where developers endlessly debate the best practices, micro-optimizations, gratuitous niceties, and everything else other than the core business logic. If working on the business logic isn’t rewarded, why should anyone focus on making that better? Obviously it’s more profitable to introduce a dead letter queue to the callpath of an RPC instead of just writing a retry decorator and monitoring if that works or not.\nNow that microservices are no longer in vogue, and numerous companies have been burnt by adopting the Netflix way of working, despite not having that level of revenue or manpower, there’s no shortage of articles2 on how bad it is to adopt SoA when a PostgreSQL-backed Django monolith would probably do the job. Also, how terrible GraphQL is when a simple denormalized secondary index would suffice, or how the high churn rate of JavaScript frontend frameworks has wasted time, effort, and money. However, few of them mention how organizational structures and policies force people to take that route.\nThere must be a middle ground where developers can focus on the core business logic that yields the most value without incurring technical debt and making the development process a nightmare. I don’t have an answer for that, nor have I worked at a company that found the perfect balance. Plus, I’m not a technical lead, manager, or business owner. So if you are one of them, I’d love to hear how you or your organization plans to tackle this.\nResume-driven development ↩︎\nThe costs of microservices ↩︎\n","permalink":"http://rednafi.com/zephyr/oh_my_poor_business_logic/","publishDate":"2023-11-05","summary":"Adopting existing tools that work, applying them to the business problems at hand, and quickly iterating in the business domain rather than endlessly swirling in the vortex of technobabble is woefully underrated. I’ve worked at two kinds of companies before:\nOne that only cares about measurable business outcomes, accruing technical debt and blaming engineers when no one wants to work with their codebase, ultimately hurting the product. Another that has staff engineers spending all day on linter configurations and writing seldom-read RFCs while juniors want to ditch Celery for Kafka because the latter is hip.\n","tags":["Essay"],"title":"Oh my poor business logic"},{"content":"I like writing custom scripts to automate stuff or fix repetitive headaches. Most of them are shell scripts, and a few of them are written in Python. Over the years, I’ve accumulated quite a few of them. I use Git and GNU stow1 to manage them across different machines, and the workflow2 is quite effective. However, as the list of scripts grows larger, invoking them becomes a pain because the tab completion results get cluttered with other system commands. Plus, often I even forget the initials of a script’s name and stare at my terminal while the blinking cursor facepalms at my stupidity.\nI was watching this amazing talk3 by Brandon Rhodes that proposes quite an elegant solution to this problem. It goes like this:\nAll your scripts should start with a character as a prefix that doesn’t have any special meaning in the shell environment. Another requirement is that no other system command should start with your chosen character.\nThat way, when you type the prefix character and hit tab, only your custom scripts should appear and nothing else. This works with your aliases too!\nThe dilemma here is picking the right character that meets both of the requirements. Luckily, Brandon did the research for us. Turns out, the shell environment uses pretty much all the characters on the keyboard as special characters other than these 6:\n@ _ + - : , Among them, the first 5 requires pressing the Shift key, which is inconvenient. But the plain old comma , is right there. You can start your script or alias names with a comma , and it’ll be golden.\nMy tab completion looks like this:\nrednafi@air:~/canvas/rednafi.com $ , ,brclr ,clear-cache ,docker-prune-containers ,redis ,brpre ,docker-nuke ,docker-prune-images ,www All my aliases start with , too so that they also appear in the list with the custom scripts. Fin!\nGNU stow ↩︎\nDotfile stewardship for the indolent ↩︎\nActivation energy — Brandon Rhodes ↩︎\n","permalink":"http://rednafi.com/misc/pesky_little_scripts/","publishDate":"2023-10-29","summary":"I like writing custom scripts to automate stuff or fix repetitive headaches. Most of them are shell scripts, and a few of them are written in Python. Over the years, I’ve accumulated quite a few of them. I use Git and GNU stow1 to manage them across different machines, and the workflow2 is quite effective. However, as the list of scripts grows larger, invoking them becomes a pain because the tab completion results get cluttered with other system commands. Plus, often I even forget the initials of a script’s name and stare at my terminal while the blinking cursor facepalms at my stupidity.\n","tags":["Shell"],"title":"Pesky little scripts"},{"content":"There are a few ways you can add URLs to your Markdown documents:\nInline links\n[inline link](https://example.com) This will render as inline link.\nReference links\n[reference link] Define the link destination elsewhere in the document like this:\n[reference link]: https://example.com This will render the same way as before, reference link.\nFootnote style reference links\nfootnote style reference link[^1] Define the link destination using a footnote reference:\n[^1]: https://example.com This will render a bit differently with a clickable number beside the origin text that refers to the backref at the bottom of the document. Like this: footnote style reference link1.\nTry clicking on the number within the square brackets and see how the page scrolls down to the corresponding backref link that lives with other backrefs at the tail of the page.\nThe inline link approach is the most prevalent one as it’s also the easiest one to write. But it suffers from a few issues:\nLinks scattered throughout your documents can make updates cumbersome. Reusing a link elsewhere requires multiple copy-pastes. Placing several links side by side can feel awkward, and URL stylings like blue highlighting or underlining make things noisy. To add a reference section, you’ll have to create a separate segment, usually at the bottom of your page, and duplicate the URLs. On mobile devices, accidentally fat-fingering a URL can promptly redirect readers away from your content, potentially against their intention. Enforcing a line width limit can be challenging due to lengthy inlined URLs. The reference link approach solves some of these issues since you won’t have to scatter the URLs across your document or repeat them multiple times for multiple usage. This also allows you to use a Markdown formatter to enforce a maximum line width. I use prettier2 to cap the line width at 92 characters and the formatter works better when it doesn’t have to shimmy around multiple long inline URLs.\nThis is certainly better than using inline links, but it still suffers from all the other issues that plague the former approach. Creating a reference section still requires some repetition, and juxtaposing multiple links remains awkward. Also, accidental misclicks that take you to a different page remains an issue.\nThe footnote-style reference link comes to the rescue. It keeps the document clean by moving all URLs to the bottom in a dedicated reference section. The small superscript numbers don’t distract the reader as much but provide an easy way to navigate to the corresponding links if needed. Accidental clicks are no longer an issue since clicking on a reference superscript will bring the user down to the footnote section where they can click on the concomitant URL or jump back to the origin by tapping on the backref (↩︎) symbol. The reference section also allows you to provide more context on each link, like a title or description.\nMoreover, adding multiple links to the same origin is straightforward since you simply add the footnote numbers like this34. Plus, you don’t have to manually create a separate reference section; it automatically gets created for you as you start adding footnotes. See the reference section in this post and click on the backref links to go back to the origin. Most parsers like GitHub flavored Markdown5 now support footnotes out of the box.\nRecently, I’ve spent an entire evening converting almost all of the inline links on this site into footnote style references in a semi-automated manner. I still use reference links here and there but mostly prefer footnotes since they allow me to avoid repetition and subjectively look less distracting compared to underlined or highlighted URLs. And suddenly, prettier’s2 job has become easier too!\nhttps://example.com ↩︎\nhttps://prettier.io/ ↩︎ ↩︎\nFootnotes with extra texts https://rednafi.com ↩︎\nMultiple footnotes are less distracting than multiple side-by-side URLs https://rednafi.com/index ↩︎\nGitHub flavored Markdown ↩︎\n","permalink":"http://rednafi.com/zephyr/footnotes_for_the_win/","publishDate":"2023-10-07","summary":"There are a few ways you can add URLs to your Markdown documents:\nInline links\n[inline link](https://example.com) This will render as inline link.\nReference links\n[reference link] Define the link destination elsewhere in the document like this:\n[reference link]: https://example.com This will render the same way as before, reference link.\nFootnote style reference links\nfootnote style reference link[^1] Define the link destination using a footnote reference:\n[^1]: https://example.com This will render a bit differently with a clickable number beside the origin text that refers to the backref at the bottom of the document. Like this: footnote style reference link1.\n","tags":["Essay"],"title":"Footnotes for the win"},{"content":"I’m one of those people who will sit in front of a computer for hours, fiddling with algorithms or debugging performance issues, yet won’t spend 10 minutes to improve their workflows. While I usually get away with this, every now and then, my inertia slithers back to bite me. The latest episode was me realizing how tedious it is to move config files across multiple devices when I was configuring a new MacBook Air and Mac Mini at the same time. I dislike customizing tools and tend to use the defaults as much as possible. However, over the years, I’ve accumulated a few config files here and there, which were historically backed up in a git repository and restored manually whenever necessary. MacOS’s time machine made sure that I didn’t need to do it very often. So I never paid much attention to it. But recently, I came across GNU stow1 and realized that people have been using it for years to manage their configs. I tried it and found that it works perfectly for what I need. It’s a nifty little tool written in perl that allows you to store all of your config files in a git repository and symlink them to the targeted directories. The tool is pretty versatile and you can do a lot more than just dotfile management. But for this purpose, only two commands will do. The workflow roughly goes like this: ┌─────────────────┐ │git repo [source]│ └┬────────────────┘ ┌▽────────────────────────────────────────────────────────┐ │dotfiles [zsh/.zshrc, zsh/.zprofile, git/.gitconfig, ...]│ └┬────────────────────────────────────────────────────────┘ ┌▽───────────────────────┐ │gnu stow creates symlink│ └┬───────────────────────┘ ┌▽───────────────────────────┐ │home directory [destination]│ └┬───────────────────────────┘ ┌▽────────────────────────────────────────────────────────────┐ │symlinked dotfiles [~/.zshrc, ~/.zprofile, ~/.gitconfig, ...]│ └─────────────────────────────────────────────────────────────┘ All of your config files will need to live in a git repo and their directory trees will have to match the desired folder structure of the destination. That means, if you need to restore a certain config file to ~/.config/app/.conf, then in the source repo, the file needs to live in the pkg1/.config/app/.conf directory. The source’s top-level directory pkg1 is called a package and can be named anything. While invoking stow, we’ll refer to a particular dotfile by the package it lives within. Run: stow -v -R -t ~ pkg1 Here: -v (or --verbose) makes stow run in verbose mode. When you use -v, stow will list the symlinks it creates or updates, making it easier to see the changes it’s making. -R (or --restow) tells stow to restow the packages. It’s useful when you’ve already stowed the packages previously, and want to reapply them. The -R flag ensures that stow re-symlinks files, even if they already exist. This makes each run idempotent and you won’t have to worry about polluting your workspace with straggler links. -t (or --target=) specifies the target directory where stow should create symlinks. The default target directory is the parent of $pwd. In the above command, -t ~ is used to set the home directory as the destination. is the package name you want to stow. For a more concrete example, let’s say, your source repo ~/canvas/dot has two packages named git and zsh where the former contains .gitconfig and the latter houses .zshrc and .zprofile files: # ~/canvas/dot zsh ├── .zprofile └── .zshrc git └── .gitconfig To symlink both of them to the home directory, you’ll need to run the following command from the root of the source directory; ~/canvas/dot in this case: stow -v -R -t ~ zsh git Then you can see the newly created symlinks in the home directory with this: ls -lah ~ | grep '^l' It prints: lrwxr-xr-x 1 rednafi staff 25 Sep 23 19:45 .gitconfig -\u003e canvas/dot/git/.gitconfig lrwxr-xr-x 1 rednafi staff 24 Sep 23 19:52 .zprofile -\u003e canvas/dot/zsh/.zprofile lrwxr-xr-x 1 rednafi staff 21 Sep 23 19:45 .zshrc -\u003e canvas/dot/zsh/.zshrc If you want to remove a config file, you can unstow it with: unstow -v -R -t ~ pkg1 or, manually remove the symlink with: unlink ~/pkg1 One neat side effect of managing configs in this manner is that, since symlinks are pointers to the original files living in the source repo, any changes made to the source files are automatically reflected in the destination configs. Here are my dotfiles2 and a few management scripts in all their splendor! GNU stow ↩︎ Dotfiles ↩︎ ","permalink":"http://rednafi.com/misc/dotfile_stewardship_for_the_indolent/","publishDate":"2023-09-27","summary":"I’m one of those people who will sit in front of a computer for hours, fiddling with algorithms or debugging performance issues, yet won’t spend 10 minutes to improve their workflows. While I usually get away with this, every now and then, my inertia slithers back to bite me. The latest episode was me realizing how tedious it is to move config files across multiple devices when I was configuring a new MacBook Air and Mac Mini at the same time.\n","tags":["Shell","TIL"],"title":"Dotfile stewardship for the indolent"},{"content":"Every once in a while, I love browsing the Wayback Machine1 to catch a glimpse of the early internet. I enjoy the waves of nostalgic indie hacker vibes that wash over me as I type a URL into the search box and click to see an old snapshot of the site frozen in time. Being a kid of the early ’00s, I missed the spectacular cosmic genesis of the ’90s internet in its entire nascent glory. However, I did briefly get a coup d’œil of the raw, untainted web 1.0 right before SEO firms, ads, pop-ups, modals, autoplays, and heavy frontend frameworks started prowling and gentrifying the whole space into an anxiety-inducing corporate circus.\nBefore social media and other corporate silos became mainstream, the web had, for the lack of a better word, more character to it. From IRC rooms to personal blogs with weird pixelated designs and sound effects, the realm was fragmented, chaotic, heterogeneous, and mostly, a lot of fun. There were no best practices around doing anything, and the notion of creativity wasn’t tied to the craft of how many keywords or interactive animations you could shove into a page to get the most eyeballs. You were the cool kid if you could just wire together a few pages with Perl, PHP, Python, or even Bash, and put something useful out there.\nNow, I don’t want to come across as one of those anachronistic hipster millennials, reeking of RMS energy, who will reject anything that’s even remotely modern. Neither am I under the illusion that web 1.0 was a blissful nirvana, and we should’ve halted the progress wheel there. I wouldn’t want to relive the past of loading jQuery-heavy pages on my crappy 2G mobile network or the security horrors of PHP CMSes. Also, I’d prefer not all the websites to adopt the user-hostile-pitch-black-text-on-blinding-white design. It wouldn’t be functional, and the UX would probably drive away most folks.\nBut there’s absolutely no denying that the raw, unfiltered chaos of the old web had a magical spark to it that the sterile, framework-stuffed UIs of today often lack. While the mighty legion of the framework overlords and their thousand minions seem unstoppable in their quest to homogenize the web, it doesn’t have to be this way everywhere. There are still pockets where the web’s wild spirit can run free, untouched by the unrelenting march of frameworks stomping out all traces of creativity.\nWhenever I see technology mughals like Peter Norvig or Rob Pike showcase their thoughts through ancient relics like this2 or this3, it does put a smile on my face. No gaudy animations, no fancy fonts, no dark mode—just strings of thoughts laid bare against a backdrop of naked HTML. If you don’t like the design, just load the content with Safari or Firefox reader and you’re golden. Of course, not all websites have to be like this but not every website has to look like Stripe’s4 either.\nPeople often conflate this brutalist5 approach to designing websites with user-hostile, overly minimalist, and lazy design. The opposite of UI maximalism doesn’t need to be UX belligerence. Your site can look like absolute poop but still be responsive on any device and provide a great UX, like this absolutely hilarious rant page6. It wraps perfectly on your laptop, phone, or tablet, and doesn’t need to load 2 MB static assets. Plus, who cares about the layout when the content is this interesting?\nThe neo-grotesque web keeps the minimalist design of the brutalist philosophy while maintaining a neat UX and emphasizing content rather than fluff. Bring back those single-column personal blogs where writers would just list out a few heartfelt contents and call it a day. Bring back those gigachad dynamic websites like Craigslist7 with their unadorned buttons and blue URLs that, UI-wise, give those million-dollar react-y pages a run for their money. As long as the pages wrap on your phone, what’s there to complain about? Ugly can still be beautiful. Excrement contains nutrients. All hail the neo-grotesque web!\nWayback machine ↩︎\nRob Pike’s blog ↩︎\nPeter Norvig’s blog ↩︎\nStripe website ↩︎\nBrutalism in web design ↩︎\nhttps://motherfuckingwebsite.com/ ↩︎\nCraigslist ↩︎\n","permalink":"http://rednafi.com/zephyr/an_ode_to_the_neo_grotesque_web/","publishDate":"2023-09-18","summary":"Every once in a while, I love browsing the Wayback Machine1 to catch a glimpse of the early internet. I enjoy the waves of nostalgic indie hacker vibes that wash over me as I type a URL into the search box and click to see an old snapshot of the site frozen in time. Being a kid of the early ’00s, I missed the spectacular cosmic genesis of the ’90s internet in its entire nascent glory. However, I did briefly get a coup d’œil of the raw, untainted web 1.0 right before SEO firms, ads, pop-ups, modals, autoplays, and heavy frontend frameworks started prowling and gentrifying the whole space into an anxiety-inducing corporate circus.\n","tags":["Essay"],"title":"An ode to the neo-grotesque web"},{"content":"This site1 is built with Hugo2 and served via GitHub pages3. Recently, I decided to change the font here to make things more consistent across different devices. However, I didn’t want to go with Google Fonts for a few reasons: CDN is another dependency. Hosting static assets on GitHub Pages has served me well. Google Fonts tracks users and violates4 GDPR in Germany. Google Analytics does that too. But since I’m using the latter anyway, this might come off a bit apocryphal. I wanted to get a few extra Lighthouse points. Turns out, it’s pretty easy to host the fonts yourself. Download the fonts I found this fantastic webfont helper tool5 that allows you to search for any Google font and download it. You can specify the font style, thickness, and browser support status. I’ve used it to download Schibsted Grotesk for text and JetBrains Mono for code snippets, targeting only modern browsers. You might want to pick Legacy Support if you need compatibility with older browsers and Historic Support for the really old ones. After downloading, unzip the file and place the fonts in the /static/fonts folder in your root directory. If you’ve selected the Modern Browsers option, then the fonts will come in web-optimized woff2 format. Sweet! Paste the CSS While downloading the fonts, you may have already noticed that the helper tool also generates the CSS snippet required to link the fonts from the host storage. Here’s a sample: /* schibsted-grotesk-regular - latin */ @font-face { font-display: swap; font-family: 'Schibsted Grotesk'; font-style: normal; font-weight: 400; src: url('../fonts/schibsted-grotesk-v3-latin-regular.woff2') format('woff2'); } /* schibsted-grotesk-italic - latin */ @font-face { font-display: swap; font-family: 'Schibsted Grotesk'; font-style: italic; font-weight: 400; src: url('../fonts/schibsted-grotesk-v3-latin-italic.woff2') format('woff2'); } /* truncated */ Copy the generated CSS and paste it somewhere in your header.css or assets/css/extended/header-override.css file if you’re overriding a theme. Edit the src attribute to reflect your font’s path: @font-face { font-display: swap; font-family: 'Schibsted Grotesk'; font-style: normal; font-weight: 400; /* url('../fonts/schibsted-grotesk-v3-latin-regular.woff2'); */ src: url('/fonts/schibsted-grotesk-v3-latin-regular.woff2') format('woff2'); } Here, you’ll need to change ../fonts/ to /fonts/, and Hugo will take care of the rest. Notice there’s no /static prefix in the font’s path. Find this blog’s header-override.css6 if you’re facing any trouble while doing it. Serve your website locally and ensure that the fonts are being loaded and displayed correctly. Deploy! Site source ↩︎ Hugo ↩︎ GitHub Pages ↩︎ Google Fonts GDPR violation ↩︎ Webfont helper tool ↩︎ Header CSS ↩︎ ","permalink":"http://rednafi.com/misc/self_hosted_google_fonts_in_hugo/","publishDate":"2023-09-14","summary":"This site1 is built with Hugo2 and served via GitHub pages3. Recently, I decided to change the font here to make things more consistent across different devices. However, I didn’t want to go with Google Fonts for a few reasons:\nCDN is another dependency. Hosting static assets on GitHub Pages has served me well. Google Fonts tracks users and violates4 GDPR in Germany. Google Analytics does that too. But since I’m using the latter anyway, this might come off a bit apocryphal. I wanted to get a few extra Lighthouse points. Turns out, it’s pretty easy to host the fonts yourself.\n","tags":["TIL"],"title":"Self-hosted Google Fonts in Hugo"},{"content":"Suppose, you have a function that takes an option struct and a message as input. Then it stylizes the message according to the option fields and prints it. What’s the most sensible API you can offer for users to configure your function? Observe:\n// app/src package src // Option struct type Style struct { Fg string // ANSI escape codes for foreground color Bg string // Background color } // Display the message according to Style func Display(s *Style, msg string) {} In the src package, the function Display takes a pointer to a Style instance and a msg string as parameters. Then it decorates the msg and prints it according to the style specified in the option struct. In the wild, I’ve seen 3 main ways to write APIs that let users configure options:\nExpose the option struct directly Use the option constructor pattern Apply functional option constructor pattern Each comes with its own pros and cons.\nExpose the option struct In this case, you’d export the Style struct with all its fields and let the user configure them directly. The previous snippet already made the struct and fields public. From another package, you could import the src package and instantiate Style like this:\npackage main import \"app/src\" // Users instantiate the option struct c := \u0026src.Style{ \"\\033[31m\", // Maroon \"\\033[43m\", // Yellow } // Then pass the struct to the function Display(c, \"Hello, World!\") To configure option fields, mutate the values in place:\nc.Fg = \"\\033[35m\" // Magenta c.Bg = \"\\033[40m\" // Black This works but will break users’ code if new fields are added to the option struct. But your users can instantiate the struct with named parameters to avoid breakage:\nc := \u0026src.Style{ Fg: \"\\033[31m\", // Maroon // Bg will be implicitly set to an empty string } In this case, the field that wasn’t passed would assume the corresponding zero value. For instance, Bg will be initialized as an empty string. However, this pattern puts the responsibility of retaining API compatibility on the users’ shoulders. So if your code is meant for external use, there are better ways to achieve option configurability.\nOption constructor Go standard library extensively uses this pattern. Instead of letting the users instantiate Style directly, you expose a NewStyle constructor function that constructs the struct instance for them:\npackage src // same as before // NewStyle option constructor instantiates a Style instance func NewStyle(fg, bg string) *Style { return \u0026Style{fg, bg} } It’ll be used as follows:\npackage main import \"app/src\" // The users will now use NewStyle to instantiate Style c := src.NewStyle( \"\\033[31m\", // Maroon \"\\033[43m\", // Yellow ) Display(c, \"Hello, World!\") If a new field is added to Style, update NewStyle to have a sensible default value for it or initialize the struct with named parameters to set the optional fields to their respective zero values. This avoids breaking users’ code as long as the constructor function’s signature doesn’t change.\npackage src type Style struct { Fg string Bg string Und bool // Underline or not } // Function signature unchanged though new option field added // Set sensible default in constructor function func NewStyle(fg, bg string) *Style{ return \u0026Style{ Fg: fg, Bg: bg, // Und will be implicitly set to false } } In NewStyle, we implicitly set the value of Und to false but you can be explicit there depending on your needs. The struct fields can be updated in the same manner as before:\npackage main c := src.NewStyle( \"\\033[31m\", // Maroon \"\\033[43m\", // Yellow ) c.Und = true // Default is false, we're setting it to true src.Display(c, \"Hello, World!\") This should cover most use cases. However, if you don’t want to export the underlying option struct, or your struct has tons of optional fields requiring extensibility, you’ll need an extra layer of indirection to avoid the need to accept a zillion config parameters in your option constructor.\nFunctional option constructor As mentioned at the tail of the last section, this approach works better when your struct contains many optional fields and you need your users to be able to configure them if they want. Go doesn’t allow setting non-zero default values for struct fields. So an extra level of indirection is necessary to let the users configure them. This approach also allows us to make the option struct private so that there’s no ambiguity around API usage.\nLet’s say style now has two optional fields und and zigzag that allows users to decorate the message string with underlines or zigzagged lines:\npackage src type style struct { fg string bg string und bool // Optional field zigzag bool // Optional field } Now, we’ll define a new type called styleoption like this:\n// package src type styleoption func(*style) The styleoption function accepts a pointer to the option struct and updates a particular field with a user-provided value. The implementation of this type would look as such:\nfunc (s *style) {s.fieldName = fieldValue} Next, we’ll need to define a higher order config function for each optional field in the struct where the function will accept the field value and return another function with the styleoption signature. The WithUnd and WithZigzag wrapper functions will be a part of the public API that the users will use to configure style:\n// We only define config functions for the optional fields func WithUnd(und bool) styleoption { return func(s *style) { s.und = und } } func WithZigzag(zigzag bool) styleoption { return func(s *style) { s.zigzag = zigzag } } Finally, our option constructor function needs to be updated to accept variadic options. Observe how we’re looping through the options slice and applying the field config functions to the struct pointer:\nfunc NewStyle(fg, bg string, options ...styleoption) *style { s := \u0026style{fg: fg, bg: bg} // und and zigzag are set to false // Apply all the styleoption functions returned from // field config functions. for _, opt := range options { opt(s) } return s } The users will use the code like this to instantiate style and update the optional fields:\nc := src.NewStyle( \"\\033[31m\", \"\\033[43m\", src.WithUnd(true), // Default is false, but we're setting it to true src.WithZigzag(true), // Default is false ) The required fields fg and bg must be passed while constructing the option struct. The optional fields can be configured with the field config functions like WithUnd and WithZigzag.\nThe complete snippet looks as follows:\npackage src // We can keep the option struct private type style struct { fg string bg string und bool // Optional field zigzag bool // Optional field } // This can be private too since the users won't need it directly type styleoption func(*style) // We only define public config functions for the optional fields func WithUnd(und bool) styleoption { return func(s *style) { s.und = und } } func WithZigzag(zigzag bool) styleoption { return func(s *style) { s.zigzag = zigzag } } // Options are variadic but the required fiels must be passed func NewStyle(fg, bg string, options ...styleoption) *style { // You can also initialize the optional values explicitly s := \u0026style{fg: fg, bg: bg} for _, opt := range options { opt(s) } return s } I first came across this pattern in Rob Pike’s blog1 on the same topic.\nVerdict While the functional constructor pattern is the most intriguing one among the three, I almost never reach for it unless I need my users to be able to configure large option structs with many optional fields. It’s rare and the extra indirection makes the code inscrutable. Also, it renders the IDE suggestions useless.\nIn most cases, you can get away with exporting the option struct Stuff and a companion function NewStuff to instantiate it. For another canonical example, see bufio.Read and bufio.NewReader in the standard library.\nSelf-referential functions and the design of options - Rob Pike ↩︎\nFunctional options for friendly APIs - Dave Cheney 2 ↩︎\nFunctional options pattern in Go - Matt Boyle 3 ↩︎\n","permalink":"http://rednafi.com/go/configure_options/","publishDate":"2023-09-05","summary":"Suppose, you have a function that takes an option struct and a message as input. Then it stylizes the message according to the option fields and prints it. What’s the most sensible API you can offer for users to configure your function? Observe:\n// app/src package src // Option struct type Style struct { Fg string // ANSI escape codes for foreground color Bg string // Background color } // Display the message according to Style func Display(s *Style, msg string) {} In the src package, the function Display takes a pointer to a Style instance and a msg string as parameters. Then it decorates the msg and prints it according to the style specified in the option struct. In the wild, I’ve seen 3 main ways to write APIs that let users configure options:\n","tags":["Go"],"title":"Configuring options in Go"},{"content":"I was curious to see if I could prototype a simple load balancer in a single Go script. Go’s standard library and goroutines make this trivial. Here’s what the script needs to do:\nSpin up two backend servers that’ll handle the incoming requests. Run a reverse proxy load balancer in the foreground. The load balancer will accept client connections and round-robin them to one of the backend servers; balancing the inbound load. Once a backend responds, the load balancer will relay the response back to the client. For simplicity, we’ll only handle client’s GET requests. Obviously, this won’t have SSL termination, advanced balancing algorithms, or session persistence like you’d get with Nginx1 or Caddy2. The point is to understand the basic workflow and show how Go makes it easy to write this sort of stuff.\nArchitecture Here’s an ASCII art that demonstrates the grossly simplified end-to-end workflow:\n+----------------------------------------+ | Load balancer (8080) | | +----------------------------------+ | | | Request from client | | | +-----------------|----------------+ | | | Forward request | | | to backend | | v | | +----------------------------------+ | | | Load balancing | | | | +----------+ +----------+ | | | | | Backend | | Backend | | | | | | 8081 | | 8082 | | | | | +----------+ +----------+ | | | +-----------------|----------------+ | | | Distribute load | | v | | +----------------------------------+ | | | Backend Servers | | | | +----------+ +----------+ | | | | | Response | | Response | | | | | | body | | body | | | | | +----------+ +----------+ | | | +----------------------------------+ | | | Send response | | v | | +----------------------------------+ | | | Client receives response | | | +----------------------------------+ | +----------------------------------------+ The diagram shows a load balancer receiving client requests on port 8080. It distributes the requests between the backends, sending each request either to a backend running on port 8081 or 8082. The selected backend processes the incoming request and returns a response through the balancer. The balancer then routes the backend’s response back to the client.\nTools we’ll need Here are the stdlib tools we’ll be using. Everything will live in the main.go script:\n// main.go package main import (\"fmt\"; \"io\"; \"net/http\"; \"sync\") A few global variables // main.go // ... truncated previous sections var ( backends = []string{ \"http://localhost:8081/b8081\", \"http://localhost:8082/b8082\", } currentBackend int backendMutex sync.Mutex ) The backends slice declares a list of backend server URLs that will be load-balanced between.\nThe currentBackend integer variable keeps track of the index of the backend server that handled the most recent request. This will be used later to perform the round-robin load balancing between the backends.\nThe backendMutex lock provides mutually exclusive access to the shared variables. We’ll see how it’s used when we write the load balancing algorithm3.\nWriting the backend server The backend is a simple server that’ll just write a message to the connected client, denoting which server is handling the request.\n// main.go // ... truncated previous sections // Start a backend server on the specified port func startBackend(port int, wg *sync.WaitGroup) { // Signals the lb when a backend is done processing a request defer wg.Done() http.HandleFunc(fmt.Sprintf(\"/b%d\", port), func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \"Hello from backend server on :%d\\n\", port) }) addr := fmt.Sprintf(\":%d\", port) fmt.Printf(\"Backend is listening on :%d \\n\", port) err := http.ListenAndServe(addr, nil) if err != nil { fmt.Printf(\"Error for server on :%d; %s\\n\", port, err) } } The startBackend function starts a backend HTTP server listening on a given port. It takes the port number and a sync.WaitGroup. When startBackend returns, it calls Done() on the wait group to signal the load balancer that the backend has finished processing a request. The function then registers a handler that responds with the port number. It starts listening and serving on the provided port, printing any errors. We’ll run this as goroutines to spin up two backends on ports 8081 and 8082.\nSelecting backend servers in a round-robin fashion When a request from a client hits the load balancer, it’ll need a way to figure out which backend server to relay the request to. Here’s how it does that:\n// main.go // ... truncated previous sections // Get the next backend server to forward the request to // in a round-robin fashion. This function is thread-safe func getNextBackend() string { backendMutex.Lock() defer backendMutex.Unlock() backend := backends[currentBackend] currentBackend = (currentBackend + 1) % len(backends) return backend } The getNextBackend() function implements round-robin load balancing across the backends slice in a thread-safe manner. It works like this:\nAcquire a lock on backendMutex to prevent concurrent access to the shared state. Read the index of the current backend server from currentBackend. Increment currentBackend to point to the next backend server. The modulo % operation wraps around the index to the start when it reaches past the end. Release the lock on backendMutex. Return the URL of the backend at the index we read in step 2. This allows each request handling goroutine to safely get the next backend server in a round-robin fashion. The mutex prevents race conditions where two goroutines try to read/write the shared currentBackend and backends state at the same time.\nThe mutex lock synchronizes access to the shared state across concurrent goroutines. This is necessary because Go’s HTTP server handles requests concurrently by default. Without the mutex, the goroutines could overwrite each other’s changes to currentBackend, leading to incorrect load balancing behavior.\nWriting the load-balancing server The load balancer itself is a server that sits between the backends and the clients. We can write its handler function as such:\n// main.go // ... truncated previous sections // Handle incoming requests and forward them to the backend func loadBalancerHandler(w http.ResponseWriter, r *http.Request) { // Pick a backend in round-robin fashion backend := getNextBackend() // Relay the client's request to the backend resp, err := http.Get(backend) if err != nil { http.Error(w, \"Backend Error\", http.StatusInternalServerError) return } defer resp.Body.Close() // Copy the backend response headers and propagate them to the client for key, values := range resp.Header { for _, value := range values { w.Header().Set(key, value) } } // Copy the backend response body and propagate it to the client io.Copy(w, resp.Body) } The loadBalancerHandler() function forwards incoming requests from the clients to the backend servers. First, it calls getNextBackend() to retrieve the next backend server. It then makes an HTTP GET request to that backend using http.Get().\nIf there are any errors calling the backend, it just returns a 500 error to the client. Otherwise, it copies the backend’s headers and response body into the response writer to propagate them back to the client.\nThis allows transparently load balancing each request across the backends in a round-robin fashion. The client only sees a single load balancer endpoint. Behind the scenes, requests are distributed to the dynamic backend servers based on round-robin ordering. Copying headers and response bodies ensures clients get the proper responses from the chosen backends.\nWiring them up together Finally, the main function here just starts the backend servers on port 8081-8082 and the load balancing server on port 8080:\n// main.go // ... truncated previous sections func main() { var wg sync.WaitGroup ports := []int{8081, 8082} // Starts the backend servers in the background for _, port := range ports { wg.Add(1) go startBackend(port, \u0026wg) } // Starts the load balancer server in the foreground http.HandleFunc(\"/\", loadBalancerHandler) fmt.Println(\"Load balancer is listening on :8080\") err := http.ListenAndServe(\":8080\", nil) if err != nil { fmt.Printf(\"Error: %s\\n\", err) } } Taking it for a spin You can find the self-contained complete implementation in this gist4. Run the server in one terminal with:\ngo run main.go It’ll print the port numbers of the backend and the load-balancing servers:\nBackend is listening on :8082 Backend is listening on :8081 Load balancer is listening on :8080 Then from another console, make a few GET requests with curl:\nfor i in {1..4}; do curl http://localhost:8080/ done This prints:\nHello from backend server on :8081 Hello from backend server on :8082 Hello from backend server on :8081 Hello from backend server on :8082 Notice how the client requests are handled by different backends in an interleaving manner.\nNginx ↩︎\nCaddy ↩︎\nSelecting backend server in a round robin fashion ↩︎\nComplete implementation ↩︎\n","permalink":"http://rednafi.com/go/dummy_load_balancer/","publishDate":"2023-08-30","summary":"I was curious to see if I could prototype a simple load balancer in a single Go script. Go’s standard library and goroutines make this trivial. Here’s what the script needs to do:\nSpin up two backend servers that’ll handle the incoming requests. Run a reverse proxy load balancer in the foreground. The load balancer will accept client connections and round-robin them to one of the backend servers; balancing the inbound load. Once a backend responds, the load balancer will relay the response back to the client. For simplicity, we’ll only handle client’s GET requests. Obviously, this won’t have SSL termination, advanced balancing algorithms, or session persistence like you’d get with Nginx1 or Caddy2. The point is to understand the basic workflow and show how Go makes it easy to write this sort of stuff.\n","tags":["Go","TIL"],"title":"Dummy load balancer in a single Go script"},{"content":"I was cobbling together a long-running Go script to send webhook messages to a system when some events occur. The initial script would continuously poll a Kafka topic for events and spawn new goroutines to make HTTP requests to the destination. This had two problems: It could create unlimited goroutines if many events arrived quickly It might overload the destination system by making many concurrent requests In Python, I’d use just asyncio.Semaphore to limit concurrency. I’ve previously written about this here1. Turns out, in Go, you could do the same with a buffered channel. Here’s how the naive version looks: package main import (\"fmt\"; \"sync\") func worker(id int, wg *sync.WaitGroup) { defer wg.Done() // ... Send http post request fmt.Println(\"Sending webhook request\") } func main() { var wg sync.WaitGroup nWorkers := 10 for i := 1; i \u003c= nWorkers; i++ { wg.Add(1) go worker(i, \u0026wg) } wg.Wait() fmt.Println(\"All workers have completed\") } We’re sending the webhook request in the worker function. It takes an integer ID for bookkeeping and a pointer to a WaitGroup instance for synchronization. Once it finishes making the request, it signals the WaitGroup with wg.Done(). In the main function, we spawn 10 workers as goroutines and wait for all of them to finish work with wg.Wait(). Without the wait-group synchronization, the main goroutine would bail before all the background workers finish their work. In the above scenario, all the requests were made in parallel. How can we limit the system to only allow n number of concurrent requests at the same time? Sure, you can choose to spin up n number of goroutines and no more. But how do you do it from inside an infinite loop that’s also polling a queue continuously? In this case, I want to throttle the script so that it’ll send 2 requests in parallel and then wait until those are done. Then it’ll wait for a bit before firing up the next batch of 2 goroutines and continuously repeat the same process. Buffered channels allow us to do exactly that. Observe: package main import (\"fmt\"; \"sync\"; \"time\") func worker(id int, sem chan struct{}, wg *sync.WaitGroup) { defer wg.Done() // Acquire semaphore fmt.Printf(\"Worker %d: Waiting to acquire semaphore\\n\", id) sem \u003c- struct{}{} // Do work fmt.Printf(\"Worker %d: Semaphore acquired, running\\n\", id) time.Sleep(10 * time.Millisecond) // Release semaphore \u003c-sem fmt.Printf(\"Worker %d: Semaphore released\\n\", id) } func main() { nWorkers := 10 // Total number of goroutines maxConcurrency := 2 // Allowed to run at the same time batchInterval := 50 * time.Millisecond // Delay between each batch of 2 goros // Create a buffered channel with a capacity of maxConcurrency sem := make(chan struct{}, maxConcurrency) var wg sync.WaitGroup // We start 10 goroutines but only 2 of them will run in parallel for i := 1; i \u003c= nWorkers; i++ { wg.Add(1) go worker(i, sem, \u0026wg) // Introduce a delay after each batch of workers if i % maxConcurrency == 0 \u0026\u0026 i != nWorkers { fmt.Printf(\"Waiting for batch interval...\\n\") time.Sleep(batchInterval) } } wg.Wait() close(sem) // Remember to close the channel once done fmt.Println(\"All workers have completed\") } The clever bit here is the buffered channel named sem which acts as a semaphore to limit concurrency. We set its capacity to the max number of goroutines we want running at once, in this case 2. Before making the request, each worker goroutine tries to acquire the semaphore by sending a value into the channel via sem \u003c- struct{}{}. The value itself doesn’t matter. So we’re just sending an empty struct to avoid redundant allocation. Sending data to the channel will block if it’s already full, essentially meaning all permits are taken. Once the send succeeds, the goroutine has acquired the semaphore and is free to proceed with its work. When finished, it releases the semaphore by reading from the channel \u003c-sem. This frees up a slot in the channel for another goroutine to acquire it. By using this semaphore channel to limit access to critical sections, we can precisely control the number of concurrent goroutines. This channel-based semaphore gives us more flexibility than just using a WaitGroup. Combining it with a buffered channel provides fine-grained control over simultaneous goroutine execution. The buffer size of the channel determines the allowed parallelism, 2 here. We’ve also thrown in an extra bit of delay after each batch of operation finishes with: // Introduce additional delay after each batch of workers if i % maxConcurrency == 0 \u0026\u0026 i != nWorkers { fmt.Printf(\"Waiting for batch interval...\\n\") time.Sleep(batchInterval) } Running the script will show that although we’ve started 10 goroutines in the main function, only 2 of them run at once. Also, there’s a delay of 3 seconds between each batch. We can tune it according to our need to be lenient on the consumer. Waiting for batch interval... Worker 2: Waiting to acquire semaphore Worker 2: Semaphore acquired, running Worker 1: Waiting to acquire semaphore Worker 1: Semaphore acquired, running Worker 1: Semaphore released Worker 2: Semaphore released Waiting for batch interval... Worker 4: Waiting to acquire semaphore Worker 4: Semaphore acquired, running Worker 3: Waiting to acquire semaphore Worker 3: Semaphore acquired, running Worker 3: Semaphore released Worker 4: Semaphore released Waiting for batch interval... ... Now, you might want to add extra abstractions over the core behavior to make it more ergonomic. Here’s a pointer2 on how to do so. Effective Go also mentions3 this pattern briefly. Limit concurrency with semaphore ↩︎ Go concurrency pattern: semaphore ↩︎ Effective Go - channels ↩︎ How to wait until buffered channel semaphore is empty 4 ↩︎ ","permalink":"http://rednafi.com/go/limit_goroutines_with_buffered_channels/","publishDate":"2023-08-23","summary":"I was cobbling together a long-running Go script to send webhook messages to a system when some events occur. The initial script would continuously poll a Kafka topic for events and spawn new goroutines to make HTTP requests to the destination. This had two problems:\nIt could create unlimited goroutines if many events arrived quickly It might overload the destination system by making many concurrent requests In Python, I’d use just asyncio.Semaphore to limit concurrency. I’ve previously written about this here1. Turns out, in Go, you could do the same with a buffered channel. Here’s how the naive version looks:\n","tags":["Go","Concurrency Patterns","TIL"],"title":"Limit goroutines with buffered channels"},{"content":"A TOTP1 based 2FA system has two parts. One is a client that generates the TOTP code. The other part is a server. The server verifies the code. If the client and the server-generated codes match, the server allows the inbound user to access the target system. The code usually expires after 30 seconds and then, you’ll have to regenerate it to be able to authenticate.\nAs per RFC-62382, the server shares a base-32 encoded secret key with the client. Using this shared secret and the current UNIX timestamp, the client generates a 6-digit code. Independently, the server also generates a 6-digit code using the same secret string and its own current timestamp. If the user-entered client code matches the server-generated code, the auth succeeds. Otherwise, it fails. The client’s and the server’s current timestamp wouldn’t be an exact match. So the algorithm usually adjusts it for ~30 seconds duration.\nI wanted to see if I could write a TOTP client and use it like Google Authenticator3 to log into my 2FA-enabled4 GitHub account. Turns out Go’s standard library lets you do that with only a couple of lines of code. Here’s the fully annotated implementation:\npackage main import ( \"crypto/hmac\"; \"crypto/sha1\"; \"encoding/base32\"; \"encoding/binary\"; \"strings\"; ) func generateTOTP(secretKey string, timestamp int64) uint32 { // The base32 encoded secret key string is decoded to a byte slice base32Decoder := base32.StdEncoding.WithPadding(base32.NoPadding) secretKey = strings.ToUpper(strings.TrimSpace(secretKey)) // preprocess secretBytes, _ := base32Decoder.DecodeString(secretKey) // decode // The truncated timestamp / 30 is converted to an 8-byte big-endian // unsigned integer slice timeBytes := make([]byte, 8) binary.BigEndian.PutUint64(timeBytes, uint64(timestamp) / 30) // The timestamp bytes are concatenated with the decoded secret key // bytes. Then a 20-byte SHA-1 hash is calculated from the byte slice hash := hmac.New(sha1.New, secretBytes) hash.Write(timeBytes) // Concat the timestamp byte slice h := hash.Sum(nil) // Calculate 20-byte SHA-1 digest // AND the SHA-1 with 0x0F (15) to get a single-digit offset offset := h[len(h)-1] \u0026 0x0F // Truncate the SHA-1 by the offset and convert it into a 32-bit // unsigned int. AND the 32-bit int with 0x7FFFFFFF (2147483647) // to get a 31-bit unsigned int. truncatedHash := binary.BigEndian.Uint32(h[offset:]) \u0026 0x7FFFFFFF // Take modulo 1_000_000 to get a 6-digit code return truncatedHash % 1_000_000 } Use it as such:\n// Import \"time\" and \"fmt\" // ... func main() { // Collect it from a TOTP server like GitHub 2FA panel secretKey := \"6AXIS2D4ST9CXAW2\" // This is a fake one! now := time.Now().Unix() totpCode := generateTOTP(secretKey, now) fmt.Printf(\"Current TOTP code: %06d\\n\", totpCode) } This prints the following code and will keep printing the same one for the next 30 seconds if you rerun the script multiple times:\nCurrent TOTP code: 134624 Here are the detailed implementation steps:\nTrim whitespace and convert the base32 encoded secret key string to uppercase Decode the preprocessed secret key from base32 to a byte slice Get the current timestamp, divide by 30, and convert it to an 8-byte big-endian unsigned integer Concatenate the timestamp integer bytes with the decoded secret key bytes Hash the concatenated bytes to get a 20-byte SHA-15 digest Get the last byte of the SHA-1 digest and AND it with 0x0F (15) to mask off all but the last 4 bits to get an offset index from 0-15 Use the offset index to truncate the SHA-1 digest to get a 32-bit unsigned integer AND the 32-bit integer with 0x7FFFFFFF (2147483647) to mask off the most significant bit and convert to an unsigned 31-bit integer Take modulo 1_000_000 of the 31-bit integer to get a 6-digit TOTP code Return the 6-digit TOTP code To test the implementation, I collected a secret key from GitHub’s 2FA panel6. Then I logged into my account by inputting a TOTP code generated by this script. Worked flawlessly!\nTwilio docs - TOTP ↩︎\nRFC-6238 ↩︎\nGoogle authenticator ↩︎\nSecuring your account with two factor authentication ↩︎\nSHA-1 ↩︎\n2FA panel ↩︎\n","permalink":"http://rednafi.com/go/totp_client/","publishDate":"2023-08-20","summary":"A TOTP1 based 2FA system has two parts. One is a client that generates the TOTP code. The other part is a server. The server verifies the code. If the client and the server-generated codes match, the server allows the inbound user to access the target system. The code usually expires after 30 seconds and then, you’ll have to regenerate it to be able to authenticate.\nAs per RFC-62382, the server shares a base-32 encoded secret key with the client. Using this shared secret and the current UNIX timestamp, the client generates a 6-digit code. Independently, the server also generates a 6-digit code using the same secret string and its own current timestamp. If the user-entered client code matches the server-generated code, the auth succeeds. Otherwise, it fails. The client’s and the server’s current timestamp wouldn’t be an exact match. So the algorithm usually adjusts it for ~30 seconds duration.\n","tags":["Go","TIL"],"title":"Writing a TOTP client in Go"},{"content":"I love Go’s implicit interfaces. While convenient, they can also introduce subtle bugs unless you’re careful. Types expected to conform to certain interfaces can fluidly add or remove methods. The compiler will only complain if an identifier anticipates an interface, but is passed a type that doesn’t implement that interface. This can be problematic if you need to export types that are required to implement specific interfaces as part of their API contract.\nHowever, there’s a way you can statically check interface conformity at compile time with zero runtime overhead. Turns out, this was always buried in Effective Go1. Observe:\nimport \"io\" // Interface guard var _ io.ReadWriter = (*T)(nil) type T struct { //... } func (t *T) Read(p []byte) (n int, err error) { // ... } func (t *T) Write(p []byte) (n int, err error) { // ... } We’re checking if struct T implements the io.ReadWriter interface. It needs to have both Read and Write methods defined. The type conformity is explicitly checked via var _ io.ReadWriter = (*T)(nil). It verifies that a nil pointer to a value of type T conforms to the io.ReadWriter interface. The code will fail to compile if the type ever stops matching the interface.\nThis is only possible because nil values in Go can assume many different2 types. In this case, var _ io.ReadWriter = T{} will also work, but then you’ll have to fiddle with different zero values if the type isn’t a struct. One important thing to point out is that we’re using _ because we don’t want to accidentally refer to this nil pointer anywhere in our code. Also, trying to access any method on it will cause runtime panic.\nHere’s another example borrowed from Uber’s style guide3:\nNo check:\ntype Handler struct { //... } func (h *Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) { //... } Check:\ntype Handler struct { // ... } // Interface guard var _ http.Handler = (*Handler)(nil) func (h *Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) { //... } Neat, but don’t abuse this. Effective Go warns4:\nDon’t do this for every type that satisfies an interface, though. By convention, such declarations are only used when there are no static conversions already present in the code, which is a rare event.\nInterface checks - Effective Go ↩︎\nnils in Go ↩︎\nCheck interface compliance - Uber style guide ↩︎\nDon’t abuse interface checks ↩︎\nInterface guards - Caddy docs 5 ↩︎\nTweet by Matt Boyle 6 ↩︎\n","permalink":"http://rednafi.com/go/interface_guards/","publishDate":"2023-08-18","summary":"I love Go’s implicit interfaces. While convenient, they can also introduce subtle bugs unless you’re careful. Types expected to conform to certain interfaces can fluidly add or remove methods. The compiler will only complain if an identifier anticipates an interface, but is passed a type that doesn’t implement that interface. This can be problematic if you need to export types that are required to implement specific interfaces as part of their API contract.\n","tags":["Go","TIL"],"title":"Interface guards in Go"},{"content":"I enjoy writing about software—the things I learn, the tools I use, and the work I do. Owing to the constraints of the corporate software world, more often than not, you can’t showcase your work or talk about them. At least that’s how it always has been throughout my career. At the same time, as you grow older and start having a life outside of the computer screen, you realize that working on OSS at the tail of a 40+ hour workweek is hard, and maintaining consistency is even harder. On that front, how do you keep track of your progress without losing your sense of purpose as the years fly by?\nI ameliorate this by factoring out the things I learn at or outside work and writing about them publicly. Countless times I’ve found myself looking for stuff on the web only to land on my own website. But this approach isn’t bulletproof: you rarely encounter situations where you get to write about some novel concepts or one of your brilliant epiphanies. Routinely, I find myself writing about just another tool or library that I’ve figured out how to use or another book that’s already considered cliché in my area of interest. Plus, there are already a ton of more detailed or clickbaity posts out there that cover the same ground. So what good will it do if you add another drop to the ocean? Who will even read it?\nThe most recent example of this is when I spent an hour going through the docs1 of log/slog package of Go 1.21 and another two listing out my most common use cases2. I wrote about it despite seeing countless examples of how to use it on the internet; some of them even have the exact same title as mine. But I did that anyway because it helped me echo out my own experience with the tool that I’ll be able to relive in the future should the need arise. The goal here was not to craft the perfect post for a select audience just to get some SEO points. Rather, I wanted to write this for myself, to scratch a very particular itch. If people find it useful, great, but if I find it useful at some point, even better.\nBut occasionally, I do experience those lightbulb moments that beget more original proses like Avoid template pattern in Python3, which get highly lauded by the venerable orange site citizens. However, the general trend is that the majority of these pieces go completely unnoticed. This might be one of them too and that’s perfectly okay. Internet accolades are great, but they need not be the only reason you want to explore and share your thoughts on something. For me, the aim is to uphold a meticulous record of my odyssey, my own Da Vinci’s notebook4, and this post is but another page within!\nslog docs ↩︎\nStructured logging with slog ↩︎\nAvoid template pattern in Python ↩︎\nLeonardo da Vinci’s notebook ↩︎\n","permalink":"http://rednafi.com/zephyr/writing_on_well_trodden_topics/","publishDate":"2023-08-14","summary":"I enjoy writing about software—the things I learn, the tools I use, and the work I do. Owing to the constraints of the corporate software world, more often than not, you can’t showcase your work or talk about them. At least that’s how it always has been throughout my career. At the same time, as you grow older and start having a life outside of the computer screen, you realize that working on OSS at the tail of a 40+ hour workweek is hard, and maintaining consistency is even harder. On that front, how do you keep track of your progress without losing your sense of purpose as the years fly by?\n","tags":["Essay"],"title":"Writing on well-trodden topics"},{"content":"Before the release of version 1.21, you couldn’t set levels for your log messages in Go without either using third-party libraries or writing your own boilerplates. Coming from Python, I’ve always found this odd, considering that this capability has been in the Python standard library forever. However, it seems like the new log/slog subpackage in Go allows you to do that and a whole lot more.\nApart from being able to add levels to log messages, slog also allows you to emit JSON-structured log messages and group them by certain attributes. The ability to do all this in-house is quite neat and I wanted to take it for a spin. The official documentation1 on this is on the terser side but still comprehensive. So, here, instead of repeating the same information, I wanted to write something for me that mainly highlights the most common cases.\nKickoff Here’s how you’d add levels to your log messages:\npackage main import ( \"log/slog\" ) func main() { slog.Debug(\"a debug message\") slog.Info(\"an info message\") slog.Warn(\"a warning message\") slog.Error(\"an error message\") } Running this will print the following output.\n2023/08/10 17:10:11 INFO an info message 2023/08/10 17:10:11 WARN a warning message 2023/08/10 17:10:11 ERROR an error message Notice how the concomitant local time and level are prepended to each log message. Also, observe that the DEBUG message is missing there. That’s because the default log handler will only print messages if the log level is INFO or higher. We’ll see how we can set custom log levels shortly. But before that here’s a quick overview of how the different components of slog work together.\nMachineries The slog package lets you create Logger instances. These instances have methods like Info() and Error() that you can call to log stuff. When you call one of these methods, it creates a Record from the data you passed in and sends it to a Handler. The Handler figures out what to actually do with the log—like print it somewhere or send it over the network. You can write your own or use one of the predefined TextHandler or JSONHandler to format your log output.\nThere’s a default Logger you can use right away with functions like Info() and Error() at the top level. Underneath, the Info() function calls the Logger.Info() method. This means you don’t need to create a Logger instance by hand just to start logging. You’ve already seen how we can use these top-level functions to send different levels of logs to the stdout.\nEach log entry has an associated severity level which is represented by an integer. The more severe the log level is, the higher the value of the integer will be. The default logger only emits LevelInfo or higher levels of log messages. Predefined levels have the following values:\nconst ( LevelDebug Level = -4 LevelInfo Level = 0 LevelWarn Level = 4 LevelError Level = 8 ) Using custom log handlers You can use predefined custom handlers to change the format of your log output. The following snippet creates a new Logger instance from a TextHandler instance and then uses that to print log messages to the stdout:\n// Define a new TextHandler h := slog.NewTextHandler(os.Stdout, nil) // Update the default Logger to use the new handler slog.SetDefault(slog.New(h)) // Use the logger as usual slog.Info(\"an info message\") slog.Warn(\"a warning message\") Running this prints:\ntime=2023-08-10T23:57:39.914-04:00 level=INFO msg=\"an info message\" time=2023-08-10T23:57:39.915-04:00 level=WARN msg=\"a warning message\" The NewTextHandler function has two arguments: the first one takes in a type that implements the io.Writer interface and the second one accepts a HandlerOptions struct. The HandlerOptions struct can be used to customize the output format. We can pass nil for this value if we don’t need to change the handler’s default output format.\nWe’re passing os.Stdout as the first argument to direct the log messages to stdout and nil as the second argument. The NewTextHandler returns a *slog.TextHandler struct pointer which is passed to slog.New to get a new Logger instance. Then we set this newly created Logger as the default one via the slog.SetDefault() function. Finally, the updated logger is used to print an info and a warning message. Notice how the TextHandler output records are constituted as key-value attribute pairs.\nPrinting log messages in JSON format Similar to NewTextHandler, NewJSONHandler can be used to create a JSONHandler, which prints the log records as JSON objects:\n// Define a new TextHandler h := slog.NewJSONHandler(os.Stdout, nil) // Update the default Logger to use the new handler slog.SetDefault(slog.New(h)) // Use the logger as usual slog.Info(\"an info\") slog.Warn(\"a warning\") This prints:\n{\"time\":\"2023-08-11T00:13:44.734365-04:00\",\"level\":\"INFO\",\"msg\":\"an info\"} {\"time\":\"2023-08-11T00:13:44.734505-04:00\",\"level\":\"WARN\",\"msg\":\"a warning\"} Changing log levels You’ve already seen that the default logger only prints log messages of level Info and up. We’ll need to define a custom log handler to change the default log level. Here’s an example that enables printing Debug messages:\nvar programLevel = new(slog.LevelVar) // Info by default h := slog.NewTextHandler(os.Stdout, \u0026slog.HandlerOptions{Level: programLevel}) slog.SetDefault(slog.New(h)) programLevel.Set(slog.LevelDebug) // Update log level to Debug slog.Debug(\"a debug message\") slog.Info(\"an info message\") It’ll print:\ntime=2023-08-10T23:53:16.654-04:00 level=DEBUG msg=\"a debug message\" time=2023-08-10T23:53:16.654-04:00 level=INFO msg=\"an info message\" First, we create an instance of slog.LevelVar with the new allocator. Next, we create a TextHandler instance and the programLevel to the slog.HandlerOptions struct pointer. Then we create a new Logger instance as before and set that as the default logger. In the last step, the programLevel is updated so that it signals the handler to allow emitting Debug messages.\nDefining custom log levels Apart from Debug, Info, Warn, and Error, you can define your own custom log levels. Here’s an example of doing that with the default Logger instance:\n// Defining a few custom levels const ( logMeh = slog.Level(2) logFatal = slog.Level(13) ) // Getting the default logger logger := slog.Default() // Use the Log method on the logger and pass the log level logger.Log(nil, logMeh, \"a meh message\") logger.Log(nil, logFatal, \"a fatal message\") This will return:\n2023/08/11 00:45:35 INFO+2 a meh message 2023/08/11 00:45:35 ERROR+5 a fatal message Observe that you’ll have to use Logger.Log() to pass your custom log level. Another example with a custom log handler:\n// Defining a custom log level const logPanic = slog.Level(15) // Setting up a TextHandler h := slog.NewTextHandler(os.Stderr, nil) // Setting up a logger that uses the TextHandler logger := slog.New(h) // Use the Log method on the logger and pass the log level logger.Log(nil, logPanic, \"a panic message\") This prints:\ntime=2023-08-11T00:52:08.903-04:00 level=ERROR+7 msg=\"a panic message\" Adding or removing log attributes Log attributes are just key-value pairs. The following example appends a new key and a value to the log message:\nslog.Info(\"an info message\", \"new_key\", \"new_value\") 2023/08/11 01:10:18 INFO an info message new_key=new_value To remove attributes from log records, you’ll need to configure your custom handler and create a logger instance from that:\nReplaceAttr := func(group []string, a slog.Attr) slog.Attr { if a.Key == \"time\" { return slog.Attr{} } return slog.Attr{Key: a.Key, Value: a.Value} } // Before removing the time attribute h1 := slog.NewJSONHandler(os.Stdout, nil) slog.SetDefault(slog.New(h1)) slog.Info(\"an info message\") // After removing the time attribute h2 := slog.NewJSONHandler( os.Stdout, \u0026slog.HandlerOptions{ReplaceAttr: ReplaceAttr}, ) slog.SetDefault(slog.New(h2)) slog.Info(\"an info message\") Running this will print the following. The time key no longer exists on the second log record:\n{ \"time\":\"2023-08-11T01:23:58.936984-04:00\", \"level\":\"INFO\", \"msg\":\"an info message\" } {\"level\":\"INFO\",\"msg\":\"an info message\"} The main focus here is the ReplaceAttr function which is used to transform or remove attributes before they are processed by a handler. It accepts two arguments: a slice of group names and an Attr struct. The group name allows attributes to be qualified into different scopes, which we won’t use right now. The Attr contains the Key and Value of the attribute that’s being logged.\nIn this case, ReplaceAttr checks if the attribute key is time and if so, returns an empty Attr struct, effectively signaling the handler not to include that attribute. If the key is not time, it returns the original Attr unchanged.\nAdding sticky attributes Sometimes you want to have a few common attributes that should persist across multiple log calls. This can be done via Logger.With() method:\n// Make the attributes sticky with Logger.With method logger := slog.Default().With(\"sticky_key\" , \"sticky_value\") // Look how we don't need to repeat sticky_key and sticky_value here logger.Info(\"an info message\") logger.Error(\"an error message\") It prints:\n2023/08/11 01:56:44 INFO an info message sticky_key=sticky_value 2023/08/11 01:56:44 ERROR an error message sticky_key=sticky_value The Logger.With() method accepts key-value pairs of attributes. This saves you from passing the same attributes over and over again to make them persist across multiple log calls.\nGrouping log attributes You can group the log attributes for better organization. Adding a group makes the attribute keys of a log record qualified by the group name. What qualify means here can vary depending on whether you’re using a TextHandler or a JSONHandler. Here’s an example that demonstrates both:\n// The first string is the group name and the remaining // strings are key-value attribute pairs group := slog.Group(\"group_a\", \"key_a\", \"value_a\") // Grouping for default Logger slog.Info(\"info message\", group) // For TextHandler textHandler := slog.NewTextHandler(os.Stdout, nil) textLogger := slog.New(textHandler) textLogger.Warning(\"warning message\", group) // For JSONHandler jsonHandler := slog.NewJSONHandler(os.Stdout, nil) jsonLogger := slog.New(jsonHandler) jsonLogger.Error(\"error message\", group) This prints:\n2023/08/11 16:41:12 INFO info message group_a.key_a=value_a time=2023-08-11T16:41:12.072-04:00 level=WARN msg=\"warning message\" \\ group_a.key_a=value_a { \"time\": \"2023-08-11T16:41:12.072635-04:00\", \"level\": \"ERROR\", \"msg\": \"error message\", \"group_a\": { \"key_a\": \"value_a\" } } Here, in the case of the text logger, the log attribute key is qualified by the group name as group_a.key_a. On the other hand, the JSON logger emits the log record in a way where the group name group_a is used as the key of a nested object containing the {\"key_a\": \"value_a\"} log attributes.\nMaking log groups sticky Akin to attributes, you can also make attribute group sticky with the Logger.WithGroup() method:\n// Default logger logger := slog.Default().WithGroup(\"group_a\") logger.Info(\"info message\", \"key_b\", \"value_b\") // Text logger textHandler := slog.NewTextHandler(os.Stdout, nil) textLogger := slog.New(textHandler).WithGroup(\"group_a\") textLogger.Info(\"info message\", \"key_b\", \"value_b\") // JSON logger jsonHandler := slog.NewJSONHandler(os.Stdout, nil) jsonLogger := slog.New(jsonHandler).WithGroup(\"group_a\") jsonLogger.Info(\"info message\", \"key_b\", \"value_b\") This returns:\n2023/08/11 16:11:30 INFO info message group_a.key_b=value_b time=2023-08-11T16:11:30.913-04:00 level=INFO msg=\"info message\" \\ group_a.key_b=value_b { \"time\": \"2023-08-11T16:11:30.913892-04:00\", \"level\": \"INFO\", \"msg\": \"info message\", \"group_a\": { \"key_b\": \"value_b\" } } Directing logs to different sinks The predefined TextHandler and JSONHandler takes in a type that implements the io.Writer interface as the first argument. We can leverage this aspect to change the destination of a structured logger. The following example shows how you can direct the structured log stream to both stdout and a file:\ntype TeeWriter struct { stdout *os.File file *os.File } func (t *TeeWriter) Write(p []byte) (n int, err error) { n, err = t.stdout.Write(p) if err != nil { return n, err } n, err = t.file.Write(p) return n, err } func main() { file, _ := os.Create(\"output.txt\") writer := \u0026TeeWriter{ stdout: os.Stdout, file: file, } h := slog.NewTextHandler(writer, nil) logger := slog.New(h) logger.Info(\"Hello, World!\") } The TeeWriter struct associates stdout and a file handle. It implements a custom Write method to write to both streams, enabling teeing of output. In main(), a TeeWriter instance is created with stdout and a file. A pointer to TeeWriter is then passed to the TextHandler. Next, the TextHandler is used to create a new Logger, so when the Logger logs, the messages go through the TextHandler’s TeeWriter and are written to both the console and a file via the custom Write method.\nLeveraging Attrs and Values for performance When using a logger, you can pass in key-value pairs called Attrs instead of separate keys and values. For example:\nslog.Info(\"info message\", slog.Int(\"some_int\", 7)) This is the same as:\nslog.Info(\"info message\", \"some_int\", 7) There are helper functions like Int(), String(), and Bool() to create Attrs for common types. You can also use Any() to make an Attr for any type.\nThe real benefit is that Attrs are more efficient than separate keys and values. So for max speed, we can use the LogAttrs() instead of Log().\nFor example:\nlogger.LogAttrs(nil, slog.LevelInfo, \"info message\", slog.Int(\"some int\", 7)) This avoids extra allocations while giving the same result as:\nslog.Info(\"info message\", \"some int\", 7) Official documentation - slog ↩︎\n","permalink":"http://rednafi.com/go/structured_logging_with_slog/","publishDate":"2023-08-10","summary":"Before the release of version 1.21, you couldn’t set levels for your log messages in Go without either using third-party libraries or writing your own boilerplates. Coming from Python, I’ve always found this odd, considering that this capability has been in the Python standard library forever. However, it seems like the new log/slog subpackage in Go allows you to do that and a whole lot more.\nApart from being able to add levels to log messages, slog also allows you to emit JSON-structured log messages and group them by certain attributes. The ability to do all this in-house is quite neat and I wanted to take it for a spin. The official documentation1 on this is on the terser side but still comprehensive. So, here, instead of repeating the same information, I wanted to write something for me that mainly highlights the most common cases.\n","tags":["Go","TIL"],"title":"Go structured logging with slog"},{"content":"If you’re a manager, then there’s no shortage of information for you on how to conduct exit interviews. But there aren’t many resources that focus on how to handle them from an employee’s perspective. I’ve been meaning to write a quick piece that isn’t biased by anyone else’s experience and is short enough so that I can quickly jog my memory in the future should the need arise. While I’ve participated in a few of them over the past five years, this text doesn’t attempt to combat the inexorable recency bias that may have seeped into the writing.\nExit interviews are trickier than your typical run-of-the-mill one-on-ones, mostly because:\nIt typically means that you’re resigning voluntarily, and not getting fired By the time it happens, there’s usually no going back There are rarely any objectives that benefit you The gains are low but the stakes can be high Instead of throwing a wall of text sectioned by a bunch of headers, I’m intentionally laying out the actionable items as aphoristic assertions.\nAvoid it if you can Ask for a shorter one if you can’t Try to exit early if you can’t do either The less work your interviewer has put into the meeting, the better No point in rambling on why you’re leaving, be vague It’s a mistake to discuss next steps and future opportunities You can’t change the culture through one last meeting, so don’t try to Your objective opinions can cause more harm than good Last-minute feedback matters less than you think, they’ve heard these before A counteroffer is usually a bad idea for both parties If you sense a trap, silence is your friend Let it be awkward Don’t be a child or throw tantrums Be neutral and don’t react to tantrums Maintain mutual respect for both yourself and your employer Remember, it’s all business at the end of the day And finally, don’t burn any bridges if you don’t have to ","permalink":"http://rednafi.com/zephyr/notes_on_exit_interviews/","publishDate":"2023-08-07","summary":"If you’re a manager, then there’s no shortage of information for you on how to conduct exit interviews. But there aren’t many resources that focus on how to handle them from an employee’s perspective. I’ve been meaning to write a quick piece that isn’t biased by anyone else’s experience and is short enough so that I can quickly jog my memory in the future should the need arise. While I’ve participated in a few of them over the past five years, this text doesn’t attempt to combat the inexorable recency bias that may have seeped into the writing.\n","tags":["Essay"],"title":"Notes on exit interviews"},{"content":"The 100k context window of Claude 21 has been a huge boon for me since now I can paste a moderately complex problem to the chat window and ask questions about it. In that spirit, it recently refactored some pretty gnarly conditional logic for me in such an elegant manner that it absolutely blew me away. Now, I know how bitmasks2 work and am aware of the existence of enum.Flag3 in Python. However, it never crossed my mind that flags can be leveraged to trim conditional branches in such a clever manner that Claude illustrated. But once I looked at the proposed solution, the whole thing immediately clicked for me. The conundrum Here’s a problem that’s similar to what I was trying to solve. Let’s say we have instances of a Client entity that need to be notified when some special event occurs in our system. The notification can happen in three ways: email, webhook, and postal mail. These are the three attributes on the Client class that determine which notification method will be used: @dataclass class Client: email: str url: str address: str The business logic requires that the system must abide by the following rules while sending notifications: If only email is populated, send an email. If only url is populated, send a webhook. If only address is populated, send a postal mail. If email and url are populated, send an email and a webhook. If email and address are populated, only send an email. If url and address are populated, only send a webhook. If all three are populated, send both an email and a webhook. At least one attribute must be populated, or it’s an error. Notice how the business logic wants to minimize sending notifications via postal mail. Postal mails are expensive and will only be sent if address is the only attribute on the Client instance. In any other cases, emails and webhooks are preferred. First shot The notify function takes in a Client object and sprouts a few conditional branches to send notifications while maintaining the business constraints. def notify(client: Client) -\u003e None: \"\"\"Apply business logic and invoke the desired notification handlers.\"\"\" if client.email and not client.url and not client.address: send_email() elif client.url and not client.email and not client.address: send_webhook() elif client.address and not client.email and not client.url: send_mail() elif client.email and client.url and not client.address: send_email() send_webhook() elif client.email and client.address and not client.url: send_email() elif client.url and client.address and not client.email: send_webhook() elif client.email and client.url and client.address: send_email() send_webhook() else: raise ValueError(\"at least one attribute must be populated\") Whoa! Lots of if-else branches for such a simple scenario. Since there are 3 attributes in the complete set, we have to make sure we’re writing 2^3=8 branches to cover all the possible subsets. For 4, 5, 6 … attributes, the number of branches will increase as powers of 2: 2^4=16, 2^5=32, 2^6=64 … and so on. Then our tests will need to be able to verify each of these branches. We can try to apply De Morgan’s law to simplify some of the negation logic. De Morgan’s laws allow us to take the negation of a conditional statement and distribute it across the operators, changing ANDs to ORs and vice versa, and flipping the negation of each component. This can help simplify complex boolean logic statements. So this: if client.email and not client.url and not client.address: ... Can become: if client.email and not (client.url or client.address): ... However, that still doesn’t reduce the number of branches. Bitmasks can help us to get out of this pothole. A quick primer on bitwise operations \u0026 bitmasking Bitwise operations allow manipulating numbers at the individual bit level. This is useful for compactly storing and accessing data, performing fast calculations, and implementing low-level algorithms. Here’s a list of bitwise operations: Bitwise AND (\u0026): Takes two numbers and performs the logical AND operation on each pair of corresponding bits. Returns a number where a bit is 1 only if that bit is 1 in both input numbers. Bitwise OR (|): Takes two numbers and performs the logical OR operation on each pair of corresponding bits. Returns a number where a bit is 1 if that bit is 1 in either or both input numbers. Bitwise XOR (^): Takes two numbers and performs the logical XOR (exclusive OR) operation on each pair of corresponding bits. Returns a number where a bit is 1 if that bit is 1 in exactly one of the input numbers (but not both). Bitwise NOT (~): Takes a single number and flips all its bits. Left shift («): Shifts the bits of a number to the left by a specified number of positions. Zeros are shifted in on the right. Equivalent to multiplying by 2^n where n is the number of positions shifted. Right shift (»): Shifts the bits of a number to the right by a specified number of positions. Zeros are shifted in on the left. Equivalent to integer division by 2^n. Here’s an example displaying these operators: a = 60 # 60 = 0011 1100 b = 13 # 13 = 0000 1101 print(a \u0026 b) # 12 = 0000 1100 (0011 1100 \u0026 0000 1101 = 0000 1100) print(a | b) # 61 = 0011 1101 (0011 1100 | 0000 1101 = 0011 1101) print(a ^ b) # 49 = 0011 0001 (0011 1100 ^ 0000 1101 = 0011 0001) print(~a) # -61 = 1100 0011 (~0011 1100 = 1100 0011) print(a \u003c\u003c 2) # 240 = 1111 0000 (0011 1100 \u003c\u003c 2 = 1111 0000) print(a \u003e\u003e 2) # 15 = 0000 1111 (0011 1100 \u003e\u003e 2 = 0000 1111) Bitmasks are integers that represent a set of flags using bits as boolean values. Bitmasking uses bitwise operators to manipulate and access these flags. A common use of bitmasks is to compactly store multiple boolean values or options in a single integer, where each bit position has a specific meaning if it is 1. In the next section, we’ll use this capability to clip the conditional statements in the notify function. For example, here’s a bitmask representing text style options: # Flags BOLD = 1 # 0000 0001 ITALIC = 2 # 0000 0010 UNDERLINE = 4 # 0000 0100 # Bitmask STYLE = BOLD | ITALIC # 0000 0011 - bold and italic We use powers of 2 (1, 2, 4, 8, etc.) for the flag values so that each bit position corresponds to a single flag, and the flags can be combined using bitwise OR without overlapping. This allows testing and accessing each flag independently: has_bold = STYLE \u0026 BOLD == BOLD # True has_italic = STYLE \u0026 ITALIC == ITALIC # True has_underline = STYLE \u0026 UNDERLINE == UNDERLINE # False And toggle an option on or off using XOR: STYLE ^= BOLD # Toggles BOLD bit on/off You can do a ton of other cool stuff with bitwise operations and bitmasks. However, this is pretty much all we need to know to curtail the twisted conditional branching necessitated by the business logic. Check out this incredibly in-depth article4 from Real Python on this topic if you want to dig deeper into bitwise operations. Pruning conditional branches with flags With all the intros and primers out of the way, we can now start working towards making the notify function more tractable and testable. We’ll do that in 3 phases: First, we’re gonna define a flag-type enum called NotifyStatus which will house all the valid states our notification system can be in. Any state that’s not explicitly defined as an enum variant is invalid. Second, we’ll write a function named get_notify_status that’ll take in a Client object as input, apply the business logic and return the appropriate NotifyStatus enum variant. This function won’t be responsible for dispatching the actual notification handlers; rather, it’ll just map the attribute values of the Client instance to a fitting enum variant. We do this to keep the core business logic devoid of any external dependencies—following Gary Bernhardt’s functional core, imperative shell5 ethos. Finally, we’ll define the notify function that’ll just accept the enum variant returned by the previous function and invoke the desired notification handlers. The NotifyStatus enum is defined as follows: class NotifyStatus(Flag): # Valid primary variants (flags) EMAIL = 1 URL = 2 ADDRESS = 4 # Valid composite variants (bitmasks) EMAIL_URL = EMAIL | URL EMAIL_ADDRESS = EMAIL | ADDRESS URL_ADDRESS = URL | ADDRESS EMAIL_URL_ADDRESS = EMAIL | URL | ADDRESS Here, the EMAIL, URL, and ADDRESS variants correspond to the eponymous attributes on the Client instance. Then we define the composite variants (bitmasks) to compactly represent the valid states the system can be in. For example, EMAIL_URL = EMAIL | URL means that on the Client instance, email and url attributes are populated but address isn’t. Likewise, EMAIL_URL_ADDRESS denotes that all the attributes are populated. The biggest benefit we get from this is that we don’t need to write the negation logic explicitly; the bitmasks encode that information inherently. This representation will grossly simplify the implementation of the business logic. Now, let’s write the get_notify_status function that’ll take in an instance of Client and return the appropriate NotifyStatus variant based on our business logic: def get_notify_status(client: Client) -\u003e NotifyStatus: status = 0 if client.email: status |= NotifyStatus.EMAIL.value if client.url: status |= NotifyStatus.URL.value if client.address: status |= NotifyStatus.ADDRESS.value if status == 0: raise ValueError(\"Invalid status\") return NotifyStatus(status) This is the full implementation of our business logic in its entirety. It checks which of the notification attributes among email, url, and address are populated on the Client object. For each one that is populated, it picks the corresponding variant from the NotifyStatus enum and sets the variant bit in the status integer using bitwise OR. If all three attributes are empty, it raises a ValueError. The final value of status is then used to return the correct NotifyStatus enum variant. On the last step, the notify function can take the NotifyStatus variant returned by the get_notify_status function and dispatch the correct notification handlers like this: def notify(notify_status: NotifyStatus) -\u003e None: # Mapping between enum variants and notification handlers actions = { NotifyStatus.EMAIL: [send_email], NotifyStatus.URL: [send_webhook], NotifyStatus.ADDRESS: [send_mail], NotifyStatus.EMAIL_URL: [send_email, send_webhook], NotifyStatus.EMAIL_ADDRESS: [send_email], NotifyStatus.URL_ADDRESS: [send_webhook], NotifyStatus.EMAIL_URL_ADDRESS: [send_email, send_webhook], } if notify_status not in actions: raise ValueError(\"invalid notify status\") for action in actions[notify_status]: action() Observe how we’ve totally eliminated conditional statements from the notify function. The key takeaway here is that the program flow is now flatter and easier to follow. The core business logic is neatly tucked inside the get_notify_status routine, and the NotifyStatus enum explicitly defines all the valid states that the system can be in. This also means that if a new notification channel pops up, all we’ll need to do is update three flat constructs and write the corresponding tests instead of battling with the twisted conditional statements that we started with. Not too shabby, eh? Claude 2 ↩︎ Bitmasks ↩︎ enum.Flag ↩︎ Python bitwise operators ↩︎ Functional core, imperative shell ↩︎ ","permalink":"http://rednafi.com/python/tame_conditionals_with_bitmasks/","publishDate":"2023-07-29","summary":"The 100k context window of Claude 21 has been a huge boon for me since now I can paste a moderately complex problem to the chat window and ask questions about it. In that spirit, it recently refactored some pretty gnarly conditional logic for me in such an elegant manner that it absolutely blew me away. Now, I know how bitmasks2 work and am aware of the existence of enum.Flag3 in Python. However, it never crossed my mind that flags can be leveraged to trim conditional branches in such a clever manner that Claude illustrated. But once I looked at the proposed solution, the whole thing immediately clicked for me.\n","tags":["Python","TIL"],"title":"Taming conditionals with bitmasks"},{"content":"This morning, while browsing Hacker News, I came across a neat trick1 that allows you to share textual data by leveraging DNS TXT records. It can be useful for sharing a small amount of data in environments that restrict IP but allow DNS queries, or to bypass censorship.\nTo test this out, I opened my domain registrar’s panel and created a new TXT type DNS entry with a base64 encoded message containing the poem A Poison Tree by William Blake. The message can now be queried and decoded with the following shell command:\ndig +short _poem.rednafi.com TXT | sed 's/[\\\" ]//g' | base64 -d The command uses dig to query a TXT DNS record for _poem.rednafi.com, removes any double quotes and spaces from the record value via sed, and then decodes the base64-encoded value via base64 to retrieve the original plaintext message that was stored in the TXT record. Running this will return the decoded content of the record:\nI was angry with my friend; I told my wrath, my wrath did end. I was angry with my foe: I told it not, my wrath did grow. And I watered it in fears, Night \u0026 morning with my tears: And I sunned it with smiles, And with soft deceitful wiles. And it grew both day and night. Till it bore an apple bright. And my foe beheld it shine, And he knew that it was mine. And into my garden stole, When the night had veiled the pole; In the morning glad I see; My foe outstretched beneath the tree. You can also encode image data and retrieve it in a similar manner. If your data is too large to fit in a single record, you can split it into multiple records and concatenate them on the receiving end.\nHowever, there are some limitations to this approach. RFC 10352 says that the total size of a DNS resource record cannot exceed 65535 bytes. Also, the maximum length of the actual text value in a single TXT record is 255 bytes or characters. This doesn’t give us much room to tunnel large amounts of data. Plus, DNS has well-known vulnerabilities like MITM attacks, injection issues, cache poisoning, and DoS. So I’d refrain from transferring any data in this manner that requires a layer of security. Protocols like DANE and DNSSEC aim to address some of these concerns but their adoption is spotty at best. Still, I found the idea of using DNS records as a simple database quite clever!\nUse DNS TXT to share information ↩︎\nDomain names — implementation \u0026 specification ↩︎\n","permalink":"http://rednafi.com/misc/dns_record_to_share_text/","publishDate":"2023-07-17","summary":"This morning, while browsing Hacker News, I came across a neat trick1 that allows you to share textual data by leveraging DNS TXT records. It can be useful for sharing a small amount of data in environments that restrict IP but allow DNS queries, or to bypass censorship.\nTo test this out, I opened my domain registrar’s panel and created a new TXT type DNS entry with a base64 encoded message containing the poem A Poison Tree by William Blake. The message can now be queried and decoded with the following shell command:\n","tags":["Shell","TIL","Networking"],"title":"Using DNS record to share text data"},{"content":"Unless I’m hand rolling my own ORM-like feature or validation logic, I rarely need to write custom descriptors in Python. The built-in descriptor magics like @classmethod, @property, @staticmethod, and vanilla instance methods usually get the job done. However, every time I need to dig my teeth into descriptors, I reach for this fantastic how to1 guide by Raymond Hettinger. You should definitely set aside the time to read it if you haven’t already. It has helped me immensely to deepen my understanding of how many of the fundamental language constructs are wired together underneath. Descriptors are considered fairly advanced Python features and can easily turn into footguns if used carelessly. Recently, while working on an app with a descriptor-based data validator, I discovered a subtle but obvious bug that was hemorrhaging memory all across the app. The app was using a descriptor to validate class variables while simultaneously tracking instances where validation occurred. This validator was being used all over the codebase, so it slowly started blowing up memory usage in the background. The problem is that it was keeping hard references to everything it validated, so none of those objects could get garbage collected. But the really sneaky thing was how slowly and secretly the problem happened—the leakage built up bit by bit over time even when people used the validator in totally innocuous ways. Here’s a simpler example of a validation descriptor that tracks the instances it’s applied to: class Within: # The instances are tracked here _seen = {} def __init__(self, min, max): self.min = min self.max = max def __set_name__(self, instance, name): self.name = name def __get__(self, instance, instance_type): return instance.__dict__[self.name] def __set__(self, instance, value): if not self.min \u003c= value \u003c= self.max: raise ValueError( f\"{value} is not within {self.min} and {self.max}\" ) instance.__dict__[self.name] = value # Track the instances that have been seen. # This is the memory leak. self._seen[instance] = value The Within descriptor validates that the values assigned to instance attributes are within a specified min and max range. It does this by implementing the __set__ and __get__ dunder methods. When the descriptor is accessed via instance.attrname, the __get__ method is called which returns the value from the instance’s dict. When a value is assigned via instance.attrname = value, the __set__ method is called which validates the value is within the min/max bounds before setting it on the instance. A memory leak occurs because the _seen dict keeps a reference to every instance the descriptor has been accessed on. This prevents the instances from being garbage collected even if there are no other references to them. You can use the descriptor and observe the memory leakage like this: import gc class Exam: math = Within(0, 100) physics = Within(0, 100) chemistry = Within(0, 100) def __init__(self, math, physics, chemistry): self.math = math self.physics = physics self.chemistry = chemistry if __name__ == \"__main__\": exam = Exam(30, 50, 40) exam.math = 60 # Delete the exam instance del exam # Force garbage collection gc.collect() # Check the strong reference to the deleted instance print(tuple(Within._seen.items())) Here, we’re defining an Exam class that uses the Within descriptor to apply constraints on the values of the math, physics, and chemistry class variables. Then we initialize the class instance and mutate the math attribute to demonstrate that the validator is working as expected. The instance of the Exam class is saved to the _seen dictionary of the descriptor when the __set__ method is called. Next, we delete the Exam instance and force garbage collection. However, when you run the snippet, you’ll see that it prints the following: ((\u003c__main__.Exam object at 0x10466ca10\u003e, 60),) This indicates that although we’ve deleted the Exam instance, it can’t be fully garbage collected since the Within descriptor’s _seen dictionary holds a strong reference to it. Dispel the malady Once I spotted the bug, the solution was fairly simple. Don’t keep strong references to the class instances if you don’t need to. Also, use a more robust tool like Pydantic2 to perform validation but I digress here! Using a weakref.WeakKeyDictionary instead of a regular dict for _seen would prevent the memory leakage by avoiding strong references to the deleted instances. Since WeakKeyDictionary holds weak references to the keys, if all other strong references to an instance are deleted, the garbage collector can reclaim it. The weak reference in WeakKeyDictionary won’t keep the instance alive. Here’s how you’d modify Within to fix the issue: from weakref import WeakKeyDictionary class Within: _seen = WeakKeyDictionary() # Drop in dict replacement def __init__(self, min, max): ... def __set_name__(self, instance, name): ... def __get__(self, instance, instance_type): ... def __set__(self, instance, value): ... The modified descriptor is a drop-in replacement for the previous one—minus the memory leakage issue. So in the last snippet, when exam is deleted and the gc is called, weakref allows the instance to be garbage collected correctly instead of remaining in memory due to the strong reference in _seen. The weak reference doesn’t interfere with gc freeing up the memory as desired. If you run the demonstration snippet again, this time you’ll see that once we force the gc to collect the garbage, the _seen container gets emptied out. exam = Exam(30, 50, 40) exam.math = 60 # Delete the exam instance del exam # Force garbage collection gc.collect() # Check the strong reference to the deleted instance print(tuple(Within._seen.items())) This will print an empty tuple: () This also means that now Within will only keep track of instances that are alive in memory. Descriptor how to - Raymond Hettinger ↩︎ Pydantic ↩︎ ","permalink":"http://rednafi.com/python/memory_leakage_in_descriptors/","publishDate":"2023-07-16","summary":"Unless I’m hand rolling my own ORM-like feature or validation logic, I rarely need to write custom descriptors in Python. The built-in descriptor magics like @classmethod, @property, @staticmethod, and vanilla instance methods usually get the job done. However, every time I need to dig my teeth into descriptors, I reach for this fantastic how to1 guide by Raymond Hettinger. You should definitely set aside the time to read it if you haven’t already. It has helped me immensely to deepen my understanding of how many of the fundamental language constructs are wired together underneath.\n","tags":["Python","TIL"],"title":"Memory leakage in Python descriptors"},{"content":"Python offers a ton of ways like os.system or os.spawn* to create new processes and run arbitrary commands in your system. However, the documentation usually encourages you to use the subprocess1 module for creating and managing child processes. The subprocess module exposes a high-level run() function that provides a simple interface for running a subprocess and waiting for it to complete. It accepts the command to run as a list of strings, starts the subprocess, waits for it to finish, and then returns a CompletedProcess object with information about the result. For example: import subprocess # Here, result is an instance of CompletedProcess result = subprocess.run([\"ls\", \"-lah\"], capture_output=True, encoding=\"utf-8\") # No exception means clean exit result.check_returncode() print(result.stdout) This prints: drwxr-xr-x 4 rednafi staff 128B Jul 8 12:10 .. -rw-r--r--@ 1 rednafi staff 250B Jul 8 12:10 .editorconfig drwxr-xr-x@ 16 rednafi staff 512B Jul 13 14:47 .git drwxr-xr-x@ 4 rednafi staff 128B Jul 8 12:10 .github ... This works great when you’re carrying out simple and synchronous workflows, but it doesn’t offer enough flexibility when you need to fork multiple processes and want the processes to run in parallel. I was working on a project where I wanted to glue a bunch of programs together with Python and needed a way to run composite shell commands with pipes, e.g. echo 'foo\\nbar' | grep 'foo'. So I got curious to see how I could emulate that in Python. Turns out you can do that easily with subprocess.Popen. This function allows for more control over the subprocess. It starts the process and returns a Popen object immediately, without waiting for the command to complete. This allows you to continue executing code while the subprocess runs in parallel. Popen has methods like poll() to check if the process has finished, wait() to wait for completion, and communicate() for interacting with stdin/stdout/stderr. For example: import subprocess import time procs = [] for ip in (\"1.1.1.1\", \"8.8.8.8\"): print(f\"Pinging {ip}...\") proc = subprocess.Popen([\"ping\", \"-c1\", ip]) procs.append(proc) print(f\"Process {proc.pid} started\") # Do other stuff here while ping is running print(\"Napping for a second...\") time.sleep(1) # Wait for the processes to finish for proc in procs: proc.communicate() print(f\"Process {proc.pid} finished with code {proc.returncode}\") The above example shows how you can fire off subprocess tasks to run in parallel, let them chug along in the background, do other stuff, and then collect the results at the end when you need them. The goal here is to ping a couple of IP addresses in parallel using the subprocess module. First, it creates an empty list to store the processes. Then it loops through the IPs, printing a message and kicking off a ping for each one using Popen() so they run asynchronously in the background. The Popen objects get appended to the procs list. After starting the pings, it simulates doing other work by sleeping for a second. Then it loops through the processes again, waits for each one to finish with communicate(), and prints out the process ID and return code for each ping. Running the script will give you the following result (truncated for brevity): Pinging 1.1.1.1... Process 76242 started Pinging 8.8.8.8... Process 76243 started Napping for a second... 64 bytes from 8.8.8.8: icmp_seq=0 ttl=56 time=26.305 ms 64 bytes from 1.1.1.1: icmp_seq=0 ttl=54 time=27.365 ms --- 8.8.8.8 ping statistics --- 1 packets transmitted, 1 packets received, 0.0% packet loss --- 1.1.1.1 ping statistics --- round-trip min/avg/max/stddev = 26.305/26.305/26.305/0.000 ms Process 76242 finished with code 0 Process 76243 finished with code 0 Now that we can run processes asynchronously and gather results, I’ll demonstrate how I emulated a composite UNIX command using that technique. Emulating UNIX pipes Say you want to emulate the following shell command: ps -ef | head -5 I’m running MacOS. So this returns: UID PID PPID C STIME TTY TIME CMD 0 1 0 0 Fri04PM ?? 23:45.79 /sbin/launchd 0 353 1 0 Fri04PM ?? 3:26.62 /usr/libexec/logd 0 354 1 0 Fri04PM ?? 0:00.09 /usr/libexec/smd 0 355 1 0 Fri04PM ?? 0:25.56 /usr/libexec/UserEventAgent (System) The ps -ef command outputs a full list of running processes, then the pipe symbol sends that output as input to the head -5 command, which reads the first 5 lines from that input and prints just those, essentially slicing off the top 5 processes. We can emulate this in Python as follows: import subprocess # Run 'ps -ef' and pipe the output to 'head -n 5' ps_cmd = subprocess.Popen([\"ps\", \"-ef\"], stdout=subprocess.PIPE) # Run 'head -n 5' and pipe the output of 'ps -ef' to it head_cmd = subprocess.Popen( [\"head\", \"-n\", \"5\"], stdin=ps_cmd.stdout, stdout=subprocess.PIPE, encoding=\"utf-8\", ) stdout, stderr = head_cmd.communicate() print(stdout) This snippet uses the subprocess.Popen to run shell commands and pipe the outputs between them. First, ps_cmd executes ps -ef and sends the full output to the subprocess.PIPE buffer. Next, head_cmd runs head -n 5. The stdin of head_cmd is set to the stdout of ps_cmd. This pipes the stdout from ps_cmd as input to head_cmd. Finally, head_cmd.communicate() runs the composite command and waits for the whole thing to finish. The final output of this snippet is the same as the ps -ef | head -5 command. Here’s another example where we’ll emulate the sha256sum \u003c \u003c(echo 'foo') command. On the left side, sha256sum computes the SHA-256 cryptographic hash of an input. The construct \u003c(echo 'foo') creates a temporary file descriptor containing the output ‘foo’ from echo, which is then redirected via \u003c as standard input to sha256sum. Together this computes and prints the SHA-256 hash of the input string without needing an actual file. In this particular case, we want to compute the hash of 3 different inputs in parallel by spawning three separate processes. import subprocess import os def calculate_hash(plaintext: bytes) -\u003e subprocess.Popen: # Create a new process for each pass of hashing proc = subprocess.Popen( [\"sha256sum\"], stdin=subprocess.PIPE, stdout=subprocess.PIPE ) # Send the plaintext to the stdin of the child process proc.stdin.write(plaintext) # Ensure that the child gets input proc.stdin.flush() return proc procs = [] for _ in range(3): proc = calculate_hash(os.urandom(10)) procs.append(proc) for proc in procs: stdout, _ = proc.communicate() print(stdout.decode(\"utf8\").strip()) Running this snippet will display the 3 hashes: 3db9d86f16a60907f261f97f6b9b3dce97416056dc65b9608921ee80c71885a3 - 1ee3de4990a3bca56454d6b3fb94cba1275c8c2f19a8ce6dca5cb2779b5152a7 - f9aa6903c454c70f037328fa1504bf66700e6fdb20407fe1830223e3acec2028 - First, we define a function called calculate_hash that accepts a bytes plaintext input and returns a subprocess.Popen object. This function will spawn a new child process running the sha256sum command. The stdin and stdout of the child process are configured as subprocess.PIPE using the Popen constructor. This enables data to be piped between the parent and child processes. Inside calculate_hash, the plaintext input is written to the stdin pipe of the child process using proc.stdin.write(). This pipes the data into the child’s standard input stream. Next, proc.stdin.flush() method is called to ensure the child process actually receives the input. The main logic begins by initializing an empty list called procs. Then a loop runs 3 times, each time generating a random 10-byte string using os.urandom. This string is passed to calculate_hash, which spawns a new sha256sum child process, pipes the random data to it, and returns the Popen object representing the child. Each Popen is appended to the procs list, so now there are 3 child processes running in parallel. Finally, the procs list is iterated through and proc.communicate() is called on each Popen instance to read back the stdout pipe from the child. This contains the output of sha256sum, which is the hash of the random input string. The hash is then decoded, stripped, and printed to the console. subprocess ↩︎ Effective Python - Item 52 - Brett Slatkin 2 ↩︎ ","permalink":"http://rednafi.com/python/unix_style_pipeline_with_subprocess/","publishDate":"2023-07-14","summary":"Python offers a ton of ways like os.system or os.spawn* to create new processes and run arbitrary commands in your system. However, the documentation usually encourages you to use the subprocess1 module for creating and managing child processes. The subprocess module exposes a high-level run() function that provides a simple interface for running a subprocess and waiting for it to complete. It accepts the command to run as a list of strings, starts the subprocess, waits for it to finish, and then returns a CompletedProcess object with information about the result. For example:\n","tags":["Python","TIL"],"title":"Unix-style pipelining with Python's subprocess module"},{"content":"The current title of this post is probably incorrect and may even be misleading. I had a hard time coming up with a suitable name for it. But the idea goes like this: sometimes you might find yourself in a situation where you need to iterate through a generator more than once. Sure, you can use an iterable like a tuple or list to allow multiple iterations, but if the number of elements is large, that’ll cause an OOM error. On the other hand, once you’ve already consumed a generator, you’ll need to restart it if you want to go through it again. This behavior is common in pretty much every programming language that supports the generator construct. So, in the case where a function returns a generator and you’ve already consumed its values, you’ll need to call the function again to generate a new instance of the generator that you can use. Observe: from __future__ import annotations from collections.abc import Generator def get_numbers(start: int, end: int, step: int) -\u003e Generator[int, None, None]: yield from range(start, end, step) This can be used like this: numbers = get_numbers(1, 10, 2) for number in numbers: print(number) It’ll return: 1 3 5 7 9 Now, if you try to consume the iterable again, you’ll get empty value. Run this again: for number in numbers: print(number) It won’t print anything since the previous loop has exhausted the generator. This is expected and if you want to loop through the same elements again, you’ll have to call the function again to produce another generator that you can consume. So, the following will always work: for number in get_numbers(): print(number) If you run this snippet multiple times, on each pass, the get_numbers() function will be called again and that’ll return a new generator for you to iterate through. Calling the generator function like this works but here’s another thing that I learned today while reading Effective Python1 by Brett Slatkin. You can create a class with the __iter__ method and yield numbers from it just like the function. Then when you initiate the class, the instance of the class will allow you to loop through it multiple times; each time creating a new generator. I knew that you could create an iterable class by adding __iter__ to a class and yielding values from it. But I wasn’t aware that the you could also iterate through the instance of the class multiple times and the class will run __iter__ on each pass and produce a new generator for you to consume. For example: from __future__ import annotations from collections.abc import Generator class NumberGen: def __init__(self, start: int, end: int, step: int) -\u003e None: self.start = start self.end = end self.step = step def __iter__(self) -\u003e Generator[int, None, None]: yield from range(self.start, self.end, self.step) Now use the class as such: numbers = NumberGen() for number in numbers: print(number) This prints: 1 3 5 7 9 If you run the for-loop again on the number instance, you’ll see that the snippet will print the same numbers again. Here, instantiating the NumberGen class creates a NumberGen instance that is not a generator per se, but can return a generator if you call the iter() function on the instance. When you run the for loop on the instance, it runs the underlying __iter__ method to produce a new generator that the loop can iterate through. This allows you to run the for-loop multiple times on the instance, since each run creates a new generator that the loop can consume. A generator can still only be consumed once but each time you’re running a new for-loop on the above instance, the __iter__ method on it gets called and the method returns a new generator for you to iterate through. This is more convenient than having to repeatedly call a generator function if your API needs to consume a generator multiple times. Effective Python - Brett Slatkin ↩︎ ","permalink":"http://rednafi.com/python/enable_repeatable_lazy_iterations/","publishDate":"2023-07-13","summary":"The current title of this post is probably incorrect and may even be misleading. I had a hard time coming up with a suitable name for it. But the idea goes like this: sometimes you might find yourself in a situation where you need to iterate through a generator more than once. Sure, you can use an iterable like a tuple or list to allow multiple iterations, but if the number of elements is large, that’ll cause an OOM error. On the other hand, once you’ve already consumed a generator, you’ll need to restart it if you want to go through it again. This behavior is common in pretty much every programming language that supports the generator construct.\n","tags":["TIL","Python"],"title":"Enabling repeatable lazy iterations in Python"},{"content":" Around a year ago, I ditched my fancy Linux rig for a beefed-up 16\" MacBook Pro and ever since, it’s been my primary machine for both personal and work stuff. I love how this machine strikes a decent balance between power and portability. However, I often joke that this chonky boy is just a pound shy of being an ENIAC1. It’s a beast of a machine when you need all that power, but certainly isn’t the most convenient contraption to lug around while flying. I work fully remote, but can’t get any work done while traveling and rarely ever need to tap into the full power this thing offers.\nSo I wanted to find an excuse to have separate machines for work and when I’m out and about. Sure, I could’ve gone for a 14\" Pro to make it more portable and all, but here’s the deal: I absolutely detest working on anything that has a screen smaller than 15\". So when Apple dropped the new 15\" Air, I just knew I had to get my hands on that. Plus, I’m gonna stick with my 16\" machine for work anyway, so I’m totally cool with grabbing a less powerful device that still sports a larger screen and doesn’t weigh 5 pounds.\nBut here’s the thing, if you’re not down with the base model and don’t mind rolling with a 14\" screen, the 14\" Pro is actually a better deal. It’s got better I/O, a slightly better screen, and definitely better speakers. Personally, I already have all those perks with my 16\" machine, so my sights were strictly set on the aesthetics and portabitly aspects of the device. And boy, it didn’t disappoint. I went for the 16/256 config and snagged it for around 1500 USD.\nThis isn’t a product review, and I honestly don’t know a squat about reviewing things. It’s simply a brief piece sharing my thoughts on a product I purchased with my own money. You won’t find any affiliated links here—just my genius opinions. Also, this comes from the perspective of a person who won’t be using this as their main work machine.\nI was worried that I’d have a hard time adjusting to the smaller 15\" screen and wouldn’t find the keyboard as spacious. However, the good thing is that the differences were barely noticeable, and the Air still rocks a larger-than-life trackpad. The screen gets a bit less bright than its big brother, but that isn’t a problem since I mostly work indoors. For the price, the screen is terrific, and I don’t have any complaints about it. Oddly enough, I found the lack of external speaker grills aesthetically pleasing, and it still gets plenty loud; noticeably so than the 13\" Air.\nIn terms of appearance, hands down, the Air looks much better than the Pros because of its significantly slimmer body. The 15\" Air is barely half as thick and weighs half as much as the Pros. Did I mention it weighs half as much as the bigger Pro machine? Now, since there’s no fan and the whole SoC is passively cooled, the performance does take a hit when compared with an actively cooled machine. My workflow on this device includes writing stuff on VSCode, running 3-10 Docker containers, developing web applications, and the usual lightweight browsing. Turns out, the passively cooled 8-core M2 can handle all of those like a champ and some more. Also, the 16 GB memory gives me enough leeway to do serious development work every now and then should I need to.\nSo far, performance hasn’t been a bottleneck at all, and I can always resort to the 16\" apparatus if I need to. However, for the work that I usually do, the device is holding up surprisingly well, and I knew exactly what I’d be getting when I picked the Air over the 14\" Pro. One big caveat is that the I/O situation is less than ideal as it only has two USB-C ports, a 3.5 mm headphone jack, and a MagSafe power port—that’s it. Also, it only supports a single monitor, but that’s rarely an issue because where would I even get a monitor in an Airbnb?\nFinally, I won’t even talk about the insanely good battery life, as the Airs have been the reigning champion in that department for years. The 16\" MBP already has great battery life, and the Air matches that with a smaller battery due to having less powerful but more efficient internals. Overall, even considering my recency bias2, this is certainly one of—if not the most—prudent tech purchases that I made this year!\nENIAC ↩︎\nRecency Bias ↩︎\n","permalink":"http://rednafi.com/zephyr/descending_into_the_aether/","publishDate":"2023-07-09","summary":" Around a year ago, I ditched my fancy Linux rig for a beefed-up 16\" MacBook Pro and ever since, it’s been my primary machine for both personal and work stuff. I love how this machine strikes a decent balance between power and portability. However, I often joke that this chonky boy is just a pound shy of being an ENIAC1. It’s a beast of a machine when you need all that power, but certainly isn’t the most convenient contraption to lug around while flying. I work fully remote, but can’t get any work done while traveling and rarely ever need to tap into the full power this thing offers.\n","tags":["Essay"],"title":"Descending into the aether"},{"content":"Over the years, I’ve used the template pattern1 across multiple OO languages with varying degrees of success. It was one of the first patterns I learned in the primordial hours of my software engineering career, and for some reason, it just feels like the natural way to tackle many real-world code-sharing problems. Yet, even before I jumped on board with the composition over inheritance2 camp, I couldn’t help but notice how using this particular inheritance technique spawns all sorts of design and maintenance headaches as the codebase starts to grow. An epiphany This isn’t an attempt to explain why you should prefer composition over inheritance (although you should), as it’s a vast topic and much has been said regarding this. Also, only after a few years of reading concomitant literatures and making enough mistakes in real-life codebases, it dawned on me that opting for inheritance as the default option leads to a fragile design. So I won’t even attempt to tackle that in a single post and will refer to a few fantastic prior arts that proselytized me to the composition cult. The goal of this article is not to focus on the wider spectrum of how to transform subclass-based APIs to use composition but rather to zoom in specifically on the template pattern and propose an alternative way to solve a problem where this pattern most naturally manifests. In the first portion, the post will explain what the template pattern is and how it gradually leads to an intractable mess as the code grows. In the latter segments, I’ll demonstrate how I’ve designed a real-world service by adopting the obvious and natural path of inheritance-driven architecture. Then, I’ll explain how the service can be refactored to escape the quagmire that I’ve now started to refer to as the template pattern hellscape. Only a few moons ago, while watching Hynek Schlawack’s Python 2023 talk aptly titled “Subclassing, Composition, Python, and You”3 and reading his fantastic blog post “Subclassing in Python Redux”4, the concept of adopting composition to gradually phase out subclass-oriented design from my code finally clicked for me. However, it’s not always obvious to me how to locate inheritance metastasis and exactly where to intervene to make the design better. This post is my attempt to distill some of my learning from those resources and focus on improving only a small part of the gamut. The infectious template pattern You’re consciously or subconsciously implementing the template pattern when your API design follows these steps: You have an Abstract Base Class (ABC) with abstract methods. The ABC also includes one or more concrete methods. The concrete methods in the ABC depend on the concrete implementation of the abstract methods. API users are expected to inherit from the ABC and provide concrete implementations for the abstract methods. Users then utilize the concrete methods defined in the ABC class. This pattern enables the sharing of concrete method implementations with subclasses. However, the concrete methods of the baseclass are only valid when the user inherits from the base and implements the abstract methods. Attempting to instantiate the baseclass without implementing the abstract methods will result in a TypeError. Only the subclass can be initialized once all the abstract methods have been implemented. Observe this example: from abc import ABC, abstractmethod class Base(ABC): def concrete_method(self) -\u003e None: # This depends on abstract_method. The user is expected to create # a subclass from Base and implement abstract_method. return self.abstract_method() @abstractmethod def abstract_method(self) -\u003e None: raise NotImplementedError class Sub(Base): def abstract_method(self) -\u003e None: \"\"\"Providing a concrete implementation for the 'abstract_method' from the Base class.\"\"\" print( \"I'm a concrete implementation of the 'abstract_method' of Base.\" ) This is how you use it: sub = Sub() # Notice how we're only using the 'concrete_method' defined in the Base class sub.concrete_method() Here, the abstract Base class is defined by inheriting from the abc.ABC class. Inside Base, there’s a concrete_method that relies on an abstract_method. The concrete_method is defined to call abstract_method, expecting that subclasses will provide their own implementation of abstract_method. If a subclass of Base fails to implement abstract_method, calling concrete_method on an instance of that subclass will raise a NotImplementedError. The snippet also provides an example subclass called Sub, which inherits from Base. Sub overrides the abstract_method and provides its own implementation. In this case, it just prints a statement. By subclassing Base and implementing abstract_method, Sub becomes a concrete class that can be instantiated. The purpose of this design pattern is to define a common interface through the Base class, with the expectation that subclasses will implement specific behavior by overriding the abstract methods, while still providing a way to call those methods through the concrete methods defined in the baseclass. This seemingly innocuous and often convenient bi-directional relationship between the base and sub class tends to become infectious and introduces complexity into all the subclasses that inherit from the base. The dark side of the moon Template pattern seems like the obvious way of sharing code and it almost always is one of the first things that people learn while familiarizing themselves with how OO works in Python. Plus, it’s used extensively in the standard library. For example, in the collections.abc module, there are a few ABCs that you can subclass to build your own containers. I wrote about this5 a few years back. Here’s how you can subclass collections.abc.Sequence to implement a tuple-like immutable datastructure: from typing import Any from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, *args: Any) -\u003e None: self._data = list(args) def __getitem__(self, index: int) -\u003e Any: return self._data[index] def __len__(self) -\u003e int: return len(self._data) You’d use the class as such: seq = CustomSequence(1, 2, 3, 4) assert seq[0] == 1 assert len(seq) == 4 We’re inheriting from the Sequence ABC and implementing the required abstract methods. Here’s the first issue: how do we even know which methods to implement and which methods we get for free? You can consult the documentation6 and learn that __getitem__ and __len__ are the abstract methods that subclasses are expected to implement. In return, the base Sequence class gives you __contains__, __iter__, __reversed__, index, and count as mixin methods. You can also print out the abstract methods by accessing the Sequence.__abstractmethod__ attribute. Sure, you’re getting a lot of concrete methods for free, but suddenly you’re dependent on some out-of-band information to learn about the behavior of your specialized CustomSequence class. The following three sections will briefly explore the issues that deceptively creep up on your codebase when you opt for the template pattern. Elusive public API You’ve already seen the manifestation of this issue in the CustomSequence example. The subclass-oriented code-sharing pattern like this makes it difficult to discover the public API of your specialized class because many of its functionalities come from the concrete mixin methods provided by the base Sequence class. Now, this isn’t too terrible for a tool in the standard library since they’re usually quite well-documented, and you can always resort to inspecting the subclass instance to learn about the abstract and concrete methods. Not all subclass-driven design is bad, and the standard library makes judicious use of the template pattern. However, in an application codebase that you might be writing, this elusive nature of the public API can start becoming recalcitrant. Your code may not be as well-documented as the standard library, or instantiating the subclass may be expensive, making introspection difficult. You’re basically trading off readability for writing ergonomics. There’s nothing wrong in doing that as long as you’re aware of the tradeoffs. All I’m trying to say is that it’s a non-ideal default. Namespace pollution If you introspect the previously defined subclass with dir(CustomSequence), you’ll get the following result. I’ve removed the common attributes that every class inherits from object for brevity and annotated the abstract and mixin method names for clarity. [ \"__abstractmethods__\", # Allows you to list out the abstract methods \"__class_getitem__\", # Used for generic typing \"__contains__\", # Provided by the base \"__getitem__\", # You implement \"__iter__\", # Provided by the base \"__len__\", # You implement \"__reversed__\", # Provided by the base \"count\", # Provided by the base \"index\", # Provided by the base ] From the above list, it’s evident that all the methods from the baseclass and the subclass live in the same namespace. The moment you’re inheriting from some baseclass, you have no control over what that class is bringing over to your subclass’s namespace and effectively polluting it. It’s like a more sneaky version of from foo import *. This flat namespacing makes it hard to understand which method is coming from where. In the above case, without the annotations, you’d have a hard time discerning between the methods that you implemented and the alien methods from the baseclass. This isn’t a cardinal sin in the Python realm if that’s what you want, but it’s certainly a suboptimal default. SRP violation \u0026 goose chase program flow The biggest complaint I have against the template pattern is how it encourages the baseclass to do too many things at once. I can endure poor discoverability of public APIs and namespace pollution to some extent, but when a class tries to do too many things simultaneously, it eventually exhibits the tendency to give birth to God Objects7; breaching the SRP (Single Responsibility Principle). Intentionally violating the SRP rarely fosters good results, and in this case, the baseclass defines both concrete and abstract methods. Not only that, the base expects the subclasses to implement those abstract methods so that it can use them in its concrete method implementation. Just reading back this sentence is giving me a headache. If you design your APIs in this manner, you’ll have to carefully read through both the sub and the base class implementations to understand how this intricate bi-directional thread is woven into your program flow. This seems easy enough in a simple example where you can see both the base and the sub class in a single snippet, but it quickly gets out of hand when large base and sub classes are scattered across multiple modules. You’ll need to perform the mental gymnastics of tracking this back-and-forth logic, aka the abominable goose chase program flow. The disease and the cure Let’s examine a specific design problem and observe how it can be modeled using the template pattern. Then, we’ll explore an alternative solution that replaces the inheritance-driven design with composition. Designing with template pattern The following code snippet mimics a real-world webhook8 dispatcher that takes a message and posts it to a callback URL via HTTP POST request. First, we’ll commit the cardinal sin of modeling the domain with the template pattern and then we’ll try to find a way out of the quandary. Here it goes: from dataclasses import dataclass, field, asdict from uuid import uuid4 from abc import ABC, abstractmethod @dataclass(frozen=True) class Message: ref: str = field(default_factory=lambda: str(uuid4())) body: str = \"\" class BaseWebhook(ABC): def send(self) -\u003e None: url = self.get_url() data = self.get_message() print(f\"sending {data} to {url}\") @abstractmethod def get_message(self) -\u003e dict[str, str]: raise NotImplementedError @abstractmethod def get_url(self) -\u003e str: raise NotImplementedError class Webhook(BaseWebhook): def __init__(self, message: Message) -\u003e None: self.message = message def get_message(self) -\u003e dict[str, str]: # Assume that we're doing other side effects and adding more data in # runtime return asdict(self.message) def get_url(self) -\u003e str: return \"https://webhook.site/foo\" Here’s how you’ll orchestrate the classes: message = Message(body=\"Hello World\") webhook = Webhook(message) # This just prints: # sending # { # 'ref': '4635cfe0-825e-4f40-9c7b-04275b1c809e', # 'body': 'Hello World' # } to https://webhook.site/foo webhook.send() We start by defining an immutable Message container to store our webhook message. Next, we write an abstract BaseWebhook class that inherits from abc.ABC. This class serves as a template for the webhook functionality and declares two abstract methods: get_message() and get_url(). Type annotations are used to indicate the return types of these methods. Any subclasses derived from BaseWebhook must implement these abstract methods. The send() method, implemented in the baseclass, uses the concrete implementations of the abstract methods to perform webhook dispatching. In this case, we simulate the HTTP POST functionality by printing the message and destination URL. The Webhook class is a subclass of BaseWebhook and provides concrete implementations of the abstract methods. It accepts a single Message object as a parameter in its constructor. The get_url() method returns a fixed URL, while the get_message() method converts the Message object into a serializable dictionary representation using dataclasses.asdict(). In this structure, the user of the Webhook class only needs to initialize the class and call the send() method on the instance. The send() method, however, lives in the BaseWebhook class, not the specialized Webhook subclass. It utilizes the concrete implementations of abstract methods to deliver the send() functionality. In the following section, we’ll explore a method to avoid this weird back-and-forth program flow. Finding salvation in strategy pattern There are multiple ways and conflicting opinions on how to get out of the hole we’ve dug for ourselves. Some even like to spend more time prattling around the philosophy of how OO is terrible and how, if it weren’t for Java’s huge influence on Python, we wouldn’t be in this mess, rather than attempting to solve the actual problem. So instead of trying to cover every possible solution under the sun, I’ll go through the one that has worked for me fairly well. We’ll refactor the code in the previous section to take advantage of composition and structural subtyping9 support in Python. Long story short, structural subtyping refers to the ability to ensure type safety based on the structure or shape of an object rather than its explicit inheritance hierarchy. This allows us to define and enforce contracts based on the presence of specific attributes or methods, rather than relying on a specific class or inheritance relationship. This is achieved through the use of the typing.Protocol class introduced in Python 3.8. By defining a protocol using the typing.Protocol class, we can specify the expected attributes and methods that an object should have to satisfy the protocol. Any object that matches the structure defined by the protocol can be treated as if it conforms to that protocol, enabling more flexible and dynamic type-checking in Python. This conformity is usually checked by a type-checking tool like mypy10. If you want to learn more, check out Glyph’s post titled “I Want a New Duck”11. Here’s how I refactored it: from dataclasses import dataclass, field, asdict from uuid import uuid4 from typing import Protocol @dataclass(frozen=True) class Message: ref: str = field(default_factory=lambda: str(uuid4())) body: str = \"\" class Retriever(Protocol): def get_message(self, message: Message) -\u003e dict[str, str]: ... def get_url(self) -\u003e str: ... class Dispatcher(Protocol): def dispatch(self, url: str, data: dict[str, str]) -\u003e None: ... class HookRetriever: def get_message(self, message: Message) -\u003e dict[str, str]: # Assume that we're doing other side effects and adding more data in # runtime return asdict(message) def get_url(self) -\u003e str: return \"https://webhook.site/foo\" class HookDispatcher: def dispatch(self, url: str, data: dict[str, str]) -\u003e None: print(f\"Sending {data} to {url}\") @dataclass class Webhook: message: Message retriever: Retriever dispatcher: Dispatcher def send(self) -\u003e None: url = self.retriever.get_url() data = self.retriever.get_message(self.message) return self.dispatcher.dispatch(url, data) The classes can be wired together as follows: message = Message(body=\"Hello World\") retriever = HookRetriever() dispatcher = HookDispatcher() webhook = Webhook(message, retriever, dispatcher) # This prints the same thing as before: # sending # { # 'ref': '4635cfe0-825e-4f40-9c7b-04275b1c809e', # 'body': 'Hello World' # } to https://webhook.site/foo webhook.send() We’ve agreed that the BaseWebhook class tries to do too many things at once. The first step to disentangling a class is to identify its responsibilities and create multiple component classes where each new class will only have one responsibility. Here, the base class retrieves the necessary data and dispatches the webhook using that data at the same time. The Retriever and Dispatcher protocol classes will formalize the shape and structure of those component classes. These protocols work like the ABCs, but you don’t need to inherit from them to ensure interface conformity; the type checker will do it for you. The Retriever class has two methods: get_message and get_url, which fetch message and URL data respectively. Similarly, the Dispatcher protocol has only a dispatch method that sends the webhook. In either case, the protocol methods don’t implement anything; they work just like the abstract methods of the ABCs, and the protocol classes themselves can’t be instantiated. Then the HookRetriever and HookDispatcher components implicitly implement the protocol classes. Notice that neither of the components inherits from the protocol classes. The type checker will ensure that they conform to the defined protocols. The question is, how does the type checker know which class is supposed to conform to which protocol? The answer lies in the final Webhook class. We define a final dataclass that takes instances of the Message, Retriever, and Dispatcher classes in the constructor. Notice that while adding type hints to the retriever and dispatcher parameters of the dataclass constructor, we’re using the protocol classes instead of the concrete ones. This is how the type checker knows that whatever instance is passed to the retriever and dispatcher parameters must conform to the Retriever and Dispatcher protocols, respectively. Note that we’ve completely eliminated subclassing from our public API. Injecting dependencies in this manner is also known as the strategy pattern12. The Webhook class now has a hierarchical namespace instead of a flat one, unlike our inheritance-based friend. You’ll have to be explicit about where a method is coming from when calling it. So if you need to access the fetched URL, you’ll need to explicitly call self.retriever.get_url(). The self namespace has only one user-defined public method, .send(), which can be called to dispatch the webhook from a Webhook instance. This also means you no longer have to deal with goose chase program flow since all the dependencies flow towards the final Webhook class. On the flip side, you’ll need to do more work while initializing the Webhook class. The Message, HookRetriever, and HookDispatcher classes need to be instantiated first and then passed explicitly to the constructor of the Webhook class to instantiate it. You’re basically trading writing ergonomics for readability. Instantiating the template subclass was a lot easier for sure. Tradeoffs Opting in for composition isn’t free, as it usually leads to more verbose code orchestration. If you’re passing all the dependencies explicitly, as shown above, wiring the code together will be more complex. However, in return, you get a more readable and testable design substrate. So, I’m more than happy to make the tradeoff. Additionally, avoiding namespace pollution means that one attribute access has now turned into two or more attribute accesses, which can cause performance issues in tight conditions. Moreover, you can’t just take your inheritance-heavy API and suddenly turn it into a composable one. It usually requires planning and designing from the ground up, where you might decide that the ROI isn’t good enough to justify the effort of refactoring. Plus, in a language like Python, you can’t always escape inheritance, nor should you try to do so. Yet behold, it need not be the customary stratagem that thou graspest at each moment thine heart yearns to commune code amidst classes. Template pattern ↩︎ Composition over inheritance - Brandon Rhodes ↩︎ Subclassing, composition, Python, and you - Hynek Schlawack ↩︎ Subclassing in Python redux - Hynek Schlawack ↩︎ Interfaces, mixins and building powerful custom data structures in Python ↩︎ Sequence docs ↩︎ God objects ↩︎ Webhook ↩︎ Structural subtyping ↩︎ Mypy ↩︎ I want a new duck - Glyph ↩︎ Strategy pattern ↩︎ End of object inheritance - Augie Fackler, Nathaniel Manista 13 ↩︎ ","permalink":"http://rednafi.com/python/escape_template_pattern/","publishDate":"2023-07-01","summary":"Over the years, I’ve used the template pattern1 across multiple OO languages with varying degrees of success. It was one of the first patterns I learned in the primordial hours of my software engineering career, and for some reason, it just feels like the natural way to tackle many real-world code-sharing problems. Yet, even before I jumped on board with the composition over inheritance2 camp, I couldn’t help but notice how using this particular inheritance technique spawns all sorts of design and maintenance headaches as the codebase starts to grow.\n","tags":["Python"],"title":"Escaping the template pattern hellscape in Python"},{"content":"One major drawback of Python’s huge ecosystem is the significant variances in workflows among people trying to accomplish different things. This holds true for dependency management as well. Depending on what you’re doing with Python—whether it’s building reusable libraries, writing web apps, or diving into data science and machine learning—your workflow can look completely different from someone else’s. That being said, my usual approach to any development process is to pick a method and give it a shot to see if it works for my specific needs. Once a process works, I usually automate it and rarely revisit it unless something breaks. Also, I actively try to abstain from picking up tools that haven’t stood the test of time. If the workflow laid out here doesn’t work for you and something else does, that’s fantastic! I just wanted to document a more modern approach to the dependency management workflow that has reliably worked for me over the years. Plus, I don’t want to be the person who still uses distutils1 in their package management workflow and gets reprehended by pip for doing so. Defining the scope Since the dependency management story in Python is a huge mess for whatever reason, to avoid getting yelled at by the most diligent gatekeepers of the internet, I’d like to clarify the scope of this piece. I mainly write web applications in Python and dabble in data science and machine learning every now and then. So yeah, I’m well aware of how great conda2 is when you need to deal with libraries with C dependencies. However, that’s not typically my day-to-day focus. Here, I’ll primarily delve into how I manage dependencies when developing large-scale web apps and reusable libraries. In applications, I manage my dependencies with pip3 and pip-tools4, and for libraries, my preferred build backend is hatch5. PEP-6216 attempts to standardize the process of storing project metadata in a pyproject.toml file, and I absolutely love the fact that now, I’ll mostly be able to define all my configurations and dependencies in a single file. This made me want to rethink how I wanted to manage the dependencies without sailing against the current recommended standard while also not getting swallowed into the vortex of conflicting opinions in this space. In applications Whether I’m working on a large Django monolith or exposing a microservice via FastAPI or Flask, while packaging an application, I want to be able to: Store all project metadata, linter configs, and top-level dependencies in a pyproject.toml file following the PEP-6216 conventions. Separate the top-level application and development dependencies. Generate requirements.txt and requirements-dev.txt files from the requirements specified in the TOML file, where the top-level and their transient dependencies will be pinned to specific versions. Use vanilla pip to build the application hermetically from the locked dependencies specified in the requirements*.txt files. The goal is to simply be able to run the following command to install all the pinned dependencies in a reproducible manner: pip install -r requirements.txt -r requirements-dev.txt pip-tools allows me to do exactly that. Suppose, you have an app where you’re defining the top-level dependencies in a canonical pyproject.toml file like this: [project] requires-python = \"\u003e=3.8\" name = \"foo-app\" version = \"0.1.0\" dependencies = [ \"fastapi==0.97.0\", \"uvicorn==0.22.0\", ] [project.optional-dependencies] dev = [ \"black\u003e=23.3.0\", \"mypy\u003e=1.2.0\", \"pip-tools\u003e=6.13.0\", \"pytest\u003e=7.3.2\", \"pytest-cov\u003e=4.1.0\", \"ruff\u003e=0.0.272\" ] # Even for an application, specifying a build backend is required. # Otherwise, pip-compile command will give you an obscure error. [tool.setuptools.packages.find] where = [\"app\"] # [\".\"] by default Here, following PEP-621 conventions, we’ve specified the app and dev dependencies in the project.dependencies and project.optional-dependencies.dev sections respectively. Now in a virtual environment, install pip-tools and run the following commands: # This will pin the app and dev deps to requirements*.txt files and # generate hashes for hermetic builds # Pin the app deps along with their build hashes pip-compile -o requirements.txt pyproject.toml \\ --generate-hashes --strip-extras # Use the app deps as a constraint while pinning the dev deps so that the # dev deps don't install anything that conflicts with the app deps echo \"--constraint $(PWD)/requirements.txt\" \\ | pip-compile --generate-hashes --output-file requirements-dev.txt \\ --extra dev - pyproject.toml Running the commands will create two lock files requirements.txt and requirements-dev.txt where all the pinned top-level and transient dependencies will be listed out. The contents of the requirements.txt file looks like this (truncated): # # This file is autogenerated by pip-compile with Python 3.11 # by the following command: # # pip-compile --generate-hashes --output-file=requirements.txt # --strip-extras pyproject.toml # anyio==3.7.0 \\ --hash=sha256:275d9973793619a5374e1c89a4f4ad3f4b0a5510a2b5b939444bee8f4c4d37ce \\ --hash=sha256:eddca883c4175f14df8aedce21054bfca3adb70ffe76a9f607aef9d7fa2ea7f0 # via starlette click==8.1.3 \\ --hash=sha256:7682dc8afb30297001674575ea00d1814d808d6a36af415a82bd481d37ba7b8e \\ --hash=sha256:bb4d8133cb15a609f44e8213d9b391b0809795062913b383c62be0ee95b1db48 # via uvicorn ... Similarly, the content of requirements-dev.txt file goes as follows (truncated): # # This file is autogenerated by pip-compile with Python 3.11 # by the following command: # # pip-compile --extra=dev --generate-hashes --output-file=requirements-dev.txt # - pyproject.toml # anyio==3.7.0 \\ --hash=sha256:275d9973793619a5374e1c89a4f4ad3f4b0a5510a2b5b939444bee8f4c4d37ce \\ --hash=sha256:eddca883c4175f14df8aedce21054bfca3adb70ffe76a9f607aef9d7fa2ea7f0 # via # -r - # starlette black==23.3.0 \\ --hash=sha256:064101748afa12ad2291c2b91c960be28b817c0c7eaa35bec09cc63aa56493c5 \\ --hash=sha256:0945e13506be58bf7db93ee5853243eb368ace1c08a24c65ce108986eac65915 \\ ... Once the lock files are generated, you’re free to build the application in however way you see fit and the build process doesn’t even need to be aware of the existence of pip-tools. In the simplest case, you can just run pip install to build the application. Check out this working example7 that uses the workflow explained in this section. In libraries While packaging libraries, I pretty much want the same things mentioned in the application section. However, the story of dependency management in reusable libraries is a bit more hairy. Currently, there’s no standard around a lock file and I’m not aware of a way to build artifacts from a plain requirements.txt file. For this purpose, my preferred build backend is hatch5. Mostly because it follows the latest standards formalized by the associated PEPs. From the FAQ section of the hatch docs: Q: What is the risk of lock-in? A: Not much! Other than the plugin system, everything uses Python’s established standards by default. Project metadata is based entirely on PEP-6216/PEP-6318, the build system is compatible with PEP-5179/PEP-66010, versioning uses the scheme specified by PEP-44011, dependencies are defined with PEP-50812 strings, and environments use virtualenv. However, it doesn’t support lock files yet: The only caveat is that currently there is no support for re-creating an environment given a set of dependencies in a reproducible manner. Although a standard lock file format may be far off since PEP-66513 was rejected, resolving capabilities are coming to pip. When that is stabilized, Hatch will add locking functionality and dedicated documentation for managing applications. In my experience, I haven’t faced many issues regarding the lack of support for lock files while building reusable libraries. Your mileage may vary. Now let’s say we’re trying to package up a CLI that has the following source structure: src ├── __init__.py └── cli.py The content of cli.py looks like this: import click @click.command() @click.version_option() def cli() -\u003e None: \"\"\"Simple cli command to show the version of the package\"\"\" click.echo(\"Hello from foo-cli!\") if __name__ == \"__main__\": cli() The corresponding pyproject.toml file looks as follows: [project] requires-python = \"\u003e=3.8\" name = \"foo-cli\" dependencies = [ \"click\u003e=8.1.3\", ] version = \"0.0.1\" [project.optional-dependencies] dev = [ \"hatch\u003e=1.7.0\", \"black\u003e=23.3.0\", \"mypy\u003e=1.2.0\", \"pip-tools\u003e=6.13.0\", \"pytest\u003e=7.3.2\", \"pytest-cov\u003e=4.1.0\", \"ruff\u003e=0.0.272\" ] [project.scripts] foo-cli = \"src:cli.cli\" [build-system] requires = [\"hatchling \u003e= 1.7.0\"] build-backend = \"hatchling.build\" # We're using setuptools as the build backend [tool.setuptools.packages.find] where = [\"src\"] # [\".\"] by default Now install hatch in your virtualenv and run the following command to create the build artifacts: hatch build src This will create the build artifacts in the src directory: src ├── __init__.py ├── cli.py ├── foo_cli-0.0.1-py3-none-any.whl └── foo_cli-0.0.1.tar.gz You can now install the local wheel file to test the build: pip install foo_cli-0.0.1-py3-none-any.whl Once you’ve installed the CLI locally, you can test it by running foo-cli from your console: foo-cli This returns: Hello from foo-cli! You can also build and install the CLI with: pip install \".[dev]\" Hatch also provides a hatch publish command to upload the package to PyPI. For a complete reference, check out how I shipped another CLI14 following this workflow. distutils ↩︎ conda ↩︎ pip ↩︎ pip-tools ↩︎ hatch ↩︎ ↩︎ PEP-621 ↩︎ ↩︎ ↩︎ Example application - fastapi-nano ↩︎ PEP-631 ↩︎ PEP-517 ↩︎ PEP-660 ↩︎ PEP-440 ↩︎ PEP-508 ↩︎ PEP-665 ↩︎ Example library - rubric ↩︎ Using pyproject.toml in your Django project - Peter Baumgartner 15 ↩︎ TIL: pip-tools Supports pyproject.toml - Hynek Schlawack 16 ↩︎ ","permalink":"http://rednafi.com/python/dependency_management_redux/","publishDate":"2023-06-27","summary":"One major drawback of Python’s huge ecosystem is the significant variances in workflows among people trying to accomplish different things. This holds true for dependency management as well. Depending on what you’re doing with Python—whether it’s building reusable libraries, writing web apps, or diving into data science and machine learning—your workflow can look completely different from someone else’s. That being said, my usual approach to any development process is to pick a method and give it a shot to see if it works for my specific needs. Once a process works, I usually automate it and rarely revisit it unless something breaks.\n","tags":["Python"],"title":"Python dependency management redux"},{"content":"I was watching this amazing lightning talk1 by Karla Burnett and wanted to understand how traceroute works in Unix. Traceroute is a tool that shows the route of a network packet from your computer to another computer on the internet. It also tells you how long it takes for the packet to reach each stop along the way. It’s useful when you want to know more about how your computer connects to other computers on the internet. For example, if you want to visit a website, your computer sends a request to the website’s server, which is another computer that hosts the website. But the request doesn’t go directly from your computer to the server. It has to pass through several other devices, such as routers, that help direct the traffic on the internet. These devices are called hops. Traceroute shows you the list of hops that your request goes through, and how long it takes for each hop to respond. This can help you troubleshoot network problems, such as slow connections or unreachable websites. This is how you usually use traceroute: traceroute example.com This returns: traceroute to example.com (93.184.216.34), 64 hops max, 52 byte packets 1 192.168.1.1 (192.168.1.1) 2.386 ms 1.976 ms 1.703 ms 2 142-254-158-201.inf.spectrum.com (142.254.158.201) 9.970 ms 9.463 ms 9.867 ms 3 lag-63.uparohgd02h.netops.charter.com (65.25.145.149) 52.340 ms 26.224 ms 18.094 ms 4 lag-31.clmcohib01r.netops.charter.com (24.33.161.216) 24.277 ms 10.391 ms 16.529 ms 5 lag-27.rcr01clevohek.netops.charter.com (65.29.1.38) 16.485 ms 16.258 ms 16.999 ms 6 lag-416.vinnva0510w-bcr00.netops.charter.com (66.109.6.164) 23.478 ms 24.685 ms lag-415.vinnva0510w-bcr00.netops.charter.com (66.109.6.12) 25.211 ms 7 lag-11.asbnva1611w-bcr00.netops.charter.com (66.109.6.30) 24.541 ms lag-21.asbnva1611w-bcr00.netops.charter.com (66.109.3.24) 24.574 ms lag-31.asbnva1611w-bcr00.netops.charter.com (107.14.18.82) 24.253 ms 8 xe-7-3-1.cr0.chi10.tbone.rr.com (209.18.36.1) 24.283 ms 26.479 ms 45.171 ms 9 ae-65.core1.dcb.edgecastcdn.net (152.195.64.129) 24.550 ms 24.753 ms 25.007 ms 10 93.184.216.34 (93.184.216.34) 23.998 ms 24.086 ms 24.180 ms 11 93.184.216.34 (93.184.216.34) 23.627 ms 24.238 ms 24.271 ms This traceroute output draws the path of a network packet from my computer to example.com’s server, which has an IP address of 93.184.216.34. It shows that the packet goes through 11 hops before reaching the destination. The first hop is my router (192.168.1.1), the second hop is my ISP’s router (142.254.158.201), and so on. The last column shows the time it takes for each hop to respond in milliseconds (ms). The lower the time, the faster the connection. Some hops have multiple lines with different names or IP addresses. This means that there are multiple routers at that hop that can handle the traffic, and traceroute randomly picks one of them for each packet. For example, hop 7 has three routers with names starting with lag-11, lag-21, and lag-31. These are probably load-balancing routers that distribute the traffic among them. The last hop (93.184.216.34) appears twice in the output. This is because traceroute sends three packets to each hop by default, and sometimes the last hop responds to all three packets instead of discarding them. This is not a problem and does not affect the accuracy of the traceroute. This is all good and dandy but I wanted to understand how traceroute can find out what route a packet takes and how long it takes between each hop. So I started reading blogs like this2 one that does an awesome job at explaining what’s going on behind the scene. The gist of it goes as follows. How traceroute works Traceroute works by sending a series of ICMP (Internet Control Message Protocol) echo request packets, which are also known as pings, to the target IP address or URL that you want to reach. Each packet has an associated time-to-live (TTL) value, which is a number that indicates how many hops (or intermediate devices) the packet can pass through before it expires and is discarded by a router. Yeah, strangely, TTL doesn’t denote any time duration here. Traceroute starts by sending a packet with a low TTL value, usually 1. This means that the packet can only make one hop before it expires. When a router receives this packet, it decreases its TTL value by 1 and checks if it is 0. If it is 0, the router discards the packet and sends back an ICMP time exceeded message to the source of the packet. This message contains the IP address of the router that discarded the packet. This is how the sender knows the IP address of the first hop (router, computer, or whatsoever). Traceroute records the IP address and round-trip time (RTT) of each ICMP time exceeded message it receives. The RTT is the time it takes for a packet to travel from the source to the destination and back. It reflects the latency (or delay) between each hop. Traceroute then increases the TTL value by 1 and sends another packet. This packet can make 2 hops before it expires. The process repeats until traceroute reaches the destination or a maximum TTL value, usually 30. When the returned IP is the same as the initial destination IP, traceroute knows that the packet has completed the whole journey. By doing this, traceroute can trace the route that your packets take to reach the target IP address or URL and measure the latency between each hop. The tool prints out the associated IPs and latencies as it jumps through different hops. I snagged this photo from an SFU (Simon Fraser University) slide3 that I think explains the machinery of traceroute quite well: Writing a crappier version of traceroute in Python After getting a rough idea of what’s going on underneath, I wanted to write a simpler and crappier version of traceroute in Python. This version would roughly perform the following steps: Establish a UDP socket connection that’d be used to send empty packets to the hops. Create an ICMP socket that’d receive ICMP time exceeded messages. Start a loop and use the UDP socket to send an empty byte with a TTL of 1 to the first hop. The TTL value of the packet would be decremented by 1 at the first hop. Once the TTL reaches 0, the packet would be discarded, and an ICMP time exceeded message would be returned to the sender through the ICMP socket. The sender would also receive the address of the first hop. Calculate the time delta between sending a packet and receiving the ICMP time exceeded message. Also, capture the address of the first hop and log the time delta and address to the console. In the subsequent iterations, the TTL value will be incremented by 1 (2, 3, 4, …) and the steps from 1 through 5 will be repeated until it reaches the max_hops value, which is set at 64. Here’s the complete self-contained implementation. I tested it on Python 3.11: # script.py from __future__ import annotations import socket import sys import time from collections.abc import Generator from contextlib import ExitStack def traceroute( dest_addr: str, max_hops: int = 64, timeout: float = 2 ) -\u003e Generator[tuple[str, float], None, None]: \"\"\"Traceroute implementation using UDP packets. Args: dest_addr (str): The destination address. max_hops (int, optional): The maximum number of hops. Defaults to 64. timeout (float, optional): The timeout for receiving packets. Defaults to 2. Yields: Generator[tuple[str, float], None, None]: A generator that yields the current address and elapsed time for each hop. \"\"\" # ExitStack allows us to avoid multiple nested contextmanagers with ExitStack() as stack: # Create an ICMP socket connection for receiving packets rx = stack.enter_context( socket.socket(socket.AF_INET, socket.SOCK_RAW, socket.IPPROTO_ICMP) ) # Create a UDP socket connection for sending packets tx = stack.enter_context( socket.socket( socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP ) ) # Set the timeout for receiving packets rx.settimeout(timeout) # Bind the receiver socket to any available port rx.bind((\"\", 0)) # Iterate over the TTL values for ttl in range(1, max_hops + 1): # Set the TTL value in the sender socket tx.setsockopt(socket.IPPROTO_IP, socket.IP_TTL, ttl) # Send an empty UDP packet to the destination address tx.sendto(b\"\", (dest_addr, 33434)) try: # Start the timer start_time = time.perf_counter_ns() # Receive the response packet and extract the source address _, curr_addr = rx.recvfrom(512) curr_addr = curr_addr[0] # Stop the timer and calculate the elapsed time end_time = time.perf_counter_ns() elapsed_time = (end_time - start_time) / 1e6 except socket.error: # If an error occurs while receiving the packet, set the # address and elapsed time as None curr_addr = None elapsed_time = None # Yield the current address and elapsed time yield curr_addr, elapsed_time # Break the loop if the destination address is reached if curr_addr == dest_addr: break def main() -\u003e None: # Get the destination address from command-line argument dest_name = sys.argv[1] dest_addr = socket.gethostbyname(dest_name) # Print the traceroute header print(f\"Traceroute to {dest_name} ({dest_addr})\") print(f\"{'Hop':\u003c5s}{'IP Address':\u003c20s}{'Hostname':\u003c50s}{'Time (ms)':\u003c10s}\") print(\"-\" * 90) # Iterate over the traceroute results and print each hop information for i, (addr, elapsed_time) in enumerate(traceroute(dest_addr)): if addr is not None: try: # Get the hostname corresponding to the IP address host = socket.gethostbyaddr(addr)[0] except socket.error: host = \"\" # Print the hop information print(f\"{i+1:\u003c5d}{addr:\u003c20s}{host:\u003c50s}{elapsed_time:\u003c10.3f} ms\") else: # Print \"*\" for hops with no response print(f\"{i+1:\u003c5d}{'*':\u003c20s}{'*':\u003c50s}{'*':\u003c10s}\") if __name__ == \"__main__\": main() Running the script will give you the following nicely formatted output: sudo python script.py example.com Traceroute to example.com (93.184.216.34) Hop IP Address Hostname Time (ms) ---------------------------------------------------------------------------------------- 1 192.168.1.1 1.420 ms 2 142.254.158.201 142-254-158-201.inf.spectrum.com 9.669 ms 3 65.25.145.149 lag-63.uparohgd02h.netops.charter.com 139.603 ms 4 24.33.161.216 lag-31.clmcohib01r.netops.charter.com 14.493 ms 5 65.29.1.38 lag-27.rcr01clevohek.netops.charter.com 19.221 ms 6 66.109.6.70 lag-17.vinnva0510w-bcr00.netops.charter.com 25.803 ms 7 66.109.3.24 lag-21.asbnva1611w-bcr00.netops.charter.com 24.969 ms 8 209.18.36.1 xe-7-3-1.cr0.chi10.tbone.rr.com 24.351 ms 9 152.195.64.129 ae-65.core1.dcb.edgecastcdn.net 25.114 ms 10 93.184.216.34 23.546 ms Storytelling with traceroute ↩︎ How traceroute works ↩︎ Traceroute machinery slide ↩︎ ","permalink":"http://rednafi.com/python/implement_traceroute_in_python/","publishDate":"2023-06-01","summary":"I was watching this amazing lightning talk1 by Karla Burnett and wanted to understand how traceroute works in Unix. Traceroute is a tool that shows the route of a network packet from your computer to another computer on the internet. It also tells you how long it takes for the packet to reach each stop along the way.\nIt’s useful when you want to know more about how your computer connects to other computers on the internet. For example, if you want to visit a website, your computer sends a request to the website’s server, which is another computer that hosts the website. But the request doesn’t go directly from your computer to the server. It has to pass through several other devices, such as routers, that help direct the traffic on the internet. These devices are called hops. Traceroute shows you the list of hops that your request goes through, and how long it takes for each hop to respond. This can help you troubleshoot network problems, such as slow connections or unreachable websites.\n","tags":["Python","Networking","Shell"],"title":"Implementing a simple traceroute clone in Python"},{"content":"Recently, I purchased a domain for this blog and migrated the content from rednafi.github.io1 to rednafi.com2. This turned out to be a much bigger hassle than I originally thought it’d be, mostly because, despite setting redirection for almost all the URLs from the previous domain to the new one and submitting the new sitemap.xml3 to the Search Console, Google kept indexing the older domain. To make things worse, the search engine selected the previous domain as canonical, and no amount of manual requests were changing the status in the last 30 days. Strangely, I didn’t encounter this issue with Bing, as it reindexed the new site within a week after I submitted the sitemap file via their webmaster panel. While researching this, one potential solution suggested that along with submitting the sitemap via Google Search Console4, I’d have to make individual indexing requests for each URL to encourage faster indexing. The problem is, I’ve got quite a bit of content on this site, and it’ll take forever for me to click through all the links and request indexing that way. Naturally, I looked for a way to do this programmatically. Luckily, I found out that there’s an [indexing API] that allows you to make bulk indexing requests programmatically. This has one big advantage—Google responds5 to API requests faster than indexing requests with sitemap submission. All you’ve to do is: List out the URLs that need to be indexed. Fulfill the prerequisites6 and download the private key JSON file required to make requests to the API. From the docs: Every call to the Indexing API must be authenticated with an OAuth token that you get in exchange for your private key. Each token is good for a span of time. Google provides API client libraries to get OAuth tokens for a number of languages. The private key file will look like this: { \"type\": \"service_account\", \"project_id\": \"...\", \"private_key_id\": \"...\", \"private_key\": \"...\", \"client_email\": \"...\", \"client_id\": \"...\", \"auth_uri\": \"...\", \"token_uri\": \"...\", \"auth_provider_x509_cert_url\": \"...\", \"client_x509_cert_url\": \"...\", \"universe_domain\": \"...\" } Use an API client to make the requests. In my case, this site’s sitemap3 lists out all the URLs as follows: https://rednafi.com/tags/github/ 2023-05-21T00:00:00+00:00 https://rednafi.com/tags/javascript/ 2023-05-21T00:00:00+00:00 ... Here’s a NodeJS script that collects the URLs from sitemap.xml and makes requests to the indexing API: // ES6 import import { google } from \"googleapis\"; import { parseString } from \"xml2js\"; import fetch from \"node-fetch\"; import pkey from \"./google-api-pkey.json\" assert { type: \"json\" }; // Parse the sitemap.xml file and extract the URLs. async function getUrls(url) { try { const response = await fetch(url); const xml = await response.text(); let urls; parseString(xml, (err, result) =\u003e { if (err) { console.error(\"Error parsing XML:\", err); return; } urls = result.urlset.url.map((url) =\u003e url.loc[0]); }); return urls; } catch (error) { console.error(\"Error fetching sitemap:\", error); } } // Initialize auth client const jwtClient = new google.auth.JWT( pkey.client_email, null, pkey.private_key, [\"https://www.googleapis.com/auth/indexing\"], null ); // Perfrom auth and make multiple API calls jwtClient.authorize(async function (err, tokens) { if (err) { console.log(err); return; } const options = { url: \"https://indexing.googleapis.com/v3/urlNotifications:publish\", method: \"POST\", headers: { \"Content-Type\": \"application/json\", Authorization: `Bearer ${tokens.access_token}`, }, json: { url: \"\", type: \"URL_UPDATED\", // Means we want to request indexing }, }; try { const urls = await getUrls(\"https://www.rednafi.com/sitemap.xml\"); // There's a bulk endpoint but looping through the list // and making multiple requests is just as easy for (const url of urls) { options.json.url = url; const response = await fetch(options.url, { method: options.method, headers: options.headers, body: JSON.stringify(options.json), }); const body = await response.json(); console.log(body); } } catch (error) { console.error(\"Error:\", error); } }); Before executing the script, npm install googleapis and xml2js. Now running the script will give you an output similar to this: { urlNotificationMetadata: { url: 'https://rednafi.com/categories/', latestUpdate: { url: 'https://rednafi.com/categories/', type: 'URL_UPDATED', notifyTime: '2023-05-27T01:02:35.537421311Z' } } } { urlNotificationMetadata: { url: 'https://rednafi.com/search/', latestUpdate: { url: 'https://rednafi.com/search/', type: 'URL_UPDATED', notifyTime: '2023-05-27T01:02:35.789809492Z' } } }, ... Here, the getUrls function is defined to fetch the sitemap content from a specified URL, parse the XML content and extract the URLs. It uses the fetch function to retrieve the file, then uses xml2js to parse the XML and extract the URLs from the result. The script then initializes an authentication client using the imported private key and specifies the required API scope. The authorize function is called to authenticate the client and obtain access tokens. Inside the authorization callback, the script prepares the necessary options for making API requests to the Google indexing API. It then calls the getUrls function to fetch the URLs from the sitemap.xml file. For each URL, it updates the options with the URL and makes a POST request to the Indexing API to request indexing. The response from the API is then logged into the console. One thing to keep in mind is that by default, the daily request quota per project is 200. But you can request more quota7 if you need it. rednafi.github.io ↩︎ rednafi.com ↩︎ sitemap.xml ↩︎ ↩︎ Google search console ↩︎ Turnaround time of the indexing API is shorter than sitemap submission ↩︎ Indexing API ↩︎ Quota ↩︎ ","permalink":"http://rednafi.com/javascript/bulk_request_google_search_index/","publishDate":"2023-05-26","summary":"Recently, I purchased a domain for this blog and migrated the content from rednafi.github.io1 to rednafi.com2. This turned out to be a much bigger hassle than I originally thought it’d be, mostly because, despite setting redirection for almost all the URLs from the previous domain to the new one and submitting the new sitemap.xml3 to the Search Console, Google kept indexing the older domain. To make things worse, the search engine selected the previous domain as canonical, and no amount of manual requests were changing the status in the last 30 days. Strangely, I didn’t encounter this issue with Bing, as it reindexed the new site within a week after I submitted the sitemap file via their webmaster panel.\n","tags":["JavaScript","API"],"title":"Bulk request Google search indexing with API"},{"content":"Cloudflare absolutely nailed the serverless function DX with Cloudflare Workers1. However, I feel like it’s yet to receive widespread popularity like AWS Lambda since as of now, the service only offers a single runtime—JavaScript. But if you can look past that big folly, it’s a delightful piece of tech to work with. I’ve been building small tools with it for a couple of years but never got around to writing about the immense productivity boost it usually gives me whenever I need to quickly build and deploy a self-contained service. Recently, I was doing some lightweight frontend work and needed to make some AJAX calls from one domain to another. Usually, browser’s CORS (Cross-Origin Resource Sharing)2 policy will get in your way if you try this. While you’re reading this piece, open the dev console and paste the following fetch snippet: fetch(\"https://mozilla.org\") .then((response) =\u003e response.text()) .then((data) =\u003e { // Do something with the received data console.log(data); }) .catch((error) =\u003e { // Handle any errors that occurred during the request console.error(\"Error:\", error); }); This snippet will attempt to make a GET request from https://rednafi.com to https://mozilla.org. However, the client’s CORS policy won’t allow you to make an AJAX request like this and load external resources into the current site. On your console, you’ll see an error message like this: Access to fetch at 'https://mozilla.org/' from origin 'https://rednafi.com' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with CORS disabled. This is a good security measure. Without CORS, a malicious script could make a request to a server in another domain and access the resources that the user of the page is not intended to have access to. So much has been said and written about CORS that I won’t even attempt to explain it here. Here’s another high-level introduction to the concept3. CORS proxy While CORS is generally a good thing, it can be quite annoying when you’re trying to build something that needs access to external resources. In those cases, you’ll have to mess around with the origin server and add a few headers that the browser can understand before it allows you to load those external resources. But sometimes you don’t have access to the origin server or simply don’t want to deal with modifying the server’s response headers every time you need to access external resources. That’s where CORS proxies can come in handy. A CORS proxy server acts as a bridge between your client and the target server. It receives your request and forwards it to the target server with a modified origin header so that the target server thinks the request is coming from the same origin as itself. This way, you can bypass the same-origin policy of browsers and access resources from different domains. I usually use free proxies like cors.sh4 to bypass CORS restrictions. You can drop this snippet to your browser’s console and this time it’ll allow you to load the contents of https://mozilla.org from https://rednafi.com: // Notice how we're prepending CORS URL before the target URL fetch(\"https://proxy.cors.sh/https://mozilla.org\") .then((response) =\u003e response.text()) .then((data) =\u003e { // Do something with the received data console.log(data); }) .catch((error) =\u003e { // Handle any errors that occurred during the request console.error(\"Error:\", error); }); The target server’s response looks somewhat like this: \u003c!doctype html\u003e ... If you want to learn more about how CORS proxies work, here’s a fantastic resource5 that explains the inner machinery in more detail. Free proxy servers can be pernicious Using a free CORS proxy server can be dangerous as it might spill the beans on your requests and data to some random service you don’t know or trust. Since it plays as a middleman between your app and the resource you’re after, it could potentially snoop on, mess with, or keep tabs on your requests and data. Plus, some of those free CORS proxy servers might have restrictions on the size, type, or number of requests they can handle, or they might not even support HTTPS or other security bells and whistles. Build your own CORS proxy with Cloudflare Workers With all the intros out of the way, here’s how CloudFlare Workers afforded me to prop up a CORS proxy in less than half an hour. If you’re impatient and just want to take a look at the service in its full glory then head over here6. GitHub Actions deploys the service automatically to CloudFlare Workers every time a change is pushed to the main branch. Installing the prerequisites Assuming you have node installed on your system, you can fetch the wrangler7 CLI with the following command: npm install -g wrangler This will allow us to develop and test the service locally. Bootstrapping the service Create a new directory where you want to develop your service and bring it under source control. Now, run: npm create cloudflare@latest The CLI will guide you through the entire bootstrapping process interactively. You’ll have to create a Cloudflare account (if you don’t have one already) and log into the dashboard. Then it’ll prompt you to deploy your first hello-world API endpoint that you can immediately start to play with without doing anything else. Being able to see the serverless function in action within like 5 minutes gave me a huge dopamine boost that AWS Lambda never could. You can see the interactive bootstrapping section here: Complete CLI output... using create-cloudflare version 2.0.7 ╭ Create an application with Cloudflare Step 1 of 3 │ ├ Where do you want to create your application? │ dir cors-proxy │ ├ What type of application do you want to create? │ type \"Hello World\" script │ ├ Do you want to use TypeScript? │ typescript no │ ├ Copying files from \"simple\" template │ ╰ Application created ╭ Installing dependencies Step 2 of 3 │ ├ Installing dependencies │ installed via `npm install` │ ╰ Dependencies Installed ╭ Deploy with Cloudflare Step 3 of 3 │ ├ Do you want to deploy your application? │ yes deploying via `npm run deploy` │ ├ Logging into Cloudflare This will open a browser window │ allowed via `wrangler login` │ ├ Deploying your application │ deployed via `npm run deploy` │ ├ SUCCESS View your deployed application at │ https://cors-proxy.rednafi.workers.dev (this may take a few mins) │ │ Run the development server npm run dev │ Deploy your application npm run deploy │ Read the documentation https://developers.cloudflare.com/workers │ Stuck? Join us at https://discord.gg/cloudflaredev │ ╰ See you again soon! Running the interactive session will create the following directory structure: ├── src │ └── worker.js ├── package-lock.json ├── package.json └── wrangler.toml Developing the CORS proxy We’ll write our proxy server in src/worker.js file. Copy the following JS snippet and paste it to the file: export default { async fetch(request, env, ctx) { // Extract method, url and headers from the incoming request object. const { method, url, headers } = request; // Extract destination url from the query string. const destUrl = new URL(url).searchParams.get(\"url\"); // If the destination url is not present, return 400. if (!destUrl) { return new Response(\"Missing destination URL.\", { status: 400 }); } // If the request method is OPTIONS, return CORS headers. if ( method === \"OPTIONS\" \u0026\u0026 headers.has(\"Origin\") \u0026\u0026 headers.has(\"Access-Control-Request-Method\") ) { const responseHeaders = { \"Access-Control-Allow-Origin\": headers.get(\"Origin\"), \"Access-Control-Allow-Methods\": \"*\", // Allow all methods \"Access-Control-Allow-Headers\": headers.get( \"Access-Control-Request-Headers\" ), \"Access-Control-Max-Age\": \"86400\", }; return new Response(null, { headers: responseHeaders }); } const proxyRequest = new Request(destUrl, { method, headers: { ...headers, Origin: \"\", }, }); try { const response = await fetch(proxyRequest); const responseHeaders = new Headers(response.headers); responseHeaders.set(\"Access-Control-Allow-Origin\", \"*\"); responseHeaders.set(\"Access-Control-Allow-Credentials\", \"true\"); responseHeaders.set(\"Access-Control-Allow-Methods\", \"*\"); return new Response(response.body, { status: response.status, statusText: response.statusText, headers: responseHeaders, }); } catch (error) { return new Response(\"Error occurred while fetching the resource.\", { status: 500, }); } }, }; The first section of the code deals with extracting relevant information from the incoming request. It destructures the method, url, and headers properties from the request object, which represents the client’s request. Next, it extracts the destination URL from the query string. It extracts the URL parameter using the searchParams.get() method. If the destination URL isn’t provided, the function returns a Response object with an error message and a status code of 400 (Bad Request). The code then checks if the request method is OPTIONS. The OPTIONS method is used in CORS preflight8 requests to determine if the actual request is safe to send. If the request is an OPTIONS request and contains specific headers indicating a CORS preflight request (Origin and Access-Control-Request-Method), the function generates a response with appropriate CORS headers. The response headers include Access-Control-Allow-Origin to reflect the client’s origin, Access-Control-Allow-Methods set to *, allowing any HTTP method, Access-Control-Allow-Headers based on the requested headers, and Access-Control-Max-Age set to 86400 seconds (one day) to cache the preflight response. If the request is not an OPTIONS request or doesn’t meet the CORS preflight conditions, the code continues execution. It creates a new Request object named proxyRequest uses the extracted destination URL and sets the method and headers of the original request. The Origin header is removed to prevent CORS restrictions when forwarding the request. The subsequent code performs the actual request forwarding. It uses fetch to send the proxyRequest to the destination URL. If the fetch is successful, the code proceeds to process the response. It creates a new Headers object from the response’s headers and modifies them to include the necessary CORS headers. Finally, the function constructs a Response object using the response body, status, statusText, and modified headers. If an error occurs during the fetch operation, the code catches the error and returns a Response object with an error message and a status code of 500 (Internal Server Error). Once you’ve pasted the snippet, you can redeploy the service from your local machine with: wrangler deploy This will deploy the service immediately: ⛅️ wrangler 3.0.0 ------------------ Total Upload: 1.52 KiB / gzip: 0.57 KiB Uploaded cors-proxy (0.51 sec) Published cors-proxy (0.38 sec) https:// Current Deployment ID: f300ac99-c15e-4e30-a910-a56d81c10b95 I’ve removed my root domain from the above output since I’m using the free version of Workers and don’t want people to exhaust my free request quota. Haha, security by obscurity! But once you’ve deployed your proxy server, you can go to the following URL from your browser: https:///?url=https://mozilla.com This will send you to the Mozilla website through the deployed function. Now you can use it just like the free CORS proxy. Try it out by dropping the following snippet to your browser console. This is exactly the same as the previous fetch snippet but the only difference is this time, we’re using our own proxy server that we control: // Notice how we're prepending CORS URL before the target URL fetch(\"https://?url=https://mozilla.org\") .then((response) =\u003e response.text()) .then((data) =\u003e { // Do something with the received data console.log(data); }) .catch((error) =\u003e { // Handle any errors that occurred during the request console.error(\"Error:\", error); }); Don’t forget to replace the URL with your own service. This will result in a successful request. You can also interactively send requests to the destination URLs via the Cloudflare Workers dashboard. Go to your Cloudflare dashboard, head over to the Workers section, and select your deployed serverless function: Deploying the service with GitHub Actions For one-off services, wrangler deploy in the local machine works perfectly but I usually don’t consider a project fully done until I’ve automated away the whole process. So, I wrote a quick GitHub Actions workflow to run the linters and deploy the service automatically when a new commit is pushed to the main branch. Here’s how it looks: # .github/workers/ci.yml name: Deploy on: push: branches: - main # Allow running this workflow manually. workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-node@v3 with: node-version: \"lts/*\" cache: npm cache-dependency-path: cors-proxy/package-lock.json - name: Install dependencies working-directory: ./cors-proxy run: | npm install - name: Run linter working-directory: ./cors-proxy run: | npx prettier --check . deploy: runs-on: ubuntu-latest needs: build steps: - uses: actions/checkout@v3 - name: Publish uses: cloudflare/wrangler-action@2.0.0 with: apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }} workingDirectory: \"cors-proxy\" For this to work, you’ll need to create a Cloudflare API key9 and add it to the GitHub Secrets10 of your proxy server’s repository. Here’s the complete workflow11 file. Cloudflare Workers ↩︎ CORS ↩︎ Let’s talk about CORS ↩︎ cors.sh ↩︎ CORS proxy ↩︎ Complete implementation ↩︎ Wrangler ↩︎ Preflight request ↩︎ Cloudflare API Key ↩︎ GitHub Secrets ↩︎ CI ↩︎ ","permalink":"http://rednafi.com/javascript/cors_proxy_with_cloudflare_workers/","publishDate":"2023-05-21","summary":"Cloudflare absolutely nailed the serverless function DX with Cloudflare Workers1. However, I feel like it’s yet to receive widespread popularity like AWS Lambda since as of now, the service only offers a single runtime—JavaScript. But if you can look past that big folly, it’s a delightful piece of tech to work with. I’ve been building small tools with it for a couple of years but never got around to writing about the immense productivity boost it usually gives me whenever I need to quickly build and deploy a self-contained service.\n","tags":["JavaScript","Networking","GitHub"],"title":"Building a CORS proxy with Cloudflare Workers"},{"content":"This weekend, I was working on a fun project that required a fixed-time job scheduler to run a curl command at a future timestamp. I was aiming to find the simplest solution that could just get the job done. I’ve also been exploring Google Bard1 recently and wanted to see how it stacks up against other LLM tools like ChatGPT, BingChat, or Anthropic’s Claude in terms of resolving programming queries. So, I asked Bard: What’s the simplest solution I could get away with to run a shell command at a future datetime? It introduced me to the UNIX at command that does exactly what I needed. Cron wouldn’t be a good fit for this particular use case, and I wasn’t aware of the existence of at before. So I started probing the model and wanted to document my findings for future reference. Also, the final hacky solution that allowed me to schedule jobs remotely can be found at the tail2 of this post. The insipid definition The command at in UNIX is used to schedule one-time jobs or commands to be executed at a specific time in the future. Internally, the system maintains a queue that adds a new entry when a job is scheduled, and once it gets executed, the job is removed from the queue. NOTE: By default, the jobs will be scheduled using the targeted machine’s local timezone. Prerequisites The command isn’t included in GNU coreutils, so you might have to install it separately on your machine. Debian-ish On a Debian-flavored Linux machine, run: apt install at Then check the status of atd daemon. This daemon executes the scheduled jobs. service atd status * atd is running If the service isn’t running, then you can start the daemon with this command: service atd start * Starting deferred execution scheduler atd [OK] MacOS On MacOS, scheduled jobs are carried out by atrun and it’s disabled by default. I had to fiddle around quite a bit to make it work on my MacBook Pro running MacOS Ventura. First, you’ll need to launch the daemon with the following command: sudo launchctl load -w /System/Library/LaunchDaemons/com.apple.atrun.plist This will start the atrun daemon. Or enable it for future bootups by modifying /System/Library/LaunchDaemons/com.apple.atrun.plist to have: ... Enabled ... On modern MacOS like Ventura, unfortunately, this requires disabling SIP3. Next, you’ll need to provide full disk access to atrun. To do so: Open Spotlight and type in Allow full disk access. On the left panel, click on Allow applications to access all user files. On the right panel, add /usr/libexec/atrun to the list of allowed apps. Press cmd + shift + g and type in the full path of atrun. You can learn more about making atrun work on MacOS here4. Although I’m using MacOS for development, In my particular case, making at work on MacOS wasn’t the first priority because I deployed the final solution to an Ubuntu container. A few examples The following sections demonstrates some examples of scheduling commands to be executed in a few different scenarios. Schedule at a specific time To schedule a command to be executed at a specific time, use this command syntax: at For example, to schedule the command ls -lah \u003e\u003e foo.txt to be executed at 3:00 PM local time, you’d use the following command: at 3pm at\u003e ls -lah \u003e\u003e foo.txt at\u003e Pressing tells at that you have finished entering the command, and it should schedule the job to run at the specified time. You’ll see that at 3.00PM local time, a file named foo.txt containing the output of ls -l will be created. Schedule after a certain period of time To schedule a command to run in a specific amount of time from now, use: at now + For example, to schedule ps aux \u003e\u003e foo.txt to run in 2 minutes from now, you’d use the following command: at now + 2 minutes at\u003e ps aux \u003e\u003e foo.txt at\u003e This will schedule the command to run in two minutes in the current local time. Schedule a script run You can also run a script containing multiple commands at a specific time. To do this, create a script file that houses the commands you want to run, and then use at to schedule the script to be executed at the desired time. For example, suppose you have a script file called script.sh that contains a curl command which makes an API call and saves the output to a file. You can schedule it as such: #!/usr/bin/env bash curl -X GET https://httpbin.org/get \u003e\u003e foo.json at -f script.sh now + 1 minute The script will be executed in a minute from now. You can check the content of foo.json 1 minute later: { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Host\": \"httpbin.org\", \"User-Agent\": \"curl/7.85.0\", \"X-Amzn-Trace-Id\": \"Root=1-646162a8-71a232d563e0c16a4a497acf\" }, \"origin\": \"74.140.2.169\", \"url\": \"https://httpbin.org/get\" } Schedule in a non-interactive manner What if you don’t want to create a new script file and also don’t want to schedule a command interactively as shown before? You can echo the desired command and pipe it to at like this: echo \"dig +short rednafi.com \u003e\u003e foo.txt\" | at now + 1 minute We can also run multi-line commands in a single go by taking advantage of the heredoc format: at now + 1 minute \u003c\u003e foo.txt EOF In either case, 1 minute later, you’ll see that a foo.txt file will be created in your local directory with the following content: 185.199.111.153 185.199.108.153 185.199.109.153 185.199.110.153 This command above uses at to schedule the execution of a dig command for the domain name rednafi.com. In this case, dig performs a DNS lookup, and the scheduled time is set to be 1 minute from now in the current local time. The output of the command is then appended to the file foo.txt. The \u003c","permalink":"http://rednafi.com/misc/fixed_time_task_scheduling_with_at/","publishDate":"2023-05-14","summary":"This weekend, I was working on a fun project that required a fixed-time job scheduler to run a curl command at a future timestamp. I was aiming to find the simplest solution that could just get the job done. I’ve also been exploring Google Bard1 recently and wanted to see how it stacks up against other LLM tools like ChatGPT, BingChat, or Anthropic’s Claude in terms of resolving programming queries.\n","tags":["Shell","JavaScript","Networking"],"title":"Fixed-time job scheduling with UNIX 'at' command"},{"content":"I needed a way to sort a Django queryset based on a custom sequence of an attribute. Typically, Django allows sorting a queryset by any attribute on the model or related to it in either ascending or descending order. However, what if you need to sort the queryset following a custom sequence of attribute values? Suppose, you’re working with a model called Product where you want to sort the rows of the table based on a list of product ids that are already sorted in a particular order. Here’s how it might look: # List of product ids id_list = [3, 1, 2, 4, 8, 7, 5, 6] # We want to sort the products queryset in such a way that the records # appear in the same order specified in the id_list. products = Product.objects.all() Turns out, this is a great case where Django’s Case and When can come in handy. With these, Django exposes the underlying SQL’s way of performing conditional logic through CASE and WHEN statements. They allow you to return different values or expressions based on some criteria. Think of them as similar to IF-THEN-ELSE statements in other programming languages. Primarily, there are two types of CASE expressions: simple and searched. Simple CASE expression A simple CASE expression compares an input expression to a list of values and returns the corresponding result. Here’s the syntax: CASE input_expression WHEN value1 THEN result1 WHEN value2 THEN result2 ... ELSE default_result END The input_expression can be any valid SQL expression. The data types of the input_expression and each value must be the same or must be an implicit conversion. The WHEN clauses are evaluated in order, from top to bottom. The first one that matches the input_expression determines the result of the CASE expression. If none of the values match, the ELSE clause is executed. If the ELSE clause is omitted and no values match, the CASE expression returns NULL. For example, suppose we have a table called products with the following data: | id | name | price | category | | -- | ---- | ----- | -------- | | 1 | A | 10 | X | | 2 | B | 20 | Y | | 3 | C | 30 | Z | | 4 | D | 40 | X | | 5 | E | 50 | Y | We can use a simple CASE expression to assign a label to each product based on its category: SELECT id, name, price, CASE category WHEN 'X' THEN 'Low' WHEN 'Y' THEN 'Medium' WHEN 'Z' THEN 'High' END AS label FROM products; The output would be: | id | name | price | label | | -- | ---- | ----- | ------ | | 1 | A | 10 | Low | | 2 | B | 20 | Medium | | 3 | C | 30 | High | | 4 | D | 40 | Low | | 5 | E | 50 | Medium | Searched CASE expression A searched CASE expression evaluates a list of Boolean expressions and returns the corresponding result. The syntax looks as follows: CASE WHEN condition1 THEN result1 WHEN condition2 THEN result2 ... ELSE default_result END The conditions can be any valid Boolean expressions. Just like simple CASE expressions, here also, the data types of each result must be the same or must be an implicit conversion. As before, the WHEN clauses are evaluated in order, from top to bottom. The first one that evaluates to TRUE determines the result of the CASE expression. If none of the conditions are TRUE, the ELSE clause is executed. If the ELSE clause is omitted and no conditions are TRUE, the CASE expression returns NULL. For example, we can use a searched CASE expression to calculate a discount for each product based on its price: SELECT id, name, price, CASE WHEN price \u003c 20 THEN price * 0.9 --10% discount WHEN price BETWEEN 20 AND 40 THEN price *0.8 --20% discount ELSE price *0.7 --30% discount END AS discounted_price FROM products; The output would be: | id | name | price | discounted_price | | -- | ---- | ----- | ---------------- | | 1 | A | 10 | 9 | | 2 | B | 20 | 16 | | 3 | C | 30 | 24 | | 4 | D | 40 | 32 | | 5 | E | 50 | 35 | Using searched CASE expressions to order querysets With the intro explanations out of the way, here’s how you can sort the products table introduced in the previous section by a list of product ids: from django.db.models import Case, When from .models import Product product_ids = [4, 2, 1, 3, 5] products = Product.objects.all() preferred = Case( *(When(id=id, then=pos) for pos, id in enumerate(product_ids, start=1)) ) products_sorted = products.filter(id__in=product_ids).order_by(preferred) Printing the queryset will return the following output: ","permalink":"http://rednafi.com/python/sort_by_a_custom_sequence_in_django/","publishDate":"2023-05-09","summary":"I needed a way to sort a Django queryset based on a custom sequence of an attribute. Typically, Django allows sorting a queryset by any attribute on the model or related to it in either ascending or descending order. However, what if you need to sort the queryset following a custom sequence of attribute values?\nSuppose, you’re working with a model called Product where you want to sort the rows of the table based on a list of product ids that are already sorted in a particular order. Here’s how it might look:\n","tags":["Python","Django"],"title":"Sorting a Django queryset by a custom sequence of an attribute"},{"content":"I recently gave my blog1 a fresh new look and decided it was time to spruce up my GitHub profile’s2 landing page as well. GitHub has a special3 way of treating the README.md file of your repo, displaying its content as the landing page for your profile. My goal was to showcase a brief introduction about myself and my work, along with a list of the five most recent articles on my blog. Additionally, I wanted to ensure that the article list stayed up to date. There are plenty of fancy GitHub Action workflows4 that allow you to add your site’s URL to the CI file and it’ll periodically fetch the most recent content from the source and update the readme file. However, I wanted to make a simpler version of it from scratch which can be extended for periodically updating any Markdown file in any repo, just not the profile readme. So, here’s the plan: A custom GitHub Action workflow will periodically run a nodejs script. The script will then: Grab the XML index5 of this blog that you’re reading. Parse the XML content and extract the URLs and publication dates of 5 most recent articles. Update the associated Markdown table with the extracted content on the profile’s README.md file. Finally, the workflow will commit the changes and push them to the profile repo. You can see the final outcome here6. Here’s the script that performs the above steps: // importBlogs.js /* Import the latest 5 blog posts from rss feed */ import fetch from \"node-fetch\"; import { Parser } from \"xml2js\"; import { promises } from \"fs\"; const rssUrl = \"https://rednafi.com/index.xml\"; const header = ` Introducing myself... \\n\\n`; const outputFile = \"README.md\"; const parser = new Parser(); // Define an async function to get and parse the rss data async function getRssData() { try { const res = await fetch(rssUrl); const data = await res.text(); return await parser.parseStringPromise(data); } catch (err) { console.error(err); } } // Define an async function to write the output file async function writeOutputFile(output) { try { await promises.writeFile(outputFile, output); console.log(`Saved ${outputFile}`); } catch (err) { console.error(err); } } // Call the async functions getRssData() .then((result) =\u003e { // Get the first five posts from the result object const posts = result.rss.channel[0].item.slice(0, 5); // Initialize an empty output string let output = \"\"; // Add a title to the output string output += header; // Add a header row to the output string output += `#### Recent articles\\n\\n`; output += \"| Title | Published On |\\n\"; output += \"| ----- | ------------ |\\n\"; // Loop through the posts and add a row for each post to the output string for (let post of posts) { // Strip the time from the pubDate const date = post.pubDate[0].slice(0, 16); output += `| [${post.title}](${post.link}) | ${date} |\\n`; } // Call the writeOutputFile function with the output string writeOutputFile(output); }) .catch((err) =\u003e { // Handle the error console.error(err); }); The snippet above utilizes node-fetch to make HTTP calls,xml2js for XML parsing, and the built-in fs module’s promises for handling file system operations. Next, it defines an async function getRssData responsible for fetching the XML data from the [https://rednafi.com/index.html] URL. It extracts the blog URLs and publication dates, and returns the parsed data as a list of objects. Another async function, writeOutputFile, writes the parsed XML content as a Markdown table and saves it to the README.md file. The script is executed by the following GitHub Action workflow every day at 0:00 UTC. Before the CI runs, make sure you create a new Action Secret7 named ACCESS_TOKEN that houses an access token8 with write access to the repo where the CI runs. # Run a bash script to randomly generate empty commit to this repo. name: CI on: # Since we're pushing from this CI, don't run this on the push event because # that'll trigger an infinite loop # push: [ main ] # Add a schedule to run the job every day at 0:00 UTC schedule: - cron: \"0 0 * * *\" # Allow running this workflow manually workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - name: Checkout repo uses: actions/checkout@v4 with: # Otherwise, there would be errors pushing refs to the destination # repository fetch-depth: 0 ref: ${{ github.head_ref }} token: ${{ secrets.ACCESS_TOKEN }} - uses: actions/setup-node@v3 with: node-version: \"lts/*\" cache: npm cache-dependency-path: package-lock.json - name: Install dependencies run: | npm install - name: Run linter run: | npx prettier --write . - name: Run script run: | node scripts/importBlogs.js - name: Commit changes run: | git config --local user.name \\ \"github-actions[bot]\" git config --local user.email \\ \"41898282+github-actions[bot]@users.noreply.github.com\" git add . git diff-index --quiet HEAD \\ || git commit -m \"Autocommit: updated at $(date -u)\" - name: Push changes uses: ad-m/github-push-action@master with: force_with_lease: true In the first four steps, the workflow checks out the codebase, sets up nodejs, installs the dependencies, and then runs prettier on the scripts. Next, it executes the importBlogs.js script. The script updates the README and the subsequent shell commands commit the changes to the repo. The following line ensures that we’re only trying to commit when there’s a change in the tracked files. git diff-index --quiet HEAD \\ || git commit -m \"Autocommit: updated at $(date -u)\" Then in the last step, we use an off-the-shelf workflow to push our changes to the repo. Check out the workflow directory9 of my profile’s repo to see the whole setup in action. I’m quite satisfied with the final output: Blog ↩︎ GitHub profile landing page ↩︎ Managing your profile README ↩︎ Blog post workflow ↩︎ Blog index ↩︎ Profile repository ↩︎ Action secrets ↩︎ Personal access token ↩︎ Workflow directory ↩︎ ","permalink":"http://rednafi.com/javascript/periodic_readme_updates_with_gh_actions/","publishDate":"2023-05-04","summary":"I recently gave my blog1 a fresh new look and decided it was time to spruce up my GitHub profile’s2 landing page as well. GitHub has a special3 way of treating the README.md file of your repo, displaying its content as the landing page for your profile. My goal was to showcase a brief introduction about myself and my work, along with a list of the five most recent articles on my blog. Additionally, I wanted to ensure that the article list stayed up to date.\n","tags":["JavaScript","GitHub"],"title":"Periodic readme updates with GitHub Actions"},{"content":"One of my favorite pastimes these days is to set BingChat to creative mode, ask it to teach me a trick about topic X, and then write a short blog post about it to reinforce my understanding. Some of the things it comes up with are absolutely delightful. In the spirit of that, I asked it to teach me a Shell trick that I can use to mimic maps or dictionaries in a shell environment. I didn’t even know what I was expecting.\nIt didn’t disappoint and introduced me to the idea of associative arrays in Bash. This data structure is basically the Bash equivalent of a map.\nFirst, we have our usual arrays which are containers that can store multiple values, indexed by numbers. Associative arrays are similar, but they use strings as keys instead of numbers. For example, if you want to store the names of some fruits in a regular array, you can use:\nfruits=(apple banana cherry) This will create an array called fruits with three elements. You can access the elements by using the index number inside brackets, such as:\necho ${fruits[0]} This prints:\napple You can also use a range of indices to get a slice of the array, such as:\necho ${fruits[@]:1:2} This will print:\nbanana cherry Moreover, you can use * or @ to get all the elements of the array, such as:\necho ${fruits[*]} This returns:\napple banana cherry Associative arrays are declared with the declare -A command, and then assigned values using the = operator and brackets. For example, if you want to store the prices of some fruits in an associative array, you can use:\ndeclare -A prices prices=([apple]=1.00 [banana]=0.50 [cherry]=2.00) Or you can create the key-value pairs in place like this:\ndeclare -A prices prices[apple]=1.00 prices[banana]=0.50 prices[cherry]=2.00 This will create an associative array called prices with three key-value pairs. You can access the values by using the keys inside brackets, such as:\necho ${prices[apple]} This will print:\n1.00 Similar to regular arrays, you can use * or @ to get all the keys or values of the associative array. Run the following command to get all the keys of the prices associative array:\necho ${!prices[*]} This will print:\napple banana cherry To get the values, run:\necho ${prices[@]} This returns:\n1.00 0.50 2.00 Arrays and associative arrays can be useful when you want to store and manipulate complex data structures in bash. You can use them to perform arithmetic operations, string operations, or loop over them with for or while commands. For example, you can use:\nfor fruit in ${!prices[*]}; do echo \"$fruit costs ${prices[$fruit]}\"; done In the above snippet, we iterate through the keys of prices in a for loop. The ${!prices[*]} notation expands to a list of all the keys in the prices array. Inside the loop, we print the key-value pairs, where $fruit represents the current key and ${prices[$fruit]} represents the corresponding value. So in each iteration, the snippet will output the name of each fruit along with its corresponding price.\nRunning the snippet will print:\napple costs 1.00 banana costs 0.50 cherry costs 2.00 A more practical example Here’s a script that downloads three famous RFCs using cURL. We’re using an associative array for bookkeeping purposes.\n#!/usr/bin/env bash set -euo pipefail declare -A rfc_urls=( [http-error]=\"https://www.rfc-editor.org/rfc/rfc7808.txt\" [http-one]=\"https://www.rfc-editor.org/rfc/rfc7231.txt\" [datetime-format]=\"https://www.rfc-editor.org/rfc/rfc3339.txt\" ) echo \"======================\" echo \"start downloading rfcs\" echo \"======================\" echo \"\" for key in \"${!rfc_urls[@]}\"; do value=${rfc_urls[$key]} echo \"Downloading rfcs ${key}: ${value}\" curl -OJLs \"${value}\" done echo \"\" echo \"======================\" echo \"done downloading rfcs\" echo \"======================\" Running this will download the RFCs in the current directory:\n====================== start downloading rfcs ====================== Downloading rfcs http-error: https://www.rfc-editor.org/rfc/rfc7808.txt Downloading rfcs datetime-format: https://www.rfc-editor.org/rfc/rfc3339.txt Downloading rfcs http-one: https://www.rfc-editor.org/rfc/rfc7231.txt ====================== done downloading rfcs ====================== The script begins by declaring an associative array called rfc_urls. This array serves as a convenient way to keep track of the RFCs we want to download. Each key in the array represents a unique identifier for an RFC, while the corresponding value holds the complete URL to download that specific RFC.\nNext, we set the base_url variable to https://www.rfc-editor.org/rfc, which will be used as the base URL for all RFC downloads.\nInside a loop that iterates over the keys of the rfc_urls array, we retrieve the URL value associated with each key. To provide a progress update, we echo a message indicating the RFC being downloaded.\nUsing the curl command with the options -OJLs, we initiate the download process. The -O flag ensures that the remote file is saved with its original filename, while the -J flag takes advantage of the Content-Disposition header in the HTTP response to determine the filename. We include the -L flag to follow redirects, and the -s flag to silence curl’s progress output.\nAdvanced Bash scripting guide – devconnected 1 ↩︎\nAdvanced Bash scripting techniques for Linux administrators 2 ↩︎\nUseful Bash command line tips and tricks examples – part 1 - Linux config 3 ↩︎\n3 command line games for learning Bash the fun way 4 ↩︎\n","permalink":"http://rednafi.com/misc/associative_arrays_in_bash/","publishDate":"2023-05-03","summary":"One of my favorite pastimes these days is to set BingChat to creative mode, ask it to teach me a trick about topic X, and then write a short blog post about it to reinforce my understanding. Some of the things it comes up with are absolutely delightful. In the spirit of that, I asked it to teach me a Shell trick that I can use to mimic maps or dictionaries in a shell environment. I didn’t even know what I was expecting.\n","tags":["Shell","TIL"],"title":"Associative arrays in Bash"},{"content":"Whenever I need to deduplicate the items of an iterable in Python, my usual approach is to create a set from the iterable and then convert it back into a list or tuple. However, this approach doesn’t preserve the original order of the items, which can be a problem if you need to keep the order unscathed. Here’s a naive approach that works: from __future__ import annotations from collections.abc import Iterable # Python \u003e3.9 def dedup(it: Iterable) -\u003e list: seen = set() result = [] for item in it: if item not in seen: seen.add(item) result.append(item) return result it = (2, 1, 3, 4, 66, 0, 1, 1, 1) deduped_it = dedup(it) # Gives you [2, 1, 3, 4, 66, 0] This code snippet defines a function dedup that takes an iterable it as input and returns a new list containing the unique items of the input iterable in their original order. The function uses a set seen to keep track of the items that have already been seen, and a list result to store the unique items. Then it iterates over all the items of the input iterable using a for loop. For each item, the function checks if it has already been seen (i.e., if it’s in the seen set). If the item hasn’t been seen, it’s added to both the seen set and the result list. The final result list contains the unique items of it in their original order. This can be made a little terser by using listcomp as follows: from __future__ import annotations from collections.abc import Iterable # Python \u003e3.9 def dedup(it: Iterable) -\u003e list: seen = set() # Binding seen.add to a variable reduces the cost of attribute # fetching within a tight loop seen_add = seen.add # Here, 'or' allows us to add the item to 'seen' when it doesn't # already exist there in a single line. return [item for item in it if not (item in seen or seen_add(item))] Dedup with ordered dict from collections import OrderedDict dedup = lambda it: list(OrderedDict.fromkeys(seq)) it = (2, 1, 3, 4, 66, 0, 1, 1, 1) deduped_it = dedup(it) # Gives you [2, 1, 3, 4, 66, 0] Similar to the first snippet, this also defines dedup that takes an iterable it as input and returns a new list containing the unique items of it in their original order. The function uses the OrderedDict.fromkeys() method to create a new ordered dict with the items of it as keys and None as values. Since an ordered dict maintains the insertion order of its keys, this effectively removes any duplicate items from the iterable without affecting the order of the remaining ones. The iterable containing the keys of the resulting ordered dict is then converted into a list using the list() function to obtain a list of the unique items in their original order. While this is quite terse and does the job with O(1) complexity, neither this nor the previous solution would work for compound iterables as follows: # Here the dedup function will have to remove the duplicate items by # nested items. So the desired output will be ((1,1), (2, 1), (3, 1)) it = ((1, 1), (2, 1), (3, 1), (1, 1), (1, 3)) The next solution works on one-level nested iterables. Dedup by any element of an item in a nested iterable Consider this one-level nested iterable: # Here, (1,1), (2, 1) are items of the iterable 'it' and 1 is an element # of the first item (1,1). We're referring to items of items as elements. it = ((1, 1), (2, 1), (3, 1), (1, 1), (1, 3)) We want to write a dedup function that’ll allow us to deduplicate the iterable based on a particular element of an item. Here, (1,1), (2, 1) are items of the iterable it and 1 is the second element of item (2, 1). Here’s how we can modify the first dedup to allow deduplication by nested elements. from __future__ import annotations from collections.abc import Iterable from typing import Any, Generator def dedup( it: Iterable[tuple[Any, ...]], index: int, lazy: bool = True ) -\u003e list[Any] | Generator[Any, None, None]: seen = set() # type: set[Any] seen_add = seen.add expr = ( item for item in it if not ((elem := item[index]) in seen or seen_add(elem)) ) return expr if lazy else list(expr) it = ((1, 1), (2, 1), (3, 1), (1, 1), (1, 3)) # We're deduplicating by the second element of the items. dedup(it, 2, False) # Returns [(1,1), (1,3)] This time, the dedup function takes in an iterable of tuples it, an element index index, and a boolean lazy (defaulting to True) as arguments. The function returns a list or generator of the unique items from the input iterable based on the specified element index. Just as before, the function first creates an empty set seen and binds its add method to a variable seen_add. It then creates a generator expression that iterates over it and yields each item if its element at the specified index isn’t already present in the seen set. If item[index] isn’t present in seen, it’s added to it using the seen_add method. If lazy is True, the function returns the generator expression verbatim. Otherwise, it returns a list created from the generator expression. In the example provided, the function is called with arguments it, 1, and False. This means that it will deduplicate the input iterable based on the second element of each tuple and return a list. The result is [(1,1), (1,3)]. How do I remove duplicates from a list while preserving order 1 ↩︎ ","permalink":"http://rednafi.com/python/deduplicate_iterables_while_preserving_order/","publishDate":"2023-05-01","summary":"Whenever I need to deduplicate the items of an iterable in Python, my usual approach is to create a set from the iterable and then convert it back into a list or tuple. However, this approach doesn’t preserve the original order of the items, which can be a problem if you need to keep the order unscathed. Here’s a naive approach that works:\nfrom __future__ import annotations from collections.abc import Iterable # Python \u003e3.9 def dedup(it: Iterable) -\u003e list: seen = set() result = [] for item in it: if item not in seen: seen.add(item) result.append(item) return result it = (2, 1, 3, 4, 66, 0, 1, 1, 1) deduped_it = dedup(it) # Gives you [2, 1, 3, 4, 66, 0] This code snippet defines a function dedup that takes an iterable it as input and returns a new list containing the unique items of the input iterable in their original order. The function uses a set seen to keep track of the items that have already been seen, and a list result to store the unique items.\n","tags":["Python"],"title":"Deduplicating iterables while preserving order in Python"},{"content":"I needed to compare two large directories with thousands of similarly named PDF files and find the differing filenames between them. In the first pass, this is what I did: Listed out the content of the first directory and saved it in a file: ls dir1 \u003e dir1.txt Did the same for the second directory: ls dir2 \u003e dir2.txt Compared the difference between the two outputs: diff dir1.txt dir2.txt This returned the name of the differing files likes this: 3c3,4 \u003c f3.pdf --- \u003e f4.pdf \u003e f5.pdf It does the job, but I asked BingChat if there’s a better way to accomplish the task without creating intermediate files, and it didn’t let me down. Turns out that in Bash, process substitution allows you to do just that. Instead of running three commands, you can achieve the same result with a simple one-liner: diff \u003c(ls dir1) \u003c(ls dir2) Process substitution In Bash, process substitution is a feature that allows you to treat the output of a command or commands as if it were a file. It enables you to use the output of a command as an input to another command or perform other operations that expect file input or output. One important thing to point out is that process substitution is specific to Bash, Zsh, and certain versions of Ksh. Other shells and Bash in POSIX mode don’t understand it. Bash, Zsh, and Ksh (88,93) support process substitution, but pdksh derivatives like mksh don’t currently have this capability.* The syntax for process substitution is as follows: \u003c(command): This form allows you to use the output of a command as a file-like input. \u003e(command): This form allows you to use the output of a command as a file-like output. When using process substitution, Bash creates a named pipe (FIFO) or a special file descriptor /dev/fd/ behind the scenes. The command within the parentheses is executed, and its output is redirected to the named pipe or file descriptor. Then, the path to the named pipe or file descriptor is substituted into the original command line. This is different from the plain-old stdin or stdout redirection. Here’s how: Input Plain redirection: When using plain stdin redirection (\u003c), you can redirect input from a file, for example, \u003c input.txt. The command reads the content of the file as standard input (stdin). Process substitution: With process substitution, you can use the output of a command as input. For example, command \u003c \u003c(echo \"input\"). Here, the output of the echo command is treated as a file-like object and used as the input to command. Output Plain redirection: Using plain stdout redirection (\u003e or \u003e\u003e), you can redirect the output (stdout) of a command to a file, for example, command \u003e output.txt. The command’s output is written to the specified file. Process substitution: With process substitution, you can use the output of a command as output. For example, command \u003e(process_output). Here, the output of command is treated as a file-like output, and it is passed as input to the process_output command or operation. By using process substitution, the output of a command can be seamlessly integrated into other commands as if it were a file, even if the command doesn’t explicitly support stdin or stdout redirection. This allows for greater compatibility and enables the use of the output in situations where direct piping or redirection may not be possible. A few practical examples Inspecting the descriptors involved in process substitution You can inspect the descriptor used by a process substitution like this: echo \u003e(true) \u003c(false) This returns: /dev/fd/13 /dev/fd/11 Here, the expression \u003e(true) creates a temporary file-like object, and the true command serves as a placeholder for its input. Similarly, \u003c(false) creates another temporary file-like object with the false command serving as a placeholder for its output. When the echo command is executed, it displays the filenames associated with these temporary file-like objects, which are /dev/fd/13 and /dev/fd/11 in this specific scenario. These filenames represent the underlying descriptors of the respective process substitutions, indicating the file descriptors associated with the temporary objects created during the process substitution. Calculating the total number of lines in a file wc -l \u003c \u003c(cat input.txt) This command calculates the total number of lines in the input.txt file. Here, the \u003c(cat input.txt) commad creates an input-type file descriptor containing the output of the cat command and wc -l reads that content from there. The extra \u003c redirects the file-like object as an input stream again. This is a roundabout way of doing the following: cat input.txt | wc -l Processing the content of a file line by line while read line; do echo $line; done \u003c \u003c(cat input.txt) This command reads each line from the file input.txt and echoes it. It uses a while loop with the read command to iterate over the lines, assigning each line to the variable line and the echo $line command displays the line. Process substitution \u003c() is used to treat the output of cat input.txt as a temporary file, providing the input to the loop. Comparing directory sizes diff -r \u003c(du -sh dir1) \u003c(du -sh dir2) The command compares the disk usage of two directories, dir1 and dir2, using the diff command. The process substitution \u003c() is employed to capture the output of the du -sh command, which calculates the disk usage of each directory and provides a summary in a human-readable format. The output of each du -sh command, representing the disk usage of dir1 and dir2, is treated as temporary files and passed as arguments to the diff command. This enables the comparison of the disk usage between the two directories, highlighting any discrepancies in file sizes or subdirectories. Picking or rejecting lines common between two sorted files comm \u003c(echo 'hello world\\nhello mars' | sort) \\ \u003c(echo 'hello world\\nhello venus' | sort) This returns: hello mars hello venus hello world This performs a comparison between the sorted outputs of two separate commands using comm. The com command expects two files but we’re using process substitution to make two file-like objects from stdout. Within the first process substitution \u003c(), echo is used to generate a string containing two lines: hello world and hello mars. This string is then piped to the sort command, which sorts the lines alphabetically. Similarly, the second part of the command \u003c() uses process substitution as well. It follows the same pattern as the first process substitution, but this time the string contains hello world and hello venus. The file-like objects containing the sorted output from the two process substitutions are then passed as arguments to the comm command. Then comm compares the input files line by line and generates three columns of output: lines unique to the first input, lines unique to the second input, and lines common to both inputs. Process substitution 1 ↩︎ ","permalink":"http://rednafi.com/misc/process_substitution_in_bash/","publishDate":"2023-04-30","summary":"I needed to compare two large directories with thousands of similarly named PDF files and find the differing filenames between them. In the first pass, this is what I did:\nListed out the content of the first directory and saved it in a file:\nls dir1 \u003e dir1.txt Did the same for the second directory:\nls dir2 \u003e dir2.txt Compared the difference between the two outputs:\ndiff dir1.txt dir2.txt This returned the name of the differing files likes this:\n","tags":["Shell","TIL"],"title":"Process substitution in Bash"},{"content":"Whenever I need to whip up a quick command line tool, my go-to is usually Python. Python’s CLI solutions tend to be more robust than their Shell counterparts. However, dealing with its portability can sometimes be a hassle, especially when all you want is to distribute a simple script. That’s why while toying around with argparse to create a dynamic menu, I decided to ask ChatGPT if there’s a way to achieve the same using native shell scripting. Delightfully, it introduced me to the dead-simple select command that I probably should’ve known about years ago. But I guess better late than never! Here’s what I was trying to accomplish:\nPrint a menu that allows a user to choose an option and then trigger a specific function associated with the chosen option. When you run the script, it should present you with something similar to this:\n1) Option 1 2) Option 2 3) Option 3 4) Quit Please enter your choice: 1 You selected Option 1. Please enter your choice: 2 You selected Option 2. Please enter your choice: 3 You selected Option 3. Please enter your choice: 4 Whenever the user selects an option, the script dispatches an associated function with the option. Currently, the associated function just prints You selected option x but it has the freedom to do whatever it wants. The following native Shell script uses select to produce the output above:\n#!/usr/bin/env bash # script.sh # Define an array of menu options options=(\"Option 1\" \"Option 2\" \"Option 3\" \"Quit\") # Function to handle Option 1 function option1 { echo \"You selected Option 1.\" # Add your Option 1 code here } # Function to handle Option 2 function option2 { echo \"You selected Option 2.\" # Add your Option 2 code here } # Function to handle Option 3 function option3 { echo \"You selected Option 3.\" # Add your Option 3 code here } # Display the menu and process user selection PS3=\"Please enter your choice: \" select option in \"${options[@]}\"; do case $option in \"Option 1\") option1 ;; \"Option 2\") option2 ;; \"Option 3\") option3 ;; \"Quit\") break ;; *) echo \"Invalid option. Try again.\" ;; esac done The snippet allows users to make selections from a list of options. It starts by defining an array called options which holds the available possibilities. Each option corresponds to a specific function, such as option1, option2, and option3 which can be customized to perform specific actions or tasks.\nThe script then prompts the user to enter their choice using the select statement. The user’s selection is stored in the variable option. Then it uses a case statement to match the selected option and execute the corresponding function. For example, if the user chooses Option 1 by typing 1 into the console, the script calls the option1 function and displays a message confirming the selection. The same applies to Option 2 and Option 3. If the user selects Quit by typing 4, the script breaks out of the loop and terminates. Moreover, if the user enters an invalid option, the script displays an error message indicating that the option is not recognized and prompts the user to try again.\nHere’s a little more useful script to run some common Docker commands based on the user’s selection. The script assumes that Docker engine is installed on the targeted system:\n#!/usr/bin/env bash # Define an array of menu options options=( \"Show Docker Images\" \"Remove Docker Image\" \"Show Docker Containers\" \"Remove Docker Container\" \"Stop All Containers\" \"Reprint Options\" \"Quit\" ) # Function to show all Docker images function show_docker_images { echo \"Listing Docker images:\" docker images } # Function to remove a Docker image function remove_docker_image { read -p \"Enter the image ID or name to remove: \" image_id echo \"Removing Docker image: $image_id\" docker rmi \"$image_id\" } # Function to show all Docker containers function show_docker_containers { echo \"Listing Docker containers:\" docker ps -a } # Function to remove a Docker container function remove_docker_container { read -p \"Enter the container ID or name to remove: \" container_id echo \"Removing Docker container: $container_id\" docker rm \"$container_id\" } # Function to stop all Docker containers function stop_all_containers { echo \"Stopping all Docker containers:\" docker stop $(docker ps -aq) } # Function to reprint the options function reprint_options { echo \"Available Options:\" for index in \"${!options[@]}\"; do echo \"$((index+1))) ${options[index]}\" done } # Display the menu and process user selection PS3=\"Please enter your choice: \" select option in \"${options[@]}\"; do case $option in \"Show Docker Images\") show_docker_images ;; \"Remove Docker Image\") remove_docker_image ;; \"Show Docker Containers\") show_docker_containers ;; \"Remove Docker Container\") remove_docker_container ;; \"Stop All Containers\") stop_all_containers ;; \"Reprint Options\") reprint_options ;; \"Quit\") break ;; *) echo \"Invalid option. Try again.\" ;; esac done ","permalink":"http://rednafi.com/misc/dynamic_menu_with_select_in_bash/","publishDate":"2023-04-29","summary":"Whenever I need to whip up a quick command line tool, my go-to is usually Python. Python’s CLI solutions tend to be more robust than their Shell counterparts. However, dealing with its portability can sometimes be a hassle, especially when all you want is to distribute a simple script. That’s why while toying around with argparse to create a dynamic menu, I decided to ask ChatGPT if there’s a way to achieve the same using native shell scripting. Delightfully, it introduced me to the dead-simple select command that I probably should’ve known about years ago. But I guess better late than never! Here’s what I was trying to accomplish:\n","tags":["Shell","TIL"],"title":"Dynamic menu with select statement in Bash"},{"content":"When writing shell scripts, I’d often resort to using hardcoded ANSI escape codes1 to format text, such as: #!/usr/bin/env bash BOLD=\"\\033[1m\" UNBOLD=\"\\033[22m\" FG_RED=\"\\033[31m\" BG_YELLOW=\"\\033[43m\" BG_BLUE=\"\\033[44m\" RESET=\"\\033[0m\" # Print a message in bold red text on a yellow background. echo -e \"${BOLD}${FG_RED}${BG_YELLOW}This is a warning message${RESET}\" # Print a message in white text on a blue background. echo -e \"${BG_BLUE}This is a debug message${RESET}\" This shell snippet above shows how to add text formatting and color to shell script output via ANSI escape codes. It defines a few variables that contain different escape codes for bold, unbold, foreground, and background colors. Then, we echo two log messages with different colors and formatting options. The first message is printed in bold red text on a yellow background, while the second message is printed in white text on a blue background. To ensure that subsequent output is not affected by the previous formatting, the RESET variable is used to reset all color and formatting options back to their defaults after each message is printed. The -e option is used with echo to enable the interpretation of backslash escapes, which includes the ANSI escape codes. While this works fairly well, every time I have to write a fancy shell script, I have to either look up the ANSI color codes, copy-paste from an existing script, or explain to an LLM what I need. Then chatGPT serendipitously recommended a shell tool called tput that makes this workflow quite a bit better. Underneath tput also uses ANSI escape codes to control various text formatting options but it doesn’t require you to hardcode these ugly escape codes. Basic usage The basic syntax of the tput command goes as follows: tput Formatting options Here are some commonly used tput formatting options: setaf : set the foreground (text) color to a specific color. For example, setaf 1 sets the color to red, while setaf 2 sets the color to green. setab : set the background color to a specific color. bold: set the text to bold. sgr0: reset all formatting options to their defaults. smul: underline the text. Example usage #!/usr/bin/env bash # Print text in red on a yellow background tput setaf 1 tput setab 3 echo \"Error: some error occurred\" tput sgr0 # Print bold text tput bold echo \"This text is bold\" tput sgr0 # Print underlined text in blue tput setaf 4 tput smul echo \"This text is underlined and blue\" tput sgr0 # Print text with a custom RGB color tput setaf 38 # specify an RGB color using 8-bit mode tput setaf 5 # specify a color index in 256-color mode echo \"This text is in a custom color\" # Print text with a background color gradient tput setaf 0 for i in {0..7}; do tput setab $i echo \"Background color $i\" done tput sgr0 # Print blinking text tput blink echo \"This text is blinking\" tput sgr0 Running the script will give you the following output: This also hardcodes the color and formatting codes but it’s much easier than having to remember or search for the ANSI escape codes. Currently, I’m using a 256-bit macOS terminal and it supports fairly large sets of formatting options. You can run man tput to find out other features that are supported by your terminal. The following loop will print all the supported colors: for i in {0..255}; do tput setab $i printf \" \" tput sgr0 done On my terminal, it prints this nice color palette: Escape codes ↩︎ ","permalink":"http://rednafi.com/misc/terminal_text_formatting_with_tput/","publishDate":"2023-04-23","summary":"When writing shell scripts, I’d often resort to using hardcoded ANSI escape codes1 to format text, such as:\n#!/usr/bin/env bash BOLD=\"\\033[1m\" UNBOLD=\"\\033[22m\" FG_RED=\"\\033[31m\" BG_YELLOW=\"\\033[43m\" BG_BLUE=\"\\033[44m\" RESET=\"\\033[0m\" # Print a message in bold red text on a yellow background. echo -e \"${BOLD}${FG_RED}${BG_YELLOW}This is a warning message${RESET}\" # Print a message in white text on a blue background. echo -e \"${BG_BLUE}This is a debug message${RESET}\" This shell snippet above shows how to add text formatting and color to shell script output via ANSI escape codes. It defines a few variables that contain different escape codes for bold, unbold, foreground, and background colors. Then, we echo two log messages with different colors and formatting options.\n","tags":["Shell","TIL"],"title":"Simple terminal text formatting with tput"},{"content":"Whenever I plan to build something, I spend 90% of my time researching and figuring out the idiosyncrasies of the tools that I decide to use for the project. LLM tools like ChatGPT has helped me immensely in that regard. I’m taking on more tangential side projects because they’re no longer as time-consuming as they used to be and provide me with an immense amount of joy and learning opportunities. While LLM interfaces like ChatGPT may hallucinate, confabulate, and confidently give you misleading information, they also allow you to avoid starting from scratch when you decide to work on something. Personally, this benefits me enough to keep language models in my tool belt and use them to churn out more exploratory work at a much faster pace. For some strange reason, I never took the time to explore ObservableHQ1, despite knowing what it does and how it can help me quickly build nifty client-side tools without going through the hassle of containerizing and deploying them as dedicated applications. So, I asked ChatGPT to build me a tool that would allow me to: Upload two CSV files Calculate the row and column counts from the files Show the number of rows and columns in a table and include the headers of the columns and their corresponding index numbers, so that you can compare them easily. Here’s the initial prompt that I used: Give me the JavaScript code for an Observable notebook that’ll allow me to upload a CSV file, calculate the row and column counts from it, and then display the stats with column headers and their corresponding index starting from 0. Display the info in an HTML table. Then I refactored the JavaScript it returned so that it’ll allow me to upload two CSV files and compare their stats. I made ChatGPT do it for me with this follow-up prompt: Can you change the code so that it allows uploading two CSV files and displays the stats of both of them in two HTML tables? Don’t blindly repeat the logic from the previous section twice. Finally, I asyncified the code and changed some HTML parsing to make the table look a bit better. Here’s the complete 85-line code snippet: { // create file input elements for the two files const fileInput1 = html``; const fileInput2 = html``; // create empty HTML tables for the two files const table1 = html` `; const table2 = table1.cloneNode(true); // function to handle file load event and display stats in table const handleFileLoad = (table) =\u003e async (event) =\u003e { const file = event.target.files[0]; const reader = new FileReader(); // read the file contents as text reader.readAsText(file); // create a promise to wait for the file to load and parse const fileLoaded = new Promise((resolve, reject) =\u003e { reader.onload = () =\u003e { const contents = reader.result; const lines = contents.trim().split(\"\\n\"); const headers = lines[0].split(\",\"); const numColumns = headers.length; const numRows = lines.length - 1; // create a row for the number of rows const numRowsRow = html` Number of rows: ${numRows} `; // create a row for the number of columns const numColsRow = html` Number of columns: ${numColumns} `; // create a row for the column names const headerRow = html` Column names: ${headers.map((h, i) =\u003e `${i}: ${h}`).join(\", \")} `; // add the rows to the table body const tableBody = html` ${numRowsRow}${numColsRow}${headerRow} `; table.replaceChild(tableBody, table.lastChild); // resolve the promise with the parsed data resolve({ numRows, numColumns, headers }); }; reader.onerror = () =\u003e { reject(reader.error); }; }); // wait for the promise to resolve before displaying the results in the table try { const { numRows, numColumns, headers } = await fileLoaded; console.log( `File loaded: ${file.name}, Rows: ${numRows}, Columns: ${numColumns}, Headers: ${headers}` ); } catch (err) { console.error(err); } }; // add event listeners to the file input elements fileInput1.addEventListener(\"change\", handleFileLoad(table1)); fileInput2.addEventListener(\"change\", handleFileLoad(table2)); // display the file input and table elements in the notebook return html`${fileInput1} ${table1} ${fileInput2} ${table2}`; } The snippet above starts by creating two file input elements using HTML input tags. These are used to allow the user to select and upload CSV files. Two empty HTML tables are also created to hold the extracted statistics for each CSV file. Next, it defines a function called handleFileLoad which takes a table element as its argument. This function is called when the user uploads a file, and it reads the contents of the file and extracts some basic statistics from it. These statistics are then used to populate the HTML table with the extracted information. Inside the handleFileLoad function, the FileReader API is used to read the contents of the uploaded file. The file contents are then parsed as text and split into lines. The first line contains the column headers, which are extracted by splitting the line by commas. The number of columns is then determined by the number of headers, and the number of rows is determined by counting the number of lines in the file (excluding the header). It then creates three rows for the extracted statistics: one row for the number of rows, one row for the number of columns, and one row for the column headers with their corresponding indexes starting from zero. The rows are then added to the HTML table. Finally, the code adds event listeners to the file input elements to trigger the handleFileLoad function when the user uploads a file. The file input elements and HTML tables are then returned as an HTML fragment using the HTML template literal, and displayed in the notebook. You can find the working application embedded in the following section. Try uploading two CSV files by clicking on the Choose File button and see how the app displays the stats in separate HTML tables. Here’s a gif of it in action: Click on the following thumbnail to take the notebook for a spin: ObservableHQ ↩︎ Observable notebook 2 ↩︎ ","permalink":"http://rednafi.com/javascript/exploring_observable_notebooks/","publishDate":"2023-04-10","summary":"Whenever I plan to build something, I spend 90% of my time researching and figuring out the idiosyncrasies of the tools that I decide to use for the project. LLM tools like ChatGPT has helped me immensely in that regard. I’m taking on more tangential side projects because they’re no longer as time-consuming as they used to be and provide me with an immense amount of joy and learning opportunities. While LLM interfaces like ChatGPT may hallucinate, confabulate, and confidently give you misleading information, they also allow you to avoid starting from scratch when you decide to work on something. Personally, this benefits me enough to keep language models in my tool belt and use them to churn out more exploratory work at a much faster pace.\n","tags":["JavaScript"],"title":"Building a web app to display CSV file stats with ChatGPT \u0026 Observable"},{"content":"In multi-page web applications, a common workflow is where a user: Loads a specific page or clicks on some button that triggers a long-running task. On the server side, a background worker picks up the task and starts processing it asynchronously. The page shouldn’t reload while the task is running. The backend then communicates the status of the long-running task in real-time. Once the task is finished, the client needs to display a success or an error message depending on the final status of the finished task. The de facto tool for handling situations where real-time bidirectional communication is necessary is WebSocket1. However, in the case above, you can see that the communication is mostly unidirectional where the client initiates some action in the server and then the server continuously pushes data to the client during the lifespan of the background job. In Django, I usually go for the channels2 library whenever I need to do any real-time communication over WebSockets. It’s a fantastic tool if you need real-time full duplex communication between the client and the server. But it can be quite cumbersome to set up, especially if you’re not taking full advantage of it or not working with Django. Moreover, WebSockets can be quite flaky and usually have quite a bit of overhead. So, I was looking for a simpler alternative and found out that Server-Sent Events (SSEs) work quite nicely when all I needed was to stream some data from the server to the client in a unidirectional manner. Server-Sent Events (SSEs) Server-Sent Events (SSE)3 is a way for a web server to send real-time updates to a web page without the need for the page to repeatedly ask for updates. Instead of the page asking the server for new data every few seconds, the server can just send updates as they happen, like a live stream. This is useful for things like live chat, news feeds, and stock tickers but won’t work in situations where you also need to send real-time updates from the client to the server. In the latter scenarios, WebSockets are kind of your only option. SSEs are sent over traditional HTTP. That means they don’t need any special protocol or server implementation to get working. WebSockets on the other hand, need full-duplex connections and new WebSocket servers like Daphne to handle the protocol. In addition, SSEs have a variety of features that WebSockets lack by design such as automatic reconnection, event IDs, and the ability to send arbitrary events. This is quite nice since on the browser, you won’t have to write additional logic to handle reconnections and stuff. The biggest reason why I wanted to explore SSE is because of its simplicity and the fact that it plays in the HTTP realm. If you want to learn more about how SSEs stack up against WebSockts, I recommend this post4 by Germano Gabbianelli. The wire protocol The wire protocol works on top of HTTP and is quite simple. The server needs to send the data maintaining the following structure: HTTP/1.1 200 OK date: Sun, 02 Apr 2023 20:17:53 GMT server: uvicorn content-type: text/event-stream access-control-allow-origin: * cache-control: no-cache Transfer-Encoding: chunked event: start data: streaming started id: 0 data: message 1 : this is a comment data: message 2 retry: 5000 Here, the server header needs to set the MIME type to text/event-stream and ask the client not to cache the response by setting the cache-control header to no-cache. Next, in the message payload, only the data field is required, everything else is optional. Let’s break down the message structure: event: This is an optional field that specifies the name of the event. If present, it must be preceded by the string ’event:’. If not present, the event is considered to have the default name ‘message’. id: This is an optional field that assigns an ID to the event. If present, it must be preceded by the string ‘id:’. Clients can use this ID to resume an interrupted connection and receive only events that they have not yet seen. data: This field is required and contains the actual message data that the server wants to send to the client. It must be preceded by the string ‘data:’ and can contain any string of characters. retry: This is an optional field that specifies the number of milliseconds that the client should wait before attempting to reconnect to the server in case the connection is lost. If present, it must be preceded by the string ‘retry:’. Each message must end with double newline characters (\"\\n\\n\"). Yep, this is part of the protocol. The server can send multiple messages in a single HTTP response, and each message will be treated as a separate event by the client. A simple example In this section, I’ll prop up a simple HTTP streaming server with starlette5 and collect the events from the browser. Here’s the complete server implementation: # server.py from __future__ import annotations import asyncio import logging from typing import AsyncGenerator from starlette.applications import Starlette from starlette.requests import Request from starlette.responses import Response, StreamingResponse from starlette.routing import Route logging.basicConfig(level=logging.INFO) # This is just so that you can head over to an index page # and run the client side code. async def index(request: Request) -\u003e Response: return Response(\"SSE demo\", media_type=\"text/plain\") async def stream(request: Request) -\u003e StreamingResponse: async def _stream() -\u003e AsyncGenerator[str, None]: attempt = 0 # Give up after 3 attempts. while True: # Start sending messages. yield \"event: start\\n\" # Sets the type of the next message to 'start'. yield \"data: streaming started\\n\\n\" # A 'start' event message. yield f\"id: {attempt}\\n\\n\" # Sends the id field. yield \"data: message 1\\n\\n\" # A default event message. yield \": this is a comment\\n\\n\" # Keep-alive comment. yield \"data: message 2\\n\\n\" # Another default event message. yield \"retry: 5000\\n\\n\" # Controls autoretry from the client side (ms). # Wait for a second so that we're not flooding the client with messages. await asyncio.sleep(1) attempt += 1 # Give up after 3 attempts to avoid dangling connections. if attempt == 3: # Close the connection yield \"data: closing connection\\n\\n\" break response = StreamingResponse( _stream(), headers={ \"Content-Type\": \"text/event-stream\", \"Access-Control-Allow-Origin\": \"*\", \"Cache-Control\": \"no-cache\", }, ) return response routes = [ Route(\"/\", endpoint=index), Route(\"/stream\", endpoint=stream), ] app = Starlette(debug=True, routes=routes) The server exposes a /stream endpoint that will just continuously send data to any connected client. The stream function returns a StreamingResponse object that the framework uses to send SSE messages to the client. Internally, it defines an asynchronous generator function _stream which produces a sequence of messages that follows the SSE wire protocol and yields them line by line. The index / page is there so that you can head over to it in your browser and paste the client-side code. You can run this server with uvicorn via the following command: uvicorn server:app --port 5000 --reload This will expose the server to the localhost’s port 5000. Now you can head over to your browser, go to the localhost:5000 URL and paste this following snippet to the dev console to catch the streamed data from the client side: // client.js // Connect to the event stream server. const eventSource = new EventSource(\"http://localhost:5000/stream\"); // Log something when the client connects to the server. eventSource.onconnect = (event) =\u003e console.log(\"connected to the server\"); // Log a message while closing the connection. eventSource.onclose = (event) =\u003e console.log(\"closing connection\"); // Log an error message on account of an error. eventSource.onerror = (event) =\u003e console.log(\"an error occured\"); // This is how you can attach an event listener to a custom event. eventSource.addEventListener(\"start\", (event) =\u003e { console.log(`start event: ${event.data}`); }); // Log the default message. eventSource.onmessage = (event) =\u003e { console.log(`Default event: ${event.data}`); // Don't reconnect when the server closes the connection. if (event.data === \"closing connection\") eventSource.close(); }; Notice, how the client API is quite similar to the WebSocket API but simpler. Once you’ve pasted the code snippet to the browser console, you’ll be able to see the streamed data from the server that looks like this: start event: streaming started Default event: message 1 Default event: message 2 start event: streaming started Default event: message 1 Default event: message 2 start event: streaming started Default event: message 1 Default event: message 2 Default event: closing connection A more practical example This section will demonstrate the scenario that was mentioned at the beginning of this post where loading a particular page in your browser will trigger a long-running asynchronous celery6 task in the background. While the task runs, the server will communicate the progress with the client. Once the task is finished, the server will send a specific message to the client and it’ll update the DOM to let the user know that the task has been finished. The workflow only requires unidirectional communication and SSE is a perfect candidate for this situation. To test it out, you’ll need to install a few dependencies. You can pip install them as such: pip install 'celery[redis]' jinja2 starlette uvicorn You’ll also need to set up a Redis server that Celery will use for broker communication. If you have Docker installed in your system, you can run the following command to start a Redis server: docker run --name dev-redis -d -h localhost -p 6379:6379 redis:alpine The application will live in a directory called sse with the following structure: sse ├── __init__.py ├── index.html # Client side SSE code. └── views.py # Server side SSE code. The view.py contains the server implementation that looks like this: from __future__ import annotations import json import logging import time from typing import TYPE_CHECKING, AsyncGenerator from celery import Celery from celery.result import AsyncResult from starlette.applications import Starlette from starlette.responses import StreamingResponse from starlette.routing import Route from starlette.templating import Jinja2Templates if TYPE_CHECKING: from starlette.requests import Request from starlette.responses import Response logging.basicConfig(level=logging.INFO) templates = Jinja2Templates(directory=\"./\") celery_app = Celery(\"tasks\", backend=\"redis://\", broker=\"redis://\") @celery_app.task() def background() -\u003e str: time.sleep(5) return \"Hello from background task...\" async def index(request: Request) -\u003e Response: task_id = background.apply_async(queue=\"default\") logging.info(\"Task id: %s\", task_id) response = templates.TemplateResponse(\"index.html\", {\"request\": request}) response.set_cookie(\"task_id\", task_id) return response async def task_status(request: Request) -\u003e StreamingResponse: task_id = request.path_params[\"task_id\"] async def stream() -\u003e AsyncGenerator[str, None]: task = AsyncResult(task_id, app=celery_app) logging.info(\"Task state: %s\", task.state) attempt = 0 # Give up and close the connection after 10 attempts. while True: data = { \"state\": task.state, \"result\": task.result, } logging.info(\"Server sending data: %s\", data) # Send a stringified JSON SSE message. yield f\"data: {json.dumps(data)}\\n\\n\" attempt += 1 # Close the connection when the task has successfully finished. if data.get(\"state\") == \"SUCCESS\": break # Give up after 10 attempts to avoid dangling connections. if attempt \u003e 10: data[\"state\"] = \"UNFINISHED\" data[\"result\"] = \"Task is taking too long to complete.\" yield f\"data: {json.dumps(data)}\\n\\n\" break # Sleep for a second so that we're not flooding the client with messages. time.sleep(1) response = StreamingResponse( stream(), headers={ \"Content-Type\": \"text/event-stream\", \"Access-Control-Allow-Origin\": \"*\", \"Cache-Control\": \"no-cache\", }, ) return response routes = [ Route(\"/index\", endpoint=index), Route(\"/task_status/{task_id}\", endpoint=task_status), ] # Add session middleware app = Starlette(debug=True, routes=routes) Here, first, we’re setting up celery and connecting it to the local Redis instance. Next up, the background function simulates some async work where it just waits for a while and returns a message. The index view calls the asynchronous background task and sets the id of the task as a session cookie with response.set_cookie(\"task_id\", task_id). The frontend JavaScript will look for this task_id cookie to identify a running background task. Then we expose a task_status endpoint that takes in the value of a task_id and streams the status of the running task to the frontend as SSE messages. To avoid dangling connections, we stream the task status for 10 seconds before giving up. Now on the client side, the index.html looks like this: \u003c!DOCTYPE html\u003e SSE Demo Message Waiting for server-sent message...\nWhen the index page is loaded, the server starts a background task and sets the task_id= session cookie. The HTML above then defines a paragraph element to show the message streamed from the server: SSE Demo Message Waiting for server-sent message...\nThe JavaScript code defines a function named waitForResult() that listens for updates on the status of a long-running task that is being executed on the server. The function first waits for the task_id to be set in a cookie by calling waitForTaskIdCookie(). Once the task_id is obtained, the function creates a new EventSource object that connects to the streaming endpoint on the server using the ID to get updates on the status of the task. The EventSource object is set up with four event listeners: onmessage, onerror, onopen, and onclose. The onmessage listener is triggered when the server sends an update on the task status. The listener first logs the updated task status and then checks if the state of the task is SUCCESS or UNFINISHED. In either case, the client fetches the message element on the DOM and updates it with the result of the background task streamed by the server. The client-side SSE API will automatically keep reconnecting if the connection fails for some reason. This is handy since you don’t have to write any additional logic to make the connection more robust. However, you do need to be mindful about closing the connection from the client side once you’ve received the final task status. The onmessage event listener explicitly closes the connection with eventSource.close() once the final message about a specific task has reached the client from the server. The onerror listener handles errors that occur with the connection. The onopen callback is called when the connection is successfully opened, and onclose gets called when the connection is closed. The waitForTaskIdCookie() function that is called by the entrypoint waits for the task_id to be set in a cookie by repeatedly calling getCookie() until the ID is obtained. The function waits for 300ms between each iteration so that it doesn’t overwhelm the client. The getCookie() function is a utility function that returns the value of a cookie given its name. Finally, the code sets the window.onload event listener to call the waitForResult() function when the page has finished loading. Now, go to the sse directory and start the server with the following command: uvicorn views:app --port 5000 --reload On another terminal, start the celery workers: celery -A views.celery_app worker -l info -Q default -c 1 Finally, head over to your browser and go to http://localhost:5000/index page and see that the server has triggered a background job. Once the job finishes after 5 seconds, the client shows a message: Notice, how the server pushes the result of the task automatically once it finishes. Limitations While SSE-driven pages are much easier to bootstrap than their WebSocket counterparts—apart from only supporting unidirectional communication, they suffer from a few other limitations: SSE is limited to sending text data only. If an application needs to send binary data, it must encode the data as text before sending it over SSE. SSE connections are subject to the same connection limitations as HTTP connections. In some cases, a large number of SSE connections can overload the server, leading to performance issues. However, this can be mitigated by taking advantage of connection multiplexing in HTTP/2. WebSocket ↩︎ channels ↩︎ SSE ↩︎ SSE vs WebSockets ↩︎ starlette ↩︎ celery ↩︎ Using server-sent events 7 ↩︎ ","permalink":"http://rednafi.com/python/server_sent_events/","publishDate":"2023-04-08","summary":"In multi-page web applications, a common workflow is where a user:\nLoads a specific page or clicks on some button that triggers a long-running task. On the server side, a background worker picks up the task and starts processing it asynchronously. The page shouldn’t reload while the task is running. The backend then communicates the status of the long-running task in real-time. Once the task is finished, the client needs to display a success or an error message depending on the final status of the finished task. The de facto tool for handling situations where real-time bidirectional communication is necessary is WebSocket1. However, in the case above, you can see that the communication is mostly unidirectional where the client initiates some action in the server and then the server continuously pushes data to the client during the lifespan of the background job.\n","tags":["Python","Networking"],"title":"Pushing real-time updates to clients with Server-Sent Events (SSEs)"},{"content":"I’ve always had a vague idea about what Unix domain sockets are from my experience working with Docker for the past couple of years. However, lately, I’m spending more time in embedded edge environments and had to explore Unix domain sockets in a bit more detail. This is a rough documentation of what I’ve explored to gain some insights. The dry definition Unix domain sockets (UDS) are similar to TCP sockets in a way that they allow two processes to communicate with each other, but there are some core differences. While TCP sockets are used for communication over a network, Unix domain sockets are used for communication between processes running on the same computer. A Unix domain socket is a way for programs to exchange data in a fast and efficient way without having to worry about the overhead of network protocols like TCP/IP or UDP. It works by creating a special file on the file system called a socket, which acts as a bidirectional data channel between the processes. The processes can send and receive data through the socket just like they would with a network socket. Also, just like TCP/UDP sockets, Unix domain sockets can also be either stream-based (TCP equivalent) or datagram-based (UDP equivalent). Unix domain sockets are commonly used in server-client applications, such as web servers, databases, and email servers, where they provide a secure and efficient way for processes on the same machine to communicate with each other. They’re also used in many other types of programs where different parts of the program need to work together or share data. Another cool thing about them is that you can control access to your server just by tuning the permission of the socket file on the system. Prerequisites I’m running these experiments on an M-series Macbook pro. However, any Unix-y environment will work as long as you can run the following tools: socat: To create the socket servers and clients. curl: To make HTTP requests to a supported socket server. jq: To pretty print JSON payloads. lsof: To display currently listening socket server processes. Inspecting Unix domain sockets in your system Most likely, there are currently multiple processes listening on different sockets in your system. You can explore them using lsof with the following command: sudo lsof -U This will return a list of all Unix domain socket files and the server process PIDs that are currently listening on them: COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME launchd 1 root 3u unix 0x25269ff9edd05165 0t0 /private//var/run/syslog launchd 1 root 4u unix 0x25269ff9edd0522d 0t0 -\u003e0x25269ff9edd05165 launchd 1 root 6u unix 0x25269ff9edd052f5 0t0 /private/var/run/cupsd launchd 1 root 7u unix 0x25269ff9edd053bd 0t0 /var/rpc/ncalrpc/NETLOGON launchd 1 root 8u unix 0x25269ff9edd05485 0t0 /var/run/vpncontrol.sock launchd 1 root 9u unix 0x25269ff9edd0554d 0t0 /var/run/portmap.socket ... You can also filter out the socket files by their process names. Docker processes listen on a few socket files: sudo lsof -U -a -c 'com.docker' This will return: COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME com.docke 15451 rednafi 10u unix 0x25269ff9edcfd55d 0t0 vpnkit-bridge-fd.sock com.docke 15451 rednafi 11u unix 0x25269ff9edcfd625 0t0 vpnkit-bridge.sock com.docke 15451 rednafi 12u unix 0x25269ff9edcfd6ed 0t0 vpnkit.port.sock com.docke 15451 rednafi 13u unix 0x25269ff9edcfd7b5 0t0 vpnkit.data.sock com.docke 15451 rednafi 14u unix 0x25269ff9edcfd87d 0t0 httpproxy.sock com.docke 15451 rednafi 15u unix 0x25269ff9edcfd3cd 0t0 backend.sock ... Creating a Unix domain socket Running the following command on your terminal will create a stream-based Unix domain socket: socat unix-listen:/tmp/stream.sock,fork STDOUT This process listens on the /tmp/stream.sock and prints the incoming data to the stdout. The fork portion on the command ensures that multiple clients can be connected to the server process and they’ll be served by forking child processes. From another console, you can try to send data to the socket file as a client: echo \"hello world\" | socat - unix-connect:/tmp/stream.sock Running this command will send the hello world string to the /tmp/stream.sock file and the server process will print it on the standard output stream. Similarly, you can also create a datagram-based socket server with socat like this: socat unix-recvfrom:/tmp/datagram.sock,fork STDOUT Now send data to the server with this: echo \"hello world\" | socat - unix-sendto:/tmp/datagram.sock Connecting to Docker engine via a Unix domain socket By default, Docker runs through a non-networked UNIX socket. It can also optionally communicate using SSH or a TLS (HTTPS) socket. On MacOS, the socket file can be found in ~/.docker/run/docker.sock. We can make HTTP requests against the listening socket server and use Docker engine’s RESTful API suite. Checking the engine’s version number: The following command uses curl to spawn a client process and send a request against the Docker engine running in my local system. curl --unix-socket ~/.docker/run/docker.sock http://localhost/version | jq This returns (truncated output for readability): { \"Platform\":{ \"Name\":\"Docker Desktop 4.17.0 (99724)\" }, \"Components\":[ \"...\" ], \"Version\":\"20.10.23\", \"ApiVersion\":\"1.41\", \"MinAPIVersion\":\"1.12\", \"GitCommit\":\"6051f14\", \"GoVersion\":\"go1.18.10\", \"Os\":\"linux\", \"Arch\":\"arm64\", \"KernelVersion\":\"5.15.49-linuxkit\", \"BuildTime\":\"2023-01-19T17:31:28.000000000+00:00\" } Listing the containers: This command lists all the running containers on my machine. curl --unix-socket \\ ~/.docker/run/docker.sock http://localhost/containers/json | jq Listing the images: curl --unix-socket \\ ~/.docker/run/docker.sock http://localhost/images/json | jq Downloading a container: This allows you to programmatically download the hello-world image from Dockerhub: curl -sX POST \\ -H 'Content-Type: application/json' \\ --unix-socket ~/.docker/run/docker.sock \\ 'http://localhost/images/create?fromImage=hello-world:latest' Listening for docker events: This API call lets you listen for all incoming events from the docker engine. You can run the following command on one terminal and send events from another: curl --no-buffer --unix-socket \\ ~/.docker/run/docker.sock http://localhost/events | jq Here, the --no-buffer flag is necessary for instructing curl to send the output events to the input stream of jq without doing any buffering. This allows jq to pretty-print the outputs in real-time. Now from another console if you run the following command, you’ll see events pouring into the console that’s listening for them: docker run hello-world The complete list of APIs can be found here1. Writing a Unix domain socket server in Python You can quickly write a simple server that allows clients to connect to it via Unix domain sockets. If the clients exist on the same machine then, a UDS server has the advantage of having lower overhead than its networked TCP counterpart. The following server uses Python’s socketserver module to create a stream-based echo server: # server.py from __future__ import annotations import logging import socketserver from pathlib import Path logging.basicConfig(level=logging.INFO) class RequestHandler(socketserver.BaseRequestHandler): def setup(self) -\u003e None: logging.info(\"Start request.\") def handle(self) -\u003e None: conn = self.request while True: data = conn.recv(1024) if not data: break logging.info(f\"recv: {data!r}\") conn.sendall(data) def finish(self) -\u003e None: logging.info(\"Finish request.\") class Server(socketserver.ThreadingUnixStreamServer): def server_activate(self) -\u003e None: logging.info(\"Server started on %s\", self.server_address) super().server_activate() if __name__ == \"__main__\": # Remove the socket file if it already exists. # UDS doesn't let you reuse the socket file. socket_path = Path(\"/tmp/stream.sock\") if socket_path.exists(): socket_path.unlink() with Server(str(socket_path), RequestHandler) as server: server.serve_forever() Here, socketserver.ThreadingUnixStreamServer enables us to create a server that allows multiple clients to be connected to it via Unix domain sockets. The server spins up a new thread to serve each new client and does bi-directional communication via UDS. The client code is quite similar to a TCP client: # client.py import socket import time import logging logging.basicConfig(level=logging.INFO) ADDRESS = \"/tmp/stream.sock\" with socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) as s: s.connect(ADDRESS) while True: time.sleep(1) s.sendall(b\"hello world\") data = s.recv(1024) logging.info(f\"Received {data!r}\") The client connects to the server through the /tmp/stream.sock socket and sends a static hello world string to it. The server then sends that data back and the client sends it to the stdout stream. Running the server and client as two separate processes will yield the following output: Exposing an HTTP application via a Unix domain socket Webservers usually allow you to expose HTTP applications via Unix domain sockets. In Python, the uvicorn2 ASGI server lets you do this quite easily. This can come as handy whenever you need to spin up a local server and all the clients are running on the same machine or you’re running your server behind a proxy. Here’s an example of a simple webserver built with starlette3 and served with uvicorn. # server.py (http server) from __future__ import annotations from starlette.applications import Starlette from starlette.requests import Request from starlette.responses import HTMLResponse from starlette.routing import Route async def index(request: Request) -\u003e HTMLResponse: return HTMLResponse( \"\"\"Hello, world! This is the index page.\n\"\"\" ) app = Starlette( debug=True, routes=[ Route(\"/index\", index), ], ) You can expose this server through a UDS like this: uvicorn --uds /tmp/stream.sock server:app Calling this API with curl from another console will return the HTML content in the response: curl --unix-socket /tmp/stream.sock http://localhost/index Hello, world! This is the index page.\nIf you want to access this server from a browser, you’ll need to make sure that your reverse proxy server (Nginx / Apache / Caddy) is configured to relay the incoming request from the network to the UDS server. For a quick and dirty approach, you can use socat to proxy the request from a HOST:PORT pair to the UDS server like this: uvicorn --uds /tmp/stream.sock server:app \\ \u0026 socat tcp-listen:9999,fork unix-connect:/tmp/stream.sock \u0026 The uvicorn command spins up a webserver in the background as before and listens on the socket file /tmp/stream.sock. Then we’re using socat to create a forking TCP server that handles the incoming HTTP requests from the network and relays them to the webserver via UDS. It also relays the server’s responses back to the client—doing the work of a reverse proxy. You can then head over to your browser and go to http://localhost:9999. This will display the HTML page: Docker engine API ↩︎ Uvicorn ↩︎ Starlette ↩︎ Understanding sockets 4 ↩︎ Fun with Unix domain sockets 5 ↩︎ ","permalink":"http://rednafi.com/misc/tinkering_with_unix_domain_socket/","publishDate":"2023-03-11","summary":"I’ve always had a vague idea about what Unix domain sockets are from my experience working with Docker for the past couple of years. However, lately, I’m spending more time in embedded edge environments and had to explore Unix domain sockets in a bit more detail. This is a rough documentation of what I’ve explored to gain some insights.\nThe dry definition Unix domain sockets (UDS) are similar to TCP sockets in a way that they allow two processes to communicate with each other, but there are some core differences. While TCP sockets are used for communication over a network, Unix domain sockets are used for communication between processes running on the same computer.\n","tags":["Python","Shell","Networking"],"title":"Tinkering with Unix domain sockets"},{"content":"While working on a multithreaded socket server in an embedded environment, I realized that the default behavior of Python’s socketserver.ThreadingTCPServer requires some extra work if you want to shut down the server gracefully in the presence of an interruption signal. The intended behavior here is that whenever any of SIGHUP, SIGINT, SIGTERM, or SIGQUIT signals are sent to the server, it should: Acknowledge the signal and log a message to the output console of the server. Notify all the connected clients that the server is going offline. Give the clients enough time (specified by a timeout parameter) to close the requests. Close all the client requests and then shut down the server after the timeout exceeds. Here’s a quick implementation of a multithreaded echo server and see what happens when you send SIGINT to shut down the server: # server.py from __future__ import annotations import logging import socketserver logging.basicConfig(level=logging.INFO) class RequestHandler(socketserver.BaseRequestHandler): \"\"\"Handler that handles an incoming client request.\"\"\" def handle(self) -\u003e None: conn = self.request while True: data = conn.recv(1024) if not data: break logging.info(f\"recv: {data!r}\") conn.sendall(data) if __name__ == \"__main__\": with socketserver.ThreadingTCPServer( (\"localhost\", 9999), RequestHandler ) as server: server.serve_forever() Here’s the client code: # client.py import logging import socket import time logging.basicConfig(level=logging.INFO) HOST = \"localhost\" # The server's hostname or IP address. PORT = 9999 # The port used by the server. with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.connect((HOST, PORT)) while True: time.sleep(1) s.sendall(b\"hello world\") data = s.recv(1024) if not data: break logging.info(f\"Received {data!r}\") Here, the server logs and echoes back whatever the client sends and the client just sends the string hello world continuously in a while loop. This is pretty much the canonical multithreaded server-client example that’s found in the socketserver docs. In the client code, the only thing that’s a little different is that within the while loop, a time.sleep(1) function was added to simulate the client performing some processing tasks. Also, without the sleep, the server would’ve flooded the stdout with the client message logs and made the demonstration difficult. Let’s run the server and the client in two separate processes and then send a SIGINT signal to the server by clicking Ctrl + C on the server console: At first, the server just ignores the signal, and clicking Ctrl + C multiple times crashes the server down with this nasty traceback (full traceback trimmed for brevity): Traceback (most recent call last): File \"/Users/rednafi/Canvas/personal/reflections/server.py\", line 137, in server.serve_forever() File \"/Users/rednafi/.asdf/installs/Python/3.11.1/lib/python3.11/socketserver.py\", line 233, in serve_forever ready = selector.select(poll_interval) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/Users/rednafi/.asdf/installs/Python/3.11.1/lib/python3.11/selectors.py\", line 415, in select fd_event_list = self._selector.poll(timeout) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ KeyboardInterrupt ... Multithreaded socket server with graceful shutdown What we want here is that whenever the server gets SIGHUP, SIGINT, SIGTERM, or SIGQUIT, it should notify the clients and gracefully shut itself down. I played around with the socketserver.ThreadingTCPServer API for a while to come up with a solution that worked nicely for my use case. Here’s the full server implementation: # server.py from __future__ import annotations import logging import os import signal import socket import socketserver import threading import time from types import FrameType from typing import Callable logging.basicConfig(level=logging.INFO) class RequestHandler(socketserver.BaseRequestHandler): server: SocketServer def notify_clients_when_server_is_interrupted(self) -\u003e None: logging.info(\"Server interrupted, notifying all clients...\") self.request.sendall(b\"SHUTDOWN\") self.request.sendall(b\"\") def setup(self) -\u003e None: # Prevent new connections from being accepted when the server is # shutting down. if self.server._is_interrupted: self.notify_clients_when_server_is_interrupted() def handle(self) -\u003e None: conn = self.request while True: data = conn.recv(1024) if self.server._is_interrupted: self.notify_clients_when_server_is_interrupted() break if not data: break logging.info(f\"recv: {data!r}\") conn.sendall(data) class SocketServer(socketserver.ThreadingTCPServer): reuse_address = True daemon_threads = True block_on_close = False _is_interrupted = False def server_activate(self) -\u003e None: logging.info( \"PID:%s. Server started on %s:%s\", os.getpid(), *self.server_address, ) super().server_activate() def get_request(self) -\u003e tuple[socket.socket, str]: conn, addr = super().get_request() logging.info(\"Starting connection from %s:%s\", *addr) return conn, addr def shutdown_request( self, request: socket.socket | tuple[bytes, socket.socket] ) -\u003e None: if isinstance(request, socket.socket): logging.info( \"Closing connection from %s:%s\", *request.getpeername(), ) super().shutdown_request(request) def shutdown(self) -\u003e None: logging.info(\"Server is shutting down...\") super().shutdown() def handle_signal( self, timeout: int ) -\u003e Callable[[int, FrameType | None], None]: \"\"\"A simple signal handler factory that takes in some additional parameters and passes them to the actual signal handler. Defines and returns the final handler. \"\"\" def handler(signum: int, _: FrameType | None) -\u003e None: deadline = time.monotonic() + timeout signame = signal.Signals(signum).name self._is_interrupted = True while (current_time := time.monotonic()) \u003c deadline: delta = int(deadline - current_time) + 1 logging.info( \"%s received, closing server in %s seconds...\" % (signame, delta) ) time.sleep(1) self.server_close() self.shutdown() return handler if __name__ == \"__main__\": with SocketServer((\"localhost\", 9999), RequestHandler) as server: for sig in ( signal.SIGHUP, signal.SIGINT, signal.SIGTERM, signal.SIGQUIT, ): signal.signal(sig, server.handle_signal(timeout=5)) t = threading.Thread(target=server.serve_forever) t.start() t.join() Apart from a few extra methods that perform logging and signal handling, the overall structure of this server is similar to the vanilla multithreaded server from the previous section. In the RequestHandler, we have defined a custom notify_clients_when_server_is_interrupted method that notifies all clients whenever the server receives an interruption signal. This is a custom method that’s not defined in the BaseRequestHandler class. The notify method logs the status of the interruption signal and then sends a SHUTDOWN message to the clients. Afterward, it closes the client connection. The setup method extends the eponymous method from the BaseRequestHandler class and calls the notify_clients_when_server_is_interrupted method. This ensures that whenever the server is shutting down, it refuses any new client connections. Within the handle method, in the data processing while loop, we check the value of the _is_interrupted flag on the server instance. If the value is True, we call the notify method. The value of this flag is managed by the SocketServer class. Calling the notify method from within the data processing loop will notify all currently connected clients. Next, we define a new server class called SocketServer that inherits from the socketserver.ThreadingTCPServer class. The reuse_address, daemon_threads, and block_on_close class variables override the default values inherited from the base ThreadingTCPServer class. Here are the explanations for each: reuse_address: This variable determines whether the server can reuse a socket that’s still in the TIME_WAIT1 state after a previous connection has been closed. If this variable is set to True, the server can reuse the socket. Otherwise, the socket will be unavailable for a short period of time after it’s closed. daemon_threads: This variable determines whether the server’s worker threads should be daemon threads. Daemon threads are threads that run in the background and don’t prevent the Python interpreter from exiting when they are still running. If this variable is set to True, the server’s worker threads will be daemon threads. I found that daemon threads work better when I need to shut down the server that’s connected to multiple long-running clients. block_on_close: This variable determines whether the server should block until all client connections have been closed before shutting down. If this variable is set to True, the server will block until all client connections have been closed. Otherwise, the server will shut down immediately, even if there are still active client connections. We want to set it to False since we’ll handle the graceful shutdown in a custom signal handler method on the server class. Going forward, the SocketServer class overrides the server_activate, get_request, shutdown_request, and shutdown methods from the base class. All of them just log a few key pieces of information to the console and calls the methods from the parent class verbatim. The interesting part happens in the custom handle_signal method. When an interruption signal is sent to the server, the handle_signal method is activated. The method takes an integer parameter timeout which specifies how many seconds the server should wait before shutting down after receiving the signal. The method then returns the actual signal handler function that takes two parameters: an integer signum representing the signal number and a FrameType object which represents the current stack frame. The function is responsible for handling the signal by making the server wait for timeout seconds before shutting it down gracefully. First, the method sets a variable _is_interrupted to True to indicate that the server has received an interruption signal. Then, the method enters a while loop that continues until the current time exceeds the deadline time, which is calculated by adding the timeout to the current monotonic time. During each iteration of the while loop, the method logs a message to the console to indicate that the signal has been received and the server will be closed in a certain number of seconds. The delta variable is calculated as the difference between the deadline and the current monotonic time, plus 1. This ensures that the logging message displays an accurate countdown of the remaining time until the server shuts down. Once the deadline exceeds and the while loop completes, the method calls server_close() and shutdown() methods of the server to close the requests and shut itself down gracefully. The server_close() method closes the listening socket and stops accepting new client connections, while the shutdown() method stops all active client connections and waits for them to finish processing their current requests. However, in this case, since we are giving the clients enough time to close the connections and using daemon threads to process the requests, calling shutdown() will immediately close all the client requests and bring down the server. Finally, in the __main__ section, we instantiate the SocketServer class and register the RequestHandler. Then we register the signal handler with a timeout of 5 seconds. This means, upon receiving the interruption signal, the server will wait 5 seconds before shutting itself down. Notice, how we’re running the server.serve_forever method in a new thread. That’s because our custom signal handler explicitly calls the shutdown of the server instance and the shutdown method can only be called when the serve_forever loop is running in a different thread. From the shutdown2 documentation: Tell the serve_forever() loop to stop and wait until it does. shutdown() must be called while serve_forever() is running in a different thread otherwise it will deadlock. Now that the server is coded to shut down gracefully, we also expect the client to behave properly. That means, whenever the client receives the SHUTDOWN message, it should immediately close the connection. Here’s a slightly modified version of the vanilla socket client code that we’ve seen before: # client.py import logging import socket import time logging.basicConfig(level=logging.INFO) HOST = \"localhost\" # The server's hostname or IP address. PORT = 9999 # The port used by the server. with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.connect((HOST, PORT)) while True: time.sleep(1) s.sendall(b\"hello world\") data = s.recv(1024) if data == b\"SHUTDOWN\": logging.info(\"Closing connection...\") break if not data: break logging.info(f\"Received {data!r}\") The only difference between this and the previous client is that this client will break out of the process loop when it encounters the SHUTDOWN message from the server. Now to see the whole thing in action, you can fire up the server and the client from two different terminals. Once both the server and client are running, try sending a SIGINT or any of the three other handled signals. You see that the server acknowledges the interruption signal, gives the clients enough time to disconnect, then shut itself down in a graceful manner: TIME_WAIT ↩︎ shutdown ↩︎ socketserver 3 ↩︎ ","permalink":"http://rednafi.com/python/multithreaded_socket_server_signal_handling/","publishDate":"2023-02-26","summary":"While working on a multithreaded socket server in an embedded environment, I realized that the default behavior of Python’s socketserver.ThreadingTCPServer requires some extra work if you want to shut down the server gracefully in the presence of an interruption signal. The intended behavior here is that whenever any of SIGHUP, SIGINT, SIGTERM, or SIGQUIT signals are sent to the server, it should:\nAcknowledge the signal and log a message to the output console of the server. Notify all the connected clients that the server is going offline. Give the clients enough time (specified by a timeout parameter) to close the requests. Close all the client requests and then shut down the server after the timeout exceeds. Here’s a quick implementation of a multithreaded echo server and see what happens when you send SIGINT to shut down the server:\n","tags":["Python","Networking"],"title":"Signal handling in a multithreaded socket server"},{"content":"I was working on a project where I needed to poll multiple data sources and consume the incoming data points in a single thread. In this particular case, the two data streams were coming from two different Redis lists. The correct way to consume them would be to write two separate consumers and spin them up as different processes. However, in this scenario, I needed a simple way to poll and consume data from one data source, wait for a bit, then poll and consume from another data source, and keep doing this indefinitely. That way I could get away with doing the whole workflow in a single thread without the overhead of managing multiple processes. Here’s what I’m trying to do: # pseudocode.py def stream_a(): while True: # Poll the first redis list. def stream_b(): while True: # Poll the second redis list. def consume(): # How do I alternate between two infinite streams and consume them? while True: stream_a() # Somehow break out of the infinite while loop. stream_b() # Somehow run this infinite loop after one iteration of # the first one. One way is to poll the data sources in two generator functions and yield the result. Then in the consumer, we’ll have to alternate between the generators to fetch the next result like this: # pseudocode.py import redis def stream_a(): while True: # Fetch result from the first redis list. yield redis.rpop(\"stream_a\") def stream_b(): while True: # Fetch result from the second redis list. yield redis.rpop(\"stream_b\") def consume(): streams = (stream_a(), stream_b()) while True: # Iterate through the stream generators. for stream in streams: # Wait for a second before polling each data source. time.sleep(1) # Get the result. If the result is None then go back to the # beginning of the loop if (result := next(stream, None)) is None: continue print(f\"From {stream.__name__}:\", result) Let’s make a concrete example out of the pesudocode: # src.py from __future__ import annotations import time from itertools import count from typing import Generator def stream_even() -\u003e Generator[int, None, None]: yield from count(start=0, step=2) def stream_odd() -\u003e Generator[int, None, None]: yield from count(start=1, step=2) def consume() -\u003e None: streams = (stream_even(), stream_odd()) while True: for stream in streams: time.sleep(1) if (result := next(stream, None)) is None: continue print(f\"From {stream.__name__}:\", result) if __name__ == \"__main__\": consume() The code above defines two generator functions, stream_even() and stream_odd(), that use the count() function from the itertools module to generate an infinite sequence of even and odd integers respectively. The consume() function creates a tuple containing the two generator objects, and enters an infinite loop. On each iteration of the loop, it iterates over the tuple using a for loop; effectively alternating between the two streams. In each iteration, it waits for 1 second using the time.sleep() function and then uses the next() function to retrieve the next item from the current stream. If the result is not None, it prints a message to the console indicating which stream it came from and what the value was. Else, it loops back to the beginning of the iteration. Running the snippet will print the folling output to the console: $ python src.py From stream_even: 0 From stream_odd: 1 From stream_even: 2 From stream_odd: 3 From stream_even: 4 From stream_odd: 5 From stream_even: 6 From stream_odd: 7 From stream_even: 8 From stream_odd: 9 From stream_even: 10 ^CTraceback (most recent call last): File \"/Users/rednafi/Canvas/personal/reflections/src.py\", line 29, in consume() File \"/Users/rednafi/Canvas/personal/reflections/src.py\", line 22, in consume time.sleep(1) KeyboardInterrupt The consumer infinite loop can be written in a more concise manner with itertools.cycle. Instead of using the while loop, we can use this function to indefinitely cycle between the elements of an iterable. # src.py ... from itertools import cycle def consume() -\u003e None: streams = (stream_even(), stream_odd()) for stream in cycle(streams): # Use itertools.cycle instead of while ... time.sleep(1) if (result := next(stream, None)) is None: break print(f\"From {stream.__name__}:\", result) ... Here, the finalized executable script: # src.py from __future__ import annotations import time from itertools import count, cycle from typing import Generator def stream_even() -\u003e Generator[int, None, None]: yield from count(start=0, step=2) def stream_odd() -\u003e Generator[int, None, None]: yield from count(start=1, step=2) def consume() -\u003e None: streams = (stream_even(), stream_odd()) for stream in cycle(streams): time.sleep(1) if (result := next(stream, None)) is None: continue print(f\"From {stream.__name__}:\", result) if __name__ == \"__main__\": consume() $ python src.py From stream_even: 0 From stream_odd: 1 From stream_even: 2 From stream_odd: 3 From stream_even: 4 From stream_odd: 5 From stream_even: 6 ^CTraceback (most recent call last): File \"/Users/rednafi/Canvas/personal/reflections/src.py\", line 28, in consume() File \"/Users/rednafi/Canvas/personal/reflections/src.py\", line 21, in consume time.sleep(1) KeyboardInterrupt itertools-cycle 1 ↩︎ ","permalink":"http://rednafi.com/python/switch_between_multiple_datastreams/","publishDate":"2023-02-19","summary":"I was working on a project where I needed to poll multiple data sources and consume the incoming data points in a single thread. In this particular case, the two data streams were coming from two different Redis lists. The correct way to consume them would be to write two separate consumers and spin them up as different processes.\nHowever, in this scenario, I needed a simple way to poll and consume data from one data source, wait for a bit, then poll and consume from another data source, and keep doing this indefinitely. That way I could get away with doing the whole workflow in a single thread without the overhead of managing multiple processes.\n","tags":["Python","Database","TIL"],"title":"Switching between multiple data streams in a single thread"},{"content":"Consider this iterable: it = (1, 2, 3, 0, 4, 5, 6, 7) Let’s say you want to build another iterable that includes only the numbers that appear starting from the element 0. Usually, I’d do this: # This returns (0, 4, 5, 6, 7). from_zero = tuple(elem for idx, elem in enumerate(it) if idx \u003e= it.index(0)) While this is quite terse and does the job, it won’t work with a generator. There’s an even more generic and terser way to do the same thing with itertools.dropwhile function. Here’s how to do it: from itertools import dropwhile # This returns the same thing as before (0, 4, 5, 6, 7). from_zero = tuple(dropwhile(lambda x: x != 0, it)) Here, itertools.dropwhile is a generator function that returns elements from an iterable starting from the first element for which the predicate returns False. The predicate is a function that takes one argument and returns a boolean value. The dropwhile function takes two arguments: A function (the predicate), which takes one argument and returns a boolean value. An iterable, which can be any object that can be iterated over, such as a list, tuple, string, or even another generator. The dropwhile function starts iterating over the elements of the iterable, and drops the elements for which the predicate returns True. It then returns all the remaining elements of the iterable, regardless of whether they satisfy the condition or not. Apart from being concise, this implementation is more generic and can be used for other purposes like skipping the header lines in a file. For example: from itertools import dropwhile with open(\"/etc/passwd\") as f: for line in dropwhile(lambda x: x.startswith(\"#\"), f): print(line) This will print all the lines from the /etc/passwd file after the header comments: nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false root:*:0:0:System Administrator:/var/root:/bin/sh daemon:*:1:1:System Services:/var/root:/usr/bin/false ... Finally, let’s see how you can skip straight to the data rows in a CSV file that contains arbitrary comments and headers like this: # persons.csv This is a comment These are some other comments The fake header starts from the next line id,name,age,height The real header starts from here ID,Name,Age,Height 1,John,20,1.8 2,Jane,21,1.7 3,Jack,22,1.6 import csv from itertools import dropwhile with open(\"persons.csv\", \"r\") as f: reader = csv.DictReader(f, fieldnames=(\"ID\", \"Name\", \"Age\", \"Height\")) # Rows without comments. rows = dropwhile(lambda x: x[\"ID\"] != \"ID\", reader) # Skip the header. next(rows) for row in rows: print(row) Running this will give you the dicts containing the data rows only: {'ID': '1', 'Name': 'John', 'Age': '20', 'Height': '1.8'} {'ID': '2', 'Name': 'Jane', 'Age': '21', 'Height': '1.7'} {'ID': '3', 'Name': 'Jack', 'Age': '22', 'Height': '1.6'} Python Cookbook - David Beazley, Ch 4: Iterators and Generators 1 ↩︎ itertools.dropwhile 2 ↩︎ ","permalink":"http://rednafi.com/python/skip_first_part_of_an_iterable/","publishDate":"2023-02-12","summary":"Consider this iterable:\nit = (1, 2, 3, 0, 4, 5, 6, 7) Let’s say you want to build another iterable that includes only the numbers that appear starting from the element 0. Usually, I’d do this:\n# This returns (0, 4, 5, 6, 7). from_zero = tuple(elem for idx, elem in enumerate(it) if idx \u003e= it.index(0)) While this is quite terse and does the job, it won’t work with a generator. There’s an even more generic and terser way to do the same thing with itertools.dropwhile function. Here’s how to do it:\n","tags":["Python","TIL"],"title":"Skipping the first part of an iterable in Python"},{"content":"I needed to write a socket server in Python that would allow me to intermittently pause the server loop for a while, run something else, then get back to the previous request-handling phase; repeating this iteration until the heat death of the universe. Initially, I opted for the low-level socket module to write something quick and dirty. However, the implementation got hairy pretty quickly. While the socket module gives you plenty of control over how you can tune the server’s behavior, writing a server with robust signal and error handling can be quite a bit of boilerplate work. Thankfully, I found out that Python is already shipped with a higher level library named socketserver1 that uses the socket module underneath but gives you more tractable hooks to latch onto and build fairly robust servers where the low-level details are handled for you. Not only that, socketserver makes it easy to write a sever that can concurrently handle multiple clients either by spinning child threads or forking child processes. While all this sounds good and dandy, my primary objective was to be able to write a server that can pause serving the clients every now and then, do some work and then come back to the previous work. Here’s how I did it with a multi-threaded socket server: from __future__ import annotations import logging import socket import socketserver import time logging.basicConfig(level=logging.INFO) class RequestHandler(socketserver.BaseRequestHandler): def setup(self) -\u003e None: logging.info(\"Start request.\") def handle(self) -\u003e None: conn = self.request while True: data = conn.recv(1024) if not data: break logging.info(f\"recv: {data!r}\") conn.sendall(data) def finish(self) -\u003e None: logging.info(\"Finish request.\") class ThreadingTCPServer(socketserver.ThreadingTCPServer): _timeout = 5 # seconds _start_time = time.monotonic() def server_activate(self) -\u003e None: logging.info(\"Server started on %s:%s\", *self.server_address) super().server_activate() def get_request(self) -\u003e tuple[socket.socket, str]: conn, addr = super().get_request() logging.info(\"Connection from %s:%s\", *addr) return conn, addr def service_actions(self) -\u003e None: if time.monotonic() - self._start_time \u003e self._timeout: logging.info(\"Server paused, something else is running...\") self._start_time = time.monotonic() if __name__ == \"__main__\": with ThreadingTCPServer((\"localhost\", 9999), RequestHandler) as server: server.serve_forever() This is a simple echo server that receives client connections and reflects back the data sent by the clients. The server can handle multiple client connections simultaneously using the ThreadingTCPServer class. This class is derived from the socketserver.ThreadingTCPServer class and is responsible for implementing the server’s main loop, which listens for incoming client connections and creates a separate thread for each one to handle the incoming request. The RequestHandler class is used to handle each incoming request. This class is derived from the socketserver.BaseRequestHandler class and is responsible for handling the connection between a client and the server. It implements the setup, handle, and finish methods to perform any necessary initialization work, handle the incoming data, and clean up after the request has been processed. In the setup and finish methods, we’re only printing some message to indicate that these methods are called before and after the handle method respectively. In the handle method, we’re collecting the data sent by the clients and echoing them back. Here, inside the while loop, conn.recv is a blocking method and will keep reading from the clients indefinitely. We need the server to break out from this, do something else, and then get back to it gracefully. In the __main__ section of the code snippet, a ThreadingTCPServer object is created and the server is started using the serve_forever method. This method will continuously run the server loop, listen for incoming connections and create a separate thread for each one to handle the request. The ThreadingTCPServer class implements server_activate and get_request methods. These two methods are already implemented in the base and we’re just calling the methods from there with some additonal logging. Here, server_activate prints out the server’s IP address and port. Similarly, the get_request method calls the eponymous method from the superclass and logs the IP and the port of the incoming clients. The server also implements a service_actions method that is called by the server loop. This is where we’re periodically pausing the server and performing some blocking actions. In this case, the service_actions method checks the current time and compares it to the start time of the server. If the difference is greater than the specified timeout, the server is paused and a message is printed to the console indicating that something else is running. Then after one iteration, the start time is updated so that the server gets paused again after the timeout period. To test the server out, here’s a simple client that sends some data to the server: # client.py import socket import time import logging logging.basicConfig(level=logging.INFO) HOST = \"localhost\" # The server's hostname or IP address. PORT = 9999 # The port used by the server. with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.connect((HOST, PORT)) while True: time.sleep(1) s.sendall(b\"hello world\") data = s.recv(1024) logging.info(f\"Received {data!r}\") This client connects to the server via port 9999 and sends the b'hello world' byte string. The server will capture and echo it back to the client which the client will print as Received .... You can run the server in one console with python server.py and the client in another one with the python client.py command. You’ll see that the server will pause every 5 seconds, do something else in a blocking manner and then come back to handle the client requests. If you attach a second client from another console, you’ll see that the server can also handle that while retaining the expected behavior. The server will pause even if there’s no client sending requests to the server. You can test that behavior by detaching all the clients from the server. Now, we could also make the work in the serving_actions non-blocking by spinning a new thread or process and doing the work there. However, for the task that I was tackling, simply running the function in a blocking manner was enough. socketserver ↩︎ ","permalink":"http://rednafi.com/python/pause_and_resume_a_socket_server/","publishDate":"2023-02-05","summary":"I needed to write a socket server in Python that would allow me to intermittently pause the server loop for a while, run something else, then get back to the previous request-handling phase; repeating this iteration until the heat death of the universe. Initially, I opted for the low-level socket module to write something quick and dirty. However, the implementation got hairy pretty quickly. While the socket module gives you plenty of control over how you can tune the server’s behavior, writing a server with robust signal and error handling can be quite a bit of boilerplate work.\n","tags":["Python","Networking"],"title":"Pausing and resuming a socket server in Python"},{"content":"Back in the days when I was working as a data analyst, I used to spend hours inside Jupyter notebooks exploring, wrangling, and plotting data to gain insights. However, as I shifted my career gear towards backend software development, my usage of interactive exploratory tools dwindled. Nowadays, I spend the majority of my time working on a fairly large Django monolith accompanied by a fleet of microservices. Although I love my text editor and terminal emulators, I miss the ability to just start a Jupyter Notebook server and run code snippets interactively. While Django allows you to open up a shell environment and run code snippets interactively, it still isn’t as flexible as a notebook. So, I wanted to see if I could connect a Jupyter notebook server to a containerized Django application running on my local machine and interactively start making queries from there. Turns out, you can do that by integrating three tools into your Dockerized environment: ipykernel1, jupyter2, and django-extensions3. Before I start explaining how everything is tied together, here’s a fully working example4 of a containerized Django application where you can log into the Jupyter server and start debugging the app. The app is just a Dockerized version of the famous polls-app from the Django tutorial. The directory structure looks as follows: ../django-jupyter/ ├── Dockerfile ├── docker-compose.yml ├── mysite │ ├── db.sqlite3 │ ├── manage.py │ ├── mysite │ │ ├── __init__.py │ │ ├── _debug_settings.py │ │ ├── asgi.py │ │ ├── settings.py │ │ ├── urls.py │ │ └── wsgi.py │ ├── polls │ │ ├── __init__.py │ │ ├── admin.py │ │ ├── apps.py │ │ ├── migrations │ │ │ ├── 0001_initial.py │ │ │ └── __init__.py │ │ ├── models.py │ │ ├── tests.py │ │ ├── urls.py │ │ └── views.py │ └── script.ipynb ├── requirements.txt └── requirements-dev.txt We define and pin the dependencies required for the Jupyter integration in the requirements-dev.txt file: # These pinned deps will probably get outdated by the time you're reading it. # Use the latest version but always pin them in applications. ipykernel==6.20.1 jupyter==1.0.0 django-extensions==3.2.1 The application dependencies are defined in the requirements.txt file: django==4.1.5 In the mysite/mysite/_debug_settings.py file, we import the configs from the primary settings file and add the Jupyter configuration attributes there. Here’s the full content of the extended _debug_settings.py file: from .settings import * # noqa INSTALLED_APPS.append(\"django_extensions\") # noqa SHELL_PLUS = \"ipython\" SHELL_PLUS_PRINT_SQL = True IPYTHON_ARGUMENTS = [ \"--ext\", \"django_extensions.management.notebook_extension\", \"--debug\", ] IPYTHON_KERNEL_DISPLAY_NAME = \"Django Shell-Plus\" NOTEBOOK_ARGUMENTS = [ \"--ip\", \"0.0.0.0\", \"--port\", \"8895\", \"--allow-root\", \"--no-browser\", \"--NotebookApp.iopub_data_rate_limit=1e5\", \"--NotebookApp.token=''\", ] DJANGO_ALLOW_ASYNC_UNSAFE = True Notice how we’re appending the django_extensions app to the INSTALLED_APPS list defined in the main settings file. Then we’re setting the shell to ipython with the SHELL_PLUS attribute. The NOTEBOOK_ARGUMENTS defines the port of the Jupyter server and some auth-specific settings. Next, in the Dockerfile, we’re defining the application like this: # Dockerfile FROM python:3.11-bullseye # Set the working directory inside the container. WORKDIR /code # Don't write .pyc files and make the output unbuffered. ENV PYTHONDONTWRITEBYTECODE 1 ENV PYTHONUNBUFFERED 1 # Install dependencies. RUN pip install --upgrade pip COPY requirements.txt requirements-dev.txt ./ RUN pip install -r requirements.txt -r requirements-dev.txt # Copy the project code. COPY . /code Finally, we’re orchestrating the application and the Jupyter server in the docker-compose.yml file. Here’s how it looks: version: \"3.9\" services: web: build: . working_dir: /code/mysite volumes: - .:/code webserver: extends: service: web command: python manage.py runserver 0.0.0.0:8000 ports: - \"8000:8000\" jupyter: extends: service: web environment: - DJANGO_SETTINGS_MODULE=mysite._debug_settings - DJANGO_ALLOW_ASYNC_UNSAFE=true command: python manage.py shell_plus --notebook ports: - \"8895:8895\" debug: extends: service: web working_dir: /code command: sleep infinity We’re orchestrating three services here: webserver, jupyter, and debug. All of them extend the base web service that builds the Dockerfile. The webserver service is where the Django app is run and exposed via the 8000 port. The jupyter service runs the Jupyter server and makes it accessible through your browser via the 8895 port. Additionally, note how we are using our extended version of the main settings by overriding the DJANGO_SETTINGS_MODULE environment variable and setting it to mysite._debug_settings. The debug container is spun up to run the migration commands and perform other maintenance tasks within the container network. All the maintenance commands are defined in the Makefile for your convenience. You can run any of these by running make from the root directory. And that’s it! If you have Docker5 and docker-compose6 installed on your local system, you can give it a try. Clone the example-app4 repo, navigate to the root directory and run: docker compose up -d Then run the migration command: docker compose exec debug python mysite/manage.py makemigrations \\ \u0026\u0026 docker compose exec debug python mysite/manage.py migrate Now head over to your browser and go to http://localhost:8000. You should see an empty page with a simple header like this: If you go to http://localhost:8895, you’ll be able to open a new notebook that automatically connects to your database and allows you to write interactive code immediately. You can run the following snippet and it’ll create two questions and two choices in the database. from polls import models as polls_models from datetime import datetime, timezone for question_text in (\"Are you okay?\", \"Do you wanna go there?\"): question = polls_models.Question.objects.create( question_text=question_text, pub_date=datetime.now(tz=timezone.utc), ) question.choice_set.set( polls_models.Choice.objects.create(choice_text=ctext) for ctext in (\"yes\", \"no\") ) If you run this and refresh your application server, you’ll that the objects have been created and they appear in the view: ipykernel ↩︎ jupyter ↩︎ django-extensions ↩︎ example-app ↩︎ ↩︎ Docker ↩︎ docker-compose ↩︎ How to access Jupyter notebook in a Docker Container 7 ↩︎ ","permalink":"http://rednafi.com/python/django_and_jupyter_notebook/","publishDate":"2023-01-14","summary":"Back in the days when I was working as a data analyst, I used to spend hours inside Jupyter notebooks exploring, wrangling, and plotting data to gain insights. However, as I shifted my career gear towards backend software development, my usage of interactive exploratory tools dwindled.\nNowadays, I spend the majority of my time working on a fairly large Django monolith accompanied by a fleet of microservices. Although I love my text editor and terminal emulators, I miss the ability to just start a Jupyter Notebook server and run code snippets interactively. While Django allows you to open up a shell environment and run code snippets interactively, it still isn’t as flexible as a notebook.\n","tags":["Python","Django"],"title":"Debugging a containerized Django application in Jupyter Notebook"},{"content":"I was working with a table that had a similar (simplified) structure like this: | uuid | file_path | |----------------------------------|---------------------------| | b8658dfc3e80446c92f7303edf31dcbd | media/private/file_1.pdf | | 3d750874a9df47388569a23c559a4561 | media/private/file_2.csv | | d177b7f7d8b046768ab65857451a0354 | media/private/file_3.txt | | df45742175d7451dad59761f15653d9d | media/private/image_1.png | | a542966fc193470dab84351c15523042 | media/private/image_2.jpg | Let’s say the above table is represented by the following Django model: from django.db import models class FileCabinet(models.Model): uuid = models.UUIDField( primary_key=True, default=uuid.uuid4, editable=False ) file_path = models.FileField(upload_to=\"files/\") I needed to extract the file names with their extensions from the file_path column and create new paths by adding the prefix dir/ before each file name. This would involve stripping everything before the file name from a file path and adding the prefix, resulting in a list of new file paths like this: ['dir/file_1.pdf', ..., 'dir/image_2.jpg']. Using Django ORM and some imperative Python code you could do the following: ... # This will give you a queryset with the file paths. # e.g. ","permalink":"http://rednafi.com/python/manipulate_text_with_django_query_expression/","publishDate":"2023-01-07","summary":"I was working with a table that had a similar (simplified) structure like this:\n| uuid | file_path | |----------------------------------|---------------------------| | b8658dfc3e80446c92f7303edf31dcbd | media/private/file_1.pdf | | 3d750874a9df47388569a23c559a4561 | media/private/file_2.csv | | d177b7f7d8b046768ab65857451a0354 | media/private/file_3.txt | | df45742175d7451dad59761f15653d9d | media/private/image_1.png | | a542966fc193470dab84351c15523042 | media/private/image_2.jpg | Let’s say the above table is represented by the following Django model:\nfrom django.db import models class FileCabinet(models.Model): uuid = models.UUIDField( primary_key=True, default=uuid.uuid4, editable=False ) file_path = models.FileField(upload_to=\"files/\") I needed to extract the file names with their extensions from the file_path column and create new paths by adding the prefix dir/ before each file name. This would involve stripping everything before the file name from a file path and adding the prefix, resulting in a list of new file paths like this: ['dir/file_1.pdf', ..., 'dir/image_2.jpg'].\n","tags":["Python","Django"],"title":"Manipulating text with query expressions in Django"},{"content":"At my workplace, I was writing a script to download multiple files from different S3 buckets. The script relied on Django ORM, so I couldn’t use Python’s async paradigm to speed up the process. Instead, I opted for boto3 to download the files and concurrent.futures.ThreadPoolExecutor to spin up multiple threads and make the requests concurrently. However, since the script was expected to be long-running, I needed to display progress bars to show the state of execution. It’s quite easy to do with tqdm when you’re just looping over a list of file paths and downloading the contents synchronously: from tqdm import tqdm for file_path in tqdm(file_paths): download_file(file_path) But you can’t do this when multiple threads or processes are doing the work. Here’s what I’ve found that works quite well: from __future__ import annotations import time from concurrent.futures import ThreadPoolExecutor, as_completed from typing import Generator import httpx from tqdm import tqdm def make_request(url: str) -\u003e dict: with httpx.Client() as client: response = client.get(url) # Additional delay to simulate a slow request. time.sleep(1) return response.json() def make_requests( urls: list[str], ) -\u003e Generator[list[dict], None, None]: with tqdm(total=len(urls)) as pbar: with ThreadPoolExecutor(max_workers=5) as executor: futures = [executor.submit(make_request, url) for url in urls] for future in as_completed(futures): pbar.update(1) yield future.result() def main() -\u003e None: urls = [ \"https://httpbin.org/get\", \"https://httpbin.org/get?foo=bar\", \"https://httpbin.org/get?foo=baz\", \"https://httpbin.org/get?foo=qux\", \"https://httpbin.org/get?foo=quux\", ] results = [] for result in make_requests(urls): results.append(result) print(results) if __name__ == \"__main__\": main() Running this will print: 100%|█████████████████████████████████████████████████████| 5/5 [00:01\u003c00:00, 3.51it/s] ... This script makes 5 concurrent requests by leveraging ThreadPoolExecutor from the concurrent.futures module. The make_request function just sends one request to a URL and sleeps for a second to simulate a long-running task. Then the make_requests function spins up 5 threads and calls the make_request function in each one with a different URL. Here, we’re instantiating tqdm as a context manager and passing the total length of the urls. This allows tqdm to calculate the progress bar. Then in a nested context manager, we spin up the threads and pass the make_request to the executor.submit method. We collect the future objects returned by the executor.submit methods in a list and update the progress bar with pbar.update(1) while iterating through the futures. And that’s it, mission successful. I usually use contextlib.ExitStack to avoid nested context managers like this: ... from contextlib import ExitStack def make_requests( urls: list[str], ) -\u003e Generator[list[dict], None, None]: with ExitStack() as stack: executor = stack.enter_context(ThreadPoolExecutor(max_workers=5)) pbar = stack.enter_context(tqdm(total=len(urls))) futures = [executor.submit(make_request, url) for url in urls] for future in as_completed(futures): pbar.update(1) yield future.result() ... Running this script will yield the same result as before. How to use tqdm with multithreading? 1 ↩︎ ","permalink":"http://rednafi.com/python/tqdm_progressbar_with_concurrent_futures/","publishDate":"2023-01-06","summary":"At my workplace, I was writing a script to download multiple files from different S3 buckets. The script relied on Django ORM, so I couldn’t use Python’s async paradigm to speed up the process. Instead, I opted for boto3 to download the files and concurrent.futures.ThreadPoolExecutor to spin up multiple threads and make the requests concurrently.\nHowever, since the script was expected to be long-running, I needed to display progress bars to show the state of execution. It’s quite easy to do with tqdm when you’re just looping over a list of file paths and downloading the contents synchronously:\n","tags":["Python"],"title":"Using tqdm with concurrent.fututes in Python"},{"content":"The colon : command is a shell utility that represents a truthy value. It can be thought of as an alias for the built-in true command. You can test it by opening a shell script and typing a colon on the command line, like this:\n: If you then inspect the exit code by typing $? on the command line, you’ll see a 0 there, which is exactly what you’d see if you had used the true command.\n: ; echo $? The output will be:\n0 I find the colon command useful when running a shell script with the -x flag, which prints out the commands being executed by the interpreter. For example, consider the following script:\n#!/bin/bash # script.sh echo \"section 1: print the first 2 lines of the current directory\" ls -lah | head -n 2 echo \"section 2: print the size of the /usr/bin directory\" du -sh /usr/bin Running this script with bash -x script.sh will print the following lines:\n+ echo 'section 1: print the first 2 lines of the current directory' section 1: print the first 2 lines of the current directory + ls -lah + head -n 2 total 120 drwxr-xr-x 26 rednafi staff 832B Dec 23 13:35 . + echo 'section 2: print the size of the /usr/bin directory' section 2: print the size of the /usr/bin directory + du -sh /usr/bin 76M /usr/bin Notice that the above script prints out each command first (denoted by a preceding + sign) and then its respective output. However, the echo \"section...\" commands in this script are only used for debugging purposes, to enhance the readability of the output by providing separation between different sections. Therefore, repeating these commands and their outputs can be a little redundant. You can use the colon command to eliminate this repetition, as follows:\n#!/bin/bash : \"section 1: print the first 2 lines of the current directory\" ls -lah | head -n 2 : \"section 2: print the size of the /usr/bin directory\" du -sh /usr/bin Running this script with the -x flag will produce the following output:\n+ : 'section 1: print the first 2 lines of the current directory' + ls -lah + head -n 2 total 120 drwxr-xr-x 26 rednafi staff 832B Dec 23 13:35 . + : 'section 2: print the size of the /usr/bin directory' + du -sh /usr/bin 76M /usr/bin If you look closely, you’ll see that the debug commands and their outputs are no longer getting repeated.\nWhy I use the colon command - @anthonywritescode 1 ↩︎\n","permalink":"http://rednafi.com/misc/colon_command_in_shell_scripts/","publishDate":"2022-12-23","summary":"The colon : command is a shell utility that represents a truthy value. It can be thought of as an alias for the built-in true command. You can test it by opening a shell script and typing a colon on the command line, like this:\n: If you then inspect the exit code by typing $? on the command line, you’ll see a 0 there, which is exactly what you’d see if you had used the true command.\n","tags":["Shell","TIL"],"title":"Colon command in shell scripts"},{"content":"Django has a Model.objects.bulk_update method that allows you to update multiple objects in a single pass. While this method is a great way to speed up the update process, oftentimes it’s not fast enough. Recently, at my workplace, I found myself writing a script to update half a million user records and it was taking quite a bit of time to mutate them even after leveraging bulk update. So I wanted to see if I could use multiprocessing with .bulk_update to quicken the process even more. Turns out, yep I can!\nHere’s a script that creates 100k users in a PostgreSQL database and updates their usernames via vanilla .bulk_update. Notice how we’re timing the update duration:\n# app_name/vanilla_bulk_update.py import os import django # This allows us to run this module as a script inside a Django app. os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"mysite.settings\") django.setup() import time from django.contrib.auth.models import User # Delete the previous users, if there's any. User.objects.all().delete() # Create 100k users. users = User.objects.bulk_create( (User(username=f\"user_{i}\") for i in range(100_000)), ) # Start time. s1 = time.perf_counter() # Update all the users' usernames to use upper case. for user in users: user.username = user.username.upper() # Save all the users. The batch_size determines how many records will # be saved at once. User.objects.bulk_update(users, [\"username\"], batch_size=1_000) # End time. e1 = time.perf_counter() # Print the time taken. print(f\"Time taken to update 100k users: {e1 - s1} seconds.\") # Print a few usernames to see that the script has changed them as expected. print(\"Updated usernames:\") print(\"===================\") for username in User.objects.values_list(\"username\", flat=True)[:5]: print(username) This can be executed as a script like this:\npython -m app_name.vanilla_bulk_update It’ll return:\nTime taken to update 100k users: 9.220380916005524 seconds. Updated usernames: =================== USER_99840 USER_99841 USER_99842 USER_99843 USER_99844 A little over 9 seconds isn’t too bad for 100k users but we can do better. Here’s how I’ve updated the above script to make it 4x faster:\n# app_name/multiprocessing_bulk_update.py import os import django # This allows us to run this module as a script inside a Django app. os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"mysite.settings\") django.setup() import multiprocessing as mp import time from django.contrib.auth.models import User MAX_WORKERS = 2 * mp.cpu_count() - 1 CHUNK_SIZE = 1_000 def main(): # Delete the previous users, if there's any. User.objects.all().delete() # Create 100k users. users = User.objects.bulk_create( (User(username=f\"user_{i}\") for i in range(100_000)) ) # Start time. s1 = time.perf_counter() # Mutate the usernames to use upper case. for user in users: user.username = user.username.upper() # Split the users into chunks for each process to work on. This returns # [[USER_0, USER_1, USER_2, ...], [USER_3, USER_4, USER_5, ...], ...] user_chunks = ( users[i : i + CHUNK_SIZE] for i in range(0, len(users), CHUNK_SIZE) ) # Close the connection before forking. django.db.connections.close_all() # Create a pool of processes and run the update_users function on # each chunk. with mp.Pool(MAX_WORKERS) as pool: pool.map(update_users, user_chunks, chunksize=10) # End time. e1 = time.perf_counter() # Print the time taken. print( \"Time taken to update 100k users with multiprocessing: \" f\"{e1 - s1} seconds.\" ) # Print a few usernames to see that the script has changed them # as expected. print(\"Updated usernames:\") print(\"===================\") for username in User.objects.values_list(\"username\", flat=True)[:5]: print(username) def update_users(user_chunk): # The batch_size determines how many records will be saved at once. User.objects.bulk_update(user_chunk, [\"username\"], batch_size=CHUNK_SIZE) if __name__ == \"__main__\": main() This script divides the updated user list into a list of multiple user chunks and assigns that to the user_chunks variable. The update_users function takes a single user chunk and runs .bulk_update on that. Then we fork a bunch of processes and run the update_users function over the user_chunks via multiprocessing.Pool.map. Each process consumes 10 chunks of users in a single go—determined by the chunksize parameter of the pool.map function. Running the updated script will give you similar output as before but with a much smaller runtime:\npython -m app_name.multiprocessing_bulk_update This will print the following:\nTime taken to update 100k users with multiprocessing: 2.2682724999976926 seconds. Updated usernames: =================== USER_960 USER_961 USER_962 USER_963 USER_964 Whoa! This updated the records in under 2.5 seconds. Quite a bit of performance gain there.\nThis won’t work if you’re using SQLite database as your backend since SQLite doesn’t support concurrent writes from multiple processes. Trying to run the second script with SQLite backend will incur a database error.\nDjango bulk_update 1 ↩︎\nUsing a pool of forked workers 2 ↩︎\n","permalink":"http://rednafi.com/python/faster_bulk_update_in_django/","publishDate":"2022-11-30","summary":"Django has a Model.objects.bulk_update method that allows you to update multiple objects in a single pass. While this method is a great way to speed up the update process, oftentimes it’s not fast enough. Recently, at my workplace, I found myself writing a script to update half a million user records and it was taking quite a bit of time to mutate them even after leveraging bulk update. So I wanted to see if I could use multiprocessing with .bulk_update to quicken the process even more. Turns out, yep I can!\n","tags":["Python","Django","Database"],"title":"Faster bulk_update in Django"},{"content":"I’ve just migrated from Ubuntu to macOS for work and am still in the process of setting up the machine. I’ve been a lifelong Linux user and this is the first time I’ve picked up an OS that’s not just another flavor of Debian. Primarily, I work with Python, NodeJS, and a tiny bit of Go. Previously, any time I had to install these language runtimes, I’d execute a bespoke script that’d install: Python via deadsnake1 ppa. NodeJS via nvm2. Go from the official binary source3. Along with the hassle of having to manage three version managers, setting up multiple versions of Python almost always felt like a chore. I’ve used pyenv4 before which kind of feels like nvm and works quite well in practice. However, on Twitter, I came across this5 reply by Adam Johnson which mentions that asdf6 can manage multiple runtimes of different languages—one version manager to rule them all. Also, it’s written in pure bash so there’s no external dependency required for the tool to work. Since I’m starting from scratch on a new OS, I wanted to give this a tool to try. Spoiler alert, it works with zero drama. Here, I’ll quickly explain how to get up and running with multiple versions of Python and make them work seamlessly. Prerequisites For this to work, I’m assuming that you’ve got homebrew7 installed on your system. Install asdf with the following command: brew install asdf Once asdf is installed, you’ll need to install the Python plugin8. Run this: asdf plugin-add python Also, you’ll need to make sure that your system has these9 plugin-specific dependencies in place. Bootstrapping Python Once the prerequisites are fulfilled, you’re ready to install the Python versions from the source. Let’s say you want to install Python 3.11. To do so, run: asdf install python 3.11.0 This will install Python in the /Users/$USER/.asdf/shims/python3.11 location. Just concat the command to install multiple versions of Python: asdf install 3.10.15 \u0026\u0026 asdf install 3.9.9 Selecting a specific Python version Once you’ve installed your desired Python versions with asdf, if you try to invoke global Python with python or python3 command, you’ll encounter the following error: No version is set for command python3 Consider adding one of the following versions in your config file at python 3.8.15 python 3.11.0 python 3.10.8 To address this, you can run the next command to select the latest available version of Python (here it’s 3.11.0) as the global default runtime: asdf global python latest Running this will add a $HOME/.tool-versions file with the following content: python 3.11.0 You can also select other Python versions as the global runtime like this: asdf global python In a project, if you want to use a specific Python version other than the global one, you can run: asdf local python This will add a $PATH/.tool-versions similar to the global file. Now you can just go ahead and start using that specific version of Python. Running this command will create a virtual environment using the locally specified Python runtime and start the interpreter inside that: python -m venv .venv \u0026\u0026 source .venv/bin/activate \u0026\u0026 python Removing a runtime Running asdf uninstall python will do the trick. deadsnake ↩︎ nvm ↩︎ Download Go ↩︎ pyenv ↩︎ Adam Johnson’s tweet ↩︎ asdf - manage multiple runtime versions with a single CLI tool ↩︎ homebrew ↩︎ asdf Python plugin ↩︎ asdf plugin dependencies ↩︎ ","permalink":"http://rednafi.com/python/install_python_with_asdf/","publishDate":"2022-11-13","summary":"I’ve just migrated from Ubuntu to macOS for work and am still in the process of setting up the machine. I’ve been a lifelong Linux user and this is the first time I’ve picked up an OS that’s not just another flavor of Debian. Primarily, I work with Python, NodeJS, and a tiny bit of Go. Previously, any time I had to install these language runtimes, I’d execute a bespoke script that’d install:\n","tags":["Python","TIL"],"title":"Installing Python on macOS with asdf"},{"content":"TIL that you can specify update_fields while saving a Django model to generate a leaner underlying SQL query. This yields better performance while updating multiple objects in a tight loop. To test that, I’m opening an IPython shell with python manage.py shell -i ipython command and creating a few user objects with the following lines:\nIn [1]: from django.contrib.auth import User In [2]: for i in range(1000): ...: fname, lname = f'foo_{i}', f'bar_{i}' ...: User.objects.create( ...: first_name=fname, last_name=lname, username=f'{fname}-{lname}') ...: Here’s the underlying query Django generates when you’re trying to save a single object:\nIn [3]: from django.db import reset_queries, connections In [4]: reset_queries() In [5]: user_0 = User.objects.first() In [6]: user_0.first_name = 'foo_updated' In [7]: user_0.save() In [8]: connection.queries This will print:\n[ ..., { \"sql\": 'UPDATE \"auth_user\" SET \"password\" = \\'\\', \"last_login\" = NULL, \"is_superuser\" = 0, \"username\" = \\'foo_0-bar_0\\', \"first_name\" = \\'foo_updated\\', \"last_name\" = \\'bar_0\\', \"email\" = \\'\\', \"is_staff\" = 0, \"is_active\" = 1, \"date_joined\" = \\'2022-11-09 22:27:39.291676\\' WHERE \"auth_user\".\"id\" = 1002', \"time\": \"0.009\", }, ] If you inspect the query, you’ll see that although we’re only updating the first_name field on the user_0 object, Django is generating a query that updates all the underlying fields on the object. The SQL query always passes the pre-existing values of the fields that weren’t touched. This might seem trivial, but what if the model consisted of 20 fields and you need to call save() on it frequently? At a certain scale the database query that updates all of your columns every time you call save() can start becoming expensive.\nSpecifying update_fields inside the save() method can make the query leaner. Consider this:\nIn[9]: reset_queries() In[10]: user_0.first_name = \"foo_updated_again\" In[11]: user_0.save(update_fields=[\"first_name\"]) In[12]: connection.queries This prints:\n[ {'sql': 'UPDATE \"auth_user\" SET \"first_name\" = \\'changed_again\\' WHERE \"auth_user\".\"id\" = 1002', 'time': '0.008' } ] You can see this time, Django generates a SQL that only updates the specific field we want and doesn’t send any redundant data over the wire. The following snippet quantifies the performance gain while updating 1000 objects in a tight loop:\n# src.py import os import time import django os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"mysite.settings\") django.setup() from django.contrib.auth.models import User # Create 1000 users. for i in range(1000): User.objects.create_user( first_name=f\"foo_{i}\", last_name=f\"bar_{i}\", username=f\"foo_{i}-bar_{i}\", ) ############### Update all users with '.save()' ############### s1 = time.perf_counter() for i, user in zip(range(1000), User.objects.all()): user.first_name = f\"foo_updated_{i}\" user.save() e1 = time.perf_counter() t1 = e1 - s1 print(f\"User.save(): {t1:.2f}s\") ############################################################### ###### Update all users with '.save(update_fields=[...])'###### s2 = time.perf_counter() for i, user in zip(range(1000), User.objects.all()): user.first_name = f\"foo_updated_again_{i}\" user.save(update_fields=[\"first_name\"]) e2 = time.perf_counter() t2 = e2 - s2 print(f\"User.save(update_fields=[...]): {t2:.2f}s\") ############################################################### print( f\"User.save(update_fields=[...] is {t1 / t2:.2f}x faster than User.save()\" ) Running this script will print the following:\nUser.save(): 1.86s User.save(update_fields=[...]): 1.77s User.save(update_fields=[...] is 1.05x faster than User.save() You can see that User.save(updated_fields=[...]) is a tad bit faster than plain User.save.\nShould you always use it? Probably not. While the performance gain is measurable when you’re updating multiple objects in a loop, it’s quite negligible if the object count is low. Also, this adds maintenance overhead as any time you change the model, you’ll have to remember to keep the Model.save(update_fields=[...]) in sync. If you forget to add a field to the update_fields, Django will silently ignore the incoming data against that field and data will be lost.\nReferences Specifying which fields to save - Django docs 1 ↩︎\nSave your Django models using update_fields for better performance - Reddit 2 ↩︎\n","permalink":"http://rednafi.com/python/save_with_update_fields_in_django/","publishDate":"2022-11-09","summary":"TIL that you can specify update_fields while saving a Django model to generate a leaner underlying SQL query. This yields better performance while updating multiple objects in a tight loop. To test that, I’m opening an IPython shell with python manage.py shell -i ipython command and creating a few user objects with the following lines:\nIn [1]: from django.contrib.auth import User In [2]: for i in range(1000): ...: fname, lname = f'foo_{i}', f'bar_{i}' ...: User.objects.create( ...: first_name=fname, last_name=lname, username=f'{fname}-{lname}') ...: Here’s the underlying query Django generates when you’re trying to save a single object:\n","tags":["Python","Django","TIL"],"title":"Save models with update_fields for better performance in Django"},{"content":"At my workplace, while working on a Lambda1 function, I noticed that my Python logs weren’t appearing on the corresponding Cloudwatch2 log dashboard. At first, I thought that the function wasn’t picking up the correct log level from the environment variables. We were using serverless3 framework and GitLab CI to deploy the function, so my first line of investigation involved checking for missing environment variables in those config files.\nHowever, I quickly realized that the environment variables were being propagated to the Lambda function as expected. So, the issue had to be coming from somewhere else. After perusing through some docs, I discovered from the source code of Lambda Python Runtime Interface Client4 that AWS Lambda Python runtime pre-configures5 a logging handler that modifies the format of the log message, and also adds some metadata to the record if available. What’s not pre-configured though is the log level. This means that no matter the type of log message you try to send, it won’t print anything.\nAccording to the docs6, to make your logging work in the Lambda environment, you’ll only need to set the log level for the root logger like this:\n# src.py import logging # Get the root logger. logger = logging.getLogger() # Set the log level to the logger object. logger.setLevel(logging.INFO) # Use the logger. This will print the message only in the Lambda runtime. logger.info(\"Hello from Lambda!\") While this does make the log messages appear on the Cloudwatch dashboard, it doesn’t work whenever you’ll need to introspect the logs in your local Python interpreter. If you execute the above snippet locally, you won’t see any log message on your console. That’s because here we’re only setting the log level for the root logger and we haven’t defined any handler. To fix the local logging, you’ll need to add a handler to the logger as and set the log level on it as follows:\n# src.py import logging # Get the root logger. logger = logging.getLogger() # Create a handler. s_handler = logging.StreamHandler() # Link the handler to the logger. logger.addHandler(s_handler) # Set the log level to the logger. logger.setLevel(logging.INFO) # Use the logger. This will print the message in the local environment. logger.info(\"This is an info message.\") In the Lambda Python runtime, the root logger is already pre-configured to have modified handlers. The snippet above first adds another handler to the logger and sets the log level. So technically, the root logger will contain two handlers in the Lambda environment and print every log message twice with different handlers. However, you won’t see the duplicate messages in your local environment since the local logger will have only the one handler that we’ve defined. So, the logger will behave differently in the two environments; not good.\nHaving multiple stream handlers on the root logger that send the message to the stdout will print every log message twice.\nSo, this still doesn’t do what we want. Besides, sometimes in the local environment, I just want to use logging.basicConfig and start logging with minimal configuration. The goal here is to configure the root logger in a way that doesn’t conflict with Lambda’s pre-configured handlers and also works locally without any side effects. Here’s what I’ve found that works:\n# src.py import logging # If the logger has pre-configured handlers, set the log level to the # root logger only. This branch will get executed in the Lambda runtime. if logging.getLogger().hasHandlers(): logging.getLogger().setLevel(logging.INFO) else: # Just configure with basicConfig for local usage. This branch will # get executed in the local environment. logging.basicConfig(level=logging.INFO) # Use the logger. logging.info(\"This is an info message.\") The above snippet first inspects whether the root logger contains any handlers and if it does then sets the log level for the root logger. Otherwise, it just configures the logger with basicConfig for local development. This will print out the log messages both in the local and Lambda environment and won’t suffer from any side effects like message duplication. It’ll also make sure that the pre-configured formatting of the log message is kept intact.\nAWS Lambda ↩︎\nCloudwatch ↩︎\nServerless framework ↩︎\nLambda Python Runtime Interface Client ↩︎\nPre-configured root logger in the Lambda environment ↩︎\nAWS Lambda function logging in Python ↩︎\nUsing Python logging with AWS Lambda 7 ↩︎\n","permalink":"http://rednafi.com/python/logging_quirks_in_lambda_environment/","publishDate":"2022-10-20","summary":"At my workplace, while working on a Lambda1 function, I noticed that my Python logs weren’t appearing on the corresponding Cloudwatch2 log dashboard. At first, I thought that the function wasn’t picking up the correct log level from the environment variables. We were using serverless3 framework and GitLab CI to deploy the function, so my first line of investigation involved checking for missing environment variables in those config files.\nHowever, I quickly realized that the environment variables were being propagated to the Lambda function as expected. So, the issue had to be coming from somewhere else. After perusing through some docs, I discovered from the source code of Lambda Python Runtime Interface Client4 that AWS Lambda Python runtime pre-configures5 a logging handler that modifies the format of the log message, and also adds some metadata to the record if available. What’s not pre-configured though is the log level. This means that no matter the type of log message you try to send, it won’t print anything.\n","tags":["Python","AWS"],"title":"Python logging quirks in AWS Lambda environment"},{"content":"Python makes it freakishly easy to load the whole content of any file into memory and process it afterward. This is one of the first things that’s taught to people who are new to the language. While the following snippet might be frowned upon by many, it’s definitely not uncommon:\n# src.py with open(\"foo.csv\", \"r\") as f: # Load the whole content of the file as a string in memory and return it. f_content = f.read() # ...do your processing here. ... Adopting this pattern as the default way of handling files isn’t the most terrible thing in the world for sure. Also, this is often the preferred way of dealing with image files or blobs. However, overzealously loading file content is only okay as long as the file size is smaller than the volatile memory of the working system.\nMoreover, you’ll need to be extra careful if you’re accepting files from users and running further procedures on the content of those files. Indiscriminantly loading up the full content into memory can be dangerous as it can cause OOM errors and crash the working process if the system runs out of memory while processing a large file. This simple overlook was the root cause of a major production incident at my workplace today.\nThe affected part of our primary Django monolith asks the users to upload a CSV file to a panel, runs some procedures on the content of the file, and displays the transformed rows in a paginated HTML table. Since the application is primarily used by authenticated users and we knew the expected file size, there wasn’t any guardrail that’d prevent someone from uploading a humongous file and crashing down the whole system. To make things worse, the associated background function in the Django view was buffering the entire file into memory before starting to process the rows. Buffering the entire file surely makes the process a little faster but at the cost of higher memory usage.\nAlthough we were using background processes to avoid chugging files in the main server process, that didn’t help when the users suddendly started to send large CSV files in parallel. The workers were hitting OOM errors and getting restarted by the process manager. In our particular case, we didn’t have much reason to buffer the whole file before processing. Apparently, the naive way scaled up pretty gracefully and we didn’t pay much attention since no one was uploading file that our server instances couln’t handle. We were storing the incoming file in a models.FileField type attribute of a Django model. When a user uploads a CSV file, we’d:\nOpen the file in binary mode via the open(filepath, \"rb\") callable. Buffer the whole file in memory and transform the binary content into a unicode string. Pass the stringified file-like object to csv.DictReader to load that as a CSV file. Apply transformation on the rows line by line and render the HTML table. This is how the code looks:\n# src.py import csv import io # Django mandates us to open the file in binary mode. with model_instance.file.open(mode=\"rb\") as f: reader = csv.DictReader( io.StringIO(f.read().decode(errors=\"ignore\", encoding=\"utf-8\")), ) with row in reader: # ... data processing goes here. The csv.DictReader callable only accepts a file-like object that’s been opened in text mode. However, Django’s FileField type doesn’t make any assumptions about the file content. It mandates us to open the file in binary mode and then decode it if necessary. So, we open the file in binary mode with model_instance.file.open(mode=\"rb\") which returns an io.BufferedReader type file object. This file-like object can’t be passed directly to the csv.DictReader because a byte stream doesn’t have the concept of EOL and the CSV reader need that to know where a row ends. As a consequence, the csv.DictReader expects a file-like object opened in text mode where the rows are explicitly delineated by platform-specific EOLs like \\n or \\n\\r.\nTo solve this, we load the content of the file in memory with f.read() and decode it by calling .decode() on the result of the preceding operation. Then we create an in-memory text file-like buffer by passing the decoded string to io.StringIO. Now the CSV reader can consume this transformed file-like object and build dictionaries of rows off of that. Unfortunately, this stringified file buffer stays alive in the memory throughout the entire lifetime of the processor function. Imagine 100s of large CSV files getting thrown at the workers that execute the above code snippet. You see, at this point, overwhelming the background workers doesn’t seem too difficult.\nWhen our workers started to degrade in production and the alerts went bonkers, we began investigating the problem. After pinpointing the issue, we immediately responded to it by vertically scaling up the machines. The surface area of this issue was quite large and we didn’t want to hotfix it in fear of triggering inadvertent regressions. Once we were out of the woods, we started patching the culprit.\nThe solution to this is quite simple—convert the binary file-like object into a text file-like object without buffering everything in memory and then pass the file to the CSV reader. We were already processing the CSV rows in a lazy manner and just removing f.read() fixed the overzealous buffering issue. The corrected code snippet looks like this:\n# src.py import csv import io # Django mandates us to open the file in binary mode. with model_instance.file.open(mode=\"rb\") as f: reader = csv.DictReader( io.TextIOWrapper(f, errors=\"ignore\", encoding=\"utf-8\"), ) with row in reader: # ... data processing goes here. Here, io.TextIOWrapper wraps the binary file-like object in a way that makes it behave as if it were opened in text mode. In fact when you open a file in text mode, the native implementation of open returns a file-like object wrapped in io.TextIOWrapper. You can find more details about the implementation1 of open in PEP-31162.\nThe csv.DictReader callable can consume this transformed file-like object without any further modifications. Since we aren’t calling f.read() anymore, no overzealous content buffering is going on here and we can lazily ask for new rows from the reader object as we sequentially process them.\nopen ↩︎\nNew I/O - PEP-3116 ↩︎\nHow to use python csv.DictReader with a binary file? 3 ↩︎\n","permalink":"http://rednafi.com/python/outage_caused_by_eager_loading_file/","publishDate":"2022-10-14","summary":"Python makes it freakishly easy to load the whole content of any file into memory and process it afterward. This is one of the first things that’s taught to people who are new to the language. While the following snippet might be frowned upon by many, it’s definitely not uncommon:\n# src.py with open(\"foo.csv\", \"r\") as f: # Load the whole content of the file as a string in memory and return it. f_content = f.read() # ...do your processing here. ... Adopting this pattern as the default way of handling files isn’t the most terrible thing in the world for sure. Also, this is often the preferred way of dealing with image files or blobs. However, overzealously loading file content is only okay as long as the file size is smaller than the volatile memory of the working system.\n","tags":["Python","Incident Post-mortem"],"title":"Dissecting an outage caused by eager-loading file content"},{"content":"After reading Simon Willison’s amazing piece1 on how he adds new features to his open-source softwares, I wanted to adopt some of the good practices and incorporate them into my own workflow. One of the highlights of that post was how to kick off a feature work. The process roughly goes like this: Opening a new GitHub issue for the feature in the corresponding repository. Adding a rough description of the feature to the issue. Creating a feature branch off of main/master/trunk. If the feature is trivial or just a doc update, this step can be skipped. Referring to the issue in every commit message as you start working on the feature: Appending #refs to every commit message. This will attach the commit to the concerning issue on the GitHub UI. Appending #closes to the final commit message when the feature is complete. If you need to refer to an issue after it’s closed, you can still do that by appending #refs to the commit message. So a commit message should look similar to Feature foo, refs #120 or Update foo, closes #115. The comma (,) before refs/closes is essential here. I like to enforce it. This pattern can also work for bugfixes without any changes. Here’s an example2 of it in action. I follow the pattern to write the blogs on this site as well. This is what a feature issue might look like on GitHub: While I’m quite happy with how the process is working for me, often time, I get careless and push commits without a reference to any issue. This pollutes the Git history and breaks my streak of maintaining good hygiene. So, I was looking for a way to make sure that the CI fails and reprimands me whenever I’m not following the process correctly. It’s just one less thing to worry about. I’ve decided to use GitHub Actions to audit the conformity of the commit messages. The CI pipeline is orchestrated as follows: After every push and pull-request, the audit-commits job in an audit.yml workflow file will verify the conformity of the commit messages. This job runs a regex pattern against every commit message and fails with exit code 1 if the message doesn’t respect the expected format. If the audit-commits job passes successfully, only then the primary jobs in the ci.yml workflow will execute. The entire pipeline will fail and the primary CI workflow won’t be triggered at all if the audit-commit job fails at any point. On GitHub, you’re expected to place your workflow files in the .github/workflows directory. If you inspect this blog’s workflows3 folder, you’ll see this pattern in action. Here, the directory has three workflow files: .github/workflows ├── audit.yml ├── automerge.yml └── ci.yml The automerge.yml file automatically merges a pull-request when the primary CI jobs pass. I wrote about it in more detail in another4 write-up. We’ll ignore the automerge.yml file for now. Here, the audit file runs after every push and pull-request and verifies the structure of the commit message. I picked a generic name like audit.yml instead of a more specific one like audit-commit.yml because in the future if I want to add another check, I can easily extend this file without renaming it. Here’s the unabridged content of the audit.yml file: # .github/workflows/audit.yml # Auditing commit structure. name: Audit on: workflow_call: jobs: audit-commits: runs-on: ubuntu-latest if: ${{ github.actor != 'dependabot[bot]' }} steps: - name: \"Return exit code 1 if the commit messages aren't formatted correctly.\" shell: bash run: | set -euo pipefail # Get the commit payload from GH Actions event. # https://docs.github.com/en/developers/webhooks-and-events/events/ # github-event-types#pushevent commits='${{ toJSON(github.event.commits) }}' # Exit with 0 if no new commit is found. if [[ $commits =~ \"null\" ]]; then echo \"No commit found. Exiting...\" exit 0 fi # Get the unique messages from the commits event. parsed=$(echo -n \"$commits\" | jq -r \".[].message\" | sort -u) mtch='(, refs|, closes) #[0-9]+' echo \"$parsed\" | while IFS= read -r raw_line; do line=$(echo \"$raw_line\" | tr -d \"\\r\\n\") # Ignore empty lines. if [[ -z \"$line\" ]]; then continue # Check with regex if the commit message contains 'refs #issue_number' # or 'closes #issue_number'. If not, exit with an error. elif [[ \"$line\" =~ $mtch ]]; then echo \"Commit message: $line ✅\" else echo \"Commit message: $line ❌\" echo -n \"Commit message must contain \" echo -n \"'refs #issue_number' or 'closes #issue_number'.\" exit 1 fi done I’ve defined this workflow as a reusable one. A reusable workflow can be called like a function with parameters from another workflow. The workflow_call node the audit.yml file makes it a reusable one and you can define additional parameters in this section if you need to do so. However, in this particular case, I don’t need to pass any parameters while calling the audit.yml workflow from the ci.yml workflow. You can find more details on how to define reusable workflows5 in the docs. In the jobs section of the audit.yml file, we define a single audit-commits job that runs a bash script against every incoming commit message and verifies its structure. The commit messages can be accessed from the '${{ toJSON(github.event.commits) }}' context variable. Then the script loops over every commit message and verifies the structure. It’ll terminate the job with exit code 1 if the incoming message doesn’t match the expected structure. Otherwise, the script will gracefully terminate the job with exit code 0. In the main ci.yml file the audit.yml workflow is called like this: ... jobs: audit: uses: rednafi/reflections/.github/workflows/audit.yml@master ... The ci.yml file roughly looks like this: name: CI on: push: pull_request: # Everyday at 0:37 UTC. schedule: - cron: \"37 0 * * *\" # Cancel any running workflow if the CI gets triggered again. concurrency: group: ${{ github.head_ref || github.run_id }} cancel-in-progress: true jobs: audit: uses: rednafi/reflections/.github/workflows/audit.yml@master build: needs: [\"audit\"] runs-on: ubuntu-latest steps: ... test: needs: [\"build\"] runs-on: ubuntu-latest steps: ... deploy: needs: [\"deploy\"] runs-on: ubuntu-latest steps: ... Here the needs: [\"audit\"] node in the build section ensures that the build will only trigger if the audit job passes successfully. Otherwise, none of the build, test, or deploy jobs will run and the CI will fail with a non-zero exit code. Here’s the fully working ci.yml6 file. Notes GitHub Actions terminology can be confusing. A workflow is a separate file that contains one or more jobs. A job is a set of steps in a workflow that executes on the same runner. A runner is a server that runs your workflows when they’re triggered. Each runner can run a single job at a time. A reusable workflow can be called from another workflow file. The docs have more information on the terminologies7. How I build a feature - Simon Willison ↩︎ Example issue that reflects the pattern explained here ↩︎ Worflows directory of this blog ↩︎ Automerge Dependabot PRs on GitHub ↩︎ Reusing workflows ↩︎ The main CI file of this blog ↩︎ Understanding GitHub Actions ↩︎ ","permalink":"http://rednafi.com/misc/audit_commit_messages_on_github/","publishDate":"2022-10-06","summary":"After reading Simon Willison’s amazing piece1 on how he adds new features to his open-source softwares, I wanted to adopt some of the good practices and incorporate them into my own workflow. One of the highlights of that post was how to kick off a feature work. The process roughly goes like this:\nOpening a new GitHub issue for the feature in the corresponding repository.\nAdding a rough description of the feature to the issue.\n","tags":["GitHub"],"title":"Auditing commit messages on GitHub"},{"content":"My grug1 brain can never remember the correct semantics of quoting commands and variables in a UNIX shell environment. Every time I work with a shell script or run some commands in a Docker compose file, I’ve to look up how to quote things properly to stop my ivory tower from crashing down. So, I thought I’d list out some of the most common rules that I usually look up all the time.\nI mostly work with bash; so that’s what I’ll focus on. However, the rules should be similar for any POSIX compliant shell.\nSingle quotes vs double quotes vs backticks Use single quotes when you don’t want your shell to expand variables. For example:\necho '$HOST' This prints:\n'$HOST' In the previous snippet, the single quotes ensure that the value of the HOST variable doesn’t get expanded by the shell and instead the literal name of the variable is used. On the contrary, your shell will evaulate the variable if you use double quotes here:\necho \"$HOST\" xps In this case, the command prints the name of my host machine. Lastly, a backtick pair is used to open a subshell and run some command. The following command allows you to check out to the HEAD-1th commit in Git:\ngit checkout `git rev-parse --short HEAD~1` In the above command, first, the command within the backtick runs in a subshell and then returns the result to the main shell. The git checkout part of the command in the main shell then uses the output value of the git rev-parse --short HEAD~1 sub-command to carry out the intended action.\nWhile this works, `...` is the legacy2 syntax for command substitution, required by only the very oldest of non-POSIX-compatible Bourne shells. A better alternative is to use the $(...) syntax.\ngit checkout $(git rev-parse --short HEAD~1) When to quote variables Quote if the variable can either be empty or contain any whitespace or special characters like spaces, backslashs or wildcards. Not quoting strings with spaces often leads to the shell breaking apart a single argument into many. Consider this command:\nexport x=some filename echo $x This will print:\nsome Ideally, this should’ve returned some filename. You can fix this by quoting the value:\nexport x=\"some filename\" echo $x some filename In the shell environment, the value of a variable is delimited by space. So if the value of your variable contains a space, it won’t work correctly unless you quote it properly. This can also happen while accepting a value from a user and assigning it to a variable. For example:\nread -p \"Enter the name of a file: \" file; cat $file If the user provides a file name that contains a space or any special character like *, ? or /, the command above will behave unexpectedly. To ensure that the cat is applied on a single file, wrap the file variable with double quotes.\nread -p \"Enter the name of a file: \" file; cat \"$file\" Instead of double quotes, if you wrap the variable with single quotes, the command will try to apply cat on a file that’s literally named $file which is most likely not what you want.\nGrug brained developer ↩︎\nWhy is $(…) preferred over ... (backticks)? ↩︎\n","permalink":"http://rednafi.com/misc/to_quote_or_not_to_quote/","publishDate":"2022-10-05","summary":"My grug1 brain can never remember the correct semantics of quoting commands and variables in a UNIX shell environment. Every time I work with a shell script or run some commands in a Docker compose file, I’ve to look up how to quote things properly to stop my ivory tower from crashing down. So, I thought I’d list out some of the most common rules that I usually look up all the time.\n","tags":["Shell"],"title":"To quote or not to quote"},{"content":"TIL that returning a value from a function in bash doesn’t do what I thought it does. Whenever you call a function that’s returning some value, instead of giving you the value, Bash sets the return value of the callee as the status code of the calling command. Consider this example:\n#!/usr/bin/bash # script.sh return_42() { return 42 } # Call the function and set the return value to a variable. value=$return_42 # Print the return value. echo $value I was expecting this to print out 42 but instead it doesn’t print anything to the console. Turns out, a shell function doesn’t return the value when it encounters the return keyword. Rather, it stops the execution of the function and sets the status code of the last command in the function as the value that the function returns.\nTo test it out, you can print out the status code of the last command when a script exits with echo $?. Here’s the same snippet from the previous section where the last line is the command that calls the return_42 function:\n#!/usr/bin/bash # script.sh return_42() { return 42 } # Call the function. return_42 Run the snippet and print the exit code of the last line of the script with the following command:\n./script.sh; echo $? This prints out:\n42 Status code evaluation pattern Here’s one pattern that you can use whenever you need to return a value from a shell function. In the following snippet, I’m evaluating whether a number provided by the user is a prime or not and printing out a message accordingly:\n#!/usr/bin/bash # script.sh # Check whether a number is prime or not. is_prime(){ factor_count=$(factor $1 | wc -w) if [[ $factor_count -eq 2 ]]; then return 0 # Sets the status code to 0. else return 1 # Any non-zero value will work here. fi } # Call the function. is_prime $1 # Inspect the status code. status=$? # Print message according to the status code. if [[ $status -eq 0 ]]; then echo \"$1 is prime.\" else echo \"$1 is not prime.\" fi Since the returned values are treated as status codes where 0 is used to denote no error and a non-zero value represents an error, you’ll need to return 0 as a truthy value and 1 as a falsy value. While this works, returning 0 to denote a truthy value is the opposite of what you’d usually do in other programming languages and can confuse someone who might not be familiar with shell quirks. If you only need to return a boolean value from a function, here’s a better pattern:\n#!/usr/bin/bash # script.sh # Check whether a number is prime or not. is_prime(){ factor_count=$(factor $1 | wc -w) if [[ $factor_count -eq 2 ]]; then true else false fi } # Call the function. is_prime $1 # Inspect the status code. status=$? # Print message according to the status code. if [[ $status -eq 0 ]]; then echo \"$1 is prime.\" else echo \"$1 is not prime.\" fi In this snippet, notice how the is_prime function doesn’t explicitly return anything. Instead, it just adds the true or false expression to the end of the return path accordingly. This implicitly sets the status code to 0 when the input number is a prime and to 1 when it’s not. The rest of the status checking works the same as in the previous script.\nThe second pattern won’t work if you need to set the status code to something other than 0 or 1. In that case you can resort the first pattern without confusing anyone.\nReturning a boolean from a Bash function 1 ↩︎\n","permalink":"http://rednafi.com/misc/return_values_from_a_shell_function/","publishDate":"2022-09-25","summary":"TIL that returning a value from a function in bash doesn’t do what I thought it does. Whenever you call a function that’s returning some value, instead of giving you the value, Bash sets the return value of the callee as the status code of the calling command. Consider this example:\n#!/usr/bin/bash # script.sh return_42() { return 42 } # Call the function and set the return value to a variable. value=$return_42 # Print the return value. echo $value I was expecting this to print out 42 but instead it doesn’t print anything to the console. Turns out, a shell function doesn’t return the value when it encounters the return keyword. Rather, it stops the execution of the function and sets the status code of the last command in the function as the value that the function returns.\n","tags":["Shell","TIL"],"title":"Returning values from a shell function"},{"content":"While working with GitHub webhooks, I discovered a common pattern1 a webhook receiver can adopt to verify that the incoming webhooks are indeed arriving from GitHub; not from some miscreant trying to carry out a man-in-the-middle attack. After some amount of digging, I found that it’s quite a common practice that many other webhook services employ as well. Also, check out how Sentry does it here2. Moreover, GitHub’s documentation demonstrates the pattern in Ruby. So I thought it’d be a good idea to translate that into Python in a more platform-agnostic manner. The core idea of the pattern goes as follows: The webhook sender will hash the JSONified webhook payload with a well-known hashing algorithm like MD5, SHA-1, or SHA-256. A secret token known to the receiver will be used to sign the calculated hash of the payload. The sender will include the payload hash digest prefixed by the name of the hash algorithm to the header of the webhook request. For example, the GitHub webhook’s request header has a key like the following. Notice how the digest is prefixed with the name of the algorithm sha256: X-Hub-Signature-256=\\ sha-256=e863e1f6370b60981bbbcbc2da3313321e65eaaac36f9d1262af415965df9320 The webhook receiver is then expected to hash the received JSON payload with the same algorithm found in the prefix of the header and sign with the common secret token known to both the sender and the receiver. Afterward, the receiver compares the calculated hash with the incoming hash in the request header. If the two digests match, that ensures that the payload hasn’t been tampered with. Otherwise, the receiver should reject the incoming payload. This provides a second layer of protection over the usual authentication that the receiver might have in place. To demonstrate the workflow, here’s an example of how the webhook sender might be implemented: # sender.py from __future__ import annotations import hashlib import json from http import HTTPStatus import httpx from starlette.applications import Starlette from starlette.requests import Request from starlette.responses import JSONResponse from starlette.routing import Route async def send_webhook(request: Request) -\u003e JSONResponse: # Get the request body as bytes. raw_body = await request.body() # Disallow empty body. if not raw_body: return JSONResponse( {\"error\": \"Empty body\"}, status_code=HTTPStatus.BAD_REQUEST, ) # Check that the request body is a valid JSON payload. try: body = json.loads(raw_body) except json.JSONDecodeError: return JSONResponse( {\"error\": \"Invalid JSON body\"}, status_code=HTTPStatus.BAD_REQUEST, ) # Hash the body and sign it with a secret. x_payload_signature = hashlib.sha256(raw_body) x_payload_signature.update(b\"some-secret\") x_payload_signature = x_payload_signature.hexdigest() # Send the webhook. async with httpx.AsyncClient() as client: response = await client.post( \"http://localhost:6000/receive-webhook\", json=body, headers={ \"X-Payload-Signature-256\": f\"sha256={x_payload_signature}\", \"Content-Type\": \"application/json\", }, ) if response.status_code != HTTPStatus.ACCEPTED: return JSONResponse( {\"error\": \"Could not sent webhook\"}, status_code=HTTPStatus.BAD_REQUEST, ) return JSONResponse( { \"message\": \"Webhook sent\", \"response_payload\": response.json(), }, status_code=HTTPStatus.OK, ) app = Starlette( debug=True, routes=[ Route(\"/send-webhook\", send_webhook, methods=[\"POST\"]), ], ) Here, I’ve implemented a simple POST API that: Accepts a payload from the user. Hashes the payload with sha-256 algorithm and signs it with a some-secret token. Adds the digest to the request header to the receiver. The header has a key called X-Payload-Signature-256 that contains the prefixed payload digest: X-Payload-Signature-256: \\ sha-256=e863e1f6370b60981bbbcbc2da3313321e65eaaac36f9d1262af415965df9320 After hashing, the sender sends the payload to the receiver via HTTP POST request. Here, I’m using HTTPx to send the request to the receiver. For demonstration purposes, I’m assuming that the receiver endpoint is localhost:6000/receive-webhook. The receiver will: Accept the incoming request from the sender. Parse the header and store the value of X-Payload-Signature-256. Calculate the hash value of the incoming payload in the same manner as the sender. Sign the payload with the common secret that’s known to both parties. Compare the newly calculated signed-hash with the digest value of the X-Payload-Signature-256 attribute. Only accept and process the payload if the incoming and the computed hashes match. Here’s how you can implement the receiver: # receiver.py from __future__ import annotations import hashlib import json import secrets from http import HTTPStatus from starlette.applications import Starlette from starlette.requests import Request from starlette.responses import JSONResponse from starlette.routing import Route async def receive_webhook(request: Request) -\u003e JSONResponse: # Get the payload signature from the request headers. x_payload_signature_256 = request.headers.get(\"X-Payload-Signature-256\") # Disallow empty signature. if x_payload_signature_256 is None: return JSONResponse( {\"error\": \"Missing X-Payload-Signature header\"}, status_code=HTTPStatus.BAD_REQUEST, ) # Check that the signature is valid. if not x_payload_signature_256.startswith(\"sha256=\"): return JSONResponse( {\"error\": \"Invalid X-Payload-Signature header\"}, status_code=HTTPStatus.BAD_REQUEST, ) # Get x_payload_signature_256 without the \"sha256=\" prefix. x_payload_signature = x_payload_signature_256.removeprefix(\"sha256=\") raw_body = await request.body() # Disallow empty body. if not raw_body: return JSONResponse( {\"error\": \"Empty body\"}, status_code=HTTPStatus.BAD_REQUEST, ) # Check that the request body is a valid JSON payload. try: body = json.loads(raw_body) except json.JSONDecodeError: return JSONResponse( {\"error\": \"Invalid JSON body\"}, status_code=HTTPStatus.BAD_REQUEST, ) # Hash the incoming body with the secret. expected_signature = hashlib.sha256(raw_body) expected_signature.update(b\"some-secret\") expected_signature = expected_signature.hexdigest() # Compare the expected signature with the incoming signature. if ( secrets.compare_digest(x_payload_signature, expected_signature) is False ): return JSONResponse( {\"error\": \"Invalid signature\"}, status_code=HTTPStatus.UNAUTHORIZED, ) return JSONResponse( {\"message\": \"Webhook accepted\"}, status_code=HTTPStatus.ACCEPTED, ) app = Starlette( debug=True, routes=[ Route(\"/receive-webhook\", receive_webhook, methods=[\"POST\"]), ], ) In the receiver, instead of using plain string comparison to compare the payload hashes, leverage secrets.compare_digest to mitigate the possibility of timing attacks3. To test the end-to-end workflow, you’ll need to pip install httpx4 and uvicorn5. Then on your console, you can run the two scripts in the background with the following command: nohup uvicorn sender:app --reload --port 5000 \u003e /dev/null \\ \u0026 nohup uvicorn receiver:app --reload --port 6000 \u003e /dev/null \u0026 This will spin up two uvicorn servers in the background where the sender and the receiver can be accessed via ports 5000 and 6000 respectively. Now if you make a request to the sender service, you’ll see that the sender sends the webhook payload to the receiver service and returns an HTTP 200 code only if the receiver has been able to verify the signed-hash of the payload: curl -si POST http://localhost:5000/send-webhook -d '{\"hello\": \"world\"}' This will return: HTTP/1.1 200 OK date: Tue, 20 Sep 2022 06:31:07 GMT server: uvicorn content-length: 76 content-type: application/json {\"message\":\"Webhook sent\",\"response_payload\":{\"message\":\"Webhook accepted\"}} The reciver will return a HTTP 400 error code if it can’t verify the payload. Once you’re done, kill the running servers with sudo pkill uvicorn command. Securing your webhooks ↩︎ Sentry hook resources ↩︎ Timing attack ↩︎ HTTPx ↩︎ Uvicorn ↩︎ ","permalink":"http://rednafi.com/python/verify_webhook_origin/","publishDate":"2022-09-18","summary":"While working with GitHub webhooks, I discovered a common pattern1 a webhook receiver can adopt to verify that the incoming webhooks are indeed arriving from GitHub; not from some miscreant trying to carry out a man-in-the-middle attack. After some amount of digging, I found that it’s quite a common practice that many other webhook services employ as well. Also, check out how Sentry does it here2.\nMoreover, GitHub’s documentation demonstrates the pattern in Ruby. So I thought it’d be a good idea to translate that into Python in a more platform-agnostic manner. The core idea of the pattern goes as follows:\n","tags":["Python","API"],"title":"Verifying webhook origin via payload hash signing"},{"content":"While going through the documentation of Python’s sqlite31 module, I noticed that it’s quite API-driven, where different parts of the module are explained in a prescriptive manner. I, however, learn better from examples, recipes, and narratives. Although a few good recipes already exist in the docs, I thought I’d also enlist some of the examples I tried out while grokking them. Executing individual statements To execute individual statements, you’ll need to use the cursor_obj.execute(statement) primitive. # src.py import sqlite3 conn = sqlite3.connect(\":memory:\") c = conn.cursor() with conn: c.execute( \"\"\" create table if not exists stat (id integer primary key, cat text, score real); \"\"\" ) c.execute(\"\"\"insert into stat (cat, score) values ('a', 1.0);\"\"\") c.execute(\"\"\"insert into stat (cat, score) values ('b', 2.0);\"\"\") result = c.execute(\"\"\"select * from stat;\"\"\").fetchall() print(result) [(1, 'a', 1.0), (2, 'b', 2.0)] Executing batch statements You can bundle up multiple statements and execute them in a single go with the cursor_obj.executemany(template_statement, (data, ...)) API. # src.py import sqlite3 conn = sqlite3.connect(\":memory:\") c = conn.cursor() with conn: c.execute( \"\"\" create table if not exists stat (id integer primary key, cat text, score real); \"\"\" ) # Data needs to be passed as an iterable of tuples. data = ( (\"a\", 1.0), (\"b\", 2.0), (\"c\", 3.0), ) c.executemany(\"insert into stat (cat, score) values (?, ?);\", data) result = c.execute(\"\"\"select * from stat;\"\"\").fetchall() print(result) [(1, 'a', 1.0), (2, 'b', 2.0), (3, 'c', 3.0)] Applying user-defined callbacks You can define and apply arbitrary Python callbacks to different data points in an SQLite table. There are two types of callbacks that you can apply: Scalar function: A scalar function returns one value per invocation; in most cases, you can think of this as returning one value per row. Aggregate function: In contrast, an aggregate function returns one value per group of rows. Applying user-defined scalar functions In the following example, I’ve created a table called users with two text type columns—username and password. Here, we define a transformation scalar function named sha256 that applies sha256 hashing to all the elements of the password column. The function is then registered via the connection_obj.create_function(func_name, narg, func) API. # src.py import sqlite3 import hashlib conn = sqlite3.connect(\":memory:\") c = conn.cursor() def sha256(t: str) -\u003e str: return hashlib.sha256( t.encode(\"utf-8\"), usedforsecurity=True, ).hexdigest() # Register the scalar function. conn.create_function(\"sha256\", 1, sha256) with conn: c.execute( \"\"\" create table if not exists users ( username text, password text ); \"\"\" ) c.execute( \"insert into users values (?, sha256(?));\", (\"admin\", \"password\"), ) c.execute( \"insert into users values (?, sha256(?));\", (\"user\", \"otherpass\"), ) result = c.execute(\"select * from users;\").fetchall() print(result) [ ( 'admin', '5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8'), ( 'user', '0da86a02c6944c679c5a7f06418bfde6bddb445de708639a3131af3682b34108' ) ] Applying user-defined aggregate functions Aggregate functions are defined as classes and then registered with the connection_obj.create_aggregate(func_name, narg, aggregate_class) API. In the example below, I’ve created a table called series with a single integer type column val. To define an aggregate function, we’ll need to write a class with two methods—step and finalize where step will return the value of an intermediary progression step and finalize will return the final result. Below, you can see that the aggregate function returns a single value in the output. # src.py import sqlite3 import hashlib conn = sqlite3.connect(\":memory:\") c = conn.cursor() class Mult: def __init__(self): self._result = 1 def step(self, value): self._result *= value def finalize(self): return self._result # Register the aggregate class. conn.create_aggregate(\"mult\", 1, Mult) with conn: c.execute( \"\"\" create table if not exists series ( val integer ); \"\"\" ) c.execute(\"insert into series (val) values (?);\", (2,)) c.execute(\"insert into series (val) values (?);\", (3,)) result = c.execute(\"select mult(val) from series;\").fetchall() print(result) [(6,)] Printing traceback when a user-defined callback raises an error By default, sqlite3 will suppress the traceback of any error raised from an user-defined function. However, you can turn on the traceback option as follows: # src import sqlite3 sqlite3.enable_callback_tracebacks(True) ... Transforming types Conventionally, Python sqlite3 documentation uses the term adaptation to refer to the transformation that changes Python types to SQLite types and conversion to refer to the change in the reverse direction. Adapting Python types to SQLite types To transform Python types to native SQLite types, you’ll need to define a transformation callback that’ll carry out the task. Then the callback will need to be registered with the sqlite3.register_adapter(type, adapter_callback) API. Here, I’ve created an in-memory table called colors with a single text type column name that refers to the name of the color. Then I register the lambda color: color.value anonymous function that serializes an enum value to a text value. This allows me to pass an enum member directly into the cursor_obj.execute method. # src.py import enum import sqlite3 conn = sqlite3.connect(\":memory:\") c = conn.cursor() class Color(enum.Enum): RED = \"red\" GREEN = \"green\" BLUE = \"blue\" # Register an adapter to transform a Python type to an SQLite type. sqlite3.register_adapter(Color, lambda color: color.value) with conn: c.execute( \"\"\" create table if not exists colors ( name integer ); \"\"\" ) c.execute(\"insert into colors (name) values (?);\", (Color.RED,)) c.execute(\"insert into colors (name) values (?);\", (Color.GREEN,)) result = c.execute(\"select name from colors;\").fetchall() print(result) [('red',), ('green',)] Converting SQLite types to Python types Converting SQLite types to Python types works similarly to the previous section. Here, as well, I’ve created the same colors table with a single name column as before. But this time, I want to insert string values into the name column and get back native enum objects from that field while performing a get query. To do so, I’ve registered a converter function with the sqlite3.register_converter(\"sqlite_type_as_a_string\", converter_callback) API. Another point to keep in mind is that you’ll have to set detect_type=sqlite3.PARSE_DECLTYPES in the sqlite3.connection method for the adaptation to work. Notice the output of the last select ... statement and you’ll see that we’re getting enum objects in the returned list. # src.py import enum import sqlite3 class Color(enum.Enum): RED = \"red\" GREEN = \"green\" BLUE = \"blue\" color_map = {v.value: v for v in Color.__members__.values()} # Register a convert to convert text to Color enum sqlite3.register_converter( \"text\", lambda v: color_map[v.decode(\"utf-8\")], ) conn = sqlite3.connect( \":memory:\", detect_types=sqlite3.PARSE_DECLTYPES, # Parse declaration types. ) c = conn.cursor() with conn: c.execute( \"\"\" create table if not exists colors ( name text ); \"\"\" ) c.execute(\"insert into colors (name) values (?);\", (\"red\",)) c.execute(\"insert into colors (name) values (?);\", (\"green\",)) c.execute(\"insert into colors (name) values (?);\", (\"blue\",)) result = c.execute(\"select name from colors;\").fetchall() print(result) [ (","permalink":"http://rednafi.com/python/recipes_from_python_sqlite_docs/","publishDate":"2022-09-11","summary":"While going through the documentation of Python’s sqlite31 module, I noticed that it’s quite API-driven, where different parts of the module are explained in a prescriptive manner. I, however, learn better from examples, recipes, and narratives. Although a few good recipes already exist in the docs, I thought I’d also enlist some of the examples I tried out while grokking them.\nExecuting individual statements To execute individual statements, you’ll need to use the cursor_obj.execute(statement) primitive.\n","tags":["Database","Python"],"title":"Recipes from Python SQLite docs"},{"content":"TIL from this1 video that Python’s urllib.parse.urlparse2 is quite slow at parsing URLs. I’ve always used urlparse to destructure URLs and didn’t know that there’s a faster alternative to this in the standard library. The official documentation also recommends the alternative function.\nThe urlparse function splits a supplied URL into multiple seperate components and returns a ParseResult object. Consider this example:\nIn [1]: from urllib.parse import urlparse In [2]: url = \"https://httpbin.org/get?q=hello\u0026r=22\" In [3]: urlparse(url) Out[3]: ParseResult( scheme='https', netloc='httpbin.org', path='/get', params='', query='q=hello\u0026r=22', fragment='' ) You can see how the function disassembles the URL and builds a ParseResult object with the URL components. Along with this, the urlparse function can also parse an obscure type of URL that you’ll most likely never need. If you notice closely in the previous example, you’ll see that there’s a params argument in the ParseResult object. This params argument gets parsed whether you need it or not and that adds some overhead. The params field will be populated if you have a URL like this:\nIn [1]: from urllib.parse import urlparse In [2]: url = \"https://httpbin.org/get;a=mars\u0026b=42?q=hello\u0026r=22\" In [3]: urlparse(url) Out[4]: ParseResult( scheme='https', netloc='httpbin.org', path='/get', params='a=mars\u0026b=42', query='q=hello\u0026r=22', fragment='' ) Notice the parts in the URL that appears after https://httpbin.org/get. There’s a semicolon and a few more parameters succeeding that—;a=mars\u0026b=42. The resulting ParseResult now has the params field populated with the parsed param value a=mars\u0026b=42. Unless you need this param support, there’s a better and faster alternative to this in the standard library. The urlsplit3 function does the same thing as urlparse minus the param parsing and is twice as fast. Here’s how you’d use urlsplit:\nIn [1]: from urllib.parse import urlsplit In [2]: url = \"https://httpbin.org/get?q=hello\u0026r=22\" In [3]: urlsplit(url) Out[3]: SplitResult( scheme='https', netloc='httpbin.org', path='/get', query='q=hello\u0026r=22', fragment='' ) The urlsplit function returns a SplitResult object similar to the ParseResult object you’ve seen before. Notice there’s no param argument in the output here. I measured the speed difference like this:\nIn [1]: from urllib.parse import urlparse, urlsplit In [2]: url = \"https://httpbin.org/get?q=hello\u0026r=22\" In [3]: %timeit urlparse(url) 1.7 µs ± 2.91 ns per loop ( mean ± std. dev. of 7 runs, 1,000,000 loops each) In [4]: %timeit urlsplit(url) 885 ns ± 10.9 ns per loop ( mean ± std. dev. of 7 runs, 1,000,000 loops each) Wow, that’s almost 2x speed improvement. Although this shouldn’t be much of an issue in a real codebase but it can matter if you are parsing URLs in a critical hot path.\npython: don’t use urlparse - Anthony Sottile ↩︎\nurlparse ↩︎\nurlsplit ↩︎\n","permalink":"http://rednafi.com/python/use_urlsplit_over_urlparse/","publishDate":"2022-09-10","summary":"TIL from this1 video that Python’s urllib.parse.urlparse2 is quite slow at parsing URLs. I’ve always used urlparse to destructure URLs and didn’t know that there’s a faster alternative to this in the standard library. The official documentation also recommends the alternative function.\nThe urlparse function splits a supplied URL into multiple seperate components and returns a ParseResult object. Consider this example:\nIn [1]: from urllib.parse import urlparse In [2]: url = \"https://httpbin.org/get?q=hello\u0026r=22\" In [3]: urlparse(url) Out[3]: ParseResult( scheme='https', netloc='httpbin.org', path='/get', params='', query='q=hello\u0026r=22', fragment='' ) You can see how the function disassembles the URL and builds a ParseResult object with the URL components. Along with this, the urlparse function can also parse an obscure type of URL that you’ll most likely never need. If you notice closely in the previous example, you’ll see that there’s a params argument in the ParseResult object. This params argument gets parsed whether you need it or not and that adds some overhead. The params field will be populated if you have a URL like this:\n","tags":["Python"],"title":"Prefer urlsplit over urlparse to destructure URLs"},{"content":"Python has a random.choice routine in the standard library that allows you to pick a random value from an iterable. It works like this: # src.py import random # The seed ensures that you'll get the same random choice # every time you run the script. random.seed(90) # This builds a list: [\"choice_0\", \"choice_1\", ..., \"choice_9\"] lst = [f\"choice_{i}\" for i in range(10)] print(random.choice(lst)) print(random.choice(lst)) This will print: choice_3 choice_1 I was looking for a way to quickly hydrate a table with random data in an SQLite database. To be able to do so, I needed to extract unpremeditated values from an array of predefined elements. The issue is, that SQLite doesn’t support array types or have a built-in function to pick random values from an array. However, recently I came across this1 trick from Ricardo Ander-Egg’s tweet2, where he exploits SQLite’s JSON support to parse an array. This idea can be further extended to pluck random values from an array. To extract values from any JSON object in SQLite, you can use the json_extract function. Start a SQLite CLI session and run the following query: select json_extract( '{\"greetings\": [\"Hello\", \"Hola\", \"Ohe\"]}', '$.greetings[2]' ) This will give you an output as follows: Ohe The above query parses the JSON object inside the json_extract function and extracts the last element from the greetings array. If you want to know more details about how you can extract specific elements from JSON objects, head over to the SQLite docs on this topic3. You can pick any value from a JSON array by its index: select json_extract( '[\"Columbus\", \"Cincinnati\", \"Dayton\", \"Toledo\"]', '$[2]' ) Dayton Now, how do we extract random elements from the above array? If we can generate a set of random indices, those can be used to access values arbitrarily from the JSON array. These random indices can be generated using SQLite’s built-in random() function. The function doesn’t take any arguments and generates a large positive or negative arbitrary integer. From this integer, a random index can be found by computing abs(random()) modulo n where abs(random()) denotes the absolute result of the random function and n represents the length of the target array. For example, if the length of the array is 4, and random() produces the integer -123456789, then the index will be 123456789 % 4 = 1 : select abs(random()) % 4; If you run this query multiple times, you’ll see that it prints a value between 0 and 3 in random order. sqlite\u003e select abs(random()) % 4; 0 sqlite\u003e select abs(random()) % 4; 3 sqlite\u003e select abs(random()) % 4; 2 sqlite\u003e select abs(random()) % 4; 1 Similarly, if you compute abs(random()) % 5, it’ll print a value between 0 to 4 and so on. Armed with this knowledge, we can extract a random value from a JSON array like this: select json_extract( '[\"Columbus\", \"Cincinnati\", \"Dayton\", \"Toledo\"]', '$[' || cast(abs(random()) % 4 as text) || ']' ); Running the above query will give you a single value from the JSON array in random order. Execute the query multiple times to see it in action. sqlite\u003e select json_extract('[\"Columbus\", ... Toledo sqlite\u003e select json_extract('[\"Columbus\", ... Cincinnati sqlite\u003e select json_extract('[\"Columbus\", ... Columbus Voila, we’ve successfully emulated Python’s random.choice in SQL. Populating a table with random data Populating a table with randomly distributed data is useful, especially when you need to demonstrate a feature or flex your SQL fu. We can leverage the above pattern to populate a simple table with 100 data points like this: -- Create the 'stat' table with 'id', 'cat', and 'score' columns. create table if not exists stat ( id integer primary key, cat text, score real ); -- Populate the 'stat' table with random data. with recursive cte (x, y) as ( select 'a', random() % 1000 union all select json_extract ('[\"a\", \"b\", \"c\"]', '$[' || cast(abs(random()) % 3 as text) || ']'), random() % 1000 from cte limit 100) insert into stat (cat, score) select * from cte where not exists ( -- This block ensures that the query select * -- can be run multiple times without from stat -- any side effects. If you run this where stat.cat = cte.x -- multiple times, it'll or stat.score = cte.x); -- only insert the values once. -- Inspect the populated table. select * from stat; If you run the above queries via the SQLite CLI, the final statement will reveal the stat table with the randomly filled in data: | id | cat | score | |-----|-----|--------| | 1 | a | 390.0 | | 2 | a | 864.0 | | 3 | b | -856.0 | | 4 | b | -307.0 | | 5 | c | -405.0 | | 6 | a | -61.0 | | 7 | a | 794.0 | | 8 | b | -560.0 | | 9 | a | -355.0 | | 10 | c | 10.0 | ... | 100 | c | 420.0 | Passing arrays as parameters to SQLite ↩︎ Passing lists of values to SQLite ↩︎ The json_extract() function ↩︎ ","permalink":"http://rednafi.com/database/random_choice_in_sqlite/","publishDate":"2022-09-02","summary":"Python has a random.choice routine in the standard library that allows you to pick a random value from an iterable. It works like this:\n# src.py import random # The seed ensures that you'll get the same random choice # every time you run the script. random.seed(90) # This builds a list: [\"choice_0\", \"choice_1\", ..., \"choice_9\"] lst = [f\"choice_{i}\" for i in range(10)] print(random.choice(lst)) print(random.choice(lst)) This will print:\nchoice_3 choice_1 I was looking for a way to quickly hydrate a table with random data in an SQLite database. To be able to do so, I needed to extract unpremeditated values from an array of predefined elements. The issue is, that SQLite doesn’t support array types or have a built-in function to pick random values from an array. However, recently I came across this1 trick from Ricardo Ander-Egg’s tweet2, where he exploits SQLite’s JSON support to parse an array. This idea can be further extended to pluck random values from an array.\n","tags":["Database","SQL"],"title":"Pick random values from an array in SQL(ite)"},{"content":"Over the years, I’ve used Python’s contextlib.ExitStack in a few interesting ways. The official documentation1 advertises it as a way to manage multiple context managers and has a couple of examples of how to leverage it. However, neither in the docs nor in GitHub code search2 I could find examples of some of the maybe unusual ways I’ve used it in the past. So, I thought I’d document them here. Enforcing request level transaction While consuming APIs, it’s important to handle errors in a way that prevents database state corruption. In the following example, I’m making two POST requests to an API and rolling back to the original state if any one of them fails: # src.py from __future__ import annotations import logging import uuid from contextlib import ExitStack from http import HTTPStatus import httpx logging.basicConfig(level=logging.INFO) def group_create(uuid_a: str, uuid_b: str) -\u003e tuple[httpx.Response, ...]: with httpx.Client() as client: url = \"https://httpbin.org/post\" response_a = client.post( url, json={\"uuid\": uuid_a, \"foo\": \"bar\"}, ) response_b = client.post( url, json={\"uuid\": uuid_b, \"fizz\": \"bazz\"}, ) return response_a, response_b def maybe_rollback( uuid: str, incoming_status_code: int, expected_status_code: int = HTTPStatus.OK, ) -\u003e None: if incoming_status_code != expected_status_code: logging.info(f\"Rolling back request: {uuid}\") url = f\"https://httpbin.org/delete?uuid={uuid}\" response = httpx.delete(url) assert response.status_code == HTTPStatus.OK else: logging.info(f\"Request {uuid} completed successfully.\") def main() -\u003e None: with ExitStack() as stack: uuid_a = str(uuid.uuid4()) uuid_b = str(uuid.uuid4()) response_a, response_b = group_create(uuid_a, uuid_b) stack.callback( maybe_rollback, uuid=uuid_a, incoming_status_code=response_a.status_code, ) stack.callback( maybe_rollback, uuid=uuid_b, incoming_status_code=response_b.status_code, ) if __name__ == \"__main__\": main() Running this will print the following output: INFO:root:Request fec8fc9f-7762-4d53-b8f9-3dc7802108a4 completed successfully. INFO:root:Request 4b6ed0ed-b7cf-46f0-9374-85627be4c26c completed successfully. Here, the group_create function makes two calls to POST httpbin.org/post endpoint and the maybe_rollback function deletes the created record if any one of the two requests fails. In the main function, I’ve used the ExitStack.callback method to register the maybe_rollback callback. If you change the expected_status_code in the maybe_rollback function to something like HTTPStatus.FORBIDDEN, you’ll be able to see the cleanup callbacks in action: INFO:root:Rolling back request: 50eb2734-f84c-4013-b5f6-0ccf1aa5d79a INFO:root:Rolling back request: b326e567-a006-4648-bf04-202397f44e31 Invoking conditional event hooks The same strategy used in the previous section can be applied to invoke event hooks conditionally. For example, let’s say you want to run a callback function when some event function executes. However, you want only a particular type of callback function to be executed depending on the state of your conditionals or code path. I’ve found the following pattern useful in this case: # src.py from __future__ import annotations from contextlib import ExitStack from typing import Any class EventHook: def __init__(self, event_name: str) -\u003e None: self.event_name = event_name self.dispatch_config = { \"success\": self.on_success, \"failure\": self.on_failure, } def on_success(self) -\u003e None: print(f\"'{self.event_name}' hook called\") def on_failure(self) -\u003e None: print(f\"'{self.event_name}' hook called\") def __call__(self) -\u003e Any: return self.dispatch_config[self.event_name]() def successful_event() -\u003e None: print(\"'successful_event' executed\") def failed_event() -\u003e None: print(\"'failed_event' executed\") 1 / 0 def main() -\u003e None: success_hook = EventHook(\"success\") failure_hook = EventHook(\"failure\") with ExitStack() as stack: try: # Run successful event and attach success hook. successful_event() stack.callback(success_hook) failed_event() except ZeroDivisionError: # When the failed even raises an error, attach failure hook. stack.callback(failure_hook) if __name__ == \"__main__\": main() 'successful_event' executed 'failed_event' executed 'failure' hook called 'success' hook called Here the .on_failure hook will only be called if there’s an error in your execution path raises an exception. Avoiding nested context structure It can get ugly pretty quickly when you start using multiple nested context managers. For example, if you need to open two files and copy content from one file to the other, you’d typically start two nested context managers and transfer the content like this: # src.py with open(\"file1.md\") as f1: with open(\"file2.md\") as f2: # Copy content from f1 to f2 and save it. ExitStack can help you get away with only one level of nesting here. Here’s a complete example: # src.py import io import shutil import tempfile from contextlib import ExitStack def copy_over( fsrc: io.IOBase, fdst: io.IOBase, skip_line: int = 0, ) -\u003e None: if skip_line \u003e 0: for _ in range(skip_line): fsrc.readline() shutil.copyfileobj(fsrc, fdst) def main() -\u003e None: with ExitStack() as stack: # Enter into the respective context managers without explicit # 'with' blocks. fsrc = stack.enter_context( tempfile.SpooledTemporaryFile(mode=\"rb\"), ) fdst = stack.enter_context( tempfile.SpooledTemporaryFile(mode=\"rb+\"), ) # Write some data to the source file. fsrc.write(b\"hello world\\nhello mars\") # Rewind the source file and copy it to the destination file. fsrc.seek(0) copy_over(fsrc, fdst, skip_line=1) # Rewind the destination file and assert the data. fdst.seek(0) assert fdst.read() == b\"hello mars\" # Rewind the dst file and print out the shape of the fdst # content. fdst.seek(0) print(fdst.read()) if __name__ == \"__main__\": main() This example creates two in-memory temporary file instances with tempfile.SpooledTemporaryFile. The SpooledTemporaryFile can be used as a context manager. However, instead of nesting the two instances, I’m using ExitStack.enter_context to enter into the context manager without explicitly using the with statement. This .enter_context method ensures that the __exit__ method of the respective context managers will be called properly at the end of the main() function run. Then in the body of the ExitStack, we’re writing some content to the first in-memory file and then copying the content to the other in-memory file. If we had to open and manage even more context managers, in this way, we’d be able to that without crating any additional nestings. Applying multiple patches as context managers Python’s unittest.mock.patch can be used as both decorators and context managers. For granular patching and unpatching during tests, the context manager approach gives you more control than its decorator counterpart. In this case, ExitStack can help you avoid multiple nestings just like in the previous section: # src.py from __future__ import annotations from contextlib import ExitStack from http import HTTPStatus from typing import Any from unittest.mock import patch import httpx def get(url: str) -\u003e dict[str, Any]: return httpx.get(url).json() def post(url: str, data: dict[str, Any]) -\u003e dict[str, Any]: return httpx.post(url, json=data).json() def main() -\u003e dict[str, Any]: res_get = get(\"https://httpbin.org/get\") res_post = post(\"https://httpbin.org/post\", {\"foo\": \"bar\"}) return {\"get\": res_get, \"post\": res_post} def test_main() -\u003e None: with ExitStack() as stack: # Arrange mock_httpx_get = stack.enter_context( patch( \"httpx.get\", autospec=True, return_value=httpx.Response( json={\"fizz\": \"bazz\"}, status_code=HTTPStatus.OK ), ), ) mock_httpx_post = stack.enter_context( patch( \"httpx.post\", autospec=True, return_value=httpx.Response( json={\"foo\": \"bar\"}, status_code=HTTPStatus.CREATED ), ) ) # Act res = main() # Assert assert res[\"get\"][\"fizz\"] == \"bazz\" assert res[\"post\"][\"foo\"] == \"bar\" assert mock_httpx_get.call_count == 1 assert mock_httpx_post.call_count == 1 Running the above snippet with pytest will reveal that the test passes without any error: src.py::test_main PASSED ======================= 1 passed in 0.11s ======================= Here, I’m making GET and POST requests with the httpx library and in the test_main function, the httpx.get and httpx.post callable are patched with the patch context manager. However, ExitStack allows me here to do it without creating additional nested with blocks. ExitStack ↩︎ GitHub code search ↩︎ ","permalink":"http://rednafi.com/python/exitstack/","publishDate":"2022-08-27","summary":"Over the years, I’ve used Python’s contextlib.ExitStack in a few interesting ways. The official documentation1 advertises it as a way to manage multiple context managers and has a couple of examples of how to leverage it. However, neither in the docs nor in GitHub code search2 I could find examples of some of the maybe unusual ways I’ve used it in the past. So, I thought I’d document them here.\n","tags":["Python"],"title":"ExitStack in Python"},{"content":"While reading the second version of Brian Okken’s pytest book1, I came across this neat trick to compose multiple levels of fixtures. Suppose, you want to create a fixture that returns some canned data from a database. Now, let’s say that invoking the fixture multiple times is expensive, and to avoid that you want to run it only once per test session. However, you still want to clear all the database states after each test function runs. Otherwise, a test might inadvertently get coupled with another test that runs before it via the fixture’s shared state. Let’s demonstrate this: # test_src.py import pytest @pytest.fixture(scope=\"session\") def create_files(tmp_path_factory): \"\"\"Fixture that creates files in the tmp_path/tmp directory, writes something stuff, then and returns the directory.\"\"\" directory = tmp_path_factory.mktemp(\"tmp\") for filename in (\"foo.txt\", \"bar.txt\", \"baz.txt\"): file = directory / filename file.write_text(\"Hello, World!\") yield directory def test_read_default_content(create_files): directory = create_files for filename in (\"foo.txt\", \"bar.txt\", \"baz.txt\"): file = directory / filename assert file.read_text() == \"Hello, World!\" def test_read_custom_content(create_files): directory = create_files for filename in (\"foo.txt\", \"bar.txt\", \"baz.txt\"): file = directory / filename file.write_text(\"Hello, Mars!\") assert file.read_text() == \"Hello, Mars!\" In the above snippet, we’ve created a session-scoped fixture called create_files that creates three files in a temporary directory, writes some content to them, and then yields the directory. Afterward, we write two tests where the first one tests the files’ default content and the second one writes some stuff to each of the file and then test their content. If we run this with pytest, both of the tests pass. However, if we change the order of the tests where the test_read_custom_content runs before test_read_default_content, pytest will raise an error: test_src.py .F [100%] ==================== FAILURES ==================== ____________________ test_read_default_content ____________________ create_files = PosixPath('/tmp/pytest-of-rednafi/pytest-33/tmp0') def test_read_default_content(create_files): directory = create_files for filename in (\"foo.txt\", \"bar.txt\", \"baz.txt\"): file = directory / filename \u003e assert file.read_text() == \"Hello, World!\" E AssertionError: assert 'Hello, Mars!' == 'Hello, World!' E - Hello, World! E + Hello, Mars! test_src.py:32: AssertionError ==================== short test summary info ==================== FAILED test_src.py::test_read_default_content - AssertionError: assert 'Hello, Mars!' == 'Hello, World!' Our tests behave differently when the order of their execution changes. This is bad. You should always make sure that running your tests randomly or reversely doesn’t change the outcome of the test run. You can use a plugin like pytest-reverse2 to change your test execution order. This happens because the data of the fixture create_files persists across multiple tests since it’s defined as a session-scoped fixture. Here, test_read_custom_content overwrites the default contents of the files and when the other test runs after this one, it can’t find the default content and hence raises an AssertionError. To fix this, we’ll need to make sure that the fixture’s state gets cleaned up after each test function executes. One way to achieve this is by making the create_files fixture function-scoped; instead of session-scoped. If you decorate create_files with @pytest.fixture(scope=\"function\") and then run the above snippet in a reverse manner, you’ll see that the error doesn’t occur this time. However, making the fixture function-scoped means, the fixture will be executed once before running each test function. This can be a deal breaker if the fixture has to perform some time-consuming setups. To solve this, we can keep the create_files fixture session-scoped and use another function-scoped fixture to clean up its state. This way, before running each test function, the function-scoped fixture will clean up the state of the session-scoped fixture. We can write the previous example as follows: # test_src.py import pytest @pytest.fixture(scope=\"session\") def create_files(tmp_path_factory): \"\"\"Fixture that creates files in the tmp_path/tmp directory, writes something stuff, then and returns the directory.\"\"\" directory = tmp_path_factory.mktemp(\"tmp\") for filename in (\"foo.txt\", \"bar.txt\", \"baz.txt\"): file = directory / filename file.write_text(\"Hello, World!\") yield directory @pytest.fixture(scope=\"function\") def get_files(create_files): yield create_files # Clean up the files after each test function runs. for filename in (\"foo.txt\", \"bar.txt\", \"baz.txt\"): file = create_files / filename file.write_text(\"Hello, World!\") def test_read_custom_content(get_files): directory = get_files for filename in (\"foo.txt\", \"bar.txt\", \"baz.txt\"): file = directory / filename file.write_text(\"Hello, Mars!\") assert file.read_text() == \"Hello, Mars!\" def test_read_default_content(get_files): directory = get_files for filename in (\"foo.txt\", \"bar.txt\", \"baz.txt\"): file = directory / filename assert file.read_text() == \"Hello, World!\" Notice that I’ve swapped the order of the tests just for demonstration purposes. Here, we’ve defined another fixture called get_files which is function-scoped. Underneath, get_files uses create_files to create the file contents and then cleans up the state after the yield statement. We could refactor some of the clean-up code to make it DRY but I intentionally kept it verbose for simplicity’s sake. In this case, the lighter get_files fixture gets executed before every test function runs and keeps the state of the create_files clean. On the other hand, the create_files fixture gets executed only once per test session. This time, if you run the tests, all the tests should pass successfully. We have successfully composed two different levels of fixture functions! Python testing with pytest - Brian Okken ↩︎ Pytest reverse ↩︎ ","permalink":"http://rednafi.com/python/compose_multiple_levels_of_pytest_fixtures/","publishDate":"2022-07-21","summary":"While reading the second version of Brian Okken’s pytest book1, I came across this neat trick to compose multiple levels of fixtures. Suppose, you want to create a fixture that returns some canned data from a database. Now, let’s say that invoking the fixture multiple times is expensive, and to avoid that you want to run it only once per test session. However, you still want to clear all the database states after each test function runs. Otherwise, a test might inadvertently get coupled with another test that runs before it via the fixture’s shared state. Let’s demonstrate this:\n","tags":["Python","Testing","TIL"],"title":"Compose multiple levels of fixtures in pytest"},{"content":"I was reading Ned Bachelder’s blog “Why your mock doesn’t work”1 and it triggered an epiphany in me about a testing pattern that I’ve been using for a while without being aware that there might be an aphorism on the practice. Patch where the object is used; not where it’s defined. To understand it, consider the example below. Here, you have a module containing a function that fetches data from some fictitious database. # db.py from __future__ import annotations import random def get_data() -\u003e list[int]: # ...run some side effects and return data # from a fictitous database. return [random.randint(100, 200) for _ in range(4)] Let’s say another module named service.py imports the get_data function and calls that inside of a function named process_data: # service.py from __future__ import annotations from db import get_data def process_data() -\u003e list[int]: data = get_data() # ... do some processing. return data Now, let’s say we want to write a test for the service.process_data function. Since the function depends on db.get_data, we’ll patch the get_data function and replace it with a mock object that returns a canned response. This will make sure that calling process doesn’t invoke the real get_data which might have side effects that we don’t want to trigger during test runs. Also, in this case, instead of returning a list of pseudo-random integers, the replaced get_data function will deterministically return a list of known integers. You could patch get_data in multiple ways. Here’s the first attempt: # test_service.py from unittest.mock import patch from service import process # Patching happens here! @patch(\"db.get_data\", return_value=[1, 2, 3, 4], autospec=True) def test_process(mock_get_data): # Call the target function. result = process() # Check the result. assert result == [1, 2, 3, 4] # Check that get_data was called. mock_get_data.assert_called_once() Since get_data is defined in the db.py module, we pass db.get_data to the patch decorator. Unfortunately, if you run the above test with pytest2, you’ll see that the test fails with the following error: test_service.py F [100%] ========== FAILURES ========== __________ test_process __________ mock_get_data = @patch( \"db.get_data\", return_values=[1, 2, 3, 4], autospec=True ) def test_process(mock_get_data): # Call the target function. result = process() # Check the result. \u003e assert result == [1, 2, 3, 4] E assert [184, 112, 189, 135] == [1, 2, 3, 4] E At index 0 diff: 184 != 1 E Use -v to get more diff test_src.py:13: AssertionError ========== short test summary info ========== FAILED test_src.py::test_process - assert [184, 112, 189, 135] == [1, 2, 3, 4] ========== 1 failed in 0.14s ========== The original implementation of get_data returns a list of 4 pseudo-random integers where the values lie between 100 and 200 whereas our patched version of get_data always returns [1, 2, 3, 4]. So, the test is failing because the get_data function didn’t get patched properly and it’s calling the original get_data function during the test run. While the function get_data is defined in the db.py module, it’s actally used in the service.py module. So, we can avoid this missing target issue by patching get_data in the location where it’s used; not where it’s defined. Here’s how to do it: # test_service.py # Notice how we're patching 'get_data' in the 'service.py' module. @patch(\"service.get_data\", return_value=[1, 2, 3, 4], autospec=True) def test_process(mock_get_data): # ...rest of the test implementation is the same as before. This time, when you run the tests, pytest doesn’t complain. Why your mock doesn’t work ↩︎ pytest ↩︎ ","permalink":"http://rednafi.com/python/patch_where_the_object_is_used/","publishDate":"2022-07-18","summary":"I was reading Ned Bachelder’s blog “Why your mock doesn’t work”1 and it triggered an epiphany in me about a testing pattern that I’ve been using for a while without being aware that there might be an aphorism on the practice.\nPatch where the object is used; not where it’s defined.\nTo understand it, consider the example below. Here, you have a module containing a function that fetches data from some fictitious database.\n","tags":["Python","Testing"],"title":"Patch where the object is used"},{"content":"I just found out that you can use Python’s unittest.mock.ANY to make assertions about certain arguments in a mock call, without caring about the other arguments. This can be handy if you want to test how a callable is called but only want to make assertions about some arguments. Consider the following example: # test_src.py import random import time def fetch() -\u003e list[float]: # Simulate fetching data from a database. time.sleep(2) return [random.random() for _ in range(4)] def add(w: float, x: float, y: float, z: float) -\u003e float: return w + x + y + z def procss() -\u003e float: return add(*fetch()) Let’s say we only want to test the process function. But process ultimately depends on the fetch function, which has multiple side effects—it returns pseudo-random values and waits for 2 seconds on a fictitious network call. Since we only care about process, we’ll mock the other two functions. Here’s how unittest.mock.ANY can make life easier: # test_src.py from unittest.mock import patch, ANY @patch(\"test_src.fetch\", return_value=[1, 2, 3, 4]) @patch(\"test_src.add\", return_value=42) def test_process(mock_add, mock_fetch): result = procss() assert result == 42 mock_fetch.assert_called_once() # Assert that the 'add' function was called with the correct # arguments. Notice we only care about the first two arguments, # so we've set the remaining ones to ANY. mock_add.assert_called_once_with(1, 2, ANY, ANY) While this is a simple example, I found ANY to be quite useful while making assertions about callables that accept multiple complex objects as parameters. Being able to ignore some aruments while calling mock_callable.assert_called_with() can make the tests more tractable. Under the hood, the implementation of ANY is quite simple. It’s an instance of a class that defines __eq__ and __ne__ in a way that comparing any value with ANY will return True. Here’s the full implementation: from __future__ import annotations from typing import Any, Literal class _ANY: \"A helper object that compares equal to everything.\" def __eq__(self, other: Any) -\u003e Literal[True]: return True def __ne__(self, other: Any) -\u003e Literal[False]: return False def __repr__(self) -\u003e str: return \"\" ANY = _ANY() It always returns True whenever compared with some value: In [1]: from unittest.mock import ANY In [2]: ANY == 1 Out[2]: True In [3]: ANY == \"anything\" Out[3]: True In [4]: ANY == True Out[4]: True In [5]: ANY == False Out[5]: True In [6]: ANY == None Out[6]: True unittest.mock.ANY 1 ↩︎ ANY in the wild 2 ↩︎ ","permalink":"http://rednafi.com/python/partially_assert_callable_arguments/","publishDate":"2022-07-17","summary":"I just found out that you can use Python’s unittest.mock.ANY to make assertions about certain arguments in a mock call, without caring about the other arguments. This can be handy if you want to test how a callable is called but only want to make assertions about some arguments. Consider the following example:\n# test_src.py import random import time def fetch() -\u003e list[float]: # Simulate fetching data from a database. time.sleep(2) return [random.random() for _ in range(4)] def add(w: float, x: float, y: float, z: float) -\u003e float: return w + x + y + z def procss() -\u003e float: return add(*fetch()) Let’s say we only want to test the process function. But process ultimately depends on the fetch function, which has multiple side effects—it returns pseudo-random values and waits for 2 seconds on a fictitious network call. Since we only care about process, we’ll mock the other two functions. Here’s how unittest.mock.ANY can make life easier:\n","tags":["Python","Testing"],"title":"Partially assert callable arguments with 'unittest.mock.ANY'"},{"content":"Whenever your local branch diverges from the remote branch, you can’t directly pull from the remote branch and merge it into the local branch. This can happen when, for example: You checkout from the main branch to work on a feature in a branch named alice. When you’re done, you merge alice into main. After that, if you try to pull the main branch from remote again and the content of the main branch changes by this time, you’ll encounter a merge error. Reproduce the issue Create a new branch named alice from main. Run: git checkout -b alice From alice branch, add a line to a newly created file foo.txt: echo \"from branch alice\" \u003e\u003e foo.txt Add, commit, and push the branch: git commit -am \"From branch alice\" \u0026\u0026 git push From the GitHub UI, send a pull request against the main branch and merge it: In your local machine, switch to main and try to pull the latest content merged from the alice branch. You’ll encounter the following error: hint: You have divergent branches and need to specify how to reconcile them. hint: You can do so by running one of the following commands sometime before hint: your next pull: hint: hint: git config pull.rebase false # merge (the default strategy) hint: git config pull.rebase true # rebase hint: git config pull.ff only # fast-forward only hint: hint: You can replace \"git config\" with \"git config --global\" to set a default hint: preference for all repositories. You can also pass --rebase, --no-rebase, hint: or --ff-only on the command line to override the configured default per hint: invocation. fatal: Need to specify how to reconcile divergent branches. This means that the history of your local main branch and the remote main branch have diverged and they aren’t reconciliable. Solution From the main branch, you can run: git pull --rebase This will rebase your local main by adding your local commits on top of the remote commits. When should I use git pull –rebase 1 ↩︎ An example repo that reproduces the issue 2 ↩︎ ","permalink":"http://rednafi.com/misc/when_to_use_git_pull_rebase/","publishDate":"2022-07-14","summary":"Whenever your local branch diverges from the remote branch, you can’t directly pull from the remote branch and merge it into the local branch. This can happen when, for example:\nYou checkout from the main branch to work on a feature in a branch named alice. When you’re done, you merge alice into main. After that, if you try to pull the main branch from remote again and the content of the main branch changes by this time, you’ll encounter a merge error. Reproduce the issue Create a new branch named alice from main. Run:\n","tags":["Git","TIL"],"title":"When to use 'git pull --rebase'"},{"content":"Whenever I need to apply some runtime constraints on a value while building an API, I usually compare the value to an expected range and raise a ValueError if it’s not within the range. For example, let’s define a function that throttles some fictitious operation. The throttle function limits the number of times an operation can be performed by specifying the throttle_after parameter. This parameter defines the number of iterations after which the operation will be halted. The current_iter parameter tracks the current number of times the operation has been performed. Here’s the implementation: # src.py def throttle(current_iter: int, throttle_after: int = -1) -\u003e None: \"\"\" The value of 'throttle_after' must be -1 or an integer greater than 0. Here, -1 means no throttling, and 'n' means that the function will throttle some operation after 'n' iterations. The `current_iter` parameter denotes the current iteration of some operation. When 'current_iter \u003e throttle_after' this function will throttle the operation. \"\"\" # Return early if 'throttle_after=-1'. if throttle_after == -1: print(\"No throttling.\") return # Ensure 'current_iter' is a positive integer. if not (isinstance(current_iter, int) and current_iter \u003e= 0): raise ValueError( \"Value of 'current_iter' must be a\" \" positive integer.\" ) # Ensure 'throttle_after' is a non-zero positive integer. if not (isinstance(throttle_after, int) and throttle_after \u003e 0): raise ValueError( \"Value of 'throttle_after' must be either -1 or an\" \" integer greater than 0.\" ) # Do the throttling. if current_iter \u003e throttle_after: print(f\"Thottling after {throttle_after} iteration(s).\") return if __name__ == \"__main__\": # Prints 'Throttling after 1 iteration(s).' throttle(current_iter=2, throttle_after=1) We return early if the value of throttle_after is -1. Otherwise, we check to see if current_iter is a positive integer and throttle_after is a non-zero positive integer. If not, we raise a ValueError. When the parameters pass these checks then we compare current_iter with throttle_after. If the value of current_iter exceeds that of the throttle_after parameter, we throttle the operation. While this works fine, recently, I’ve started to use assert to replace the conditionals with ValueError pattern. It works as follows: # src.py def throttle(current_iter: int, throttle_after: int = -1) -\u003e None: # Return early if 'throttle_after=-1'. if throttle_after == -1: print(\"No throttling.\") return # Ensure 'current_iter' is a positive integer. assert ( isinstance(current_iter, int) and current_iter \u003e= 0 ), \"Value of 'current_iter' must be a positive integer.\" # Ensure 'throttle_after' is a non-zero positive integer. assert isinstance(throttle_after, int) and throttle_after \u003e 0, ( \"Value of 'throttle_after' must be either -1 or an \" \" integer greater than 0.\" ) # Do the throttling. if current_iter \u003e throttle_after: print(f\"Thottling after {throttle_after} iterations.\") return if __name__ == \"__main__\": # AssertionError: Value of 'current_iter' must be a positive # integer. throttle(current_iter=-2, throttle_after=1) So, instead of using the if not expression ... raise ValueError pattern, we can leverage assert expression, \"Error message\" pattern. In the latter case, assert will raise AssertionError with the “Error message” if the expression evaluates to a falsy value. Otherwise, the statement will remain silent and allow the execution to move forward. This is more succinct and makes the code flatter. I’ve no idea why I haven’t started using it earlier and this1 piece of code in the Starlette2 repository jolted my brain. Eh bien, better late than never, I guess. Breadcrumbs After this blog was published, several people mentioned3 on Twitter that the second approach has a small caveat. Python has a flag that allows you to disable assert statements in a script. You can disable the assertions in the snippet above by running the script with the -OO flag: python -00 src.py Removing assert statements will disable the constraints needed for the second throttle function to work, which could lead to unexpected behavior or even subtle bugs. However, I see this being used frequently4 in frameworks like Starlette and FastAPI. Also, from my experience, using assertions is much more common than running production code with the optimization flag. Usage of assert in the Starlette codebase ↩︎ Starlette ↩︎ Caveats of using asserts to enforce constraints ↩︎ Usage of assert in the FastAPI codebase ↩︎ ","permalink":"http://rednafi.com/python/apply_constraint_with_assert/","publishDate":"2022-07-10","summary":"Whenever I need to apply some runtime constraints on a value while building an API, I usually compare the value to an expected range and raise a ValueError if it’s not within the range. For example, let’s define a function that throttles some fictitious operation. The throttle function limits the number of times an operation can be performed by specifying the throttle_after parameter. This parameter defines the number of iterations after which the operation will be halted. The current_iter parameter tracks the current number of times the operation has been performed. Here’s the implementation:\n","tags":["Python"],"title":"Apply constraints with 'assert' in Python"},{"content":"Whether I’m trying out a new tool or just prototyping with a familiar stack, I usually create a new project on GitHub and run all the experiments there. Some examples of these are:\nrubric: linter config initializer for Python exert: declaratively apply converter functions to class attributes hook-slinger: generic service to send, retry, and manage webhooks think-async: exploring cooperative concurrency primitives in Python epilog: container log aggregation with Elasticsearch, Kibana \u0026 Filebeat While many of these prototypes become full-fledged projects, most end up being just one-time journies. One common theme among all of these endeavors is that I always include instructions in the readme.md on how to get the project up and running—no matter how small it is. Also, I tend to configure a rudimentary CI pipeline that runs the linters and tests. GitHub Actions and Dependabot1 make it simple to configure a basic CI workflow. Dependabot keeps the dependencies fresh and makes pull requests automatically when there’s a new version of a dependency used in a project.\nThings can get quickly out of hand if you’ve got a large collection of repos where the automated CI runs periodically. Every now and then, I get a sizable volume of PRs in these fairly stale repos that I still want to keep updated. Merging these manually is a chore. Luckily, there are multiple ways2 that GitHub offers to automatically merge PRs. The workflow that is documented here is the one I happen to like the most. I also think that this process leads to the path of the least surprise. Instead of depending on a bunch of GitHub settings, we’ll write a GitHub action workflow3 to automate the process.\nFirst, you’ll need to turn on the auto-merge option from the repository settings. To do so, go to the repo’s settings tab and turn on the Allow auto-merge option from the Pull Requests section:\nNow, you probably don’t want to mindlessly merge every pull request Dependabot throws at you. You most likely want to make sure that a pull request triggers certain tests and it’ll be merged only if all of those checks pass. To do so, you can turn on branch protection4. From the settings panel, select Branches on the left panel:\nOnce you’ve selected the tab, add a branch protection rule to the target branch against which Dependabot will send the pull requests:\nIn this case, I’m adding the protection layer to the main branch. I’ve turned on the Require status checks to pass before merging toggle and added the build step to the list of status checks that are required. Here, you can select any job from your CI files in the .github/workflows directory:\nOnce this is done, you can drop the following CI file in the .github/workflows directory of your repo. It’s the same file5 that’s currently living inside this site’s CI folder.\n# .github/workflows/automerge.yml name: Dependabot auto-merge on: pull_request permissions: contents: write pull-requests: write # Needed if in a private repository jobs: dependabot: runs-on: ubuntu-latest if: ${{ github.actor == 'dependabot[bot]' }} steps: - name: Enable auto-merge for Dependabot PRs run: gh pr merge --auto --merge \"$PR_URL\" env: PR_URL: ${{github.event.pull_request.html_url}} # GitHub provides this variable in the CI env. You don't # need to add anything to the secrets vault. GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} From now on, every time Dependabot sends a merge request, the checks will be triggered and if all the mandatory checks pass, the automerge.yml workflow will merge it into the target branch.\nConfiguring Dependabot security updates ↩︎\nAutomatically merging a pull request ↩︎\nEnable auto-merge on a pull request ↩︎\nAbout protected branches ↩︎\nComplete CI file ↩︎\n","permalink":"http://rednafi.com/misc/automerge_dependabot_prs_on_github/","publishDate":"2022-07-07","summary":"Whether I’m trying out a new tool or just prototyping with a familiar stack, I usually create a new project on GitHub and run all the experiments there. Some examples of these are:\nrubric: linter config initializer for Python exert: declaratively apply converter functions to class attributes hook-slinger: generic service to send, retry, and manage webhooks think-async: exploring cooperative concurrency primitives in Python epilog: container log aggregation with Elasticsearch, Kibana \u0026 Filebeat While many of these prototypes become full-fledged projects, most end up being just one-time journies. One common theme among all of these endeavors is that I always include instructions in the readme.md on how to get the project up and running—no matter how small it is. Also, I tend to configure a rudimentary CI pipeline that runs the linters and tests. GitHub Actions and Dependabot1 make it simple to configure a basic CI workflow. Dependabot keeps the dependencies fresh and makes pull requests automatically when there’s a new version of a dependency used in a project.\n","tags":["GitHub"],"title":"Automerge Dependabot PRs on GitHub"},{"content":"A common bottleneck for processing large data files is—memory. Downloading the file and loading the entire content is surely the easiest way to go. However, it’s likely that you’ll quickly hit OOM errors. Often time, whenever I have to deal with large data files that need to be downloaded and processed, I prefer to stream the content line by line and use multiple processes to consume them concurrently. For example, say, you have a CSV file containing millions of rows with the following structure: --------------------------------------- | a | b | --------------------------------------- 0.902210680227088 | 0.236522024407207 | 0.424413804319515 | 0.400788559643378 | 0.601611774624256 | 0.4992389256938 | 0.332269908707654 | 0.72328094652184 | --------------------------------------- Here, let’s say you need to download the file from some source and run some other heavy tasks that depends on the data from the file. To avoid downloading the file to the disk, you can stream and read the content line by line directly from the network. While doing so, you may want to trigger multiple other tasks that can run independent of the primary process. At my workplace, I often have to create objects in a relational database using the information in a CSV file. The idea here is to consume the information in the CSV file directly from the network and create the objects in the database. This database object creation task can be offloaded to a separate process outside of the main process that’s streaming the file contents. Since we’re streaming the content from the network line by line, there should be zero disk usage and minimal memory footprint. Also, to speed up the consumption, we’ll fork multiple OS processes. To put in concisely, we’ll need to perform the following steps: Stream a single row from the target CSV file. Write the content of the row in an in-memory string buffer. Parse the file buffer with csv.DictReader. Collect the dict the contains the information of the parsed row. Yield the dict. Flush the buffer. Another process will collect the yielded dict and consume that outside of the main process. And continue the loop for the next row. The following snippet implements the workflow mentioned above: # src.py from __future__ import annotations import csv import io import multiprocessing as mp import time from operator import itemgetter from typing import Iterator, Mapping import httpx def stream_csv(url: str) -\u003e Iterator[Mapping[str, str | int]]: \"\"\"Return an iterator that yields a dict representing a single row of a CSV file. Args: url (str): URL that holds the CSV file Yields: Iterator[dict[str, str]]: Returns a generator that yields a dict. \"\"\" with httpx.Client() as client: # Make a streaming HTTP request. with client.stream(\"GET\", url, follow_redirects=True) as r: # Create instance of an in-memory file. We save the row # of the incoming CSV file here. f = io.StringIO() # The content of the source CSV file is iterated # line by line. lines = r.iter_lines() # Ignore the header row. This is the first row. next(lines) # Enumerate allows us to attach a line number to each row. # We start from two since header is the first line. for lineno, line in enumerate(lines, 2): # Write one line to the in-memory file. f.write(line) # Seek sends the file handle to the top of the file. f.seek(0) # We initiate a CSV reader to read and parse each line # of the CSV file reader = csv.DictReader(f, fieldnames=(\"a\", \"b\")) # Since we know that there's only one row in the reader # we just call 'next' on it to get the parsed dict. # The row dict looks like this: # {'a': '0.902210680227088', 'b': '0.236522024407207'} row = next(reader) # Add a line number to the dict. It makes the dict looks # like this: # { # 'a': '0.902210680227088', # 'b': '0.236522024407207', # 'lineno': 2 # } row[\"lineno\"] = lineno # type: ignore # Yield the row. This allows us to call the function # in a lazy manner. yield row # The file handle needs to be set to the top before # cleaning up the buffer. f.seek(0) # Clean up the buffer. f.flush() def process_row(row: Mapping[str, str | int]) -\u003e None: \"\"\"Consume a single row and do some work. Args: row (dict[str, str]): Represents a single parsed row of a CSV file. \"\"\" a, b = itemgetter(\"a\", \"b\")(row) float_a, float_b = float(a), float(b) # Do some processing. print( f\"Processed row {row['lineno']}:\" f\"a={float_a:.15f}, b={float_b:.15f}\", ) # Mimick some other heavy processing. time.sleep(2) if __name__ == \"__main__\": # fmt: off csv_url = ( \"https://github.com/rednafi/reflections/files\" \\ \"/9006167/foo.csv\", ) with mp.Pool(4) as pool: for res in pool.imap(process_row, stream_csv(csv_url)): pass The first function stream_csv accepts a URL that points to a CSV file. In this case, the URL used here points to a real CSV file hosted on GitHub. HTTPx1 allows you to make a streaming2 GET request and iterate through the contents of the file without fully downloading it to the disk. Inside the client.stream block, we’ve created an in-memory file instance with io.StringIO. This allows us to write the streamed content of the source CSV file to the in-memory file. Then we pull one row from the source file, write it to the in-memory buffer, and pass the in-memory file buffer over to the csv.DictReader class. The DictReader class will parse the content of the row and emit a reader object. Running next on the reader iterator returns a dictionary with the parsed content of the row. The parsed content for the first row of the example CSV looks like this: { \"a\": \"0.902210680227088\", \"b\": \"0.236522024407207\", \"lineno\": 1, } Next, the process_row function takes in the data of a single row as a dict like the one above and does some processing on that. For demonstration, currently, it just prints the values of the rows and then sleeps for two seconds. Finally, in the __main__ block, we fire up four processes to apply the process_row function to the output of the stream_csv function. Running the script will print the following output: Processed row 2:a=0.902210680227088, b=0.236522024407207 Processed row 3:a=0.424413804319515, b=0.400788559643378 Processed row 4:a=0.601611774624256, b=0.499238925693800 Processed row 5:a=0.332269908707654, b=0.723280946521840 # Sleep 2 sec Processed row 6:a=0.024648655864128, b=0.585924680177486 Processed row 7:a=0.116178678991780, b=0.027524894156040 Processed row 8:a=0.313182023389972, b=0.373896338507016 Processed row 9:a=0.252893754537173, b=0.809821115129037 # Sleep 2 sec Processed row 10:a=0.770407022765901, b=0.021249180774146 ... ... ... Since we’re forking 4 processes, the script will print four items, and then it’ll pause roughly for 2 seconds before moving on. If we were using a single process, the script would wait for 2 seconds after printing every row. By increasing the number of processes, you can speed up the consumption rate. Also, if the consumer tasks are lightweight, you can open multiple threads to consume them. HTTPx ↩︎ Streaming responses ↩︎ ","permalink":"http://rednafi.com/python/stream_process_a_csv_file/","publishDate":"2022-07-01","summary":"A common bottleneck for processing large data files is—memory. Downloading the file and loading the entire content is surely the easiest way to go. However, it’s likely that you’ll quickly hit OOM errors. Often time, whenever I have to deal with large data files that need to be downloaded and processed, I prefer to stream the content line by line and use multiple processes to consume them concurrently.\nFor example, say, you have a CSV file containing millions of rows with the following structure:\n","tags":["Python","Networking"],"title":"Stream process a CSV file in Python"},{"content":"I’ve rarely been able to take advantage of Django’s bulk_create / bulk_update APIs in production applications; especially in the cases where I need to create or update multiple complex objects with a script. Often time, these complex objects trigger a chain of signals or need non-trivial setups before any operations can be performed on each of them. The issue is, bulk_create / bulk_update doesn’t trigger these signals or expose any hooks to run any setup code. The Django doc mentions these caveates1 in detail. Here are a few of them: The model’s save() method will not be called, and the pre_save and post_save signals will not be sent. It does not work with child models in a multi-table inheritance scenario. If the model’s primary key is an AutoField, the primary key attribute can only be retrieved on certain databases (currently PostgreSQL, MariaDB 10.5+, and SQLite 3.35+). On other databases, it will not be set. It does not work with many-to-many relationships. It casts objs to a list, which fully evaluates objs if it’s a generator. Here, obj is the iterable that passes the information necessary to create the database objects in a single go. To solve this, I wanted to take advantage of Python’s concurrent.futures module. It exposes a similar API for both thread-based and process-based concurrency. The snippet below creates ten thousand user objects in the database and runs some setup code before creating each object. # script.py from __future__ import annotations import os from typing import Iterable import django os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"mysite.settings\" django.setup() from concurrent.futures import ProcessPoolExecutor from django.contrib.auth.models import User from tqdm import tqdm MAX_WORKERS = 4 def create_user_setup() -\u003e None: # ... Run some heavy weight setup code here. pass def create_user(username: str, email: str) -\u003e None: # ... Call complex setup code here. This allows the # setup code to run concurrently. create_user_setup() User.objects.create(username=username, email=email) def bulk_create_users(users: Iterable[dict[str, str]]) -\u003e None: # A container for the pending future objects. futures = [] # With PostgreSQL, Psycopg2 often complains about closed cursors # and this fixes that. django.db.connections.close_all() with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor: for user in users: future = executor.submit(create_user, **user) futures.append(future) # Wait for all the futures to complete and give the # user a visual feedback with tqdm progressbar. for future in tqdm(futures): future.result() print(\"done!\") if __name__ == \"__main__\": users = ( { \"username\": f\"{i}\", \"email\": f\"{i}@{i}.com\", } for i in range(10_000) ) bulk_create_users(users=users) Here, the create_user_setup function runs some complex setup code before the creation of each user object. We wrap the user creation process in a function named create_user and call the setup code in that. This allows us to run the complex setup code concurrently. The magic happens in the bulk_create_users function. It takes in an iterable containing the information to create the users and runs the create_user functions concurrently. The ProcessPoolExecutor forks 4 processes and starts consuming the iterable. We use the executor.submit method for maximum flexibility. This allows us to further process the returned value from the create_user function (in this case it’s None). Running this snippet will also show a progress bar as the processes start chewing through the work. You can also try experimenting with ThreadPoolExecutor, executor.map, and chunksize. I didn’t choose executor.map because it’s tricky to show the progress bar with map. Also, I encountered some psycopg2 errors in a PostgreSQL database whenever I switched to the ThreadPoolExecutor. Another gotcha is that psycopg can complain about closed cursors and closing the database connection before running each process is a way to avoid that. Notice that the script above runs django.db.connections.close_all() before entering into the ProcessPoolExecutor context manager. This appoach will run the pre_save and post_save signals which allows me to take advantage of these hooks without losing the ability of being able to perform concurrent row operations. Breadcrumbs Example shown here performs a trivial task of creating 10k user objects. In cases like this, you might find that a simple for-loop might be faster. Always run at least a rudimentary benchmark before adding concurrency to your workflow. Also, this approach primarily targets ad-hoc scripts and tasks. I don’t recommend forking multiple processes in your views or forms since Python processes aren’t cheap. Caveats of bulk_create ↩︎ concurrent.futures 2 ↩︎ ","permalink":"http://rednafi.com/python/django_bulk_operation_with_process_pool/","publishDate":"2022-06-27","summary":"I’ve rarely been able to take advantage of Django’s bulk_create / bulk_update APIs in production applications; especially in the cases where I need to create or update multiple complex objects with a script. Often time, these complex objects trigger a chain of signals or need non-trivial setups before any operations can be performed on each of them.\nThe issue is, bulk_create / bulk_update doesn’t trigger these signals or expose any hooks to run any setup code. The Django doc mentions these caveates1 in detail. Here are a few of them:\n","tags":["Python","Django"],"title":"Bulk operations in Django with process pool"},{"content":"I frequently have to write ad-hoc scripts that download a CSV file from s31, do some processing on it, and then create or update objects in the production database using the parsed information from the file. In Python, it’s trivial to download any file from s3 via boto32, and then the file can be read with the csv module from the standard library. However, these scripts are usually run from a separate script server and I prefer not to clutter the server’s disk with random CSV files. Loading the s3 file directly into memory and reading its contents isn’t difficult but the process has some subtleties. I do this often enough to justify documenting the workflow here.\nAlong with boto3, we can leverage Python’s tempfile.NamedTemporaryFile3 to directly download the contents of the file to a temporary in-memory file. Afterward, we can do the processing, create the objects in the DB, and delete the file once we’re done. The NamedTemporaryFile class can be used as a context manager and it’ll delete the file automatically when the with block ends.\nThis is quite straightforward with a simple gotcha. Here’s how you’d usually download a file from s3 and save that to a file-like object:\n# src.py import boto3 s3 = boto3.client(\"s3\") with open(\"FILE_NAME\", \"wb\") as f: s3.download_fileobj(\"BUCKET_NAME\", \"OBJECT_NAME\", f) Okay but the doc reminds us about this:\nThe download_fileobj method accepts a writeable file-like object. The file object must be opened in binary mode, not text mode.\nOpening the file in binary mode is an issue. The CSV reader needs the file to be opened in text mode. This is not an issue when you download the file to disk since you can open the file again in text mode to feed it to the CSV reader. However, we’re trying to avoid saving the file to disk and opening that again in text mode. So, you can’t do this:\n# src.py import boto3 import tempfile import csv s3 = boto3.client(\"s3\") with tempfile.NamedTemporaryFile(\"wb\") as f: s3.download_fileobj(\"BUCKET_NAME\", \"OBJECT_NAME\", f) # The csv file. This will raise an error since csv.DictReader # expects a file opened in text mode; not binary mode. csv_reader = csv.DictReader(f) for row in csv_reader: # ... do processing ... The above snippet won’t work because:\nThe file-like object is opened in binary mode but the csv.DictReader expects the file pointer to be opened in text mode. So, it’ll raise an error.\nEven if you fixed that, the CSV reader wouldn’t be able to read anything since the file currently only allows writing in binary mode, not reading.\nEven if you fixed the second issue, the content of the CSV file would be empty. That’s because after boto3 downloads and saves the file to the file object, it sets the file handle to the end of the file. So loading the content from there would result in an empty file. Here’s how I fixed all three of these problems:\n# src.py import boto3 import tempfile import csv import io BUCKET_NAME = \"foo-bucket\" OBJECT_NAME = \"foo-file.csv\" s3 = boto3.client(\"s3\") # 'w+b' allows both reading from and writing to the file. with tempfile.NamedTemporaryFile(\"w+b\") as f: s3.download_fileobj(BUCKET_NAME, OBJECT_NAME, f) # This sets the file handle back to the beginning of the file. # Without this, the loaded file will show no content. f.seek(0) # Here, 'io.TextIOWrapper' is converting the binary content of # the file to be compatible with text content. csv_reader = csv.DictReader( io.TextIOWrapper(f, encoding=\"utf-8\"), ) # Now you're good to go. for row in csv_reader: # ... do processing ... You can see that the snippet first opens a temporary file in w+b mode which allows both binary read and write operations. Then it downloads the file from s3 and saves it to the file-like object.\nOnce the download is finished, the file handle is placed at the bottom of the file. So, we’ll need to call f.seek(0) to place the handle at the beginning of the file; otherwise, our read operation will yield no content. Also, since the currently opened file object only allows binary read and write operations, we’ll need to convert it to a text file object before passing it to the CSV reader. The io.TextIOWrapper class does exactly that. Once the file object is in text mode, we pass it to the CSV reader and do further processing.\nAWS s3 ↩︎\nboto3 ↩︎\nNamedTemporaryFile ↩︎\nHow to use Python csv.DictReader with a binary file? 4 ↩︎\n","permalink":"http://rednafi.com/python/read_s3_file_in_memory/","publishDate":"2022-06-26","summary":"I frequently have to write ad-hoc scripts that download a CSV file from s31, do some processing on it, and then create or update objects in the production database using the parsed information from the file. In Python, it’s trivial to download any file from s3 via boto32, and then the file can be read with the csv module from the standard library. However, these scripts are usually run from a separate script server and I prefer not to clutter the server’s disk with random CSV files. Loading the s3 file directly into memory and reading its contents isn’t difficult but the process has some subtleties. I do this often enough to justify documenting the workflow here.\n","tags":["Python"],"title":"Read a CSV file from s3 without saving it to the disk"},{"content":"I run git log --oneline to list out the commit logs all the time. It prints out a compact view of the git history. Running the command in this repo gives me this: d9fad76 Publish blog on safer operator.itemgetter, closes #130 0570997 Merge pull request #129 from rednafi/dependabot/... 6967f73 Bump actions/setup-python from 3 to 4 48c8634 Merge pull request #128 from rednafi/dependabot/pip/mypy-0.961 5b7a7b0 Bump mypy from 0.960 to 0.961 However, there are times when I need to list out the commit logs that only represent the changes made to a particular file. Here’s the command that does exactly that. git logs --oneline --follow Running the command on the Markdown file that you’re currently reading prints out the following: git log --oneline \\ --follow content/shell/distil_git_logs_attached_to_a_file.md 7a21b3d (HEAD -\u003e master, origin/master, origin/HEAD) Nit, refs #132 6c08934 Publish distil git logs blog, refs #132 f5d2d4a Git log follow post, closes #132 Unfortunately, this command doesn’t support flag chaining. So, you can’t use the --follow flag multiple times to concatenate the logs for multiple files. But there’s a way to do it via shell command. Here’s how: echo \" \" \\ | xargs -n1 \\ | xargs -I{} sh -c \"git log --oneline --follow {}; echo ====\" Running the command on two random files in this repo yields the following output: echo \"pelicanconf.py src.py\" \\ | xargs -n1 \\ | xargs -I{} sh -c \"git log --oneline --follow {}; echo ====\" 96c0e8c Aesthetics, refs #131 e6d5409 Add default link-sharing image, closes #83 9ed958c SEO fba05d8 Add footer 8dec778 Transformation 4a402b3 Basic customizations 1c93c23 Add pelican conf ==== b89791f Fix bug in operator itemgetter implementation c75e2ab Push draft of post on typeguard, refs #87 0c6fc7b Add blacken docs to tool stack 20ac41d Publish amphibian decorators blog, closes #54 ==== Here, the first xargs is used to split the line and extract the two filenames. The second xargs applies the git log --oneline --follow command to the two files and concatenates the output with a ==== separator. The separator helps you figure out which output came from which file. ","permalink":"http://rednafi.com/misc/distil_git_logs_attached_to_a_file/","publishDate":"2022-06-21","summary":"I run git log --oneline to list out the commit logs all the time. It prints out a compact view of the git history. Running the command in this repo gives me this:\nd9fad76 Publish blog on safer operator.itemgetter, closes #130 0570997 Merge pull request #129 from rednafi/dependabot/... 6967f73 Bump actions/setup-python from 3 to 4 48c8634 Merge pull request #128 from rednafi/dependabot/pip/mypy-0.961 5b7a7b0 Bump mypy from 0.960 to 0.961 However, there are times when I need to list out the commit logs that only represent the changes made to a particular file. Here’s the command that does exactly that.\n","tags":["Shell","Git","TIL"],"title":"Distil git logs attached to a single file"},{"content":"Python’s operator.itemgetter is quite versatile. It works on pretty much any iterables and map-like objects and allows you to fetch elements from them. The following snippet shows how you can use it to sort a list of tuples by the first element of the tuple: In [2]: from operator import itemgetter ...: ...: l = [(10, 9), (1, 3), (4, 8), (0, 55), (6, 7)] ...: l_sorted = sorted(l, key=itemgetter(0)) In [3]: l_sorted Out[3]: [(0, 55), (1, 3), (4, 8), (6, 7), (10, 9)] Here, the itemgetter callable is doing the work of selecting the first element of every tuple inside the list and then the sorted function is using those values to sort the elements. Also, this is faster than using a lambda function and passing that to the key parameter to do the sorting: In [6]: from operator import itemgetter In [7]: l = [(10, 9), (1, 3), (4, 8), (0, 55), (6, 7)] In [8]: %timeit sorted(l, key=itemgetter(0)) 386 ns ± 4.2 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each) In [9]: %timeit sorted(l, key=lambda x: x[0]) 498 ns ± 0.444 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each) You can also use itemgetter to extract multiple values from a dictionary in a single pass. Consider this example: In [13]: from operator import itemgetter In [14]: d = {'foo': 31, 'bar': 12, 'baz': 42, 'chez': 83, 'moi': 24} In [15]: foo, bar, bazz = itemgetter('foo', 'bar', 'baz')(d) In [16]: foo, bar, bazz Out[16]: (31, 12, 42) So, instead of extracting the key-value pairs with d['foo'], d['bar'], ..., itemgetter allows us to make it DRY. The source code of the callable is freakishly simple. Here’s the entire thing: # operator.py class itemgetter: \"\"\" Return a callable object that fetches the given item(s) from its operand. After f = itemgetter(2), the call f(r) returns r[2]. After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3]) \"\"\" __slots__ = (\"_items\", \"_call\") def __init__(self, item, *items): if not items: self._items = (item,) def func(obj): return obj[item] self._call = func else: self._items = items = (item,) + items def func(obj): return tuple(obj[i] for i in items) self._call = func def __call__(self, obj): return self._call(obj) def __repr__(self): return \"%s.%s(%s)\" % ( self.__class__.__module__, self.__class__.__name__, \", \".join(map(repr, self._items)), ) def __reduce__(self): return self.__class__, self._items While this is all good and dandy, itemgetter will raise a KeyError if it can’t find the corresponding value against a key in a map or an IndexError if the provided index is outside of the range of the sequence. This is how it looks in a dict: In [1]: from operator import itemgetter In [2]: d = {'foo': 31, 'bar': 12, 'baz': 42, 'chez': 83, 'moi': 24} In [3]: # These keys don't exist. In [4]: fizz, up = itemgetter('fiz', 'up')(d) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) Input In [4], in () ----\u003e 1 fizz, up = itemgetter('fiz', 'up')(d) KeyError: 'fiz' In the above snippet, itemgetter can’t find the key fiz in the dict d and it complains when we try to fetch the value against it. In a sequence, the error looks like this: In [5]: from operator import itemgetter In [6]: l = [(10, 9), (1, 3), (4, 8), (0, 55), (6, 7)] In [7]: # These indices don't exist. In [8]: item_42, item_50 = itemgetter(42, 50)(l) --------------------------------------------------------------------------- IndexError Traceback (most recent call last) Input In [8], in () ----\u003e 1 item_42, item_50 = itemgetter(42, 50)(l) IndexError: list index out of range A more tolerant version of ‘operator.itemgetter’ I wanted something that works similar to itemgetter but doesn’t raise these exceptions when it can’t find the key in a dict or the index of a sequence is out of range. Instead, it’d return a default value when these exceptions occur. So, to avoid KeyError in a map, it’d use d.get(key, default) instead of d[key] to fetch the value. Similarly, in a sequence, it’d first compare the length of the sequence with the index and return a default value if the index is out of range. Since operator.itemgetter is a class, we could inherit it and overwrite the __init__ method. However, your type-checker will complain if you do so. That’s because, in the stub file, the itemgetter class is decorated with the typing.final decorator and isn’t meant to be subclassed. So, our only option is to rewrite it. The good news is that this implementation is quite terse just like the original. Here it goes: # src.py from collections.abc import Mapping class _Nothing: \"\"\"Works as a sentinel value.\"\"\" def __repr__(self): return \"\" _NOTHING = _Nothing() class safe_itemgetter: \"\"\" Return a callable object that fetches the given item(s) from its operand. \"\"\" __slots__ = (\"_items\", \"_call\") def __init__(self, item, *items, default=_NOTHING): if not items: self._items = (item,) def func(obj): if isinstance(obj, Mapping): return obj.get(item, default) if (item \u003e 0 and len(obj) \u003c= item) or ( item \u003c 0 and len(obj) \u003c abs(item) ): return default return obj[item] self._call = func else: self._items = items = (item,) + items def func(obj): if isinstance(obj, Mapping): get = obj.get # Reduce attibute search call. return tuple(get(i, default) for i in items) return tuple( default if (i \u003e 0 and len(obj) \u003c= i) or (i \u003c 0 and len(obj) \u003c abs(i)) else obj[i] for i in items ) self._call = func # ----------------- same as operator.itemgetter --------------# def __call__(self, obj): return self._call(obj) def __repr__(self): return \"%s.%s(%s)\" % ( self.__class__.__module__, self.__class__.__name__, \", \".join(map(repr, self._items)), ) def __reduce__(self): return self.__class__, self._items This class behaves almost the same way as the original itemgetter function. The only difference is that you can pass a default value to return instead of raising KeyError/IndexError depending on the type of the container. Let’s try it out with a dict: In [12]: d = {'foo': 31, 'bar': 12, 'baz': 42, 'chez': 83, 'moi': 24} In [13]: safe_itemgetter(-5, -3, -33, 'baz', 1)(d) Out[13]: (, , , 42, ) Here, we’re trying to access a bunch of keys that don’t exist in the dict d and we want to do this without raising any exceptions. You can see that instead of raising an exception, safe_itemgetter returns a tuple containing the value(s) that it can find and the rest of the positions are filled with the default value; in this case, the sentinel. We can pass any default value there: In[14]: safe_itemgetter(-5, -3, -33, \"baz\", 1, default=\"default\")(d) Out[14]: (\"default\", \"default\", \"default\", 42, \"default\") This works similarly when a sequence is passed: In[18]: l = [(10, 9), (1, 3), (4, 8), (0, 55), (6, 7)] In[19]: safe_itemgetter(-11, default=())(l) Out[19]: () This returns an empty tuple when the sequence index is out of range. It works with multiple indices as well: In [28]: l = [(10, 9), (1, 3), (4, 8), (0, 55), (6, 7)] In [29]: safe_itemgetter(-1, -3, -7, 1)(l) Out[29]: ((6, 7), (4, 8), , (1, 3)) operator.itemgetter - Python docs 1 ↩︎ ","permalink":"http://rednafi.com/python/operators_itemgetter/","publishDate":"2022-06-16","summary":"Python’s operator.itemgetter is quite versatile. It works on pretty much any iterables and map-like objects and allows you to fetch elements from them. The following snippet shows how you can use it to sort a list of tuples by the first element of the tuple:\nIn [2]: from operator import itemgetter ...: ...: l = [(10, 9), (1, 3), (4, 8), (0, 55), (6, 7)] ...: l_sorted = sorted(l, key=itemgetter(0)) In [3]: l_sorted Out[3]: [(0, 55), (1, 3), (4, 8), (6, 7), (10, 9)] Here, the itemgetter callable is doing the work of selecting the first element of every tuple inside the list and then the sorted function is using those values to sort the elements. Also, this is faster than using a lambda function and passing that to the key parameter to do the sorting:\n","tags":["Python"],"title":"Safer 'operator.itemgetter' in Python"},{"content":"Nested conditionals suck. They’re hard to write and even harder to read. I’ve rarely regretted the time I’ve spent optimizing for the flattest conditional structure in my code. The following piece mimics the actions of a traffic signal: // src.ts enum Signal { YELLOW = \"Yellow\", RED = \"Red\", GREEN = \"Green\", } function processSignal(signal: Signal) :void { if (signal === Signal.YELLOW) { console.log(\"Slow down!\"); } else { if (signal === Signal.RED) { console.log(\"Stop!\"); } else { if (signal === Signal.GREEN) { console.log(\"Go!\"); } } } } // Log processSignal(Signal.YELLOW) // prints 'Slow down!' processSignal(Signal.RED) // prints 'Stop!' The snippet above suffers from two major issues: It contains three contiguous levels of nested conditionals. The conditionals don’t cover the case where the return value is undefined. If you add a fourth member to the Signal enum, now the processing function doesn’t exhaustively cover all the cases and it won’t communicate that fact with you. We can leverage guard clauses to fix the first two issues. The guard (clause) provides an early exit from a subroutine, and is a commonly used deviation from structured programming, removing one level of nesting and resulting in flatter code: replacing if guard { ... } with if not guard: return; … We can rewrite the earlier snippet as follows: // ...snip... function processSignal(signal: Signal) { if (signal === Signal.YELLOW) { return \"Slow down!\"; } if (signal === Signal.RED) { return \"Stop!\"; } if (signal === Signal.GREEN) { return \"Go!\"; } else { return \"Not a valid input!\"; } } This model has a flatter structure and now it’s gracefully handling the undefined return path. However, the third issue still persists. In an alien world, if someone added a fourth member to the Signal enum, that’d make the conditional flow in the processSignal function incomplete since it wouldn’t be covering that newly added fourth enum member. In that case, the above snippet will execute the final catch-all conditional statement; not something that we’d want. TypeScript provides a never type to throw a compilation error if a new member isn’t covered by the conditional flow. Here’s how you’d leverage it: // src.ts enum Signal { YELLOW = \"Yellow\", RED = \"Red\", GREEN = \"Green\", PURPLE = \"Purple\", // Newly added member. } function assertNever(value: never) { throw Error(`Invalid value: ${value}`); } function processSignal(signal: Signal) { if (signal === Signal.YELLOW) { return \"Slow down!\"; } if (signal === Signal.RED) { return \"Stop!\"; } if (signal === Signal.GREEN) { return \"Go!\"; } // Try commenting out this line and typescript compiler // will throw an error. if (signal === Signal.PURPLE) { return \"Go faster!\"; } assertNever(signal); } processSignal(Signal.PURPLE); Ideally, the assertNever should never be called. Try removing a conditional and see how TypeScript starts screaming at you regarding the unhandled case. The assertNever function will also raise a runtime error if any case remains unhandled. Example in Python The same idea can be demonstrated in Python using Python3.10’s match statement and typing.NoReturn type. # src.py (Python 3.10+) from __future__ import annotations from enum import Enum from typing import NoReturn class Signal(str, Enum): YELLOW = \"Yellow\" RED = \"Red\" GREEN = \"Green\" PURPLE = \"Purple\" def assert_never(value: NoReturn) -\u003e NoReturn: raise AssertionError(f\"Invalid value: {value!r}\") def process_signal(signal: Signal) -\u003e str: match signal: case Signal.YELLOW: return \"Slow down!\" case Signal.RED: return \"Stop!\" case Signal.GREEN: return \"Go!\" # Try commenting out this line and mypy will throw # an error. case Signal.PURPLE: return \"Go faster!\" case _: assert_never(signal) if __name__ == \"__main__\": print(process_signal(Signal.PURPLE)) Similar to TypeScript, mypy will complain if you add a new member to the enum but forget to handle that in the processor function. Python 3.11 added the Never type and assert_never function to the typing module. Underneath, Never is an alias to the NoReturn type; so you can use them interchangeably. However, in this case, Never seems to communicate the intent better. You may also choose to use the backported versions of the type and function from the typing_extensions module. Here’s how: # src.py from __future__ import annotations import sys from enum import Enum if sys.version_info \u003c (3, 11): from typing_extensions import assert_never else: from typing import assert_never ... Guard clause, guard code, or guard statement 1 ↩︎ Never type in TypeScript 2 ↩︎ Unreachable Code and Exhaustiveness Checking in Python 3 ↩︎ ","permalink":"http://rednafi.com/typescript/guard_clauses_and_never_type/","publishDate":"2022-05-22","summary":"Nested conditionals suck. They’re hard to write and even harder to read. I’ve rarely regretted the time I’ve spent optimizing for the flattest conditional structure in my code. The following piece mimics the actions of a traffic signal:\n// src.ts enum Signal { YELLOW = \"Yellow\", RED = \"Red\", GREEN = \"Green\", } function processSignal(signal: Signal) :void { if (signal === Signal.YELLOW) { console.log(\"Slow down!\"); } else { if (signal === Signal.RED) { console.log(\"Stop!\"); } else { if (signal === Signal.GREEN) { console.log(\"Go!\"); } } } } // Log processSignal(Signal.YELLOW) // prints 'Slow down!' processSignal(Signal.RED) // prints 'Stop!' The snippet above suffers from two major issues:\n","tags":["TypeScript","Python","Typing"],"title":"Guard clause and exhaustiveness checking"},{"content":"While working on a project with EdgeDB1 and FastAPI2, I wanted to perform health checks against the FastAPI server in the GitHub CI. This would notify me about the working state of the application. The idea is to: Run the server in the background. Run the commands against the server that’ll denote that the app is in a working state. Perform cleanup. Exit with code 0 if the check is successful, else exit with code 1. The following shell script demonstrates a similar workflow with a Python HTTP server. This script: Runs a Python web server in the background. Makes an HTTP request against the server and checks if it returns HTTP 200 (OK). If the request fails or the server isn’t ready then waits for a second and makes the request again, and keeps retrying for the next 20 times before giving up. Performs cleanups and kills the Python processes. Exit with code 0 if the request is successful, else exit with code 1. #!/bin/bash set -euo pipefail # Run the Python server in the background. nohup python3 -m http.server 5000 \u003e\u003e /dev/null \u0026 # Give the server enough time to be ready before accepting requests. c=20 while [[ $c != 0 ]] do # Run the healthcheck. if [[ $(curl -I http://localhost:5000/ 2\u003e\u00261) =~ \"200 OK\" ]]; then echo \"Health check passed!\" # ...do additional cleanups if required. pkill -9 -i python exit 0 fi ((c--)) echo \"Server isn't ready. Retrying...\" $c sleep 1 done echo \"Health check failed!\" # ...do additional cleanups if required. pkill -9 -i python exit 1 The nohup before the python3 -m http.server 5000 makes sure that the SIGHUP signal can’t reach the server and shut down the process. The ampersand \u0026 after the command runs the process in the background. Afterward, the script starts making requests to the http://localhost:5000/ URL in a loop. If the server returns HTTP 200, the health check is considered successful. This will break the loop and the script will be terminated with exit 0 status. If the server doesn’t return HTTP 200 or isn’t ready yet, the script will keep retrying 20 times with a 1 second interval between each subsequent request before giving up. A failed health check will cause the script to terminate with exit 1 status. EdgeDB ↩︎ FastAPI ↩︎ What’s the difference between nohup and ampersand 3 ↩︎ ","permalink":"http://rednafi.com/misc/health_check_a_server_with_nohup/","publishDate":"2022-04-18","summary":"While working on a project with EdgeDB1 and FastAPI2, I wanted to perform health checks against the FastAPI server in the GitHub CI. This would notify me about the working state of the application. The idea is to:\nRun the server in the background. Run the commands against the server that’ll denote that the app is in a working state. Perform cleanup. Exit with code 0 if the check is successful, else exit with code 1. The following shell script demonstrates a similar workflow with a Python HTTP server. This script:\n","tags":["Shell","TIL"],"title":"Health check a server with 'nohup $(cmd) \u0026'"},{"content":"At my workplace, we have a large Django monolith that powers the main website and works as the primary REST API server at the same time. We use Django Rest Framework (DRF) to build and serve the API endpoints. This means, whenever there’s an error, based on the incoming request header—we’ve to return different formats of error responses to the website and API users. The default DRF configuration returns a JSON response when the system experiences an HTTP 400 (bad request) error. However, the server returns an HTML error page to the API users whenever HTTP 403 (forbidden), HTTP 404 (not found), or HTTP 500 (internal server error) occurs. This is suboptimal; JSON APIs should never return HTML text whenever something goes wrong. On the other hand, the website needs those error text to appear accordingly. This happens because 403, 404, and 500 are handled by Django’s default handlers for those errors and not by DRF’s exception handlers. As the DRF doc suggests1, overriding the error handlers is one way of solving it. But this will only work if the application is an API-only backend or if you haven’t already overridden the error handlers for custom error pages. In our case, we already had to override the default error handlers to display custom error pages on the website. These custom pages would bleed into the API endpoints occasionally when errors occur. So, I thought, if I could handle this in the middleware layer, that’d be cleaner than most of the solutions that I’d seen at that point. Solution To fix the dilemma, I wrote a middleware called JSONErrorMiddleware that returns the expected response based on the content type in the request header. If the header has Content-Type: html/text and it experiences an error, the server returns an appropriate HTML page. On the contrary, if the incoming request header has Content-Type: application/json and the server sees an error, it responds with a JSON error payload instead. Here’s how the middleware looks: # /middleware.py from http import HTTPStatus class JSONErrorMiddleware: \"\"\"Without this middleware, APIs would respond with html/text whenever there's an error.\"\"\" def __init__(self, get_response): self.get_response = get_response self.status_code_description = { v.value: v.description for v in HTTPStatus } def __call__(self, request): response = self.get_response(request) # If the content_type isn't 'application/json', do nothing. if not request.content_type == \"application/json\": return response # If there's no error, let Django and DRF's default views deal # with it. status_code = response.status_code if ( not HTTPStatus.BAD_REQUEST \u003c status_code \u003c= HTTPStatus.INTERNAL_SERVER_ERROR ): return response # Return a JSON error response if any of 403, 404, or 500 occurs. r = JsonResponse( { \"error\": { \"status_code\": status_code, \"message\": self.status_code_description[status_code], \"detail\": {\"url\": request.get_full_path()}, } }, ) r.status_code = response.status_code return r You’ll have to add this middleware to the list of middlewares in the settings.py file: MIDDLEWARE = [..., \".middleware.JSONErrorMiddleware\"] And voila, now the API and non-API errors will be handled differently as expected! Test Here’s how you can unit test the behavior of the middleware: import json from unittest.mock import MagicMock from django.http import JsonResponse from django.test import RequestFactory, TestCase, override_settings from main.middleware import JSONErrorMiddleware @override_settings( MIDDLEWARE_CLASSES=(\"main.middleware.JSONErrorMiddleware\",), ) class TestJSONErrorMiddleware(TestCase): def setUp(self): super().setUp() self.factory = RequestFactory() def get_response(request): response = MagicMock() response.status_code = HTTPStatus.FORBIDDEN return response self.middleware = JSONErrorMiddleware(get_response) def test_json_error_middleware(self): # Arrange corrupted_url = \"/account\" # Act request = self.factory.get( path=corrupted_url, ) request.content_type = \"application/json\" response = self.middleware.__call__(request) # Assert # Assert 404 no longer returns html/text. self.assertTrue(isinstance(response, JsonResponse)) # Assert json format. json_data = json.loads(response.content) expected_json_data = { \"error\": { \"status_code\": HTTPStatus.FORBIDDEN, \"message\": HTTPStatus.FORBIDDEN.description, \"detail\": {\"url\": \"/account\"}, } } for k, v in json_data[\"error\"].items(): self.assertEqual(v, expected_json_data[\"error\"][k]) Breadcrumbs This workflow has been tested on Django 3.2, 4.0, and DRF 3.13. References Generic error views ↩︎ HTML sometimes returned when Accept: application/json is provided #3362 2 ↩︎ Added generic 500 and 400 JSON error handlers #5904 3 ↩︎ ","permalink":"http://rednafi.com/python/return_json_error_payload_in_drf/","publishDate":"2022-04-13","summary":"At my workplace, we have a large Django monolith that powers the main website and works as the primary REST API server at the same time. We use Django Rest Framework (DRF) to build and serve the API endpoints. This means, whenever there’s an error, based on the incoming request header—we’ve to return different formats of error responses to the website and API users.\nThe default DRF configuration returns a JSON response when the system experiences an HTTP 400 (bad request) error. However, the server returns an HTML error page to the API users whenever HTTP 403 (forbidden), HTTP 404 (not found), or HTTP 500 (internal server error) occurs. This is suboptimal; JSON APIs should never return HTML text whenever something goes wrong. On the other hand, the website needs those error text to appear accordingly.\n","tags":["Python","API","Django"],"title":"Return JSON error payload instead of HTML text in DRF"},{"content":"Generators can help you decouple the production and consumption of iterables—making your code more readable and maintainable. I learned this trick a few years back from David Beazley’s slides1 on generators. Consider this example: # src.py from __future__ import annotations import time from typing import NoReturn def infinite_counter(start: int, step: int) -\u003e NoReturn: i = start while True: time.sleep(1) # Not to flood stdout print(i) i += step infinite_counter(1, 2) # Prints # 1 # 3 # 5 # ... Now, how’d you decouple the print statement from the infinite_counter? Since the function never returns, you can’t collect the outputs in an iterable, return the container, and print the elements of the iterable in another function. You might be wondering why would you even need to do it. I can think of two reasons: The infinite_counter function is the producer of the numbers and the print function is consuming them. These are two separate responsibilities tangled in the same function which violates the Single Responsibility Principle (SRP)2. What’d you do if you needed a version of the infinite counter where the consumer had different behavior? One way the second point can be addressed is—by accepting the consumer function as a parameter and applying that to the produced value. # src.py from __future__ import annotations import time # In Python \u003c 3.9, import this from the 'typing' module. from collections.abc import Callable from typing import NoReturn def infinite_counter( start: int, step: int, consumer: Callable = print ) -\u003e NoReturn: i = start while True: time.sleep(1) # Not to flood stdout consumer(i) i += step infinite_counter(1, 2) # Prints # 1 # 3 # 5 # ... You can override the value of consumer with any callable and make the function more flexible. However, applying multiple consumers will still be hairy. Doing this with generators is cleaner. Here’s how you’d transform the above script to take advantage of generators: # src.py from __future__ import annotations import time # In Python \u003c 3.9, import this from the 'typing' module. from collections.abc import Generator # Producer. def infinite_counter(start: int, step: int) -\u003e Generator[int, None, None]: i = start while True: time.sleep(1) # Not to flood stdout yield i i += step # Consumer. This can be a callable doing anything. def infinite_printer(gen: Generator[int, None, None]) -\u003e None: for i in gen: print(i) gen = infinite_counter(1, 2) infinite_printer(gen) # Prints # 1 # 3 # 5 # ... The infinite_counter returns a generator that can lazily be iterated to produce the numbers and you can call any arbitrary consumer on the generated result without coupling it with the producer. Writing a workflow that mimics ’tail -f' In a UNIX system, you can call tail -f | grep to print the lines of a file in real-time where the lines match a specific pattern. Running the following command on my terminal allows me to tail the syslog file and print out any line that contains the word xps: tail -f /var/logs/syslog | grep xps Apr 3 04:42:21 xps slack.desktop[4613]: [04/03/22, 04:42:21:859] ... If you look carefully, the above command has two parts. The tail -f returns the new lines appended to the file and grep consumes the new lines to look for a particular pattern. This behavior can be mimicked via generators as follows: # src.py from __future__ import annotations import os import time # In Python \u003c 3.9, import this from the 'typing' module. from collections.abc import Generator # Producer. def tail_f(filepath: str) -\u003e Generator[str, None, None]: file = open(filepath) file.seek(0, os.SEEK_END) # End-of-file while True: line = file.readline() if not line: time.sleep(0.001) # Sleep briefly continue yield line # Consumer. def grep( lines: Generator[str, None, None], pattern: str | None = None ) -\u003e None: for line in gen: if not pattern: print(line) else: if not pattern in line: continue print(line, flush=True) lines = tail_f(filepath=\"/var/log/syslog\") grep(lines, \"xps\") # Prints # Apr 3 04:42:21 xps slack.desktop[4613]: [04/03/22, 04:42:21:859] # info: Store: SET_SYSTEM_IDLE idle Here, the tail_f continuously yields the logs, and the grep function looks for the pattern xps in the logs. Replacing grep with any other processing function is trivial as long as it accepts a generator. The tail_f function doesn’t know anything about the existence of grep or any other consumer function. Continuously polling a database and consuming the results This concept of polling a log file for new lines can be extended to databases and caches as well. I was working on a microservice that polls a Redis queue at a steady interval and processes the elements one by one. I took advantage of generators to decouple the function that collects the data and the one that processes the data. Here’s how it works: # src.py from __future__ import annotations # In Python \u003c 3.9, import this from the 'typing' module. from collections.abc import Generator import redis # Requires pip install Data = Generator[tuple[bytes, bytes], None, None] def collect(queue_name: str) -\u003e Data: r = redis.Redis() while True: yield r.brpop(queue_name) def process(data: Data) -\u003e None: for datum in data: queue_name, content = datum[0].decode(), datum[1].decode() print(f\"{queue_name=}, {content=}\") data = collect(\"default\") process(data) You’ll need to run an instance of Redis3 server and Redis CLI4 to test this out. If you’ve got Docker5 installed in your system, then you can run docker run -it redis to quickly spin up a Redis instance. Afterward, run the above script and start the CLI. Print the following command on the CLI prompt: 127.0.0.1:6379\u003e lpush default hello world The above script should print the following: queue_name='default', content='hello' queue_name='default', content='world' This allows you to define multiple consumers and run them in separate threads/processes without the producer ever knowing about their existence at all. Generator tricks for systems programmers — David Beazley ↩︎ SRP ↩︎ Redis ↩︎ Redis CLI ↩︎ Docker ↩︎ ","permalink":"http://rednafi.com/python/decouple_with_generators/","publishDate":"2022-04-03","summary":"Generators can help you decouple the production and consumption of iterables—making your code more readable and maintainable. I learned this trick a few years back from David Beazley’s slides1 on generators. Consider this example:\n# src.py from __future__ import annotations import time from typing import NoReturn def infinite_counter(start: int, step: int) -\u003e NoReturn: i = start while True: time.sleep(1) # Not to flood stdout print(i) i += step infinite_counter(1, 2) # Prints # 1 # 3 # 5 # ... Now, how’d you decouple the print statement from the infinite_counter? Since the function never returns, you can’t collect the outputs in an iterable, return the container, and print the elements of the iterable in another function. You might be wondering why would you even need to do it. I can think of two reasons:\n","tags":["Python"],"title":"Decoupling producers and consumers of iterables with generators in Python"},{"content":"In CPython, elements of a list are stored as pointers to the elements rather than the values of the elements themselves. This is evident from the struct1 that represents a list in C:\n// Fetched from CPython main branch. Removed comments for brevity. typedef struct { PyObject_VAR_HEAD PyObject **ob_item; /* Pointer reference to the element. */ Py_ssize_t allocated; }PyListObject; An empty list builds a PyObject and occupies some memory:\nfrom sys import getsizeof l = [] print(getsizeof(l)) This returns:\n56 The exact size of an empty list can vary across different Python versions and implementations.\nA single pointer to an element requires 8 bytes of space in a list. Whenever additional elements are added to the list, Python dynamically allocates extra memory to accommodate future elements without resizing the container. This implies, adding a single element to an empty list will incite Python to allocate more memory than 8 bytes.\nLet’s put this to test and append some elements to the list:\n# src.py from sys import getsizeof l = [] l.append(0) print(getsizeof(l)) This returns:\n88 Wait, the size of l should have been 64 bytes (56+8) but instead, it increased to 88 bytes. This happens because in this case, Python over-allocated 32 extra bytes to accommodate future incoming elements. Now, if you append 3 more elements to the list, you’ll see that it doesn’t increase the size because no re-allocation is happening here:\n# src.py from sys import getsizeof l = [] l.append(0) l.append(1) l.append(2) l.append(3) print(getsizeof(l)) This prints:\n88 Adding a fifth element to the above list will increase the size of the list by 32 bytes (can be different in other implementations) again:\n# src.py from sys import getsizeof l = [] for i in range(6): l.append(l) print(getsizeof(l)) 120 This dynamic memory allocation makes lists so flexible, and since a list only holds references to the elements, it can house heterogenous objects without any issue. But this flexibility of being able to append any number of elements—without ever caring about memory allocation—comes at the cost of slower execution time.\nAlthough usually, you don’t need to think about optimizing this at all, there’s a way that allows you to perform static pre-allocation of memory in a list instead of letting Python perform dynamic allocation for you. This way, you can make sure that Python won’t have to perform dynamic memory allocation multiple times as your list grows.\nStatic pre-allocation will make your code go slightly faster. I had to do this once in a tightly nested loop and the 10% performance improvement was significant for the service that I was working on.\nPre-allocating memory in a list Let’s measure the performance of appending elements to an empty list. I’m using IPython’s built-in %%timeit command to do it:\nIn [1]: %%timeit ...: ...: l=[] ...: for i in range(10_000): ...: l.append(i) ...: 499 µs ± 1.23 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) Now, if you know the final size of the list beforehand, then you don’t need to create an empty list and append elements to it via a loop. You can initialize the list with None and then fill in the elements like this:\n# src.py size = 10_000 l = [None] * size for i in range(size): l[i] = i This is quite a bit faster than the previous snippet:\nIn [2]: %%timeit ...: ...: l=[None]*10_000 ...: for i in range(10_000): ...: l[i] = i ...: 321 µs ± 71.1 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each) Breadcrumbs For simple cases demonstrated above, list comprehension is going to be quite a bit quicker than the static pre-allocation technique. See for yourself:\nIn [3]: %%timeit ...: ...: [i for i in range(10_000)] 225 µs ± 711 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each) So, I don’t recommend performing micro-optimization without instrumenting your code first. However, list pre-allocation can still come in handy in more complex cases where you already know the size of the final list, and shaving off a few micro-seconds makes a considerable difference.\nReferences List struct in CPython ↩︎\nCreate a list with initial capacity in Python 2 ↩︎\n","permalink":"http://rednafi.com/python/preallocated_list/","publishDate":"2022-03-27","summary":"In CPython, elements of a list are stored as pointers to the elements rather than the values of the elements themselves. This is evident from the struct1 that represents a list in C:\n// Fetched from CPython main branch. Removed comments for brevity. typedef struct { PyObject_VAR_HEAD PyObject **ob_item; /* Pointer reference to the element. */ Py_ssize_t allocated; }PyListObject; An empty list builds a PyObject and occupies some memory:\nfrom sys import getsizeof l = [] print(getsizeof(l)) This returns:\n","tags":["Python"],"title":"Pre-allocated lists in Python"},{"content":"Up until now, I’ve always preferred Title Case to demarcate titles and section headers in my writings. However, lately I’ve realized that each time I start writing a sentence, I waste a few seconds deciding on the appropriate case of the special words like—technical terms, trademark names, proper nouns, etc—and how they’ll blend in with the multiple flavors1 of rules around title casing. Plus, often time, special casing of selected words makes title-cased sentences look strange.\nThis led me to asking a question2 on Twitter to find out if anyone had the same thought as mine or if it’d be ignored as trifling pedantry. A few people responded and they had a few excellent points on what they liked and disliked about title case. What surprised me that majority of them preferred Sentence case where you’d only capitalize the first letter of the first word of a sentence and then the capitilization of words would conform to the usual grammatical rules. So instead of this:\nDataclasses in Python Eliminates Class Initialization Boilerplates.\nyou’d do this:\nDataclasses in Python eliminates class initialization boilerplates.\nThis allows me not to waste those few precious seconds thinking about the rules of upper casing and whether or not sentences with technical words in them will look awkward. Also, it saves me from creating horrors like this where the semantics of a sentence get massacred by the eccentricity of title casing:\nSentence case everywhere is the general trend I’m seeing on the technical blogs of some of the big players on the internet:\nDjango docs Dropbox blog GitHub blog Twitter blog GitLab blog One thing that I’ll admit is, at first it just doesn’t look right to me for a title to have sentence casing. Also, I’m not too conviced to adopt sentence case in technical papers. However, Robert Smallshire argues3 that if it works for the Economist4, it should work for most of us.\nSo, I’m kind of leaning towards sentence casing and have slowly started adopting it in my writings everywhere. This blog used to conform to a mangled version of title casing to delineate the titles and sub headers but I’ve converted all the posts to adopt sentence case from now on. I’m quite happy with the results so far and it’s definitely one less thing to worry about.\nTitle Capitalization \u0026 Title Case Converter Tool ↩︎\nAsking the Twitter folks ↩︎\nFor the love of sentence casing ↩︎\nThe Economist ↩︎\n","permalink":"http://rednafi.com/zephyr/in_favor_of_sentence_case/","publishDate":"2022-03-26","summary":"Up until now, I’ve always preferred Title Case to demarcate titles and section headers in my writings. However, lately I’ve realized that each time I start writing a sentence, I waste a few seconds deciding on the appropriate case of the special words like—technical terms, trademark names, proper nouns, etc—and how they’ll blend in with the multiple flavors1 of rules around title casing. Plus, often time, special casing of selected words makes title-cased sentences look strange.\n","tags":["Essay"],"title":"In favor of sentence case"},{"content":"I was working on a DRF POST API endpoint where the consumer is expected to add a URL containing a PDF file and the system would then download the file and save it to an S3 bucket. While this sounds quite straightforward, there’s one big issue. Before I started working on it, the core logic looked like this: # src.py from __future__ import annoatations from urllib.request import urlopen import tempfile from shutil import copyfileobj def save_to_s3(src_url: str, dest_url: str) -\u003e None: with tempfile.NamedTemporaryFile() as file: with urlopen(src_url) as response: # This stdlib function saves the content of the file # in 'file'. copyfileobj(response, file) # Logic to save file in s3. _save_to_s3(des_url) if __name__ == \"__main__\": save_to_s3( \"https://citeseerx.ist.psu.edu/viewdoc/download?\" \"doi=10.1.1.92.4846\u0026rep=rep1\u0026type=pdf\", \"https://s3-url.com\", ) In the above snippet, there’s no guardrail against how large the target file can be. You could bring the entire server down to its knees by posting a link to a ginormous file. The server would be busy downloading the file and keep consuming resources. I didn’t want to use urllib at all for this purpose and went for HTTPx1. It exposes a neat API to perform streaming file download. Also, I didn’t want to peek into the Content-Length header to assess the file size since the file server can choose not to include that header key. I was looking for something more dependable than that. Here’s how I solved it: # src from __future__ import annotations import httpx import tempfile def save_to_s3( src_url: str, dest_url: str, chunk_size: int = 1024 * 1024, # 1 MB buffer. max_size: int = 10 * 1024 * 1024, # 10 MB ) -\u003e None: # Keep track of the already downloaded byte length. downloaded_content_length = 0 # bytes with tempfile.NamedTemporaryFile() as file: with httpx.stream(\"GET\", src_url) as response: for chunk in response.iter_bytes(chunk_size): downloaded_content_length += len(chunk) if downloaded_content_length \u003e max_size: raise ValueError( f\"File size too large. Make sure your linked \" \"file is not larger than 10 MB.\" ) file.write(chunk) # logic to save file in s3. _save_to_s3(dest_url) if __name__ == \"__main__\": save_to_s3( \"https://citeseerx.ist.psu.edu/viewdoc/download?\" \"doi=10.1.1.92.4846\u0026rep=rep1\u0026type=pdf\", \"\", ) The chunk_size parameter explicitly dictates the buffer size of the file being downloaded. This means the entire file won’t be loaded into memory while being downloaded. The max_size parameter defines the maximum file size that’ll be allowed. In this example, we’re keeping track of the size of the already downloaded bytes in the downloaded_content_length variable and raising an error if the size exceeds 10MB. Sweet! HTTPx ↩︎ Streaming download with HTTPx 2 ↩︎ ","permalink":"http://rednafi.com/python/disallow_large_file_download/","publishDate":"2022-03-23","summary":"I was working on a DRF POST API endpoint where the consumer is expected to add a URL containing a PDF file and the system would then download the file and save it to an S3 bucket. While this sounds quite straightforward, there’s one big issue. Before I started working on it, the core logic looked like this:\n# src.py from __future__ import annoatations from urllib.request import urlopen import tempfile from shutil import copyfileobj def save_to_s3(src_url: str, dest_url: str) -\u003e None: with tempfile.NamedTemporaryFile() as file: with urlopen(src_url) as response: # This stdlib function saves the content of the file # in 'file'. copyfileobj(response, file) # Logic to save file in s3. _save_to_s3(des_url) if __name__ == \"__main__\": save_to_s3( \"https://citeseerx.ist.psu.edu/viewdoc/download?\" \"doi=10.1.1.92.4846\u0026rep=rep1\u0026type=pdf\", \"https://s3-url.com\", ) In the above snippet, there’s no guardrail against how large the target file can be. You could bring the entire server down to its knees by posting a link to a ginormous file. The server would be busy downloading the file and keep consuming resources.\n","tags":["Python"],"title":"Disallow large file download from URLs in Python"},{"content":"While writing microservices in Python, I like to declaratively define the shape of the data coming in and out of JSON APIs or NoSQL databases in a separate module. Both TypedDict and dataclass are fantastic tools to communicate the shape of the data with the next person working on the codebase. Whenever I need to do some processing on the data before starting to work on that, I prefer to transform the data via dataclasses. Consider this example: # src.py from __future__ import annotations from dataclasses import dataclass from typing import Any @dataclass class WebhookPayload: \"\"\"Save data to DynamoDB.\"\"\" url: str request_payload: dict response_payload: dict status_code: int def to_dynamodb_item(self) -\u003e None: ... The above class defines the structure of a payload that’ll be saved in a DynamoDB table. To make things simpler, I want to serialize the request_payload, response_payload, and status_code fields to JSON string before saving them to the DB. Usually, I’d do it in the to_dynamodb_item like this: # src.py from __future__ import annotations from dataclasses import dataclass import json from typing import Any @dataclass class WebhookPayload: \"\"\"Save data to DynamoDB.\"\"\" # Snip... def to_dynamodb_item(self) -\u003e dict[str, Any]: request_payload = json.dumps(self.request_payload) response_payload = json.dumps(self.response_payload) status_code = json.dumps(self.response_payload) # ... dyanmodb_item = ... return dynamodb_item However, keeping track of this json.dumps transformation that’s buried in a method can be difficult. Also, it can be hard to track the fields that need to be deserialized whenever you want the rich data structures back. Another disadvantage is that you’ll have to perform the same transformation again if you need serialized fields in another method. A better way is to take advantage of the __post_init__ hook exposed by dataclasses. Here’s how you can do it: # src.py from __future__ import annotations import json from dataclasses import dataclass, field from typing import Any @dataclass class WebhookPayload: \"\"\"Save data to DynamoDB.\"\"\" url: str request_payload: dict | None response_payload: dict | None status_code: int | None _json_transform: bool = field(default=True, repr=False) _json_fields: tuple[str, ...] = field( default=( \"request_payload\", \"response_payload\", \"status_code\", ), repr=False, ) def __post_init__(self) -\u003e None: if not self._json_transform: return for field in self._json_fields: # Here's where the magic happens! setattr(self, field, json.dumps(getattr(self, field))) def to_dynamodb_item(self) -\u003e dict[str, Any]: ... if __name__ == \"__main__\": wh = WebhookPayload( url=\"https//:httpbin.org/post\", request_payload={\"hello\": \"world\"}, response_payload=None, status_code=None, ) print(wh) Running the script will print the following: WebhookPayload( url='https//:httpbin.org/post', request_payload='{\"hello\": \"world\"}', response_payload='null', status_code='null' ) Notice, how the intended fields are now JSON encoded. Python calls the __post_init__ hook of a dataclass after calling the __init__ method. If you don’t generate any init by decorating the target class with @dataclass(init=False), in that case, the __post_init__ hook won’t be executed. The field function with repr=False allows us to exclude the configuration fields like _json_transform and _json_fields from the final __repr__ of the class. Notice that these two fields are absent in the final representation of the dataclass instance. You can turn off the JSON conversion by setting the _json_transform to False: # src.py ... WebhookPayload( url=\"https//:httpbin.org/post\", request_payload={\"hello\": \"world\"}, response_payload=None, status_code=None, _json_transform=False, ) You can also add or remove fields to be transformed by changing the value of the _json_fields iterable of the class: # src.py ... WebhookPayload( url=\"https//:httpbin.org/post\", request_payload={\"hello\": \"world\"}, response_payload=None, status_code=None, _json_fields=(\"status_code\",), ) This will only serialize the status_code field. Neat! References Post-init processing — Python docs 1 ↩︎ ","permalink":"http://rednafi.com/python/declaratively_transform_dataclass_fields/","publishDate":"2022-03-20","summary":"While writing microservices in Python, I like to declaratively define the shape of the data coming in and out of JSON APIs or NoSQL databases in a separate module. Both TypedDict and dataclass are fantastic tools to communicate the shape of the data with the next person working on the codebase.\nWhenever I need to do some processing on the data before starting to work on that, I prefer to transform the data via dataclasses. Consider this example:\n","tags":["Python"],"title":"Declaratively transform data class fields in Python"},{"content":"To avoid instantiating multiple DB connections in Python apps, a common approach is to initialize the connection objects in a module once and then import them everywhere. So, you’d do this: # src.py import boto3 # Pip install boto3 import redis # Pip install redis dynamo_client = boto3.client(\"dynamodb\") redis_client = redis.Redis() However, this adds import time side effects to your module and can turn out to be expensive. In search of a better solution, my first instinct was to go for functools.lru_cache(None) to immortalize the connection objects in memory. It works like this: from __future__ import annotations # In Py \u003c 3.9, use 'functools.lru_cache(None)'. from functools import cache import boto3 import redis @cache def get_dynamo_client() -\u003e boto3.session.Session.client: return boto3.client(\"dynamodb\") @cache def get_redis_client() -\u003e redis.Redis: return redis.Redis() This way, the connection objects returned by the functions are cached and any subsequent calls to the functions will provide the same connection objects from the cache without reinitializing them. One problem with the above approach is—how complex the implementation of the cache decorator is. Underneath, the functools.cache decorator is an alias for functools.lru_cache(None) and it employs a Least Recently Used cache eviction policy. While this policy is quite useful when you need it but to cache simple connection objects, arguably, the complexity and the overhead of the cache decorator offset its benefits. There’s a simpler way to do it and James Powell on Twitter pointed1 me to it. This works as follows: # src.py from __future__ import annotations import boto3 import redis _cache = {} def get_dynamo_client( service_name: str = \"dynamodb\", ) -\u003e boto3.session.Session.client: \"\"\"Immortalize the Dynamo client object so that this function always returns the same connection object .\"\"\" if service_name not in _cache: _cache[service_name] = boto3.client(service_name) return _cache[service_name] def get_redis_client(service_name: str = \"redis\") -\u003e redis.Redis: \"\"\"Immortalize Redis connection object.\"\"\" if service_name not in _cache: _cache[service_name] = redis.Redis() return _cache[service_name] Is this singleton pattern? Probably so. Caching connections in Python — Twitter ↩︎ ","permalink":"http://rednafi.com/python/caching_connection_objects/","publishDate":"2022-03-16","summary":"To avoid instantiating multiple DB connections in Python apps, a common approach is to initialize the connection objects in a module once and then import them everywhere. So, you’d do this:\n# src.py import boto3 # Pip install boto3 import redis # Pip install redis dynamo_client = boto3.client(\"dynamodb\") redis_client = redis.Redis() However, this adds import time side effects to your module and can turn out to be expensive. In search of a better solution, my first instinct was to go for functools.lru_cache(None) to immortalize the connection objects in memory. It works like this:\n","tags":["Python","TIL"],"title":"Caching connection objects in Python"},{"content":"When I first started working with Python, nothing stumped me more than how bizarre Python’s import system seemed to be. Often time, I wanted to run a module inside of a package with the python src/sub/module.py command, and it’d throw an ImportError that didn’t make any sense. Consider this package structure: src ├── __init__.py ├── a.py └── sub ├── __init__.py └── b.py Let’s say you’re importing module a in module b: # b.py from src import a ... Now, if you try to run module b.py with the following command, it’d throw an import error: python src/sub/b.py Traceback (most recent call last): File \"/home/rednafi/canvas/personal/reflections/src/sub/b.py\", line 2, in from src import a ModuleNotFoundError: No module named 'src' What! But you can see the src/a.py module right there. Why can’t Python access the module here? Turns out Python puts the path of the module that you’re trying to access to the top of the sys.path stack. Let’s print the sys.path before importing module a in the src/sub/b.py file: # b.py import sys print(sys.path) from src import a Now running this module with python src/sub/b.py will print the following: ['/home/rednafi/canvas/personal/reflections/src/sub', '/usr/lib/python310.zip', '/usr/ lib/python3.10', '/usr/lib/python3.10/lib-dynload', '/home/rednafi/canvas/personal/ reflections/.venv/lib/python3.10/site-packages'] Traceback (most recent call last): File \"/home/rednafi/canvas/personal/reflections/src/sub/b.py\", line 5, in from src import a ModuleNotFoundError: No module named 'src' From the first section of the above output, it’s evident that Python looks for the imported module in the src/sub/ directory, not in the root directory from where the command is being executed. That’s why it can’t find the a.py module because it exists in a directory above the sys.path’s first entry. To solve this, you should run the module with the -m switch as follows: python -m src.sub.b This will not raise the import error and return the following output: ['/home/rednafi/canvas/personal/reflections', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '/home/rednafi/canvas/personal/reflections/.venv/lib/python3.10/site-packages'] Here, the first entry denotes the root directory from where the script is being run from. Voila, problem solved! Don’t run python my/script.py 1 ↩︎ ","permalink":"http://rednafi.com/python/how_not_to_run_a_script/","publishDate":"2022-03-16","summary":"When I first started working with Python, nothing stumped me more than how bizarre Python’s import system seemed to be. Often time, I wanted to run a module inside of a package with the python src/sub/module.py command, and it’d throw an ImportError that didn’t make any sense. Consider this package structure:\nsrc ├── __init__.py ├── a.py └── sub ├── __init__.py └── b.py Let’s say you’re importing module a in module b:\n","tags":["Python"],"title":"How not to run a script in Python"},{"content":"This is the 4th time in a row that I’ve wasted time figuring out how to mock out a function during testing that calls the chained methods of a datetime.datetime object in the function body. So I thought I’d document it here. Consider this function: # src.py from __future__ import annotations import datetime def get_utcnow_isoformat() -\u003e str: \"\"\"Get UTCnow as an isoformat compliant string.\"\"\" return datetime.datetime.utcnow().isoformat() How’d you test it? Mocking out datetime.datetime is tricky because of its immutable nature. Third-party libraries like freezegun1 make it easier to mock and test functions like the one above. However, it’s not too difficult to cover this simple case without any additional dependencies. Here’s one way to achieve the goal: # src.py from __future__ import annotations import datetime from unittest.mock import patch import pytest def get_utcnow_isoformat() -\u003e str: \"\"\"Get UTCnow as an isoformat compliant string.\"\"\" return datetime.datetime.utcnow().isoformat() @pytest.fixture def mock_datetime(): with patch(\"datetime.datetime\") as m: # This is where the magic happens! m.utcnow.return_value.isoformat.return_value = ( \"2022-03-15T23:11:12.432048\" ) yield m def test_get_utcnow_isoformat(mock_datetime): frozen_date = \"2022-03-15T23:11:12.432048\" assert get_utcnow_isoformat() == frozen_date Here, the mock_datetime fixture function makes the output of the chained calls on the datetime object deterministic. Then I used it in the test_get_utcnow_isoformat function to get a frozen output every time the function get_utcnow_isoformat gets called. If you run the above snippet with Python, it’ll pass. ======test session starts ====== platform linux -- Python 3.10.2, pytest-7.0.1, pluggy-1.0.0 rootdir: /home/rednafi/canvas/personal/reflections plugins: anyio-3.5.0 collected 1 item src.py . [100%] ====== 1 passed in 0.01s ====== freezegun ↩︎ Python test using mock with datetime.utcnow — Stackoverflow 2 ↩︎ ","permalink":"http://rednafi.com/python/mocking_datetime_objects/","publishDate":"2022-03-16","summary":"This is the 4th time in a row that I’ve wasted time figuring out how to mock out a function during testing that calls the chained methods of a datetime.datetime object in the function body. So I thought I’d document it here. Consider this function:\n# src.py from __future__ import annotations import datetime def get_utcnow_isoformat() -\u003e str: \"\"\"Get UTCnow as an isoformat compliant string.\"\"\" return datetime.datetime.utcnow().isoformat() How’d you test it? Mocking out datetime.datetime is tricky because of its immutable nature. Third-party libraries like freezegun1 make it easier to mock and test functions like the one above. However, it’s not too difficult to cover this simple case without any additional dependencies. Here’s one way to achieve the goal:\n","tags":["Python","Testing"],"title":"Mocking chained methods of datetime objects in Python"},{"content":"While working with microservices in Python, a common pattern that I see is—the usage of dynamically filled dictionaries as payloads of REST APIs or message queues. To understand what I mean by this, consider the following example: # src.py from __future__ import annotations import json from typing import Any import redis # Do a pip install. def get_payload() -\u003e dict[str, Any]: \"\"\"Get the 'zoo' payload containing animal names and attributes.\"\"\" payload = {\"name\": \"awesome_zoo\", \"animals\": []} names = (\"wolf\", \"snake\", \"ostrich\") attributes = ( {\"family\": \"Canidae\", \"genus\": \"Canis\", \"is_mammal\": True}, {\"family\": \"Viperidae\", \"genus\": \"Boas\", \"is_mammal\": False}, ) for name, attr in zip(names, attributes): payload[\"animals\"].append( # type: ignore {\"name\": name, \"attribute\": attr}, ) return payload def save_to_cache(payload: dict[str, Any]) -\u003e None: # You'll need to spin up a Redis db before instantiating # a connection here. r = redis.Redis() print(\"Saving to cache...\") r.set(f\"zoo:{payload['name']}\", json.dumps(payload)) if __name__ == \"__main__\": payload = get_payload() save_to_cache(payload) Here, the get_payload function constructs a payload that gets stored in a Redis DB in the save_to_cache function. The get_payload function returns a dict that denotes a contrived payload containing the data of an imaginary zoo. To execute the above snippet, you’ll need to spin up a Redis database first. You can use Docker1 to do so. Install and configure Docker on your system and run: docker run -d -p 6379:6379 redis:alpine If you run the above snippet after instantiating the Redis server, it’ll run without raising any error. You can inspect the content saved in Redis with the following command (assuming you’ve got redis-cli and jq installed in your system): echo \"get zoo:awesome_zoo\" | redis-cli | jq This will return the following payload to your console: { \"name\": \"awesome_zoo\", \"animals\": [ { \"name\": \"wolf\", \"attribute\": { \"family\": \"Canidae\", \"genus\": \"Canis\", \"is_mammal\": true } }, { \"name\": \"snake\", \"attribute\": { \"family\": \"Viperidae\", \"genus\": \"Boas\", \"is_mammal\": false } } ] } Although this workflow is functional in runtime, there’s a big gotcha here! It’s really difficult to picture the shape of the payload from the output of the get_payload function; as it dynamically builds the dictionary. First, it declares a dictionary with two fields—name and animals. Here, name is a string value that denotes the name of the zoo. The other field animals is a list containing the names and attributes of the animals in the zoo. Later on, the for-loop fills up the dictionary with nested data structures. This charade of operations makes it difficult to reify the final shape of the resulting payload in your mind. In this case, you’ll have to inspect the content of the Redis cache to fully understand the shape of the data. Writing code in the above manner is effortless but it makes it really hard for the next person working on the codebase to understand how the payload looks without tapping into the data storage. There’s a better way to declaratively communicate the shape of the payload that doesn’t involve writing unmaintainably large docstrings. Here’s how you can leverage TypedDict and Annotated to achieve the goals: # src.py from __future__ import annotations import json # In \u003c Python 3.8, import 'TypedDict' from 'typing_extensions'. # In \u003c Python 3.9, import 'Annotated' from 'typing_extensions'. from typing import Annotated, Any, TypedDict import redis # Do a pip install. class Attribute(TypedDict): family: str genus: str is_mammal: bool class Animal(TypedDict): name: str attribute: Attribute class Zoo(TypedDict): name: str animals: list[Animal] def get_payload() -\u003e Zoo: \"\"\"Get the 'zoo' payload containing animal names and attributes.\"\"\" payload: Zoo = {\"name\": \"awesome_zoo\", \"animals\": []} names = (\"wolf\", \"snake\", \"ostrich\") attributes: tuple[Attribute, ...] = ( {\"family\": \"Canidae\", \"genus\": \"Canis\", \"is_mammal\": True}, {\"family\": \"Viperidae\", \"genus\": \"Boas\", \"is_mammal\": False}, ) for name, attr in zip(names, attributes): payload[\"animals\"].append({\"name\": name, \"attribute\": attr}) return payload def save_to_cache(payload: Annotated[Zoo, dict]) -\u003e None: # You'll need to spin up a Redis db before instantiating # a connection here. r = redis.Redis() print(\"Saving to cache...\") r.set(f\"zoo:{payload['name']}\", json.dumps(payload)) if __name__ == \"__main__\": payload: Zoo = get_payload() save_to_cache(payload) Notice, how I’ve used TypedDict to declare the nested structure of the payload Zoo. In runtime, instances of typed-dict classes behave the same way as normal dicts. Here, Zoo contains two fields—name and animals. The animals field is annotated as list[Animal] where Animal is another typed-dict. The Animal typed-dict houses another typed-dict called Attribute that defines various properties of the animal. Taking a look at the typed-dict Zoo and following along its nested structure, the final shape of the payload becomes clearer without us having to look for example payloads. Also, Mypy can check whether the payload conforms to the shape of the annotated type. I used Annotated[Zoo, dict] in the input parameter of save_to_cache function to communicate with the reader that an instance of the class Zoo is a dict that conforms to the contract laid out in the type itself. The type Annotated can be used to add any arbitrary metadata to a particular type. In runtime, this snippet will exhibit the same behavior as the previous one. Mypy also approves this. Handling missing key-value pairs By default, the type checker will structurally validate the shape of the dict annotated with a TypedDict class and all the key-value pairs expected by the annotation must be present in the dict. It’s possible to lax this behavior by specifying totality. This can be helpful to deal with missing fields without letting go of type safety. Consider this: from __future__ import annotations from typing import TypedDict class Attribute(TypedDict): family: str genus: str is_mammal: bool animal_attribute: Attribute = { \"family\": \"Hominidae\", \"genus\": \"Homo\", } # Mypy will complain about the missing 'is_mammal' key. Mypy will complain about the missing key: src.py:12: error: Missing key \"is_mammal\" for TypedDict \"Attribute\" animal_attribute: Attribute = { ^ Found 1 error in 1 file (checked 1 source file) You can relax this behavior like this: ... class Attribute(TypedDict, total=False): family: str genus: str is_mammal: bool ... Now Mypy will no longer complain about the missing field in the annotated dict. However, this will still disallow arbitrary keys that isn’t defined in the TypedDict. For example: ... # Mypy will complain as the key 'species' doesn't exist in the TypedDict. animal_attribute[\"species\"] = \"Sapiens\" ... src.py:17: error: TypedDict \"Attribute\" has no key \"species\" animal_attribute[\"species\"] = \"Sapiens\" ^ Found 1 error in 1 file (checked 3 source files) make: *** [Makefile:134: mypy] Error 1 Sweet type safety without being too strict about missing fields! Docker ↩︎ PEP 589 – TypedDict: Type hints for dictionaries with a fixed set of keys 2 ↩︎ ","permalink":"http://rednafi.com/python/declarative_payloads_with_typedict/","publishDate":"2022-03-11","summary":"While working with microservices in Python, a common pattern that I see is—the usage of dynamically filled dictionaries as payloads of REST APIs or message queues. To understand what I mean by this, consider the following example:\n# src.py from __future__ import annotations import json from typing import Any import redis # Do a pip install. def get_payload() -\u003e dict[str, Any]: \"\"\"Get the 'zoo' payload containing animal names and attributes.\"\"\" payload = {\"name\": \"awesome_zoo\", \"animals\": []} names = (\"wolf\", \"snake\", \"ostrich\") attributes = ( {\"family\": \"Canidae\", \"genus\": \"Canis\", \"is_mammal\": True}, {\"family\": \"Viperidae\", \"genus\": \"Boas\", \"is_mammal\": False}, ) for name, attr in zip(names, attributes): payload[\"animals\"].append( # type: ignore {\"name\": name, \"attribute\": attr}, ) return payload def save_to_cache(payload: dict[str, Any]) -\u003e None: # You'll need to spin up a Redis db before instantiating # a connection here. r = redis.Redis() print(\"Saving to cache...\") r.set(f\"zoo:{payload['name']}\", json.dumps(payload)) if __name__ == \"__main__\": payload = get_payload() save_to_cache(payload) Here, the get_payload function constructs a payload that gets stored in a Redis DB in the save_to_cache function. The get_payload function returns a dict that denotes a contrived payload containing the data of an imaginary zoo. To execute the above snippet, you’ll need to spin up a Redis database first. You can use Docker1 to do so. Install and configure Docker on your system and run:\n","tags":["Python","Typing"],"title":"Declarative payloads with TypedDict in Python"},{"content":"While most of my pytest fixtures don’t react to the dynamically-passed values of function parameters, there have been situations where I’ve definitely felt the need for that. Consider this example:\n# test_src.py import pytest @pytest.fixture def create_file(tmp_path): \"\"\"Fixture to create a file in the tmp_path/tmp directory.\"\"\" directory = tmp_path / \"tmp\" directory.mkdir() file = directory / \"foo.md\" # The filename is hardcoded here! yield directory, file def test_file_creation(create_file): \"\"\"Check the fixture.\"\"\" directory, file = create_file assert directory.name == \"tmp\" assert file.name == \"foo.md\" Here, in the create_file fixture, I’ve created a file named foo.md in the tmp folder. Notice that the name of the file foo.md is hardcoded inside the body of the fixture function. The fixture yields the path of the directory and the created file.\nLater on, the test_file_creation function just checks whether the fixture is working as expected. This snippet will pass successfully if you execute it with the pytest command.\nNow, if you needed to create three files—foo.md, bar.md, baz.md—how’d you do that in the fixture? You could hardcode the names of the three files in the fixture as follows:\n# test_src.py import pytest @pytest.fixture def create_files(tmp_path): \"\"\" Fixture to create multiple files in the tmp_path/tmp directory. \"\"\" directory = tmp_path / \"tmp\" directory.mkdir() # Notice the hardcoded file names. The fixture can only # create files with these names. filenames = (\"foo.md\", \"bar.md\", \"baz.md\") files = [directory / filename for filename in filenames] yield directory, files def test_file_creation(create_files): \"\"\"Check the fixture.\"\"\" directory, files = create_files expected_filenames = (\"foo.md\", \"bar.md\", \"baz.md\") assert directory.name == \"tmp\" assert all(f.name for f in files if f.name in expected_filenames) I had to change the name of the fixture from create_file to create_files because the output signature of the fixture was changed to yield the directory path and a list of the paths of the three newly created files.\nWhile this works, it’s cumbersome and inflexible. What if one of your tests needs one file and another one demands two files to be created? How’d you tackle that?\nIt’d be much better if we could just pass the filename to the fixture as a parameter and the fixture would then create the corresponding file in the temporary folder. Also, if we need n files to be created, then we’ll just have to execute the fixture n times. There’s a way to do so by leveraging fixture parameters and @pytest.mark.parameterize decorator. This is how you can do it:\n# test_src.py import pytest @pytest.fixture def create_file(tmp_path, filename): \"\"\"Fixture to create a file in the tmp_path/tmp directory.\"\"\" directory = tmp_path / \"tmp\" directory.mkdir() file = directory / filename # The hardcoded filename is gone! yield directory, file @pytest.mark.parametrize(\"filename\", (\"foo.md\", \"bar.md\", \"baz.md\")) def test_file_creation(create_file): \"\"\"Check the fixture.\"\"\" directory, file = create_file expected_filenames = (\"foo.md\", \"bar.md\", \"baz.md\") assert directory.name == \"tmp\" assert all(f for f in expected_filenames if file.name == f) In this case, the fixture create_file takes an additional parameter called filename and then yields the directory path and the file path; just as the first snippet. Later on, in the test_file_creation function, the desired values of the filename parameter is injected into the fixture via the @pytest.mark.parametrize decorator. In the above snippet, pytest runs the fixture 3 times and creates the desired files in 3 passes—just like how a normal function call would behave.\nPass a parameter to a fixture function - Stackoverflow 1 ↩︎\n","permalink":"http://rednafi.com/python/parametrized_fixtures_in_pytest/","publishDate":"2022-03-10","summary":"While most of my pytest fixtures don’t react to the dynamically-passed values of function parameters, there have been situations where I’ve definitely felt the need for that. Consider this example:\n# test_src.py import pytest @pytest.fixture def create_file(tmp_path): \"\"\"Fixture to create a file in the tmp_path/tmp directory.\"\"\" directory = tmp_path / \"tmp\" directory.mkdir() file = directory / \"foo.md\" # The filename is hardcoded here! yield directory, file def test_file_creation(create_file): \"\"\"Check the fixture.\"\"\" directory, file = create_file assert directory.name == \"tmp\" assert file.name == \"foo.md\" Here, in the create_file fixture, I’ve created a file named foo.md in the tmp folder. Notice that the name of the file foo.md is hardcoded inside the body of the fixture function. The fixture yields the path of the directory and the created file.\n","tags":["Python","Testing"],"title":"Parametrized fixtures in pytest"},{"content":"If you try to mutate a sequence while traversing through it, Python usually doesn’t complain. For example: # src.py l = [3, 4, 56, 7, 10, 9, 6, 5] for i in l: if not i % 2 == 0: continue l.remove(i) print(l) The above snippet iterates through a list of numbers and modifies the list l in-place to remove any even number. However, running the script prints out this: [3, 56, 7, 9, 5] Wait a minute! The output doesn’t look correct. The final list still contains 56 which is an even number. Why did it get skipped? Printing the members of the list while the for-loop advances reveal what’s happening inside: 3 4 7 10 6 [3, 56, 7, 9, 5] From the output, it seems like the for-loop doesn’t even visit all the elements of the sequence. However, trying to emulate what happens inside the for-loop with iter and next makes the situation clearer. Notice the following example. I’m using ipython shell to explore: In [1]: l = [3, 4, 56, 7, 10, 9, 6, 5] In [2]: # Make the list an iterator. In [3]: it = iter(l) In [4]: # Emulate for-loop by applying 'next()' function on 'it'. In [5]: next(it) Out[5]: 3 In [6]: next(it) Out[6]: 4 In [7]: # Remove a value that's already been visited by the iterator. In [8]: l.remove(3) In [9]: next(it) Out[9]: 7 In [10]: # Notice how the iterator skipped 56. Remove another. In [11]: l.remove(4) In [12]: next(it) Out[12]: 9 The REPL experiment reveals that: Whenever you remove an element of an iterable that’s already been visited by the iterator, in the next iteration, the iterator will jump right by 1 element. This can make the iterator skip a value. The opposite is also true if you prepend some value to a sequence after the iterator has started iterating. In that case, in the next iteration, the iterator will jump left by 1 element and may visit the same value again. Here’s what happens when you prepend values after the iteration has started: In[1]: l = [3, 4, 56, 7, 10, 9, 6, 5] In[2]: it = iter(l) In[3]: next(it) Out[3]: 3 In[4]: next(it) Out[4]: 4 In[5]: l.insert(0, 44) In[6]: next(it) Out[6]: 4 Notice how the element 4 is being visited twice after prepending a value to the list l. Solution To solve this, you’ll have to make sure the target elements don’t get removed after the iterator has already visited them. You can iterate in the reverse order and remove elements maintaining the original order. The first snippet can be rewritten as follows: # src.py l = [3, 4, 56, 7, 10, 9, 6, 5] # Here, 'reversed' returns a lazy iterator, so it's performant! for i in reversed(l): print(i) if not i % 2 == 0: continue l.remove(i) print(l) Running the script prints: 5 6 9 10 7 56 4 3 [3, 7, 9, 5] Notice, how the iterator now visits all the elements and the final list contains the odd elements as expected. Another way you can solve this is—by copying the list l before iterating. But this can be expensive if l is large: # src.py l = [3, 4, 56, 7, 10, 9, 6, 5] # Here 'l.copy()' creates a shallow copy of 'l'. It's # less performant than 'reversed(l)'. for i in l.copy(): print(i) if not i % 2 == 0: continue l.remove(i) print(l) This time, the order of the iteration and element removal is the same, but that isn’t a problem since these two operations occur on two different lists. Running the snippet produces the following output: 3 4 56 7 10 9 6 5 [3, 7, 9, 5] What about dictionaries Dictionaries don’t even allow you to change their sizes while iterating. The following snippet raises a RuntimeError: # src.py # {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9} d = {k: k for k in range(10)} for k, v in d.items(): if not v % 2 == 0: continue d.pop(k) Traceback (most recent call last): File \"/home/rednafi/canvas/personal/reflections/src.py\", line 4, in for k,v in d.items(): RuntimeError: dictionary changed size during iteration You can solve this by making a copy of the keys of the dictionary and iterating through it while removing the elements from the dictionary. Here’s one way to do it: # src.py # {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9} d = {k: k for k in range(10)} # This creates a copy of all the keys of 'd'. # At least we arent't creating a new copy of the # entire dict and tuple creation is quite fast. for k in tuple(d.keys()): if not d[k] % 2 == 0: continue d.pop(k) print(d) Running the snippet prints: {1: 1, 3: 3, 5: 5, 7: 7, 9: 9} Voila, the key-value pairs of the even numbers have been removed successfully! How to modify a list while iterating - Anthony Sottile 1 ↩︎ ","permalink":"http://rednafi.com/python/modify_iterables_while_iterating/","publishDate":"2022-03-04","summary":"If you try to mutate a sequence while traversing through it, Python usually doesn’t complain. For example:\n# src.py l = [3, 4, 56, 7, 10, 9, 6, 5] for i in l: if not i % 2 == 0: continue l.remove(i) print(l) The above snippet iterates through a list of numbers and modifies the list l in-place to remove any even number. However, running the script prints out this:\n[3, 56, 7, 9, 5] Wait a minute! The output doesn’t look correct. The final list still contains 56 which is an even number. Why did it get skipped? Printing the members of the list while the for-loop advances reveal what’s happening inside:\n","tags":["Python"],"title":"Modify iterables while iterating in Python"},{"content":"Five traits that almost all the GitHub Action workflows in my Python projects share are:\nIf a new workflow is triggered while the previous one is running, the first one will get canceled. The CI is triggered every day at UTC 1. Tests and the lint-checkers are run on Ubuntu and MacOS against multiple Python versions. Pip dependencies are cached. Dependencies, including the Actions dependencies are automatically updated via dependabot1. I use pip-tools2 for managing dependencies in applications and setuptools3 setup.py combo for managing dependencies in libraries. Here’s an annotated version of the template action syntax:\n# .github/workflows/ci.yml name: CI on: # Triggers when something is pushed to the 'main' branch. push: branches: - master # Triggers when a pull request is sent against the 'main' branch. pull_request: branches: - master # Triggers everyday at 1 UTC. schedule: - cron: \"0 1 * * *\" # Cancel any running workflow if the CI gets triggered again. concurrency: group: ${{ github.head_ref || github.run_id }} cancel-in-progress: true jobs: run-tests: # Tests are run on multiple Python versions. runs-on: ${{ matrix.os }} strategy: matrix: # Multiple OSs. os: [ubuntu-latest, macos-latest] # Multiple Python versions. python-version: [\"3.10\", \"3.11\", \"3.12\"] include: - os: ubuntu-latest path: ~/.cache/pip # Cache location on Ubuntu - os: macos-latest path: ~/Library/Caches/pip # Cache location on MacOS steps: # Checkout to the codebase. - uses: actions/checkout@v3 # Sets up Python. - uses: actions/setup-python@v3 with: python-version: ${{ matrix.python-version }} # Cache pip dependencies via 'cache' actions. - uses: actions/cache@v2 with: path: ${{ matrix.path }} key: \"${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}\\ -${{ hashFiles('**/requirements-dev.txt') }}\" restore-keys: | ${{ runner.os }}-pip- # Dev and app dependencies are kept in separate files. - name: Install the Dependencies run: | pip install --upgrade pip pip install -r requirements.txt pip install -r requirements-dev.txt # Run black, isort, flake8, etc. - name: Check Linter run: | echo \"Checking black formatting...\" python3 -m black --check . echo \"Checking isort formatting...\" python3 -m isort --check . echo \"Checking flake8 formatting...\" python3 -m flake8 . # Run the tests via pytest. - name: Run the tests run: | pytest -v -s The dependabot config looks as follows:\n# .github/dependabot.yml version: 2 updates: - package-ecosystem: \"pip\" # See documentation for possible values. directory: \"/\" # Location of package manifests. schedule: interval: \"daily\" # Maintain dependencies for GitHub Actions. - package-ecosystem: \"github-actions\" directory: \"/\" schedule: interval: \"daily\" Dependabot ↩︎\npip-tools ↩︎\nsetuptools ↩︎\nAn active version of the above workflow 4 ↩︎\n","permalink":"http://rednafi.com/python/github_action_template_python/","publishDate":"2022-03-02","summary":"Five traits that almost all the GitHub Action workflows in my Python projects share are:\nIf a new workflow is triggered while the previous one is running, the first one will get canceled. The CI is triggered every day at UTC 1. Tests and the lint-checkers are run on Ubuntu and MacOS against multiple Python versions. Pip dependencies are cached. Dependencies, including the Actions dependencies are automatically updated via dependabot1. I use pip-tools2 for managing dependencies in applications and setuptools3 setup.py combo for managing dependencies in libraries. Here’s an annotated version of the template action syntax:\n","tags":["Python","GitHub"],"title":"Github action template for Python based projects"},{"content":"PEP-6731 introduces the Self type and it’s coming to Python 3.11. However, you can already use that now via the typing_extenstions2 module. The Self type makes annotating methods that return the instances of the corresponding classes trivial. Before this, you’d have to do some mental gymnastics to statically type situations as follows: # src.py from __future__ import annotations from typing import Any class Animal: def __init__(self, name: str, says: str) -\u003e None: self.name = name self.says = says @classmethod def from_description(cls, description: str = \"|\") -\u003e Animal: descr = description.split(\"|\") return cls(descr[0], descr[1]) class Dog(Animal): def __init__(self, *args: Any, **kwargs: Any) -\u003e None: super().__init__(*args, **kwargs) @property def legs(self) -\u003e int: return 4 if __name__ == \"__main__\": dog = Dog.from_description(\"Matt | woof\") print(dog.legs) # Mypy complains here! The class Animal has a from_description class method that acts as an additional constructor. It takes a description string, and then builds and returns an instance of the same class. The return type of the method is annotated as Animal here. However, doing this makes the child class Dog conflate its identity with the Animal class. If you execute the snippet, it won’t raise any runtime error. Also, Mypy will complain about the type: src.py:27: error: \"Animal\" has no attribute \"legs\" print(dog.legs) # Mypy complains here! ^ Found 1 error in 1 file (checked 1 source file) To fix this, we’ll have to make sure that the return type of the from_description class method doesn’t confuse the type checker. This is one way to do this: from __future__ import annotations from typing import TypeVar T = TypeVar(\"T\", bound=\"Animal\") class Animal: def __init__(self, name: str, says: str) -\u003e None: self.name = name self.says = says @classmethod # In T: descr = description.split(\"|\") return cls(descr[0], descr[1]) ... In the above snippet, first I had to declare a TypeVar and bind that to the Animal class. Then I had to explicitly type the cls variable in the from_description method. This time, the type checker will be happy. While this isn’t a lot of work, it surely goes against the community convention. Usually, we don’t explicitly type the self, cls variables and instead, let the type checker figure out their types. Also, subjectively, this sticks out like a sore thumb. PEP-673 allows us to solve the issue elegantly: # src.py from __future__ import annotations import sys if sys.version_info \u003e= (3, 11): from typing import Self else: from typing_extensions import Self class Animal: def __init__(self, name: str, says: str) -\u003e None: self.name = name self.says = says @classmethod def from_description(cls, description: str = \"|\") -\u003e Self: descr = description.split(\"|\") return cls(descr[0], descr[1]) ... If you run Mypy against the second snippet, it won’t complain. Typing instance methods that return self Take a look at this: # src.py from __future__ import annotations import sys if sys.version_info \u003e= (3, 11): from typing import Self else: from typing_extensions import Self class Counter: def __init__(self, start: int = 1) -\u003e None: self.val = start def increment(self) -\u003e Self: self.val += 1 return self def decrement(self) -\u003e Self: self.val -= 1 return self The increment and decrement method of the Counter class return the instance of the same class after performing the operations on the start value. This is a perfect case where the Self type can be useful. Typing __new__ methods You can also type the __new__ method easily: from __future__ import annotations import sys if sys.version_info \u003e= (3, 11): from typing import Self else: from typing_extensions import Self from typing import Any class Config: def __new__(cls, var: int, *args: Any, **kwargs: Any) -\u003e Self: \"\"\"Validate the value before constructing the class.\"\"\" if not 0 \u003c= var \u003c 10: raise TypeError( \"'var' must be a positive integer between 0 and 9\", ) return super().__new__(cls) def __init__(self, var: int) -\u003e None: self.var = var The __new__ method in the Config class validates the var before constructing an instance of the class. The Self type makes it easy to annotate the method. PEP 673 – Self Type ↩︎ typing_extensions ↩︎ Tweet by Raymond Hettinger 3 ↩︎ ","permalink":"http://rednafi.com/python/self_type/","publishDate":"2022-02-28","summary":"PEP-6731 introduces the Self type and it’s coming to Python 3.11. However, you can already use that now via the typing_extenstions2 module.\nThe Self type makes annotating methods that return the instances of the corresponding classes trivial. Before this, you’d have to do some mental gymnastics to statically type situations as follows:\n# src.py from __future__ import annotations from typing import Any class Animal: def __init__(self, name: str, says: str) -\u003e None: self.name = name self.says = says @classmethod def from_description(cls, description: str = \"|\") -\u003e Animal: descr = description.split(\"|\") return cls(descr[0], descr[1]) class Dog(Animal): def __init__(self, *args: Any, **kwargs: Any) -\u003e None: super().__init__(*args, **kwargs) @property def legs(self) -\u003e int: return 4 if __name__ == \"__main__\": dog = Dog.from_description(\"Matt | woof\") print(dog.legs) # Mypy complains here! The class Animal has a from_description class method that acts as an additional constructor. It takes a description string, and then builds and returns an instance of the same class. The return type of the method is annotated as Animal here. However, doing this makes the child class Dog conflate its identity with the Animal class. If you execute the snippet, it won’t raise any runtime error. Also, Mypy will complain about the type:\n","tags":["Python","Typing"],"title":"Self type in Python"},{"content":"In Python, even though I adore writing tests in a functional manner via pytest, I still have a soft corner for the tools provided in the unittest.mock module. I like the fact it’s baked into the standard library and is quite flexible. Moreover, I’m yet to see another mock library in any other language or in the Python ecosystem that allows you to mock your targets in such a terse, flexible, and maintainable fashion. So, in almost all the tests that I write for both my OSS projects and at my workplace, I use unittest.mock.patch exclusively for performing mock-patch. Consider this example: # src.py from __future__ import annotations import random # In ","permalink":"http://rednafi.com/python/patch_with_pytest_fixture/","publishDate":"2022-02-27","summary":"In Python, even though I adore writing tests in a functional manner via pytest, I still have a soft corner for the tools provided in the unittest.mock module. I like the fact it’s baked into the standard library and is quite flexible. Moreover, I’m yet to see another mock library in any other language or in the Python ecosystem that allows you to mock your targets in such a terse, flexible, and maintainable fashion.\n","tags":["Python","Testing"],"title":"Patching test dependencies via pytest fixture \u0026 unittest mock"},{"content":"Static type checkers like Mypy follow your code flow and statically try to figure out the types of the variables without you having to explicitly annotate inline expressions. For example: # src.py from __future__ import annotations def check(x: int | float) -\u003e str: if not isinstance(x, int): reveal_type(x) # Type is now 'float'. else: reveal_type(x) # Type is now 'int'. return str(x) The reveal_type function is provided by Mypy and you don’t need to import this. But remember to remove the function before executing the snippet. Otherwise, Python will raise a runtime error as the function is only understood by Mypy. If you run Mypy against this snippet, it’ll print the following lines: src.py:6: note: Revealed type is \"builtins.float\" src.py:10: note: Revealed type is \"builtins.int\" Here, I didn’t have to explicitly tell the type checker how the conditionals narrow the types. Static type checkers commonly employ a technique called ’type narrowing’ to determine a more precise type of an expression within a program’s code flow. When type narrowing is applied within a block of code based on a conditional code flow statement (such as if and while statements), the conditional expression is sometimes referred to as a ’type guard’. — PEP-647 So, in the above snippet, Mypy performed type narrowing to determine the more precise type of the variable x; and the if ... else conditionals, in this case, is known as type guards. However, when the type checker encounters a complex expression, often time, it can’t figure out the types statically. Mypy will complain when it faces one of these issues: from __future__ import annotations # In ","permalink":"http://rednafi.com/python/type_guard/","publishDate":"2022-02-23","summary":"Static type checkers like Mypy follow your code flow and statically try to figure out the types of the variables without you having to explicitly annotate inline expressions. For example:\n# src.py from __future__ import annotations def check(x: int | float) -\u003e str: if not isinstance(x, int): reveal_type(x) # Type is now 'float'. else: reveal_type(x) # Type is now 'int'. return str(x) The reveal_type function is provided by Mypy and you don’t need to import this. But remember to remove the function before executing the snippet. Otherwise, Python will raise a runtime error as the function is only understood by Mypy. If you run Mypy against this snippet, it’ll print the following lines:\n","tags":["Python","Typing"],"title":"Narrowing types with TypeGuard in Python"},{"content":"Technically, the type of None in Python is NoneType. However, you’ll rarely see types.NoneType being used in the wild as the community has pretty much adopted None to denote the type of the None singleton. This usage is also documented1 in PEP-484. Whenever a callable doesn’t return anything, you usually annotate it as follows: # src.py from __future__ import annotations def abyss() -\u003e None: return But sometimes a callable raises an exception and never gets the chance to return anything. Consider this example: # src.py from __future__ import annotations import logging def raise_verbose_type_error(message: str) -\u003e None: logging.error(\"Raising type error\") raise TypeError(message) if __name__ == \"__main__\": raise_verbose_type_error(\"type error occured\") This semantically makes sense and if you run Mypy against the snippet, it won’t complain. However, there’s one difference between a callable that returns an implicit None vs one that raises an exception. In the latter case, if you run any code after calling the callable, that code won’t be reachable. But Mypy doesn’t statically catch that or warn you about the potential dead code. This is apparently fine by the type checker: ... if __name__ == \"__main__\": raise_verbose_type_error(\"type error occured\") print( \"This part of the code is unreachable due to the exception\" \"above, but Mypy doesn't warn us.\" ) NoReturn type can be used in cases like this to warn us about potential dead code ahead. To utilize it, you’d type the above snippet like this: # src.py from __future__ import annotations import logging from typing import NoReturn def raise_verbose_type_error(message: str) -\u003e NoReturn: logging.error(\"Raising type error\") raise TypeError(message) if __name__ == \"__main__\": raise_verbose_type_error(\"type error occured\") print( \"This part of the code is unreachable due to the exception\" \"above, but this time, Mypy will warn us.\" ) Notice, that I changed the return type of the raise_verbose_type_error function to typing.NoReturn. Now, if you run Mypy against the snippet with the --warn-unreachable flag, it’ll complain: mypy --warn-unreachable src.py src.py:14: error: Statement is unreachable print( ^ Found 1 error in 1 file (checked 1 source file) More practical examples Callables containing infinite loops # src.py from __future__ import annotations import itertools from typing import NoReturn def run_indefinitely() -\u003e NoReturn: for i in itertools.cycle(\"abc\"): print(i) if __name__ == \"__main__\": run_indefinitely() print(\" Dead code. Mypy will warn us.\") Mypy will warn us about the dead code. src.py:14: error: Statement is unreachable print( ^ Found 1 error in 1 file (checked 1 source file) Another case where NoReturn can be useful, is to type callables with while True loops. This is common in webservers: # src.py from __future__ import annotations from typing import NoReturn def loop_forever() -\u003e NoReturn: while True: do_something() Callables that invoke ‘sys.exit()’, ‘os._exit()’, ‘os.execvp()’, etc Both sys.exit() and os._exit() do similar things. The former function raises the SystemExit() exception and exits the program without printing any stacktrace or whatsoever. On the other hand, the latter function exits the process immediately without letting the interpreter run any cleanup code. Prefer sys.exit() over os._exit(). The os.execvp() function execute a new program, replacing the current process. It never returns. Here’s how you’d type the callables that call these functions: # src.py from __future__ import annotations import os import sys from typing import NoReturn def call_sys_exit(code: int) -\u003e NoReturn: sys.exit(code) def call_os_exit(code: int) -\u003e NoReturn: os._exit(code) def call_os_execvp() -\u003e NoReturn: os.execvp(\"echo\", (\"echo\", \"hi\")) Using None ↩︎ Python return annotations: NoReturn vs None (intermediate) anthony explains #007 2 ↩︎ Python type hints - what’s the point of NoReturn? - Adam Johnson 3 ↩︎ ","permalink":"http://rednafi.com/python/why_noreturn_type_exists/","publishDate":"2022-02-21","summary":"Technically, the type of None in Python is NoneType. However, you’ll rarely see types.NoneType being used in the wild as the community has pretty much adopted None to denote the type of the None singleton. This usage is also documented1 in PEP-484.\nWhenever a callable doesn’t return anything, you usually annotate it as follows:\n# src.py from __future__ import annotations def abyss() -\u003e None: return But sometimes a callable raises an exception and never gets the chance to return anything. Consider this example:\n","tags":["Python","Typing"],"title":"Why 'NoReturn' type exists in Python"},{"content":"While grokking the source code of http.HTTPStatus module, I came across this technique to add extra attributes to the values of enum members. Now, to understand what do I mean by adding attributes, let’s consider the following example: # src.py from __future__ import annotations from enum import Enum class Color(str, Enum): RED = \"Red\" GREEN = \"Green\" BLUE = \"Blue\" Here, I’ve inherited from str to ensure that the values of the enum members are strings. This class can be used as follows: # src.py ... # Print individual members. print(f\"{Color.RED=}\") # Print name as a string. print(f\"{Color.GREEN.name=}\") # Print value. print(f\"{Color.BLUE.value=}\") Running the script will print: Color.RED=","permalink":"http://rednafi.com/python/add_attributes_to_enum_members/","publishDate":"2022-02-17","summary":"While grokking the source code of http.HTTPStatus module, I came across this technique to add extra attributes to the values of enum members. Now, to understand what do I mean by adding attributes, let’s consider the following example:\n# src.py from __future__ import annotations from enum import Enum class Color(str, Enum): RED = \"Red\" GREEN = \"Green\" BLUE = \"Blue\" Here, I’ve inherited from str to ensure that the values of the enum members are strings. This class can be used as follows:\n","tags":["Python"],"title":"Add extra attributes to enum members in Python"},{"content":"The functools.wraps decorator allows you to keep your function’s identity intact after it’s been wrapped by a decorator. Whenever a function is wrapped by a decorator, identity properties like—function name, docstring, annotations of it get replaced by those of the wrapper function. Consider this example: from __future__ import annotations # In \u003c Python 3.9, import this from the typing module. from collections.abc import Callable from typing import Any def log(func: Callable) -\u003e Callable: def wrapper(*args: Any, **kwargs: Any) -\u003e Any: \"\"\"Internal wrapper.\"\"\" val = func(*args, **kwargs) return val return wrapper @log def add(x: int, y: int) -\u003e int: \"\"\"Add two numbers. Parameters ---------- x : int First argument. y : int Second argument. Returns ------- int Returns the summation of two integers. \"\"\" return x + y if __name__ == \"__main__\": print(add.__doc__) print(add.__name__) Here, I’ve defined a simple logging decorator that wraps the add function. The function add has its own type annotations and docstring. So, you’d expect the docstring and name of the add function to be printed when the above snippet gets executed. However, running the script prints the following instead: Internal wrapper. wrapper This is surprising and probably not something you want. If you pay attention to the function wrapper in the log decorator, you’ll see that the identity properties of the wrapper function replace the identity properties of the wrapped function add. This can easily be avoided by decorating the wrapper function inside the log decorator with the functools.wraps decorator: # src.py from functools import wraps ... def log(func: Callable) -\u003e Callable: @wraps(func) # Here's the decorator! def wrapper(*args: Any, **kwargs: Any) -\u003e Any: \"\"\"Internal wrapper.\"\"\" val = func(*args, **kwargs) return val return wrapper ... Now, running the script will return the expected output: Add two numbers. Parameters ---------- x : int First argument. y : int Second argument. Returns ------- int Returns the summation of two integers. add I wanted to take a peek into how the functools.wraps decorator works internally. Turns out that the implementation is quite straightforward. Here’s the entire implementation from the functools.py module. For brevity’s sake, I’ve stripped out the comments and added type annotations: # functools.py from __future__ import annotations # In \u003c Python 3.9, import this from the typing module. from collections.abc import Callable WRAPPER_ASSIGNMENTS = ( \"__module__\", \"__name__\", \"__qualname__\", \"__doc__\", \"__annotations__\", ) WRAPPER_UPDATES = (\"__dict__\",) def update_wrapper( wrapper: Callable, wrapped: Callable, assigned: tuple[str, ...] = WRAPPER_ASSIGNMENTS, updated: tuple[str, ...] = WRAPPER_UPDATES, ) -\u003e Callable: for attr in assigned: try: value = getattr(wrapped, attr) except AttributeError: pass else: setattr(wrapper, attr, value) for attr in updated: getattr(wrapper, attr).update(getattr(wrapped, attr, {})) wrapper.__wrapped__ = wrapped return wrapper def wraps( wrapped: Callable, assigned: tuple[str, ...] = WRAPPER_ASSIGNMENTS, updated: tuple[str, ...] = WRAPPER_UPDATES, ) -\u003e Callable: return partial( update_wrapper, wrapped=wrapped, assigned=assigned, updated=updated, ) The bulk of the work is done in the update_wrapper function. It copies the identity properties defined in WRAPPER_ASSIGNMENTS and WRAPPER_UPDATES—from the wrapped function over to the wrapper function. Here, the wrapped function is the decorated one (add function) and the wrapper function is the eponymous function inside the log decorator. Since you’ve already seen that whenever you try to introspect the identity properties of a wrapped function, the wrapper function obfuscates them and returns its own properties. However, if the identity properties are copied over from the wrapped to the wrapper function, your inspection will return the expected result. The update_wrapper function is doing exactly that. The wraps function just binds the input arguments with the update_wrapper function using the partial function defined in the same module. This allows us to use the wraps function as a decorator. You can also directly use the update_wrapper function to get the same result should you choose to do so. Here’s how to do it: # src.py from functools import update_wrapper ... def log(func: Callable) -\u003e Callable: def wrapper(*args: Any, **kwargs: Any) -\u003e Any: \"\"\"Internal wrapper.\"\"\" val = func(*args, **kwargs) return val # Only this line is different! return update_wrapper(func, wrapper) ... functools.update_wrapper 1 ↩︎ ","permalink":"http://rednafi.com/python/internals_of_functools_wraps/","publishDate":"2022-02-14","summary":"The functools.wraps decorator allows you to keep your function’s identity intact after it’s been wrapped by a decorator. Whenever a function is wrapped by a decorator, identity properties like—function name, docstring, annotations of it get replaced by those of the wrapper function. Consider this example:\nfrom __future__ import annotations # In \u003c Python 3.9, import this from the typing module. from collections.abc import Callable from typing import Any def log(func: Callable) -\u003e Callable: def wrapper(*args: Any, **kwargs: Any) -\u003e Any: \"\"\"Internal wrapper.\"\"\" val = func(*args, **kwargs) return val return wrapper @log def add(x: int, y: int) -\u003e int: \"\"\"Add two numbers. Parameters ---------- x : int First argument. y : int Second argument. Returns ------- int Returns the summation of two integers. \"\"\" return x + y if __name__ == \"__main__\": print(add.__doc__) print(add.__name__) Here, I’ve defined a simple logging decorator that wraps the add function. The function add has its own type annotations and docstring. So, you’d expect the docstring and name of the add function to be printed when the above snippet gets executed. However, running the script prints the following instead:\n","tags":["Python"],"title":"Peeking into the internals of Python's 'functools.wraps' decorator"},{"content":"I was working with a rate-limited API endpoint where I continuously needed to send short polling GET requests without hitting HTTP 429 error. Perusing the API doc, I found out that the API endpoint only allows a maximum of 100 requests per second. So, my goal was to find out a way to send the maximum amount of requests without encountering the too-many-requests error. I picked up Python’s asyncio1 and the amazing HTTPx2 library by Tom Christie to make the requests. This is the naive version that I wrote in the beginning; it quickly hits the HTTP 429 error: # src.py from __future__ import annotations import asyncio from http import HTTPStatus from pprint import pprint import httpx # Reusing http client allows us to reuse a pool of TCP connections. client = httpx.AsyncClient() async def make_one_request(url: str, num: int) -\u003e httpx.Response: headers = {\"Content-Type\": \"application/json\"} print(f\"Making request {num}\") r = await client.get(url, headers=headers) if r.status_code == HTTPStatus.OK: return r raise ValueError( f\"Unexpected Status: Http status code is {r.status_code}.\", ) async def make_many_requests(url: str, count: int) -\u003e list[httpx.Response]: tasks = [] for num in range(count): task = asyncio.create_task(make_one_request(url, num)) tasks.append(task) results = await asyncio.gather(*tasks) # All the results will look the same, so we're just printing one. print(\"\\n\") print(\"Final result:\") print(\"==============\\n\") pprint(results[0].json()) return results if __name__ == \"__main__\": asyncio.run(make_many_requests(\"https://httpbin.org/get\", count=200)) Here, for this demonstration, I’m using the https://httpbin.org/get endpoint that’s openly accessible. This particular endpoint doesn’t impose any limit on the number of requests per second. However, in the above snippet, if you inspect the for loop in the make_many_requests function, you’ll see that it’s sending 200 concurrent requests without any restrictions. Also, the snippet will raise a ValueError if it encounters an HTTP-429-too-many-requests error. Running the script produces the following output: Making request 0 Making request 1 Making request 2 Making request 3 Making request 4 Making request 5 Making request 6 ... Making request 199 Final result: ============== {'args': {}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Type': 'application/json', 'Host': 'httpbin.org', 'User-Agent': 'python-httpx/0.22.0', 'X-Amzn-Trace-Id': 'Root=1-62042fc6-007ccd7d6b2cf5c15c0963f6'}, 'origin': '103.84.246.3', 'url': 'https://httpbin.org/get'} From the output, it’s pretty evident that the script is hammering the server without any delay between the concurrent requests. While 200 requests per second may not be that high but even if there weren’t any restrictions, sending so many rogue requests like that isn’t desirable. It’s easy to overwhelm any service if you’re not being careful. Luckily, Python exposes a Semaphore construct that allows you to synchronize the concurrent workers (threads, processes, or coroutines) regarding how they should access a shared resource. All concurrency primitives in Python have semaphores to help you control resource access. This means if you’re using any of the—multiprocessing, threading, or asyncio module, you can take advantage of it. From the asyncio docs: A semaphore manages an internal counter which is decremented by each acquire() call and incremented by each release() call. The counter can never go below zero; when acquire() finds that it is zero, it blocks, waiting until some task calls release(). You can use the semaphores in the above script as follows: ... # Initialize a semaphore object with a limit of 3. limit = asyncio.Semaphore(3) async def make_one_request(url: str, num: int) -\u003e httpx.Response: headers = {\"Content-Type\": \"application/json\"} # No more than 3 concurrent workers will be able to make # get request at the same time. async with limit: print(f\"Making request {num}\") r = await client.get(url, headers=headers) # When workers hit the limit, they'll wait for a second # before making more requests. if limit.locked(): print(\"Concurrency limit reached, waiting ...\") await asyncio.sleep(1) if r.status_code == HTTPStatus.OK: return r raise ValueError( f\"Unexpected Status: Http status code is {r.status_code}.\", ) ... Here, I only had to change the make_one_request function to take advantage of the semaphore. First, I initialized an asyncio.Semaphore object with the limit 3. This means the semaphore won’t allow more than three concurrent workers to make HTTP GET requests at the same time. The semaphore instance is then used as a context manager. Inside the async with block, the line starting with if limit.locked() makes the workers wait for a second whenever the concurrency limit is reached. If you execute the script, it’ll produce the following output: Making request 0 Making request 1 Making request 2 Concurrency limit reached, waiting ... Concurrency limit reached, waiting ... Concurrency limit reached, waiting ... Making request 3 Making request 4 Making request 5 Concurrency limit reached, waiting ... Concurrency limit reached, waiting ... Concurrency limit reached, waiting ... Making request 6 Making request 7 Making request 8 ... Making request 199 ... The output makes it clear that no more than 3 async functions are making concurrent requests to the server at the same time. You can tune the number of concurrent workers by changing the limit in the asyncio.Semaphore object. Complete script # src.py from __future__ import annotations import asyncio from http import HTTPStatus from pprint import pprint import httpx # Reusing http client allows us to reuse a pool of TCP connections. client = httpx.AsyncClient() # Initialize a semaphore object with a limit of 3. limit = asyncio.Semaphore(3) async def make_one_request(url: str, num: int) -\u003e httpx.Response: headers = {\"Content-Type\": \"application/json\"} # No more than 3 concurrent workers will be able to make # get request at the same time. async with limit: print(f\"Making request {num}\") r = await client.get(url, headers=headers) # When workers hit the limit, they'll wait for a second # before making more requests. if limit.locked(): print(\"Concurrency limit reached, waiting ...\") await asyncio.sleep(1) if r.status_code == HTTPStatus.OK: return r raise ValueError( f\"Unexpected Status: Http status code is {r.status_code}.\", ) async def make_many_requests(url: str, count: int) -\u003e list[httpx.Response]: tasks = [] for num in range(count): task = asyncio.create_task(make_one_request(url, num)) tasks.append(task) results = await asyncio.gather(*tasks) # All the results will look the same, so we're just printing one. print(\"\\n\") print(\"Final result:\") print(\"==============\\n\") pprint(results[0].json()) return results if __name__ == \"__main__\": asyncio.run(make_many_requests(\"https://httpbin.org/get\", count=200)) asyncio ↩︎ HTTPx ↩︎ Limiting Concurrent Requests with Semaphore - Think Async 3 ↩︎ ","permalink":"http://rednafi.com/python/limit_concurrency_with_semaphore/","publishDate":"2022-02-10","summary":"I was working with a rate-limited API endpoint where I continuously needed to send short polling GET requests without hitting HTTP 429 error. Perusing the API doc, I found out that the API endpoint only allows a maximum of 100 requests per second. So, my goal was to find out a way to send the maximum amount of requests without encountering the too-many-requests error.\nI picked up Python’s asyncio1 and the amazing HTTPx2 library by Tom Christie to make the requests. This is the naive version that I wrote in the beginning; it quickly hits the HTTP 429 error:\n","tags":["Python"],"title":"Limit concurrency with semaphore in Python asyncio"},{"content":"Whether you like it or not, the split world of sync and async functions in the Python ecosystem is something we’ll have to live with; at least for now. So, having to write things that work with both sync and async code is an inevitable part of the journey. Projects like Starlette1, HTTPx2 can give you some clever pointers on how to craft APIs that are compatible with both sync and async code. Lately, I’ve been calling constructs that are compatible with both synchronous and asynchronous paradigms as Amphibian Constructs. So, I wanted to write an amphibian decorator that’d work with both sync and async functions. Let’s consider writing a trivial decorator that’ll tag the wrapped function. Here, by tagging I mean, the decorator will attach a _tags attribute to the wrapped function where the value of the tag can be passed as the function parameter. This type of tagging can be helpful if you want to write code that’ll classify functions based on their tags and do interesting things with them. Locust3 uses this concept of tagging to select and deselect load-testing routines in the CLI. Also, @pytest.mark.* utilizes a similar concept. Here’s how you can do that: # src.py from __future__ import annotations import inspect # In ","permalink":"http://rednafi.com/python/amphibian_decorators/","publishDate":"2022-02-06","summary":"Whether you like it or not, the split world of sync and async functions in the Python ecosystem is something we’ll have to live with; at least for now. So, having to write things that work with both sync and async code is an inevitable part of the journey. Projects like Starlette1, HTTPx2 can give you some clever pointers on how to craft APIs that are compatible with both sync and async code.\n","tags":["Python"],"title":"Amphibian decorators in Python"},{"content":"While grokking Black formatter’s codebase, I came across this1 interesting way of handling exceptions in Python. Exception handling in Python usually follows the EAFP paradigm where it’s easier to ask for forgiveness than permission. However, Rust has this recoverable error2 handling workflow that leverages generic Enums. I wanted to explore how Black emulates that in Python. This is how it works: # src.py from __future__ import annotations from typing import Generic, TypeVar, Union T = TypeVar(\"T\") E = TypeVar(\"E\", bound=Exception) class Ok(Generic[T]): def __init__(self, value: T) -\u003e None: self._value = value def ok(self) -\u003e T: return self._value class Err(Generic[E]): def __init__(self, e: E) -\u003e None: self._e = e def err(self) -\u003e E: return self._e Result = Union[Ok[T], Err[E]] In the above snippet, two generic types Ok and Err represent the return type and the error types of a callable respectively. These two generics were then combined into one Result generic type. You’d use the Result generic to handle exceptions as follows: # src.py ... def div(dividend: int, divisor: int) -\u003e Result[int, ZeroDivisionError]: if divisor == 0: return Err(ZeroDivisionError(\"Zero division error occurred!\")) return Ok(dividend // divisor) if __name__ == \"__main__\": result = div(10, 0) if isinstance(result, Ok): print(result.ok()) else: print(result.err()) This will print: Zero division error occurred! If you run Mypy on the snippet, it’ll succeed as well. You can also apply constraints on the return or exception types as follows: # src.py ... # Only int, float, and str types are allowed as input. Convertible = TypeVar(\"Convertible\", int, float, str) # Create a more specialized generic type from Result. IntResult = Result[int, TypeError] def to_int(num: Convertible) -\u003e IntResult: \"\"\"Converts a convertible input to an integer.\"\"\" if not isinstance(num, (int, float, str)): return Err( TypeError( \"Input type is not convertible to an integer type.\", ) ) return Ok(int(num)) if __name__ == \"__main__\": result = to_int(1 + 2j) if isinstance(result, Ok): print(result.ok()) else: print(result.err()) Running the script will give you this: Input type is not convertible to an integer type. In this case, Mypy will catch the type inconsistency before runtime. Breadcrumbs Black extensively uses this pattern3 in the transformation part of the codebase. This showed me another way of thinking about handling recoverable exceptions while ensuring type safety in a Python codebase. However, I wouldn’t go about and mindlessly refactor any exception handling logic that I come across to follow this pattern. You might find it useful if you need to handle exceptions in a recoverable fashion and need additional type safety around the logic. An error-handling model influenced by the Rust programming language ↩︎ Recoverable errors with result ↩︎ More rusty error handling in Black ↩︎ Beginner’s guide to error handling in Rust 4 ↩︎ ","permalink":"http://rednafi.com/python/go_rusty_with_exception_handling/","publishDate":"2022-02-02","summary":"While grokking Black formatter’s codebase, I came across this1 interesting way of handling exceptions in Python. Exception handling in Python usually follows the EAFP paradigm where it’s easier to ask for forgiveness than permission.\nHowever, Rust has this recoverable error2 handling workflow that leverages generic Enums. I wanted to explore how Black emulates that in Python. This is how it works:\n# src.py from __future__ import annotations from typing import Generic, TypeVar, Union T = TypeVar(\"T\") E = TypeVar(\"E\", bound=Exception) class Ok(Generic[T]): def __init__(self, value: T) -\u003e None: self._value = value def ok(self) -\u003e T: return self._value class Err(Generic[E]): def __init__(self, e: E) -\u003e None: self._e = e def err(self) -\u003e E: return self._e Result = Union[Ok[T], Err[E]] In the above snippet, two generic types Ok and Err represent the return type and the error types of a callable respectively. These two generics were then combined into one Result generic type. You’d use the Result generic to handle exceptions as follows:\n","tags":["Python"],"title":"Go Rusty with exception handling in Python"},{"content":"I’ve always had a hard time explaining variance of generic types while working with type annotations in Python. This is an attempt to distill the things I’ve picked up on type variance while going through PEP-483. A pinch of type theory A generic type is a class or interface that is parameterized over types. Variance refers to how subtyping between the generic types relates to subtyping between their parameters' types. Throughout this text, the notation T2 \u003c: T1 denotes T2 is a subtype of T1. A subtype always lives in the pointy end. If T2 \u003c: T1, then a generic type constructor GenType will be: Covariant, if GenType[T2] \u003c: GenType[T1] for all such T1 and T2. Contravariant, if GenType[T1] \u003c: GenType[T2] for all such T1 and T2. Invariant, if neither of the above is true. To better understand this definition, let’s make an analogy with ordinary functions. Assume that we have: # src.py from __future__ import annotations def cov(x: float) -\u003e float: return 2 * x def contra(x: float) -\u003e float: return -x def inv(x: float) -\u003e float: return x * x If x1 \u003c x2, then always cov(x1) \u003c cov(x2), and contra(x2) \u003c contra(x1), while nothing could be said about inv. Replacing \u003c with \u003c:, and functions with generic type constructors, we get examples of covariant, contravariant, and invariant behavior. A few practical examples Immutable generic types are usually type covariant For example: Union behaves covariantly in all its arguments. That means: if T2 \u003c: T1, then Union[T2] \u003c: Union[T1] for all such T1 and T2. FrozenSet[T] is also covariant. Let’s consider int and float in place of T. First, int \u003c: float. Second, a set of values of FrozenSet[int] is clearly a subset of values of FrozenSet[float]. Therefore, FrozenSet[int] \u003c: FrozenSet[float]. Mutable generic types are usually type invariant For example: list[T] is invariant. Although a set of values of list[int] is a subset of values of list[float], only an int could be appended to a list[int]. Therefore, list[int] is not a subtype of list[float]. The callable generic type is covariant in return type but contravariant in the arguments Callable[[], int] \u003c: Callable[[], float] . If Manager \u003c: Employee then Callable[[], Manager] \u003c: Callable[[], Employee]. However, for two callable types that differ only in the type of one argument, the subtype relationship for the callable types goes in the opposite direction as for the argument types. Examples: Callable[[float], None] \u003c: Callable[[int], None], where int \u003c: float. Callable[[Employee], None] \u003c: Callable[[Manager], None], where Manager \u003c: Employee. I found this odd at first. However, this actually makes sense. If a function can calculate the salary for a Manager, it should also be able to calculate the salary of an Employee. Examples Covariance # src.py from __future__ import annotations # In ","permalink":"http://rednafi.com/python/variance_of_generic_types/","publishDate":"2022-01-31","summary":"I’ve always had a hard time explaining variance of generic types while working with type annotations in Python. This is an attempt to distill the things I’ve picked up on type variance while going through PEP-483.\nA pinch of type theory A generic type is a class or interface that is parameterized over types. Variance refers to how subtyping between the generic types relates to subtyping between their parameters' types.\n","tags":["Python","Typing"],"title":"Variance of generic types in Python"},{"content":"How’d you create a sub dictionary from a dictionary where the keys of the sub-dict are provided as a list?\nI was reading a tweet1 by Ned Bachelder on this today and that made me realize that I usually solve it with O(DK) complexity, where K is the length of the sub-dict keys and D is the length of the primary dict. Here’s how I usually do that without giving it any thoughts or whatsoever:\n# src.py from __future__ import annotations main_dict = { \"this\": 0, \"is\": 1, \"an\": 2, \"example\": 3, \"of\": 4, \"speech\": 5, \"synthesis\": 6, \"in\": 7, \"english\": 8, } sub_keys = [\"this\", \"is\", \"an\", \"example\"] sub_dict = {k: v for k, v in main_dict.items() if k in sub_keys} print(sub_dict) This prints:\n{'this': 0, 'is': 1, 'an': 2, 'example': 3} While this works fine, if you look carefully you’ll notice that in the above snippet, the complexity of creating the sub-dict is O(DK). This means, in the worst-case scenario, it’ll have to traverse the entire length of the main-dict and all the keys of the sub-dict to create the sub-dict. We can do better. Consider this:\n# src.py ... # Only this line is different from the previous snippet. sub_dict = {k: main_dict[k] for k in sub_keys} ... It prints out the same thing as before:\n{'this': 0, 'is': 1, 'an': 2, 'example': 3} It’s quite a bit faster because in the worst case scenario, it’ll only have to traverse the entire sub_keys list—O(K) complexity achieved. This is so simple and elegant. How did I miss that! There’s another functional but subjectively less readable way of achieving the same thing. Here you go:\n# src.py from operator import itemgetter ... sub_dict = dict(zip(sub_keys, itemgetter(*sub_keys)(main_dict))) ... Benchmarks I ran this naive benchmark in an ipython console:\n... In [3]: %timeit {k: v for k, v in main_dict.items() if k in sub_keys} 886 ns ± 7.68 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each) In [4]: %timeit {k:main_dict[k] for k in sub_keys} 340 ns ± 2.87 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each) In [5]: %timeit dict(zip(sub_keys, itemgetter(*sub_keys)(main_dict))) 581 ns ± 2.73 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each) ... It shows that the solution I was using does suffer from the effects of O(DK) complexity even when the dict size is as small as 9 elements. The second solution is the fastest and the least complex one to understand. While the third one is better than the first solution, it’s a gratuitously complex way of doing something so trivial.\nNed Bachelder’s tweet ↩︎\nThe second solution came out of a comment on the same tweet 2 ↩︎\n","permalink":"http://rednafi.com/python/create_sub_dict/","publishDate":"2022-01-30","summary":"How’d you create a sub dictionary from a dictionary where the keys of the sub-dict are provided as a list?\nI was reading a tweet1 by Ned Bachelder on this today and that made me realize that I usually solve it with O(DK) complexity, where K is the length of the sub-dict keys and D is the length of the primary dict. Here’s how I usually do that without giving it any thoughts or whatsoever:\n","tags":["Python","TIL"],"title":"Create a sub dictionary with O(K) complexity in Python"},{"content":"I was reading a tweet about it yesterday and that didn’t stop me from pushing a code change in production with the same rookie mistake today. Consider this function: # src.py from __future__ import annotations import logging import time from datetime import datetime def log( message: str, /, *, level: str, timestamp: str = datetime.utcnow().isoformat(), ) -\u003e None: logger = getattr(logging, level) # Avoid f-string in logging as it's not lazy. logger(\"Timestamp: %s \\nMessage: %s\\n\" % (timestamp, message)) if __name__ == \"__main__\": for _ in range(3): time.sleep(1) log(\"Reality can often be disappointing.\", level=\"warning\") Here, the function log has a parameter timestamp that computes its default value using the built-in datetime.utcnow().isoformat() method. I was under the impression that the timestamp parameter would be computed each time when the log function was called. However, that’s not what happens when you try to run it. If you run the above snippet, you’ll get this instead: WARNING:root:Timestamp: 2022-01-27T19:57:34.147403 Message: Reality can often be disappointing. WARNING:root:Timestamp: 2022-01-27T19:57:34.147403 Message: Reality can often be disappointing. WARNING:root:Timestamp: 2022-01-27T19:57:34.147403 Message: Reality can often be disappointing. In the __main__ block, I’m calling the log function 3 times with a 1-second delay between each invocation. But if you take a look at the timestamp of each of the log entries in the output, you’ll notice that all 3 of them are exactly the same. Default function arguments are early-bound in Python. That means: Python interpreter will bind the default parameters at function definition time and will use that static value at run time. It’s also true for methods. This design choice was intentional. We’re getting the same value of the timestamp each time because Python is computing the value of the default timestamp parameter once in the function definition time and then reusing the same value across all the function calls. The log function was called 3 times but the timestamp function was invoked only once; during the function definition time. This is easy to fix. Remove the default value of the timestamp and explicitly pass the parameter value while calling the function: # src.py from __future__ import annotations import logging import time from datetime import datetime def log( message: str, /, *, level: str, timestamp: str, # No default value here. ) -\u003e None: logger = getattr(logging, level) # Avoid f-string in logging as it's not lazy. logger(\"Timestamp: %s \\nMessage: %s\\n\" % (timestamp, message)) if __name__ == \"__main__\": for _ in range(3): time.sleep(1) log( \"Reality can often be disappointing.\", level=\"warning\", # Pass this explicitly. timestamp=datetime.utcnow().isoformat(), ) Now if you run it, you’ll get this: WARNING:root:Timestamp: 2022-01-27T20:19:47.618326 Message: Reality can often be disappointing. WARNING:root:Timestamp: 2022-01-27T20:19:48.618761 Message: Reality can often be disappointing. WARNING:root:Timestamp: 2022-01-27T20:19:49.620116 Message: Reality can often be disappointing. Notice, how the values of the seconds in the timestamps have roughly a 1-second delay between them. Early-bound defaults can also produce surprising results if you try to use a mutable data structure as the default value of a function/method. Here’s an example: # src.py from __future__ import annotations # In ","permalink":"http://rednafi.com/python/early_bound_function_defaults/","publishDate":"2022-01-27","summary":"I was reading a tweet about it yesterday and that didn’t stop me from pushing a code change in production with the same rookie mistake today. Consider this function:\n# src.py from __future__ import annotations import logging import time from datetime import datetime def log( message: str, /, *, level: str, timestamp: str = datetime.utcnow().isoformat(), ) -\u003e None: logger = getattr(logging, level) # Avoid f-string in logging as it's not lazy. logger(\"Timestamp: %s \\nMessage: %s\\n\" % (timestamp, message)) if __name__ == \"__main__\": for _ in range(3): time.sleep(1) log(\"Reality can often be disappointing.\", level=\"warning\") Here, the function log has a parameter timestamp that computes its default value using the built-in datetime.utcnow().isoformat() method. I was under the impression that the timestamp parameter would be computed each time when the log function was called. However, that’s not what happens when you try to run it. If you run the above snippet, you’ll get this instead:\n","tags":["Python"],"title":"Gotchas of early-bound function argument defaults in Python"},{"content":"I used to use Unittest’s self.assertTrue / self.assertFalse to check both literal booleans and truthy/falsy values in Unittest. Committed the same sin while writing tests in Django.\nI feel like assertTrue and assertFalse are misnomers. They don’t specifically check literal booleans, only truthy and falsy states respectively.\nConsider this example:\n# src.py import unittest class TestFoo(unittest.TestCase): def setUp(self): self.true_literal = True self.false_literal = False self.truthy = [True] self.falsy = [] def is_true(self): self.assertTrue(self.true_literal, True) def is_false(self): self.assertFalse(self.false_literal, True) def is_truthy(self): self.assertTrue(self.truthy, True) def is_falsy(self): self.assertFalse(self.falsy, True) if __name__ == \"__main__\": unittest.main() In the above snippet, I’ve used assertTrue and assertFalse to check both literal booleans and truthy/falsy values. However, to test the literal boolean values, assertIs works better and is more explicit. Here’s how to do the above test properly:\n# src.py import unittest class TestFoo(unittest.TestCase): def setUp(self): self.true_literal = True self.false_literal = False self.truthy = [True] self.falsy = [] def is_true(self): self.assertIs(self.true_literal, True) def is_false(self): self.assertIs(self.false_literal, False) def is_truthy(self): self.assertTrue(self.truthy, True) def is_falsy(self): self.assertFalse(self.falsy, True) if __name__ == \"__main__\": unittest.main() Notice how I’ve used self.assertIs in the is_true and is_false methods to explicitly test out the literal boolean values. The is_truthy and is_falsy methods were kept unchanged from the previous snippet.\nTweet by Drewrey Lupton 1 ↩︎\n","permalink":"http://rednafi.com/python/use_assertis_to_check_literal_booleans/","publishDate":"2022-01-24","summary":"I used to use Unittest’s self.assertTrue / self.assertFalse to check both literal booleans and truthy/falsy values in Unittest. Committed the same sin while writing tests in Django.\nI feel like assertTrue and assertFalse are misnomers. They don’t specifically check literal booleans, only truthy and falsy states respectively.\nConsider this example:\n# src.py import unittest class TestFoo(unittest.TestCase): def setUp(self): self.true_literal = True self.false_literal = False self.truthy = [True] self.falsy = [] def is_true(self): self.assertTrue(self.true_literal, True) def is_false(self): self.assertFalse(self.false_literal, True) def is_truthy(self): self.assertTrue(self.truthy, True) def is_falsy(self): self.assertFalse(self.falsy, True) if __name__ == \"__main__\": unittest.main() In the above snippet, I’ve used assertTrue and assertFalse to check both literal booleans and truthy/falsy values. However, to test the literal boolean values, assertIs works better and is more explicit. Here’s how to do the above test properly:\n","tags":["Python","TIL"],"title":"Use 'assertIs' to check literal booleans in Python unittest"},{"content":"Accurately static typing decorators in Python is an icky business. The wrapper function obfuscates type information required to statically determine the types of the parameters and the return values of the wrapped function. Let’s write a decorator that registers the decorated functions in a global dictionary during function definition time. Here’s how I used to annotate it: # src.py # Import 'Callable' from 'typing' module in \u003c Py3.9. from collections.abc import Callable from functools import wraps from typing import Any, TypeVar R = TypeVar(\"R\") funcs = {} def register(func: Callable[..., R]) -\u003e Callable[..., R]: \"\"\"Register any function at definition time in the 'funcs' dict.\"\"\" # Registers the function during function defition time. funcs[func.__name__] = func @wraps(func) def inner(*args: Any, **kwargs: Any) -\u003e Any: return func(*args, **kwargs) return inner @register def hello(name: str) -\u003e str: return f\"Hello {name}!\" The functools.wraps decorator makes sure that the identity and the docstring of the wrapped function don’t get gobbled up by the decorator. This is syntactically correct and if you run Mypy against the code snippet, it’ll happily tell you that everything’s alright. However, this doesn’t exactly do anything. If you call the hello function with the wrong type of parameter, Mypy won’t be able to detect the mistake statically. Notice this: ... hello(1) # Mypy doesn't complain about it all All this for nothing! PEP-6121 proposed ParamSpec and Concatenate in the typing module to address this issue. Later on, these were introduced in Python 3.10. The former is required to precisely add type hints to any decorator while the latter is needed to type annotate decorators that change wrapped functions’ signatures. If you’re not on Python 3.10+, you can import ParamSpec and Concatenate from the typing_extensions module. The package gets automatically installed with Mypy. Use ParamSpec to type decorators I’ll take advantage of both ParamSpec and TypeVar to annotate the register decorator that we’ve seen earlier: # src.py # Import 'Callable' from 'typing' module in \u003c Py3.9. from collections.abc import Callable from functools import wraps from typing import ParamSpec, TypeVar P = ParamSpec(\"P\") R = TypeVar(\"R\") funcs = {} def register(func: Callable[P, R]) -\u003e Callable[P, R]: funcs[func.__name__] = func @wraps(func) def inner(*args: P.args, **kwargs: P.kwargs) -\u003e R: return func(*args, **kwargs) return inner @register def hello(name: str) -\u003e str: return f\"Hello {name}!\" # Try calling the function with the wrong param type. print(hello(1)) # Mypy will complain here! Above, I’ve used ParamSpec to annotate the type of the wrapped function’s input parameters and TypeVar to annotate its return value. Underneath, ParamSpec is a type variable similar to TypeVar but with a trick under its sleeve; it can relay type information to a decorator’s inner callable. Notice the annotations of the inner function inside register. Here, P.args and P.kwargs are transferring the type information from the wrapped func to the inner function. This makes sure that static type checkers like Mypy can now precisely scream at you whenever you call the decorated functions with the wrong type of parameters. Use Concatenate to type decorators that change the wrapped functions’ signatures There’s another type of decorator that changes the signature of the wrapped function by adding or removing parameters during runtime. Annotating these can be tricky; as the magic happens mostly during runtime. The Concatenate type allows us to communicate this behavior with the type checker. Consider this inject_logger decorator, that adds a logger instance to the decorated function. It sort of acts how Django injects the request instances into the view functions. Here’s the typed version of that: # src.py import logging # Import 'Callable' from 'typing' module in \u003c Py3.9. from collections.abc import Callable from functools import wraps from typing import Concatenate, ParamSpec, TypeVar P = ParamSpec(\"P\") R = TypeVar(\"R\") def inject_logger( func: Callable[Concatenate[logging.Logger, P], R], ) -\u003e Callable[P, R]: # Runs this during function definition time only. logger = logging.getLogger(func.__name__) @wraps(func) def inner(*args: P.args, **kwargs: P.kwargs) -\u003e R: return func(logger, *args, *kwargs) return inner @inject_logger def hello(logger: logging.Logger, name: str) -\u003e None: logger.warning(\"Spooky action in distance...\") return f\"Hello {name}!\" # Notice how you can call the hello function without # inserting the first parameter. The decorator does # that for you. print(hello(\"world\")) This is a contrived example and a gratuitously complicated way to achieve a simple goal. Also, it’s not recommended to mutate function signatures like this in runtime. But it’s allowed and now Python gives you a way to statically type check the decorator and the decorated function. The only thing that’s different from the previous section is the annotation of the func parameter of the inject_logger. Notice how the Callable generic now contain Concatenate[logging.Logger, P]. The first parameter of the Concatenate generic is the injected parameter—logging.Logger in this case. Since the instance of logging.Logger gets dynamically injected, an additional paradigm Concatenate is necessary to communicate that with the type checker. If you’d defined hello with the wrong types, the type checker would’ve complained. ... @inject_logger def hello(logger: int, name: str) -\u003e str: logger.warning(\"Spooky action in distance...\") return f\"Hello {name}!\" Above, I’ve changed the type of the logger parameter from logging.Logger to int. The type checker will now dutifully chastise us for our transgressions. Unfortunately, as of writing this post, Mypy doesn’t understand Concatenate but Microsoft’s Pyright2 does. You can pip install Pyright and test out the above snippet as follows: pyright src.py This will return: ... Parameter 1: type \"Logger\" cannot be assigned to type \"int\" \"Logger\" is incompatible with \"int\" (reportGeneralTypeIssues) ./src.py:83:12 - error: Cannot access member \"warning\" for type \"int\" PEP 612 – Parameter specification variables ↩︎ pyright ↩︎ Decorator typing (PEP 612) - Anthony explains #386 3 ↩︎ ","permalink":"http://rednafi.com/python/static_typing_decorators/","publishDate":"2022-01-23","summary":"Accurately static typing decorators in Python is an icky business. The wrapper function obfuscates type information required to statically determine the types of the parameters and the return values of the wrapped function.\nLet’s write a decorator that registers the decorated functions in a global dictionary during function definition time. Here’s how I used to annotate it:\n# src.py # Import 'Callable' from 'typing' module in \u003c Py3.9. from collections.abc import Callable from functools import wraps from typing import Any, TypeVar R = TypeVar(\"R\") funcs = {} def register(func: Callable[..., R]) -\u003e Callable[..., R]: \"\"\"Register any function at definition time in the 'funcs' dict.\"\"\" # Registers the function during function defition time. funcs[func.__name__] = func @wraps(func) def inner(*args: Any, **kwargs: Any) -\u003e Any: return func(*args, **kwargs) return inner @register def hello(name: str) -\u003e str: return f\"Hello {name}!\" The functools.wraps decorator makes sure that the identity and the docstring of the wrapped function don’t get gobbled up by the decorator. This is syntactically correct and if you run Mypy against the code snippet, it’ll happily tell you that everything’s alright. However, this doesn’t exactly do anything. If you call the hello function with the wrong type of parameter, Mypy won’t be able to detect the mistake statically. Notice this:\n","tags":["Python","Typing"],"title":"Static typing Python decorators"},{"content":"How come I didn’t know about the python -m pydoc command before today! It lets you inspect the docstrings of any modules, classes, functions, or methods in Python. I’m running the commands from a Python 3.10 virtual environment but it’ll work on any Python version. Let’s print out the docstrings of the functools.lru_cache function. Run: python -m pydoc functools.lru_cache This will print the following on the console: Help on function lru_cache in functools: functools.lru_cache = lru_cache(maxsize=128, typed=False) Least-recently-used cache decorator. If *maxsize* is set to None, the LRU features are disabled and the cache can grow without bound. If *typed* is True, arguments of different types will be cached separately. For example, f(3.0) and f(3) will be treated as distinct calls with distinct results. Arguments to the cached function must be hashable. View the cache statistics named tuple (hits, misses, maxsize, currsize) with f.cache_info(). Clear the cache and statistics with f.cache_clear(). Access the underlying function with f.__wrapped__. Works for third party tools as well: python -m pydoc typing_extensions.ParamSpec Also, works for any custom Python structure that is accessible from the current Python path. Let’s define a function with docstrings and put that in a module called src.py: # src.py def greetings(name: str) -\u003e None: \"\"\"Prints Hello ! on the console. Parameters ---------- name : str Name of the person you want to greet \"\"\" print(\"Hello {name}!\") You can inspect the entire src.py module or the greetings function specifically as follows: To inspect the module, run: python -m pydoc src To inspect the greetings function only, run: python -m pydoc src.greetings It’ll return: Help on function greetings in src: src.greetings = greetings(name: str) -\u003e None Prints Hello ! on the console. Parameters ---------- name : str Name of the person you want to greet Instead of inspecting the docstrings one by one, you can also pull up all of them in the current Python path and serve them as HTML pages. To do so, run: python -m pydoc -b This will render the docstrings as HTML web pages and automatically open the index page with your default browser. From there you can use the built-in search to find and read your ones you need. Tweet by Brandon Rhodes 1 ↩︎ ","permalink":"http://rednafi.com/python/inspect_docstring_with_pydoc/","publishDate":"2022-01-22","summary":"How come I didn’t know about the python -m pydoc command before today!\nIt lets you inspect the docstrings of any modules, classes, functions, or methods in Python.\nI’m running the commands from a Python 3.10 virtual environment but it’ll work on any Python version. Let’s print out the docstrings of the functools.lru_cache function. Run:\npython -m pydoc functools.lru_cache This will print the following on the console:\nHelp on function lru_cache in functools: functools.lru_cache = lru_cache(maxsize=128, typed=False) Least-recently-used cache decorator. If *maxsize* is set to None, the LRU features are disabled and the cache can grow without bound. If *typed* is True, arguments of different types will be cached separately. For example, f(3.0) and f(3) will be treated as distinct calls with distinct results. Arguments to the cached function must be hashable. View the cache statistics named tuple (hits, misses, maxsize, currsize) with f.cache_info(). Clear the cache and statistics with f.cache_clear(). Access the underlying function with f.__wrapped__. Works for third party tools as well:\n","tags":["Python"],"title":"Inspect docstrings with Pydoc"},{"content":"To check whether an integer is a power of two, I’ve deployed hacks like this: def is_power_of_two(x: int) -\u003e bool: return x \u003e 0 and hex(x)[-1] in (\"0\", \"2\", \"4\", \"8\") While this works1, I’ve never liked explaining the pattern matching hack that’s going on here. Today, I came across this tweet2 by Raymond Hettinger where he proposed an elegant solution to the problem. Here’s how it goes: def is_power_of_two(x: int) -\u003e bool: return x \u003e 0 and x.bit_count() == 1 This is neat as there’s no hack and it uses a mathematical invariant to check whether an integer is a power of 2 or not. Also, it’s a tad bit faster. Explanation Any integer that’s a power of 2, will only contain a single 1 in its binary representation. For example: \u003e\u003e\u003e bin(2) '0b10' \u003e\u003e\u003e bin(4) '0b100' \u003e\u003e\u003e bin(8) '0b1000' \u003e\u003e\u003e bin(16) '0b10000' \u003e\u003e\u003e The .bit_count() function checks how many on-bits (1) are there in the binary representation of an integer. Complete example with tests import unittest def is_power_of_two(number: int) -\u003e bool: return number \u003e 0 and number.bit_count() == 1 class IsPowerofTwoTest(unittest.TestCase): def setUp(self): self.power_of_twos = [2**x for x in range(2, 25_000)] self.not_power_of_twos = [3**x for x in range(2, 25_000)] def test_is_power_of_two(self): for x, y in zip(self.power_of_twos, self.not_power_of_twos): self.assertIs(is_power_of_two(x), True) self.assertIs(is_power_of_two(y), False) if __name__ == \"__main__\": unittest.main() My tweet on the hack ↩︎ A better solution by Raymond Hettinger ↩︎ ","permalink":"http://rednafi.com/python/check_is_a_power_of_two/","publishDate":"2022-01-21","summary":"To check whether an integer is a power of two, I’ve deployed hacks like this:\ndef is_power_of_two(x: int) -\u003e bool: return x \u003e 0 and hex(x)[-1] in (\"0\", \"2\", \"4\", \"8\") While this works1, I’ve never liked explaining the pattern matching hack that’s going on here.\nToday, I came across this tweet2 by Raymond Hettinger where he proposed an elegant solution to the problem. Here’s how it goes:\ndef is_power_of_two(x: int) -\u003e bool: return x \u003e 0 and x.bit_count() == 1 This is neat as there’s no hack and it uses a mathematical invariant to check whether an integer is a power of 2 or not. Also, it’s a tad bit faster.\n","tags":["Python","TIL"],"title":"Check whether an integer is a power of two in Python"},{"content":"Django Rest Framework exposes a neat hook to customize the response payload of your API when errors occur. I was going through Microsoft’s REST API guideline1 and wanted to make the error response of my APIs more uniform and somewhat similar to this2. I’ll use a modified version of the quickstart example3 in the DRF docs to show how to achieve that. Also, we’ll need a POST API to demonstrate the changes better. Here’s the same example with the added POST API. Place this code in the project’s urls.py file. # urls.py from django.urls import path, include from django.contrib.auth.models import User from rest_framework import routers, serializers, viewsets # Serializers define the API representation. class UserSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = User fields = [\"url\", \"username\", \"email\", \"is_staff\"] def validate_username(self, username: str) -\u003e str: if len(username) \u003c 10: raise serializers.ValidationError( \"Username must be at least 10 characters long.\", ) return username def validate_email(self, email: str) -\u003e str: try: validate_email(email) except ValidationError: raise serializers.ValidationError(\"Invalid email format.\") return email def create(self, validated_data: str) -\u003e User: return User.objects.create(**validated_data) # ViewSets define the view behavior. class UserViewSet(viewsets.ModelViewSet): queryset = User.objects.all() serializer_class = UserSerializer # Routers provide an easy way of automatically determining the URL conf. router = routers.DefaultRouter() router.register(r\"users\", UserViewSet) # Wire up our API using automatic URL routing. # Additionally, we include login URLs for the browsable API. urlpatterns = [ path(\"\", include(router.urls)), path( \"api-auth/\", include(\"rest_framework.urls\", namespace=\"rest_framework\") ), ] If you make a POST request to /users endpoint with the following payload where it’ll intentionally fail email and username validation— { \"username\": \"hello\", \"email\": \"email..\", \"is_staff\": false } you’ll see the following response: { \"username\":[ \"Username must be at least 10 characters long.\" ], \"email\":[ \"Enter a valid email address.\" ] } While this is okay, there’s one gotcha here. The error payload isn’t consistent. Depending on the type of error, the shape of the response payload will change. This can be a problem if your system has custom error handling logic that expects a consistent response. I wanted the error payload to have a predictable shape while carrying more information like—HTTP error code, error message, etc. You can do it by wrapping the default rest_framework.views.exception_handler function in a custom exception handler function. Let’s write the api_exception_handler: # urls.py from rest_framework.views import exception_handler from http import HTTPStatus from typing import Any from rest_framework.views import Response ... def api_exception_handler(exc: Exception, context: dict[str, Any]) -\u003e Response: \"\"\"Custom API exception handler.\"\"\" # Call REST framework's default exception handler first, # to get the standard error response. response = exception_handler(exc, context) if response is not None: # Using the description's of the HTTPStatus class as error message. http_code_to_message = {v.value: v.description for v in HTTPStatus} error_payload = { \"error\": { \"status_code\": 0, \"message\": \"\", \"details\": [], } } error = error_payload[\"error\"] status_code = response.status_code error[\"status_code\"] = status_code error[\"message\"] = http_code_to_message[status_code] error[\"details\"] = response.data response.data = error_payload return response ... Now, you’ll have to register this custom exception handler in the settings.py file. Head over to the REST_FRAMEWORK section and add the following key: REST_FRAMEWORK = { ... \"EXCEPTION_HANDLER\": \".urls.api_exception_handler\", } If you make a POST request to /users endpoint with an invalid payload as before, you’ll see this: { \"error\": { \"status_code\":400, \"message\":\"Bad request syntax or unsupported method\", \"details\":{ \"username\":[ \"Username must be at least 10 character long.\" ], \"email\":[ \"Enter a valid email address.\" ] } } } Much nicer! API guidelines - Microsoft ↩︎ Error payload ↩︎ DRF example ↩︎ Custom Exception Handling - DRF docs 4 ↩︎ ","permalink":"http://rednafi.com/python/uniform_error_response_in_drf/","publishDate":"2022-01-20","summary":"Django Rest Framework exposes a neat hook to customize the response payload of your API when errors occur. I was going through Microsoft’s REST API guideline1 and wanted to make the error response of my APIs more uniform and somewhat similar to this2.\nI’ll use a modified version of the quickstart example3 in the DRF docs to show how to achieve that. Also, we’ll need a POST API to demonstrate the changes better. Here’s the same example with the added POST API. Place this code in the project’s urls.py file.\n","tags":["Python","Django","TIL"],"title":"Uniform error response in Django Rest Framework"},{"content":"If you want to define a variable that can accept values of multiple possible types, using typing.Union is one way of doing that: from typing import Union U = Union[int, str] However, there’s another way you can express a similar concept via constrained TypeVar. You’d do so as follows: from typing import TypeVar T = TypeVar(\"T\", int, str) So, what’s the difference between these two and when to use which? The primary difference is: T’s type needs to be consistent across multiple uses within a given scope, while U’s doesn’t. With a Union type used as function parameters, the arguments, as well as the return type, can all be different: # src.py from typing import Union U = Union[int, str] # Native generic tuple requires py3.10 or # 'from __future__ import annotations' import. def foo(a: U, b: U) -\u003e tuple[U, ...]: return (a, b) # Use the 'foo' function. foo(\"apple\", \"bazooka\") # This is valid. foo(1, \"apple\") # Mypy won't complain here. foo(\"apple\", 1) # Mypy won't complain here as well. However, the above type definition will be too loose if you need to ensure that all of your function parameters must be of the same type in a single scope. Here’s where constrained TypeVar can come in handy: # src.py from typing import TypeVar T = TypeVar(\"T\", int, str) def add(a: T, b: T) -\u003e T: return a + b add(\"hello\", \"world\") # This is allowed. add(1, 2) # This is fine as well. add(\"hello\", 1) # Mypy will complain about this one and it'll fail in runtime. If you run Mypy against the above snippet, you’ll get this: $ mypy src.py src.py:12: error: Value of type variable \"T\" of \"add\" cannot be \"object\" add(\"hello\", 1) # Mypy will complain about this one and it'll fail in runtime. ^ Found 1 error in 1 file (checked 1 source file) As the comment implies, this error is coming from the line where I called add(\"hello\", 1). The function add can take parameters of either integer or string type. However, the type of both the parameters needs to be the same. Also, the type of the input parameters will define the type of the output value. So, the types of the input parameters must match, otherwise, Mypy will complain and in this case, the snippet will also raise a TypeError in runtime. Mypy is statically catching a bug that’d otherwise appear in runtime, how convenient! What’s the difference between a constrained TypeVar and a Union? 1 ↩︎ ","permalink":"http://rednafi.com/python/difference_between_typevar_and_union/","publishDate":"2022-01-19","summary":"If you want to define a variable that can accept values of multiple possible types, using typing.Union is one way of doing that:\nfrom typing import Union U = Union[int, str] However, there’s another way you can express a similar concept via constrained TypeVar. You’d do so as follows:\nfrom typing import TypeVar T = TypeVar(\"T\", int, str) So, what’s the difference between these two and when to use which? The primary difference is:\nT’s type needs to be consistent across multiple uses within a given scope, while U’s doesn’t.\n","tags":["Python","Typing","TIL"],"title":"Difference between constrained 'TypeVar' and 'Union' in Python"},{"content":"Recently, fell into this trap as I wanted to speed up a slow instance method by caching it. When you decorate an instance method with functools.lru_cache decorator, the instances of the class encapsulating that method never get garbage collected within the lifetime of the process holding them. Let’s consider this example: # src.py import functools import time from typing import TypeVar Number = TypeVar(\"Number\", int, float, complex) class SlowAdder: def __init__(self, delay: int = 1) -\u003e None: self.delay = delay @functools.lru_cache def calculate(self, *args: Number) -\u003e Number: time.sleep(self.delay) return sum(args) def __del__(self) -\u003e None: print(\"Deleting instance ...\") # Create a SlowAdder instance. slow_adder = SlowAdder(2) # Measure performance. start_time = time.perf_counter() # ---------------------------------------------- result = slow_adder.calculate(1, 2) # ---------------------------------------------- end_time = time.perf_counter() print(f\"Calculation took {end_time-start_time} seconds, result: {result}.\") start_time = time.perf_counter() # ---------------------------------------------- result = slow_adder.calculate(1, 2) # ---------------------------------------------- end_time = time.perf_counter() print(f\"Calculation took {end_time-start_time} seconds, result: {result}.\") Here, I’ve created a simple SlowAdder class that accepts a delay value; then it sleeps for delay seconds and calculates the sum of the inputs in the calculate method. To avoid this slow recalculation for the same arguments, the calculate method was wrapped in the lru_cache decorator. The __del__ method notifies us when the garbage collection has successfully cleaned up instances of the class. If you run this program, it’ll print this: Calculation took 2.0021052900010545 seconds, result: 3. Calculation took 5.632002284983173e-06 seconds, result: 3. Deleting instance ... You can see that the lru_cache decorator is doing its job. The second call to the calculate method with the same argument took noticeably less time compared to the first one. In the second case, the lru_cache decorator is just doing a simple dictionary lookup. This is all good but the instances of the ShowAdder class never get garbage collected in the lifetime of the program. Let’s prove that in the next section. Garbage collector can’t clear up the affected instances If you execute the above snippet with an -i flag, we can interactively prove that no garbage collection takes place. Let’s do it: $ python -i src.py Calculation took 2.002104839997628 seconds, result: 3. Calculation took 5.566998879658058e-06 seconds, result: 3. \u003e\u003e\u003e import gc \u003e\u003e\u003e \u003e\u003e\u003e slow_adder.calculate(1,2) 3 \u003e\u003e\u003e slow_adder = None \u003e\u003e\u003e \u003e\u003e\u003e gc.collect() 0 \u003e\u003e\u003e Here on the REPL, you can see that I’ve reassigned slow_adder to None and then explicitly triggered the garbage collector. However, we don’t see the message in the __del__ method printed here and the output of gc.collect() is 0. This implies that something is holding a reference to the slow_adder instance and the garbage collector can’t clear up the object. Let’s inspect who has that reference: $ python -i src.py Calculation took 2.00233274600032 seconds, result: 3. Calculation took 5.453999619930983e-06 seconds, result: 3. \u003e\u003e\u003e slow_adder.calculate.cache_info() CacheInfo(hits=1, misses=1, maxsize=128, currsize=1) \u003e\u003e\u003e slow_adder.calculate(1,2) 3 \u003e\u003e\u003e slow_adder.calculate.cache_info() CacheInfo(hits=2, misses=1, maxsize=128, currsize=1) \u003e\u003e\u003e slow_adder.calculate.cache_clear() \u003e\u003e\u003e slow_adder = None Deleting instance ... \u003e\u003e\u003e The cache_info() is showing that the cache container keeps a reference to the instance until it gets cleared. When I manually cleared the cache and reassigned the variable slow_adder to None, only then did the garbage collector remove the instance. By default, the size of the lru_cache is 128 but if I had applied lru_cache(maxsize=None), that would’ve kept the cache forever and the garbage collector would wait for the reference count to drop to zero but that’d never happen within the lifetime of the process. This can be dangerous if you create millions of instances and they don’t get garbage collected naturally. It can overflow your working memory and cause the process to crash! I accidentally did it where the infected class was being instantiated millions of times via HTTP API requests. The solution To solve this, we’ll have to make the cache containers local to the instances so that the reference from cache to the instance gets scraped off with the instance. Here’s how you can do that: # src_2.py import functools import time from typing import TypeVar Number = TypeVar(\"Number\", int, float, complex) class SlowAdder: def __init__(self, delay: int = 1) -\u003e None: self.delay = delay self.calculate = functools.lru_cache()(self._calculate) def _calculate(self, *args: Number) -\u003e Number: time.sleep(self.delay) return sum(args) def __del__(self) -\u003e None: print(\"Deleting instance ...\") The only difference here is—instead of decorating the method directly, I called the decorator function on the _calculate method just as a regular function and saved the result as an instance variable named calculate. The instances of this class get garbage collected as usual. $ python -i src.py \u003e\u003e\u003e slow_adder = SlowAdder(2) \u003e\u003e\u003e slow_adder.calculate(1,2) 3 \u003e\u003e\u003e slow_adder.calculate.cache_info() CacheInfo(hits=0, misses=1, maxsize=128, currsize=1) \u003e\u003e\u003e import gc \u003e\u003e\u003e slow_adder = None \u003e\u003e\u003e gc.collect() Deleting instance ... 11 Notice that this time, clearing out the cache wasn’t necessary. I had to call gc.collect() to invoke explicit garbage collection. That’s because this shenanigan creates cyclical references and the GC needs to do some special magic to clear the memory. In real code, Python interpreter will clean this up for you in the background without you having to call the GC. The self dilemma Even after applying the solution above, a weird thing happens in the case of instance methods. Let’s run the src_2.py script interactively to demonstrate that: $ python -i src_2.py \u003e\u003e\u003e slow_adder = SlowAdder(2) \u003e\u003e\u003e slow_adder.calculate(1,2) \u003e\u003e\u003e slow_adder \u003c__main__.SlowAdder object at 0x7f92595f9b40\u003e \u003e\u003e\u003e slow_adder_2 = SlowAdder(2) \u003e\u003e\u003e slow_adder_2.calculate(1,2) 3 \u003e\u003e\u003e slow_adder.calculate.cache_info() CacheInfo(hits=1, misses=2, maxsize=128, currsize=2) \u003e\u003e\u003e slow_adder_2.calculate.cache_info() CacheInfo(hits=1, misses=2, maxsize=128, currsize=2) \u003e\u003e\u003e Here, I’ve created another instance of the SlowAdder class and called calculate with the same arguments. But whenever I called the calculate method on the slow_adder_2 instance with the same parameters as before, the first time, it recalculated it instead of returning the result from the cache. How come! Underneath, the lru_cache decorator uses a dictionary to cache the calculated values. A hash function is applied1 to all the parameters of the target function to build the key of the dictionary, and the value is the return value of the function when those parameters are the inputs. This means, the first argument self also gets included while building the cache key. However, for different instances, this self object is going to be different and that makes the hashed key of the cache different for every instance even if the other parameters are the same. But what about class methods \u0026 static methods Class methods and static methods don’t suffer from the above issues as they don’t have any ties to their respective instances. In their case, the cache container is local to the class, not the instances. Here, you can stack the lru_cache decorator as usual. Let’s demonstrate that for classmethod first: # src_3.py import functools import time class Foo: @classmethod @functools.lru_cache def bar(cls, delay: int) -\u003e int: # Do something with the cls. cls.delay = delay time.sleep(delay) return 42 def __del__(self) -\u003e None: print(\"Deleting instance ...\") foo_1 = Foo() foo_2 = Foo() start_time = time.perf_counter() # ---------------------------------------------- result = foo_1.bar(2) # ---------------------------------------------- end_time = time.perf_counter() print(f\"Calculation took {end_time - start_time} seconds, result: {result}.\") start_time = time.perf_counter() # ---------------------------------------------- result = foo_2.bar(2) # ---------------------------------------------- end_time = time.perf_counter() print(f\"Calculation took {end_time - start_time} seconds, result: {result}.\") You can inspect the garbage collection behavior here: $ python src_3.py Calculation took 2.0022965140015003 seconds, result: 42. Calculation took 4.4819971662946045e-06 seconds, result: 42. \u003e\u003e\u003e foo_1 = None Deleting instance ... \u003e\u003e\u003e Static methods behave exactly the same. You can use the lru_cache decorator in similar fashion as below: import functools import time class Foo: @staticmethod @functools.lru_cache def bar(delay: int) -\u003e int: return 42 def __del__(self) -\u003e None: print(\"Deleting instance ...\") LRU cache key ↩︎ functools.lru_cache - Python Docs 2 ↩︎ Don’t lru_cache methods! (intermediate) anthony explains #382 3 ↩︎ Python LRU cache in a class disregards maxsize limit when decorated with a staticmethod or classmethod decorator 4 ↩︎ ","permalink":"http://rednafi.com/python/lru_cache_on_methods/","publishDate":"2022-01-15","summary":"Recently, fell into this trap as I wanted to speed up a slow instance method by caching it.\nWhen you decorate an instance method with functools.lru_cache decorator, the instances of the class encapsulating that method never get garbage collected within the lifetime of the process holding them.\nLet’s consider this example:\n# src.py import functools import time from typing import TypeVar Number = TypeVar(\"Number\", int, float, complex) class SlowAdder: def __init__(self, delay: int = 1) -\u003e None: self.delay = delay @functools.lru_cache def calculate(self, *args: Number) -\u003e Number: time.sleep(self.delay) return sum(args) def __del__(self) -\u003e None: print(\"Deleting instance ...\") # Create a SlowAdder instance. slow_adder = SlowAdder(2) # Measure performance. start_time = time.perf_counter() # ---------------------------------------------- result = slow_adder.calculate(1, 2) # ---------------------------------------------- end_time = time.perf_counter() print(f\"Calculation took {end_time-start_time} seconds, result: {result}.\") start_time = time.perf_counter() # ---------------------------------------------- result = slow_adder.calculate(1, 2) # ---------------------------------------------- end_time = time.perf_counter() print(f\"Calculation took {end_time-start_time} seconds, result: {result}.\") Here, I’ve created a simple SlowAdder class that accepts a delay value; then it sleeps for delay seconds and calculates the sum of the inputs in the calculate method. To avoid this slow recalculation for the same arguments, the calculate method was wrapped in the lru_cache decorator. The __del__ method notifies us when the garbage collection has successfully cleaned up instances of the class.\n","tags":["Python","TIL"],"title":"Don't wrap instance methods with 'functools.lru_cache' decorator in Python"},{"content":"Problem A common interview question that I’ve seen goes as follows: Write a function to crop a text corpus without breaking any word. Take the length of the text up to which character you should trim. Make sure that the cropped text doesn’t have any trailing space. Try to maximize the number of words you can pack in your trimmed text. Your function should look something like this: def crop(text: str, limit: int) -\u003e str: \"\"\"Crops 'text' upto 'limit' characters.\"\"\" # Crop the text. cropped_text = perform_crop() return cropped_text For example, if text looks like this— \"A quick brown fox jumps over the lazy dog.\" and you’re asked to crop it up to 9 characters, then the function crop should return: \"A quick\" and not: \"A quick \" or: \"A quick b\" Solution This is quite easily solvable by using Python’s textwrap.shorten function. The shorten function takes quite a few parameters1. However, we’ll only need the following ones to do our job: text: str: Target text that we’re going to operate on. width: int : Desired width after cropping. initial_indent: str: Character to use for the initial indentation. Provide empty string for no initial indentation. subsequent_indent: str: Character to use for the subsequent indentation. Provide empty string for no subsequent indentation. break_long_words: bool: Whether to break long words or not. break_on_hyphens: bool: Whether to break words on hyphens or not. placeholder: bool: Placeholder character. The default here is [...]. However, provide an empty string if you don’t want any placeholder after the cropped string. The length of the placeholder is going to be included in the total length of the cropped text. With the descriptions out of the way, let’s write the crop function here: # src.py import textwrap def crop(text: str, limit: int) -\u003e str: cropped_text = textwrap.shorten( text, width=limit, initial_indent=\"\", subsequent_indent=\"\", break_long_words=False, break_on_hyphens=False, placeholder=\"\", ) return cropped_text if __name__ == \"__main__\": cropped_text = crop( text=\"A quick brown fox jumps over the lazy dog.\", limit=9, ) print(cropped_text) This prints out the desired output as follows: A quick You can see that we achieved our goal of cropping a text corpus without breaking any word. Try playing around with the initial_indent, subsequent_indent, and placeholder parameters and see how they change the output. Complete solution with tests # src.py import textwrap import unittest def crop(text: str, limit: int) -\u003e str: cropped_text = textwrap.shorten( text, width=limit, initial_indent=\"\", subsequent_indent=\"\", break_long_words=False, break_on_hyphens=False, placeholder=\"\", ) return cropped_text class TestCrop(unittest.TestCase): def setUp(self): self.text = \"This is an example of speech synthesis in English.\" self.text_complex = \"\"\" wrap(), fill() and shorten() work by creating a TextWrapper instance and calling a single method on it. \"\"\" def test_ok(self): cropped_text = crop(self.text, limit=10) self.assertEqual(cropped_text, \"This is an\") def test_complex_ok(self): cropped_text = crop(self.text_complex, limit=15) self.assertEqual(cropped_text, \"wrap(), fill()\") def test_no_word_break(self): cropped_text = crop(self.text, limit=9) self.assertNotEqual(cropped_text, \"This is a\") def test_no_trailing_space(self): cropped_text = crop(self.text, limit=8) self.assertNotEqual(cropped_text, \"This is \") if __name__ == \"__main__\": unittest.main() textwrap.shorten ↩︎ ","permalink":"http://rednafi.com/python/text_cropping_with_textwrap_shorten/","publishDate":"2022-01-06","summary":"Problem A common interview question that I’ve seen goes as follows:\nWrite a function to crop a text corpus without breaking any word.\nTake the length of the text up to which character you should trim. Make sure that the cropped text doesn’t have any trailing space. Try to maximize the number of words you can pack in your trimmed text. Your function should look something like this:\ndef crop(text: str, limit: int) -\u003e str: \"\"\"Crops 'text' upto 'limit' characters.\"\"\" # Crop the text. cropped_text = perform_crop() return cropped_text For example, if text looks like this—\n","tags":["Python","TIL"],"title":"Cropping texts in Python with 'textwrap.shorten'"},{"content":"I was reading the source code1 of the reference implementation of “PEP-661: Sentinel Values”2 and discovered an optimization technique known as String interning. Modern programming languages like Java, Python, PHP, Ruby, Julia, etc, performs string interning to make their string operations more performant.\nString interning String interning makes common string processing operations time and space-efficient by caching them. Instead of creating a new copy of string every time, this optimization method dictates to keep just one copy of string for every appropriate immutable distinct value and use the pointer reference wherever referred.\nConsider this example:\n# src.py x = \"This is a string\" y = \"This is a string\" print(x is y) # prints True Running this will print True on the console. The is operator in Python is used to check whether two objects refer to the same memory location or not. If it returns True, it means, the two objects surrounding the operator are actually the same object.\nThis is kind of neat if you think about it. In the above snippet, instead of creating a new copy when y is assigned to a string that has the same value as x, internally, Python points to the same string that is assigned to x. This is only true for smaller strings; larger strings will create individual copies as usual. The exact length that determines whether a string will be interned or not depends on the implementation and you shouldn’t rely on this implicit behavior if your code needs interning. See this example:\n# src.py x = \"This is a string\" * 300 y = \"This is a string\" * 300 print(x is y) # prints False This will print False on the console and the strings are not interned.\nExplicit string interning Python’s sys module in the standard library has a routine called intern that you can use to intern even large strings. For example:\n# src.py import sys x = sys.intern(\"This is a string\" * 300) y = sys.intern(\"This is a string\" * 300) print(x is y) # prints True Here, the strings are interned and running the snippet will print True on the console.\nWhat strings are interned? CPython performs string interning on constants such as Function Names, Variable Names, String Literals, etc. This snippet3 from the CPython codebase suggests that when a new Python object is created, the interpreter is interning all the compile-time constants, names, and literals. Also, Dictionary Keys and Object Attributes are interned. Notice this:\n# src.py # Dict key interning. d = {\"hello\": \"world\"} print(d.popitem()[0] is \"hello\") # prints True # Object attribute interning. class Foo: def __init__(self, bar, baz): self.bar = bar self.baz = baz foo = Foo(\"hello\", \"hello\") print(foo.bar is foo.baz) # prints True In both of these above cases, the print statement will print True on the console—confirming the fact that dictionary keys and object attributes are interned. Having interned attributes and keys means that the access operation is faster since the string comparison operation is now just doing a pointer comparison.\nWhen explicit string interning might come in handy? One use case that I’ve found is—interning large dictionary keys. Dictionary keys are in general, interned automatically. However, if the key is large—something like a 4097 bytes hash value—Python can choose not to perform interning. Here’s an example:\n# src.py # No dict key interning as the key is quite large. d = {} k = \"#\" * 4097 d[\"#\" * 4097] = 1 print(d.popitem()[0] is k) # prints False This will print False as the key in this case, will not be interned. Dictionary value access is slower if the key isn’t interned. Let’s test that out:\n# src.py import time # Interned. t0 = time.perf_counter() for _ in range(10000): d = {\"#\" * 4096: \"Interned\"} d[\"#\" * 4096] t1 = time.perf_counter() # Non-interned. t2 = time.perf_counter() for _ in range(10000): d = {\"#\" * 4097: \"Non-interned\"} d[\"#\" * 4097] t3 = time.perf_counter() print(f\"Interned dict creation \u0026 access: {t1-t0} seconds\") print(f\"Non-interned dict creation \u0026 access: {t3-t2} seconds\") print(f\"Non-interned creation \u0026 access is {(t3-t2)/(t1-t0)} times slower\") This prints:\nInterned dict creation \u0026 access: 0.0014631289996032137 seconds Non-interned dict creation \u0026 access: 0.048660025000572205 seconds Non-interned creation \u0026 access is 33.25750840409036 times slower The above script creates and accesses a dictionary with interned and non-interned keys 10000 times. The time difference is quite huge. Non-interned dict creation and accession are in fact, 33 times slower than its interned counterpart.\nWe can circumnavigate this limitation by using explicit string interning via the sys module as follows:\n# src.py import sys import time # Implicitly interned. t0 = time.perf_counter() for _ in range(10000): d = {\"#\" * 4096: \"Implicitly-interned\"} d[\"#\" * 4096] t1 = time.perf_counter() # Explicitly interned. t2 = time.perf_counter() k1 = sys.intern(\"#\" * 4097) k2 = sys.intern(\"#\" * 4097) for _ in range(10000): d = {k1: \"Explicitly-interned\"} d[k2] t3 = time.perf_counter() print(t1 - t0) print((t3 - t2) / (t1 - t0)) print(f\"Implicitly interned dict creation \u0026 access: {t1-t0} seconds\") print(f\"Explicitly interned dict creation \u0026 access: {t3-t2} seconds\") print( f\"Explicitly interned creation \u0026 access is {(t3-t2)/(t1-t0)} times slower\" ) This prints:\nImplicitly interned dict creation \u0026 access: 0.002887188999011414 seconds Explicitly interned dict creation \u0026 access: 0.002545474999351427 seconds Explicitly interned creation \u0026 access is 1.1264793204711423 times slower Here, implicitly and explicitly interned dict creation and key access are almost equally fast.\nString interning in PEP-661’s implementation ↩︎\nPEP-661 – Sentinel Values ↩︎\nPyObject ↩︎\nString interning in Python 4 ↩︎\nPython docs: sys.intern 5 ↩︎\n","permalink":"http://rednafi.com/python/string_interning/","publishDate":"2022-01-05","summary":"I was reading the source code1 of the reference implementation of “PEP-661: Sentinel Values”2 and discovered an optimization technique known as String interning. Modern programming languages like Java, Python, PHP, Ruby, Julia, etc, performs string interning to make their string operations more performant.\nString interning String interning makes common string processing operations time and space-efficient by caching them. Instead of creating a new copy of string every time, this optimization method dictates to keep just one copy of string for every appropriate immutable distinct value and use the pointer reference wherever referred.\n","tags":["Python"],"title":"String interning in Python"},{"content":"I love using Go’s interface feature to declaratively define my public API structure. Consider this example: package main import ( \"fmt\" ) // Declare the interface. type Geometry interface { area() float64 perim() float64 } // Struct that represents a rectangle. type rect struct { width, height float64 } // Method to calculate the area of a rectangle instance. func (r *rect) area() float64 { return r.width * r.height } // Method to calculate the perimeter of a rectange instance. func (r *rect) perim() float64 { return 2 * (r.width + r.height) } // Notice that we're calling the methods on the interface, // not on the instance of the Rectangle struct directly. func measure(g Geometry) { fmt.Println(g) fmt.Println(g.area()) fmt.Println(g.perim()) } func main() { r := \u0026rect{width: 3, height: 4} measure(r) } You can play around with the example here1. Running the example will print: \u0026{3 4} 12 14 Even if you don’t speak Go, you can just take a look at the Geometry interface and instantly know that the function measure expects a struct that implements the Geometry interface where the Geometry interface is satisfied when the struct implements two methods—area and perim. The function measure doesn’t care whether the struct is a rectangle, a circle, or a square. As long as it implements the interface Geometry, measure can work on it and calculate the area and the perimeter. This is extremely powerful as it allows you to achieve polymorphism like dynamic languages without letting go of type safety. If you try to pass a struct that doesn’t fully implement the interface, the compiler will throw a type error. In the world of Python, this polymorphism is achieved dynamically. Consider this example: def find(haystack, needle): return needle in haystack Here, the type of the haystack can be anything that supports the in operation. It can be a list, tuple, set, or dict; basically, any type that has the __contains__ method. Python’s duck typing is more flexible than any static typing as you won’t have to tell the function anything about the type of the parameters and it’ll work spectacularly; it’s a dynamically typed language, duh! The only problem is the lack of type safety. Since there’s no compilation step in Python, it won’t stop you from accidentally putting a type that haystack doesn’t support and Python will only raise a TypeError when your try to run the code. In bigger codebases, this can often become tricky as it’s difficult to tell the types of these uber-dynamic function parameters without reading through all the methods that are being called on them. In this situation, we want the best of the both world; we want the flexibility of dynamic polymorphism and at the same time, we want some sort of type safety. Moreover, the Go code is self-documented to some extent and I’d love this kind of polymorphic | type-safe | self-documented trio in Python. Let’s try to use nominal type hinting to statically type the following example: # src.py from __future__ import annotations from typing import TypeVar T = TypeVar(\"T\") def find(haystack: dict, needle: T) -\u003e bool: return needle in haystack if __name__ == \"__main__\": haystack = {1, 2, 3, 4} needle = 3 print(contains(haystack, needle)) In this snippet, we’re declaring haystack to be a dict and then passing a set to the function parameter. If you try to run this function, it’ll happily print True. However, if you run mypy2 against this file, it’ll complain as follows: src.py:17: error: Argument 1 to \"find\" has incompatible type \"Set[int]\"; expected \"Dict[Any, Any]\" print(contains(haystack, needle)) ^ Found 1 error in 1 file (checked 4 source files) make: *** [makefile:61: mypy] Error 1 That’s because mypy expects the type to be a dict but we’re passing a set which is incompatible. During runtime, Python doesn’t raise any error because the set that we’re passing as the value of haystack, supports in operation. But we’re not communicating that with the type checker properly and mypy isn’t happy about that. To fix this mypy error, we can use Union type. ... def contains(haystack: dict | set, needle: T) -\u003e bool: return needle in haystack ... This will make mypy happy. However, it’s still not bulletproof. If you try to pass a list object as the value of haystack, mypy will complain again. So, nominal typing can get a bit tedious in this kind of situation, as you’d have to explicitly tell the type checker about every type that a variable can expect. There’s a better way! Enter structural subtyping3. We know that the value of haystack can be anything that has the __contains__ method. So, instead of explicitly defining the name of all the allowed types—we can create a class, add the __contains__ method to it, and signal mypy the fact that haystack can be anything that has the __contains__ method. Python’s typing.Protocol class allows us to do that. Let’s use that: # src.py from __future__ import annotations from typing import Protocol, TypeVar, runtime_checkable T = TypeVar(\"T\") @runtime_checkable class ProtoHaystack(Protocol): def __contains__(self, obj) -\u003e bool: ... def find(haystack: ProtoHaystack, needle: T) -\u003e bool: return needle in haystack if __name__ == \"__main__\": haystack = {1, 2, 3, 4} needle = 3 print(find(haystack, needle)) print(isinstance(ProtoHaystack, haystack)) Here, the ProtoHaystack class statically defines the structure of the type of objects that are allowed to be passed as the value of haystack. The instance method __contains__ accepts an object (obj) as the second parameter and returns a boolean value based on the fact whether that obj exists in the self instance or not. Now if you run mypy on this snippet, it’ll be satisfied. The runtime_checkable decorator on the ProtoHaystack class allows you to check whether a target object is an instance of the ProtoHaystack class in runtime via the isinstance() function. Without the decorator, you’ll only be able to test the conformity of an object to ProtoHaystack statically but not in runtime. This pattern of strurctural duck typing is so common, that the mixins in the collections.abc module are now compatible with structural type checking. So, in this case, instead of creating a ProtoHaystack class, you can directly use the collections.abc.Container class from the standard library and it’ll do the same job. ... from collections.abc import Container def find(haystack: Container, needle: T) -\u003e bool: return needle in haystack ... Avoid abc inheritance Abstract base classes in Python let you validate the structure of subclasses in runtime. Python’s standard library APIs uses abc.ABC in many places. See this example: # src.py from __future__ import annotations from abc import ( ABC, abstractmethod, abstractclassmethod, abstractproperty, ) class FooInterface(ABC): @abstractmethod def bar(self) -\u003e str: pass @abstractclassmethod def baz(cls) -\u003e str: pass @abstractproperty def qux(self) -\u003e str: pass class Foo(FooInterface): \"\"\"Foo implements FooInterface.\"\"\" def bar(self) -\u003e str: return \"from instance method\" @classmethod def baz(cls) -\u003e str: return \"from class method\" @property def qux(self) -\u003e str: return \"from property method\" Here, the class FooInterface inherits from abc.ABC and then the methods are decorated with abstract* decorators. The combination of abc.ABC class and these decorators make sure that any class that inherits from FooInterface will have to implement the bar, baz, and qux methods. Failing to do so will raise a TypeError. The Foo class implements the FooInterface. This works well in theory and practice but often time, people inadvertently start to use the *Interface classes to share implementation methods with the subclasses. When you pollute your interface with implementation methods, theoretically, it no longer stays and interface class. Go doesn’t even allow you to add implementation methods to interfaces. Abstract base classes have their places and often time, you can’t avoid them if you need a dependable runtime interface conformity check. However, more often than not, using Protocol classes with @runtime_checkable decorator works really well. Here, the Protocol class implicitly (just like Go interfaces) makes sure that your subclasses conform to the structure that you want, and the decorator, along with isinstance check can guarantee the conformity in runtime. Let’s replace the abc.ABC and the shenanigans with the decorators with typing.Protocol: from __future__ import annotations from typing import Protocol, runtime_checkable @runtime_checkable class ProtoFoo(Protocol): def bar(self) -\u003e str: ... @classmethod def baz(cls) -\u003e str: ... @property def qux(self) -\u003e str: ... class Foo: def bar(self) -\u003e str: return \"from instance method\" @classmethod def baz(cls) -\u003e str: return \"from class method\" @property def qux(self) -\u003e str: return \"from property method\" def run(foo: ProtoFoo) -\u003e None: if not isinstance(foo, ProtoFoo): raise Exception(\"Foo do not conform to Protofoo interface\") print(foo.bar()) print(foo.baz()) print(foo.qux) if __name__ == \"__main__\": foo = Foo() run(foo) Notice that Foo is not inheriting from ProtoFoo and when you run mypy against the snippet, it’ll statically check whether Foo conforms to the ProtoFoo interface or not. Voila, we avoided inheritance. The isinstance in the run function later checks whether foo is an instance of ProtoFoo or not. Complete example with tests This example employs static duck-typing to check the type of WebhookPayload where the class represents the structure of the payload that is going to be sent to an URL by the send_webhook function. # Placeholder python file to test the snippets from __future__ import annotations import json import unittest from dataclasses import asdict, dataclass from typing import Protocol, runtime_checkable @runtime_checkable class ProtoPayload(Protocol): url: str message: str @property def json(self): ... @dataclass class WebhookPayload: url: str = \"https://dummy.com/post/\" message: str = \"Dummy message\" @property def json(self) -\u003e str: return json.dumps(asdict(self)) # Notice the type accepted by the function. # Go-like static polymorphism right there! def send_webhook(payload: ProtoPayload) -\u003e None: \"\"\" This function doesn't care what type of Payload it gets as long as the payload conforms to 'ProtoPayload' structure. \"\"\" print(f\"Webhook message: {payload.json}\") print(f\"Sending webhook to {payload.url}\") class TestWebHookPayload(unittest.TestCase): def setUp(self): self.payload = WebhookPayload() def test_payload(self): # We can do isinstance check because we decorated the # 'ProtoPayload' class with the 'runtime_checkable' decorator. implements_protocol = isinstance(self.payload, ProtoPayload) self.assertEqual(implements_protocol, True) if __name__ == \"__main__\": unittest.main() Disclaimer All the code snippets here are using Python 3.10’s type annotation syntax. However, if you’re using from __future__ import annotations, you’ll be able to run all of them in earlier Python versions, going as far back as Python 3.7. Interface - Go playground ↩︎ mypy ↩︎ Nominal vs structural subtyping ↩︎ PEP 544 – Protocols: Structural subtyping (static duck typing) 4 ↩︎ ","permalink":"http://rednafi.com/python/structural_subtyping/","publishDate":"2021-12-04","summary":"I love using Go’s interface feature to declaratively define my public API structure. Consider this example:\npackage main import ( \"fmt\" ) // Declare the interface. type Geometry interface { area() float64 perim() float64 } // Struct that represents a rectangle. type rect struct { width, height float64 } // Method to calculate the area of a rectangle instance. func (r *rect) area() float64 { return r.width * r.height } // Method to calculate the perimeter of a rectange instance. func (r *rect) perim() float64 { return 2 * (r.width + r.height) } // Notice that we're calling the methods on the interface, // not on the instance of the Rectangle struct directly. func measure(g Geometry) { fmt.Println(g) fmt.Println(g.area()) fmt.Println(g.perim()) } func main() { r := \u0026rect{width: 3, height: 4} measure(r) } You can play around with the example here1. Running the example will print:\n","tags":["Python","Typing"],"title":"Structural subtyping in Python"},{"content":"While trying to avoid inheritance in an API that I was working on, I came across this neat trick to perform attribute delegation on composed classes. Let’s say there’s a class called Engine and you want to put an engine instance in a Car. In this case, the car has a classic ‘has a’ (inheritance usually refers to ‘is a’ relationships) relationship with the engine. So, composition makes more sense than inheritance here. Consider this example: # src.py from typing import Any class Engine: def __init__(self, name: str, sound: str) -\u003e None: self.name = name self.sound = sound def noise(self) -\u003e str: return f\"Engine {self.name} goes {self.sound}!\" class Car: def __init__(self, engine: Engine, tier: str, price: int) -\u003e None: self.engine = engine self.tier = tier self.price = price def info(self) -\u003e dict[str, Any]: return {\"tier\": self.tier, \"price\": self.price} Ideally, you’d to use the classes as a good citizen as follows: engine = Engine(\"w16\", \"vroom\") car = Car(engine, \"supercar\", 3_000_000) # If you want to access an attribute on the 'engine' instance from the 'car' # instance, you'll do it like this: print(car.engine.name) print(car.engine.sound) This will print the following: $ python src.py w16 vroom However, I wanted free attribute access, just like we get in inheritance. We should be able to do car.name, not car.engine.name, and get the name of the engine instance. With a little bit of __getattr__ magic, it’s easy to do so: # src.py from typing import Any class Engine: def __init__(self, name: str, sound: str) -\u003e None: self.name = name self.sound = sound def noise(self) -\u003e str: return f\"Engine {self.name} goes {self.sound}!\" class Car: def __init__(self, engine: Engine, tier: str, price: int) -\u003e None: self.engine = engine self.tier = tier self.price = price def info(self) -\u003e dict[str, Any]: return {\"tier\": self.tier, \"price\": self.price} # NOTE: This is new!! def __getattr__(self, attr: str) -\u003e Any: return getattr(self.engine, attr) This snippet is exactly the same as before and the only thing that was added here is the __getattr__ method in the Car class. Whenever you’ll try to access an attribute or a method on an instance of the Car class, the __getattr__ will intervene. It’ll first look for the attribute in the instance of the Car class and if it can’t find it there, then it’ll look for the attribute in the instance of the Engine class; just like type inheritance. This will work in case of method access as well. So now you can use the classes as below: engine = Engine(\"w16\", \"vroom\") car = Car(engine, \"supercar\", 3_000_000) print(car.name) # Actually prints the 'name' of the engine print(car.sound) # Prints the 'sound' of the engine print(car.info()) # Method 'info' is in the 'Car' instance print(car.noise()) # Method 'noise' is in the 'Engine' instance This will print: $ python src.py w16 vroom {'tier': 'supercar', 'price': 3000000} Engine w16 goes vroom! While this was all fun and dandy, I don’t recommend putting it in any serious code as it can obfuscate the program’s intent and can make obvious things not-so-obvious. Also, in case of attributes and methods with the same names in different classes, this can get hairy. I just found this gymnastics intellectually stimulating. Complete example with tests # src.py import unittest from typing import Any class Engine: def __init__(self, name: str, sound: str) -\u003e None: self.name = name self.sound = sound def noise(self) -\u003e str: return f\"Engine {self.name} goes {self.sound}!\" class Car: def __init__(self, engine: Engine, tier: str, price: int) -\u003e None: self.engine = engine self.tier = tier self.price = price def info(self) -\u003e dict[str, Any]: return {\"tier\": self.tier, \"price\": self.price} def __getattr__(self, attr: str) -\u003e Any: return getattr(self.engine, attr) class Test(unittest.TestCase): def setUp(self): self.engine = Engine(\"w16\", \"vroom\") self.car = Car(self.engine, \"supercar\", 3_000_000) def test_auto_delegation(self): expected_info = {\"tier\": \"supercar\", \"price\": 3000000} expected_noise = \"Engine w16 goes vroom!\" self.assertEqual(self.car.info(), expected_info) self.assertEqual(self.car.noise(), expected_noise) if __name__ == \"__main__\": unittest.main(Test()) ","permalink":"http://rednafi.com/python/attribute_delegation_in_composition/","publishDate":"2021-11-28","summary":"While trying to avoid inheritance in an API that I was working on, I came across this neat trick to perform attribute delegation on composed classes. Let’s say there’s a class called Engine and you want to put an engine instance in a Car. In this case, the car has a classic ‘has a’ (inheritance usually refers to ‘is a’ relationships) relationship with the engine. So, composition makes more sense than inheritance here. Consider this example:\n","tags":["Python","TIL"],"title":"Automatic attribute delegation in Python composition"},{"content":"I wanted to add a helper method to an Enum class. However, I didn’t want to make it a classmethod as property method made more sense in this particular case. Problem is, you aren’t supposed to initialize an enum class, and property methods can only be accessed from the instances of a class; not from the class itself. While sifting through Django 3.2’s codebase, I found this neat trick to make a classmethod that acts like a property method and can be accessed directly from the class without initializing it. # src.py # This requires Python 3.4+. from enum import Enum, EnumMeta class PlanetsMeta(EnumMeta): @property def choices(cls): return [(v.name, v.value) for v in cls] class Planets(Enum, metaclass=PlanetsMeta): EARTH = \"earth\" MARS = \"mars\" # This can be accessed as follows. print(Planets.choices) If you run the script, you’ll see the following output: $ python3.8 src.py [('EARTH', 'earth'), ('MARS', 'mars')] While the previous example is quite impressive, I still don’t like the solution as it requires creating a metaclass and doing a bunch of magic to achieve something so simple. Luckily, Python3.9+ makes it possible without any additional magic. Notice the example below: # src.py # Requires Python 3.9+ class ModernPlanets(Enum): EARTH = \"earth\" MARS = \"mars\" @classmethod @property def choices(cls): return [(v.name, v.value) for v in cls] # This can be accessed as follows. print(ModernPlanets.choices) The only thing that matters here is the order of the property and classmethod decorator. Python applies them from bottom to top. Changing the order will make it behave unexpectedly. Complete example with tests # src.py # Requires Python 3.4+ import sys import unittest from enum import Enum, EnumMeta class PlanetsMeta(EnumMeta): @property def choices(cls): return [(v.name, v.value) for v in cls] class Planets(Enum, metaclass=PlanetsMeta): EARTH = \"earth\" MARS = \"mars\" # Requires Python 3.9+ class ModernPlanets(Enum): EARTH = \"earth\" MARS = \"mars\" @classmethod @property def choices(cls): return [(v.name, v.value) for v in cls] class TestPlanets(unittest.TestCase): python_version = (sys.version_info.major, sys.version_info.minor) def setUp(self): self.expected_result = [(\"EARTH\", \"earth\"), (\"MARS\", \"mars\")] def test_planets(self): self.assertEqual(Planets.choices, self.expected_result) @unittest.skipIf( python_version \u003c (3, 9), \"Not supported under Python 3.9\", ) def test_modern_planets(self): \"\"\"This test method will fail if we try to run it on a version earlier than Python 3.9. So we skip it accordingly.\"\"\" self.assertEqual(ModernPlanets.choices, self.expected_result) if __name__ == \"__main__\": unittest.main() Running this will print out the following: $ python src.py .. ---------------------------------------------------------------------- Ran 2 tests in 0.000s OK ","permalink":"http://rednafi.com/python/access_classmethod_like_property/","publishDate":"2021-11-26","summary":"I wanted to add a helper method to an Enum class. However, I didn’t want to make it a classmethod as property method made more sense in this particular case. Problem is, you aren’t supposed to initialize an enum class, and property methods can only be accessed from the instances of a class; not from the class itself.\nWhile sifting through Django 3.2’s codebase, I found this neat trick to make a classmethod that acts like a property method and can be accessed directly from the class without initializing it.\n","tags":["Python"],"title":"Access 'classmethod's like 'property' methods in Python"},{"content":"I was browsing through the source code of Tom Christie’s typesystem1 library and discovered that the shell scripts2 of the project don’t have any extensions attached to them. At first, I found it odd, and then it all started to make sense.\nExecutable scripts can be written in any language and the users don’t need to care about that. Also, not gonna lie, it looks cleaner this way.\nGitHub uses this [pattern]3 successfully to normalize their scripts. According to the pattern, every project should have a folder named scripts with a subset or superset of the following files:\nscript/bootstrap – installs/updates all dependencies script/setup – sets up a project to be used for the first time script/update – updates a project to run at its current version script/server – starts app script/test – runs tests script/cibuild – invoked by continuous integration servers to run tests script/console – opens a console typesystem ↩︎\nScripts without extension ↩︎\nScripts to rule them all - GitHub Blog ↩︎\n","permalink":"http://rednafi.com/misc/do_not_add_extenstions_to_bash_executables/","publishDate":"2021-11-23","summary":"I was browsing through the source code of Tom Christie’s typesystem1 library and discovered that the shell scripts2 of the project don’t have any extensions attached to them. At first, I found it odd, and then it all started to make sense.\nExecutable scripts can be written in any language and the users don’t need to care about that. Also, not gonna lie, it looks cleaner this way.\nGitHub uses this [pattern]3 successfully to normalize their scripts. According to the pattern, every project should have a folder named scripts with a subset or superset of the following files:\n","tags":["Shell","TIL"],"title":"Don't add extensions to shell executables"},{"content":"At my workplace, we have a fairly large Celery config file where you’re expected to subclass from a base class and extend that if there’s a new domain. However, the subclass expects the configuration in a specific schema. So, having a way to enforce that schema in the subclasses and raising appropriate runtime exceptions is nice. Wrote a fancy Python 3.6+ __init_subclasshook__ to validate the subclasses as below. This is neater than writing a metaclass. # main.py from collections.abc import Mapping from typing import Any class Base: def __init_subclass__( cls, validate_config: bool = False, **kwargs: Any, ) -\u003e None: if validate_config: cls._raise_error_for_invalid_config(cls) @staticmethod def _raise_error_for_invalid_config(cls) -\u003e None: if not \"config\" in cls.__dict__: raise Exception( f\"'{cls.__name__}' should define a class attribute named 'config'\", ) if not isinstance(cls.config, Mapping): raise Exception( \"attribute 'config' should be of 'Mapping' type\", ) config = cls.config config_keys = config.keys() expected_keys = (\"foo\", \"bar\", \"bazz\") if not tuple(config_keys) == expected_keys: raise Exception( f\"'config' map should have only '{', '.join(expected_keys)}' keys\", ) def __repr__(self) -\u003e str: return f\"{self.config}\" class Sub(Base, validate_config=True): config = {\"foo\": 1, \"bar\": 2, \"bazz\": 3} s = Sub() print(s) Running the script will print: {'foo': 1, 'bar': 2, 'bazz': 3} However, if we initialize the Sub class like this: class Sub(Base): config = {\"not\": 1, \"allowed\": 2} This will raise an error: Traceback (most recent call last): File \"main.py\", line 29, in class Sub(Base, validate_config=True): File \"main.py\", line 8, in __init_subclass__ cls._raise_error_for_invalid_config(cls) File \"main.py\", line 23, in _raise_error_for_invalid_config raise Exception(f\"'config' map should have only '{', '.join(expected_keys)}' keys\") Exception: 'config' map should have only 'foo, bar, bazz' keys Test # test_base.py # Install pytest before running the script. import pytest from main import Base def test_base(): # Don't raise any exception if validate_config is False. class A(Base, validate_config=False): hello = \"world\" # Raise error when there's no attribute called config. with pytest.raises( Exception, match=\"'B' should define a class attribute named 'config'\", ): class B(Base, validate_config=True): hello = \"world\" # Raise error when config isn't a Mapping. with pytest.raises( Exception, match=\"attribute 'config' should be of 'Mapping' type\", ): class C(Base, validate_config=True): config = [1, 2, 3] # Raise error when config is empty. with pytest.raises( Exception, match=\"'config' map should have only 'foo, bar, bazz' keys\", ): class D(Base, validate_config=True): config = {} # Raise error when config doesn't have `foo, bar, bazz` keys. with pytest.raises( Exception, match=\"'config' map should have only 'foo, bar, bazz' keys\", ): class E(Base, validate_config=True): config = {\"foo\": 1, \"bar\": 2, \"wrong_attribute\": 3} # Should pass successfully. class F(Base): config = {\"foo\": 1, \"bar\": 2, \"bazz\": 3} # Assert f = F() # Check the repr. assert str(f) == f\"{{'foo': 1, 'bar': 2, 'bazz': 3}}\" ","permalink":"http://rednafi.com/python/use_init_subclass_hook_to_validate_subclasses/","publishDate":"2021-11-20","summary":"At my workplace, we have a fairly large Celery config file where you’re expected to subclass from a base class and extend that if there’s a new domain. However, the subclass expects the configuration in a specific schema. So, having a way to enforce that schema in the subclasses and raising appropriate runtime exceptions is nice.\nWrote a fancy Python 3.6+ __init_subclasshook__ to validate the subclasses as below. This is neater than writing a metaclass.\n","tags":["Python","TIL"],"title":"Use __init_subclass__ hook to validate subclasses in Python"},{"content":"Making tqdm play nice with multiprocessing requires some additional work. It’s not always obvious and I don’t want to add another third-party dependency just for this purpose. The following example attempts to make tqdm work with multiprocessing.imap_unordered. However, this should also work with similar mapping methods like—multiprocessing.map, multiprocessing.imap, multiprocessing.starmap, etc. \"\"\" Run `pip install tqdm` before running the script. The function `foo` is going to be executed 100 times across `MAX_WORKERS=5` processes. In a single pass, each process will get an iterable of size `CHUNK_SIZE=5`. So 5 processes each consuming 5 elements of an iterable will require (100 / (5*5)) 4 passes to finish consuming the entire iterable of 100 elements. Tqdm progress bar will be updated after every `MAX_WORKERS*CHUNK_SIZE` iterations. \"\"\" # src.py from __future__ import annotations import multiprocessing as mp from tqdm import tqdm import time import random from dataclasses import dataclass MAX_WORKERS = 5 CHUNK_SIZE = 5 @dataclass class StartEnd: start: int end: int def foo(start_end: StartEnd) -\u003e int: time.sleep(0.2) return random.randint(start_end.start, start_end.end) def main() -\u003e None: inputs = [ StartEnd(start, end) for start, end in zip( range(0, 100), range(100, 200), ) ] with mp.Pool(processes=MAX_WORKERS) as pool: results = tqdm( pool.imap_unordered(foo, inputs, chunksize=CHUNK_SIZE), total=len(inputs), ) # 'total' is redundant here but can be useful # when the size of the iterable is unobvious for result in results: print(result) if __name__ == \"__main__\": main() This will print: 0%| | 0/100 [00:00\u003c?, ?it/s] 14 1%|▌ | 1/100 [00:01\u003c01:39, 1.00s/it] 6 9 70 ... 26%|██████████████▎ | 26/100 [00:02\u003c00:04, 15.10it/s] 70 42 41 ... 51%|████████████████████████████ | 51/100 [00:03\u003c00:02, 19.61it/s] 114 135 59 ... 76%|█████████████████████████████████████████▊ | 76/100 [00:04\u003c00:01, 21.72it/s] 134 106 167 ... 100%|██████████████████████████████████████████████████████| 100/100 [00:04\u003c00:00] Using tqdm with multiprocessing 1 ↩︎ ","permalink":"http://rednafi.com/python/tqdm_with_multiprocessing/","publishDate":"2021-11-18","summary":"Making tqdm play nice with multiprocessing requires some additional work. It’s not always obvious and I don’t want to add another third-party dependency just for this purpose.\nThe following example attempts to make tqdm work with multiprocessing.imap_unordered. However, this should also work with similar mapping methods like—multiprocessing.map, multiprocessing.imap, multiprocessing.starmap, etc.\n\"\"\" Run `pip install tqdm` before running the script. The function `foo` is going to be executed 100 times across `MAX_WORKERS=5` processes. In a single pass, each process will get an iterable of size `CHUNK_SIZE=5`. So 5 processes each consuming 5 elements of an iterable will require (100 / (5*5)) 4 passes to finish consuming the entire iterable of 100 elements. Tqdm progress bar will be updated after every `MAX_WORKERS*CHUNK_SIZE` iterations. \"\"\" # src.py from __future__ import annotations import multiprocessing as mp from tqdm import tqdm import time import random from dataclasses import dataclass MAX_WORKERS = 5 CHUNK_SIZE = 5 @dataclass class StartEnd: start: int end: int def foo(start_end: StartEnd) -\u003e int: time.sleep(0.2) return random.randint(start_end.start, start_end.end) def main() -\u003e None: inputs = [ StartEnd(start, end) for start, end in zip( range(0, 100), range(100, 200), ) ] with mp.Pool(processes=MAX_WORKERS) as pool: results = tqdm( pool.imap_unordered(foo, inputs, chunksize=CHUNK_SIZE), total=len(inputs), ) # 'total' is redundant here but can be useful # when the size of the iterable is unobvious for result in results: print(result) if __name__ == \"__main__\": main() This will print:\n","tags":["Python","TIL"],"title":"Running tqdm with Python multiprocessing"},{"content":"Python’s daemon threads are cool. A Python script will stop when the main thread is done and only daemon threads are running. To test a simple hello function that runs indefinitely, you can do the following: # test_hello.py from __future__ import annotations import asyncio import threading from functools import partial from unittest.mock import patch async def hello() -\u003e None: while True: await asyncio.sleep(1) print(\"hello\") @patch(\"asyncio.sleep\", autospec=True) async def test_hello(mock_asyncio_sleep, capsys): run = partial(asyncio.run, hello()) t = threading.Thread(target=run, daemon=True) t.start() t.join(timeout=0.1) out, err = capsys.readouterr() assert err == \"\" assert \"hello\" in out mock_asyncio_sleep.assert_awaited() To execute the script, make sure you’ve your virtual env actiavated. Also you’ll need to install pytest and pytest-asyncio. Then run: pytest -v -s --asyncio-mode=auto The idea came from this quora answer1. How do we test an infinite loop in Python using the unittest library? ↩︎ ","permalink":"http://rednafi.com/python/use_daemon_threads_to_test_infinite_loop/","publishDate":"2021-11-18","summary":"Python’s daemon threads are cool. A Python script will stop when the main thread is done and only daemon threads are running. To test a simple hello function that runs indefinitely, you can do the following:\n# test_hello.py from __future__ import annotations import asyncio import threading from functools import partial from unittest.mock import patch async def hello() -\u003e None: while True: await asyncio.sleep(1) print(\"hello\") @patch(\"asyncio.sleep\", autospec=True) async def test_hello(mock_asyncio_sleep, capsys): run = partial(asyncio.run, hello()) t = threading.Thread(target=run, daemon=True) t.start() t.join(timeout=0.1) out, err = capsys.readouterr() assert err == \"\" assert \"hello\" in out mock_asyncio_sleep.assert_awaited() To execute the script, make sure you’ve your virtual env actiavated. Also you’ll need to install pytest and pytest-asyncio. Then run:\n","tags":["Python"],"title":"Use daemon threads to test infinite while loops in Python"},{"content":"One thing that came to me as news is that the command which—which is the de-facto tool to find the path of an executable—is not POSIX compliant. The recent Debian debacle1 around which brought it to my attention. The POSIX-compliant way of finding an executable program is command -v, which is usually built into most of the shells.\nSo, instead of doing this:\nwhich python3.12 Do this:\ncommand -v which python3.12 Debian’s which hunt ↩︎\n","permalink":"http://rednafi.com/misc/use_command_v_over_which/","publishDate":"2021-11-16","summary":"One thing that came to me as news is that the command which—which is the de-facto tool to find the path of an executable—is not POSIX compliant. The recent Debian debacle1 around which brought it to my attention. The POSIX-compliant way of finding an executable program is command -v, which is usually built into most of the shells.\nSo, instead of doing this:\nwhich python3.12 Do this:\ncommand -v which python3.12 Debian’s which hunt ↩︎\n","tags":["Shell","TIL"],"title":"Use 'command -v' over 'which' to find a program's executable"},{"content":"Writing consistent commit messages helps you to weave a coherent story with your git history. Recently, I’ve started paying attention to my commit messages. Before this, my commit messages in this repository used to look like this: git log --oneline -5 d058a23 (HEAD -\u003e master) bash strict mode a62e59b Updating functool partials til. 532b21a Added functool partials til ec9191c added unfinished indexing script 18e41c8 Bash tils With all the misuse of letter casings and punctuations, clearly, the message formatting is all over the place. To tame this mayhem, I’ve adopted these 7 rules of writing great commit messages: The seven rules of writing consistent git commit messages Separate subject from body with a blank line Limit the subject line to 50 characters (I often break this when there’s no message body) Capitalize the subject line Do not end the subject line with a period Use the imperative mood in the subject line Wrap the body at 72 characters Use the body to explain what and why vs. how Now, after rebasing, currently, the commit messages in this repo look like this: git log --oneline -5 d058a23 (HEAD -\u003e master) Employ bash strict mode a62e59b Update functool partials til 532b21a Add functool partials til ec9191c Add unfinished indexing script 18e41c8 Update bash tils Reference How to write a git commit message 1 ↩︎ ","permalink":"http://rednafi.com/misc/write_git_commit_messages_properly/","publishDate":"2021-11-11","summary":"Writing consistent commit messages helps you to weave a coherent story with your git history. Recently, I’ve started paying attention to my commit messages. Before this, my commit messages in this repository used to look like this:\ngit log --oneline -5 d058a23 (HEAD -\u003e master) bash strict mode a62e59b Updating functool partials til. 532b21a Added functool partials til ec9191c added unfinished indexing script 18e41c8 Bash tils With all the misuse of letter casings and punctuations, clearly, the message formatting is all over the place. To tame this mayhem, I’ve adopted these 7 rules of writing great commit messages:\n","tags":["Git"],"title":"Write git commit messages properly"},{"content":"The constructor for functools.partial() detects nesting and automatically flattens itself to a more efficient form. For example: from functools import partial def f(*, a: int, b: int, c: int) -\u003e None: print(f\"Args are {a}-{b}-{c}\") g = partial(partial(partial(f, a=1), b=2), c=3) # Three function calls are flattened into one; free efficiency. print(g) # Bare function can be called as 3 arguments were bound previously. g() This returns: functools.partial(, a=1, b=2, c=3) Args are 1-2-3 Tweet by Raymond Hettinger 1 ↩︎ ","permalink":"http://rednafi.com/python/functools_partial_flattens_nestings_automatically/","publishDate":"2021-11-08","summary":"The constructor for functools.partial() detects nesting and automatically flattens itself to a more efficient form. For example:\nfrom functools import partial def f(*, a: int, b: int, c: int) -\u003e None: print(f\"Args are {a}-{b}-{c}\") g = partial(partial(partial(f, a=1), b=2), c=3) # Three function calls are flattened into one; free efficiency. print(g) # Bare function can be called as 3 arguments were bound previously. g() This returns:\nfunctools.partial(, a=1, b=2, c=3) Args are 1-2-3 Tweet by Raymond Hettinger 1 ↩︎\n","tags":["Python"],"title":"Python's 'functools.partial' flattens nestings Automatically"},{"content":"Pasting shell commands can be a pain when they include hidden return \\n characters. In such a case, your shell will try to execute the command immediately. To prevent that, use curly braces { } while pasting the command. Your command should look like the following: { dig +short google.com } Here, the spaces after the braces are significant. ","permalink":"http://rednafi.com/misc/use_curly_braces_while_pasting_shell_commands/","publishDate":"2021-11-08","summary":"Pasting shell commands can be a pain when they include hidden return \\n characters. In such a case, your shell will try to execute the command immediately. To prevent that, use curly braces { } while pasting the command. Your command should look like the following:\n{ dig +short google.com } Here, the spaces after the braces are significant.\n","tags":["Shell","TIL"],"title":"Use curly braces while pasting shell commands"},{"content":"Use unofficial bash strict mode while writing scripts. Bash has a few gotchas and this helps you to avoid that. For example:\n#!/bin/bash set -euo pipefail echo \"Hello\" Where,\n-e Exit immediately if a command exits with a non-zero status. -u Treat unset variables as an error when substituting. -o pipefail The return value of a pipeline is the status of the last command to exit with a non-zero status, or zero if no command exited with a non-zero status. References The idea is a less radical version of this post1. I don’t recommend messing with the IFS without a valid reason.\nUnofficial bash strict mode ↩︎\n","permalink":"http://rednafi.com/misc/use_strict_mode_while_running_bash_scripts/","publishDate":"2021-11-08","summary":"Use unofficial bash strict mode while writing scripts. Bash has a few gotchas and this helps you to avoid that. For example:\n#!/bin/bash set -euo pipefail echo \"Hello\" Where,\n-e Exit immediately if a command exits with a non-zero status. -u Treat unset variables as an error when substituting. -o pipefail The return value of a pipeline is the status of the last command to exit with a non-zero status, or zero if no command exited with a non-zero status. References The idea is a less radical version of this post1. I don’t recommend messing with the IFS without a valid reason.\n","tags":["Shell","TIL"],"title":"Use strict mode while running bash scripts"},{"content":"Managing configurations in your Python applications isn’t something you think about much often, until complexity starts to seep in and forces you to re-architect your initial approach. Ideally, your config management flow shouldn’t change across different applications or as your application begins to grow in size and complexity. Even if you’re writing a library, there should be a consistent config management process that scales up properly. Since I primarily spend my time writing data-analytics, data-science applications and expose them using Flask1 or FastAPI2 framework, I’ll be tacking config management from an application development perspective. Few ineffective approaches In the past, while exposing APIs with Flask, I used to use .env, .flaskenv and Config class approach to manage configs which is pretty much a standard in the Flask realm. However, it quickly became cumbersome to maintain and juggle between configs depending on development, staging or production environments. There were additional application specific global constants to deal with too. So I tried using *.json, *.yaml or *.toml based config management approaches but those too, quickly turned into a tangled mess. I was constantly accessing variables buried into 3-4 levels of nested toml data structure and it wasn’t pretty. Then there are config management libraries like Dynaconf3 or environ-config4 that aim to ameliorate the issue. While these are all amazing tools but they also introduce their own custom workflow that can feel over-engineered while dealing with maintenance and extension. A pragmatic wishlist I wanted to take a declarative approach while designing a config management pipleline that’ll be modular, scalable and easy to maintain. To meet my requirements, the system should be able to: Read configs from .env files and shell environment at the same time. Handle dependency injection for introducing passwords or secrets. Convert variable types automatically in the appropriate cases, e.g. string to integer conversion. Keep development, staging and production configs separate. Switch between the different environments e.g development, staging effortlessly. Inspect the active config values Create arbitrarily nested config structure if required (Not encouraged though). Building the config management pipeline Preparation The code block that appears in this section is self contained. It should run without any modifications. If you want to play along, then just spin up a Python virtual environment and install Pydantic and python-dotenv. The following commands works on any *nix based system: python3.10 -m venv venv source venv/bin/activate pip install pydantic python-dotenv Make sure you have fairly a recent version of Python 3 installed, preferably Python 3.10+. You might need to install python3.10 venv. Introduction to Pydantic To check off all the boxes of the wishlist above, I made a custom config management flow using Pydantic5, python-dotenv6 and the .env file. Pydantic is a fantastic data validation library that can be used for validating and implicitly converting data types using Python’s type hints. Type hinting is a formal solution to statically indicate the type of a value within your Python code. It was specified in PEP-4847 and introduced in Python 3.5. Let’s define and validate the attributes of a class named User: from Pydantic import BaseModel class User(BaseModel): name: str username: str password: int user = User(name=\"Redowan Delowar\", username=\"rednafi\", password=\"123\") print(user) This will give you: \u003e\u003e\u003e User(name='Redowan Delowar', username='rednafi', password=123) In the above example, I defined a simple class named User and used Pydantic for data validation. Pydantic will make sure that the data you assign to the class attributes conform with the types you’ve annotated. Notice, how I’ve assigned a string type data in the password field and Pydantic converted it to integer type without complaining. That’s because the corresponding type annotation suggests that the password attribute of the User class should be an integer. When implicit conversion is not possible or the hinted value of an attribute doesn’t conform to its assigned type, Pydantic will throw a ValidationError. The orchestration Now let’s see how you can orchestrate your config management flow with the tools mentioned above. For simplicity, let’s say you’ve 3 sets of configurations. Configs of your app’s internal logic Development environment configs Production environment configs In this case, other than the first set of configs, all should go into the .env file. I’ll be using this .env file for demonstration. If you’re following along, then go ahead, create an empty .env file there and copy the variables mentioned below: #.env ENV_STATE=\"dev\" # or prod DEV_REDIS_HOST=\"127.0.0.1\" DEV_REDIS_PORT=\"4000\" PROD_REDIS_HOST=\"127.0.0.2\" PROD_REDIS_PORT=\"5000\" Notice how I’ve used the DEV_ and PROD_ prefixes before the environment specific configs. These help you discern between the variables designated for different environments. Configs related to your application’s internal logic should either be explicitly mentioned in the same configs.py or imported from a different app_configs.py file. You shouldn’t pollute your .env files with the internal global variables necessitated by your application’s core logic. Now let’s dump the entire config orchestration and go though the building blocks one by one: # configs.py from typing import Optional from pydantic import BaseSettings, Field, BaseModel class AppConfig(BaseModel): \"\"\"Application configurations.\"\"\" VAR_A: int = 33 VAR_B: float = 22.0 class GlobalConfig(BaseSettings): \"\"\"Global configurations.\"\"\" # These variables will be loaded from the .env file. However, if # there is a shell environment variable having the same name, # that will take precedence. APP_CONFIG: AppConfig = AppConfig() # define global variables with the Field class ENV_STATE: Optional[str] = Field(None, env=\"ENV_STATE\") # environment specific variables do not need the Field class REDIS_HOST: Optional[str] = None REDIS_PORT: Optional[int] = None REDIS_PASS: Optional[str] = None class Config: \"\"\"Loads the dotenv file.\"\"\" env_file: str = \".env\" class DevConfig(GlobalConfig): \"\"\"Development configurations.\"\"\" class Config: env_prefix: str = \"DEV_\" class ProdConfig(GlobalConfig): \"\"\"Production configurations.\"\"\" class Config: env_prefix: str = \"PROD_\" class FactoryConfig: \"\"\"Returns a config instance dependending on the ENV_STATE variable.\"\"\" def __init__(self, env_state: Optional[str]): self.env_state = env_state def __call__(self): if self.env_state == \"dev\": return DevConfig() elif self.env_state == \"prod\": return ProdConfig() cnf = FactoryConfig(GlobalConfig().ENV_STATE)() print(cnf.__repr__()) The print statement of the last line in the above code block is to inspect the active configuration class. You’ll soon learn what I meant by the term active configuration. You can comment out the last line while using the code in production. Let’s explain what’s going on with each of the classes defined above. AppConfig The AppConfig class defines the config variables required for you API’s internal logic. In this case I’m not loading the variables from the .env file, rather defining them directly in the class. You can also define and import them from another app_configs.py file if necessary but they shouldn’t be placed in the .env file. For data validation to work, you have to inherit from Pydantic’s BaseModel and annotate the attributes using type hints while constructing the AppConfig class. Later, this class is called from the GlobalConfig class to build a nested data structure. GlobalConfig GlobalConfig defines the variables that propagates through other environment classes and the attributes of this class are globally accessible from all other environments. In this class, the variables are loaded from the .env file. In the .env file, global variables don’t have any environment specific prefixes like DEV_ or PROD_ before them. The class GlobalConfig inherits from Pydantic’s BaseSettings which helps to load and read the variables from the .env file. The .env file itself is loaded in the nested Config class. Although the environment variables are loaded from the .env file, Pydantic also loads your actual shell environment variables at the same time. From Pydantic’s [documentation]: Even when using a dotenv file, Pydantic will still read environment variables as well as the dotenv file, environment variables will always take priority over values loaded from a dotenv file. This means you can keep the sensitive variables in your .bashrc or zshrc and Pydantic will inject them during runtime. It’s a powerful feature, as it implies that you can easily keep the insensitive variables in your .env file and include that to the version control system. Meanwhile the sensitive information should be injected as a shell environment variable. For example, although I’ve defined an attribute called REDIS_PASS in the GlobalConfig class, there is no mention of any REDIS_PASS variable in the .env file. So normally, it returns None but you can easily inject a password into the REDIS_PASS variable from the shell. Assuming that you’ve set up your venv and installed the dependencies, you can test it by copying the contents of the above code snippet in file called configs.py and running the commands below: export DEV_REDIS_PASS=ubuntu python configs.py This should printout: \u003e\u003e\u003e DevConfig( ... ENV_STATE='dev', ... APP_CONFIG=AppConfig(VAR_A=33, VAR_B=22.0), ... REDIS_PASS='ubuntu', ... REDIS_HOST='127.0.0.1', REDIS_PORT=4000) Notice how your injected REDIS_PASS has appeared in the printed config class instance. Although I injected DEV_REDIS_PASS into the environment variable, it appeared as REDIS_PASS inside the DevConfig instance. This is convenient because you won’t need to change the name of the variables in your codebase when you change the environment. To understand why it printed an instance of the DevConfig class, refer to the FactoryConfig section. DevConfig DevConfig class inherits from the GlobalConfig class and it can define additional variables specific to the development environment. It inherits all the variables defined in the GlobalConfig class. In this case, the DevConfig class doesn’t define any new variable. The nested Config class inside DevConfig defines an attribute env_prefix and assigns DEV_ prefix to it. This helps Pydantic to read your prefixed variables like DEV_REDIS_HOST, DEV_REDIS_PORT etc without you having to explicitly mention them. ProdConfig ProdConfig class also inherits from the GlobalConfig class and it can define additional variables specific to the production environment. It inherits all the variables defined in the GlobalConfig class. In this case, like DevConfig this class doesn’t define any new variable. The nested Config class inside ProdConfig defines an attribute env_prefix and assigns PROD_ prefix to it. This helps Pydantic to read your prefixed variables like PROD_REDIS_HOST, PROD_REDIS_PORT etc without you having to explicitly mention them. FactoryConfig FactoryConfig is the controller class that dictates which config class should be activated based on the environment state defined as ENV_STATE in the .env file. If it finds ENV_STATE=\"dev\" then the control flow statements in the FactoryConfig class will activate the development configs (DevConfig). Similarly, if ENV_STATE=\"prod\" is found then the control flow will activate the production configs (ProdConfig). Since the current environment state is ENV_STATE=\"dev\", when you run the code, it prints an instance of the activated DevConfig class. This way, you can assign different values to the same variable based on different environment contexts. You can also dynamically change the environment by changing the value of ENV_STATE on your shell. Run: EXPORT ENV_STATE=\"prod\" python configs.py This time the config instance should change and print the following: \u003e\u003e\u003e ProdConfig( ... ENV_STATE='prod', ... APP_CONFIG=AppConfig(VAR_A=33, VAR_B=22.0), ... REDIS_PASS='ubuntu', REDIS_HOST='127.0.0.2', ... REDIS_PORT=5000) Accessing the configs Using the config variables is easy. Suppose you want use the variables in file called app.py. You can easily do so as shown in the following code block: # app.py from configs import cnf APP_CONFIG = cnf.APP_CONFIG VAR_A = APP_CONFIG.VAR_A # this is a nested config VAR_B = APP_CONFIG.VAR_B REDIS_HOST = cnf.REDIS_HOST # this is a top-level config REDIS_PORT = cnf.REDIS_PORT print(APP_CONFIG) print(VAR_A) print(VAR_B) print(REDIS_HOST) print(REDIS_PORT) This should print out: \u003e\u003e\u003e ProdConfig( ... ENV_STATE='prod', ... APP_CONFIG=AppConfig(VAR_A=33, VAR_B=22.0), ... REDIS_PASS='ubuntu', ... REDIS_HOST='127.0.0.2', ... REDIS_PORT=5000) VAR_A=33 VAR_B=22.0 33 22.0 127.0.0.2 5000 Extending the pipeline The modular design demonstrated above is easy to maintain and extend in my opinion. Previously, for simplicity, I’ve defined only two environment scopes; development and production. Let’s say you want to add the configs for your staging environment. First you’ll need to add those staging variables to the .env file. ... STAGE_REDIS_HOST=\"127.0.0.3\" STAGE_REDIS_PORT=\"6000\" ... Then you’ve to create a class named StageConfig that inherits from the GlobalConfig class. The architecture of the class is similar to that of the DevConfig or ProdConfig class. # configs.py ... class StageConfig(GlobalConfig): \"\"\"Staging configurations.\"\"\" class Config: env_prefix: str = \"STAGE_\" ... Finally, you’ll need to insert an ENV_STATE logic into the control flow of the FactoryConfig class. See how I’ve appended another if-else block to the previous (prod) block. # configs.py ... class FactoryConfig: \"\"\"Returns a config instance depending on the ENV_STATE variable.\"\"\" def __init__(self, env_state: Optional[str]): self.env_state = env_state def __call__(self): if self.env_state == \"dev\": return DevConfig() elif self.env_state == \"prod\": return ProdConfig() elif self.env_state == \"stage\" return StageConfig() ... To see your new addition in action just change the ENV_STATE to “stage” in the .env file or export it to your shell environment. export ENV_STATE=\"stage\" python configs.py This will print out an instance of the class StageConfig. Remarks The above workflow works perfectly for my usage scenario. So subjectively, I feel like it’s an elegant solution to a very icky problem. Your mileage will definitely vary. Flask ↩︎ FastAPI ↩︎ Dynaconf ↩︎ environ-config ↩︎ Pydantic ↩︎ python-dotenv ↩︎ PEP-484 ↩︎ Settings management with pydantic 8 ↩︎ Flask config management 9 ↩︎ ","permalink":"http://rednafi.com/python/config_management_with_pydantic/","publishDate":"2020-07-13","summary":"Managing configurations in your Python applications isn’t something you think about much often, until complexity starts to seep in and forces you to re-architect your initial approach. Ideally, your config management flow shouldn’t change across different applications or as your application begins to grow in size and complexity.\nEven if you’re writing a library, there should be a consistent config management process that scales up properly. Since I primarily spend my time writing data-analytics, data-science applications and expose them using Flask1 or FastAPI2 framework, I’ll be tacking config management from an application development perspective.\n","tags":["Python"],"title":"Pedantic configuration management with Pydantic"},{"content":"Imagine a custom set-like data structure that doesn’t perform hashing and trades performance for tighter memory footprint. Or imagine a dict-like data structure that automatically stores data in a PostgreSQL or Redis database the moment you initialize it; also it lets you to get-set-delete key-value pairs using the usual retrieval-assignment-deletion syntax associated with built-in dictionaries. Custom data structures can give you the power of choice and writing them will make you understand how the built-in data structures in Python are constructed. One way to understand how built-in objects like dictionary, list, set etc work is to build custom data structures based on them. Python provides several mixin classes in the collection.abc module to design custom data structures that look and act like built-in structures with additional functionalities baked in. Concepts To understand how all these work, you’ll need a fair bit of knowledge about Interfaces, Abstract Base Classes, Mixin Classes etc. I’ll build the concept edifice layer by layer where you’ll learn about interfaces first and how they can be created and used via the abc.ABC class. Then you’ll learn how abstract base classes differ from interfaces. After that I’ll introduce mixins and explain how all these concepts can be knitted together to architect custom data structures with amazing capabilities. Let’s dive in. Interfaces Python interfaces can help you write classes based on common structures. They ensure that classes that provide similar functionalities will also have similar footprints. Interfaces are not as popular in Python as they are in other statically typed language. The dynamic nature and duck-typing capabilities of Python often make them redundant. However, in larger applications, interfaces can make you avoid writing code that is poorly encapsulated or build classes that look awfully similar but provide completely unrelated functionalities. Moreover, interfaces implicitly spawn other powerful techniques like mixin classes which can help you achieve DRY nirvana. Overview At a high level, an interface acts as a blueprint for designing classes. In Python, an interface is a specialized abstract class that defines one or more abstract methods. Abstract classes differs from concrete classes in the sense that they aren’t intended to stand on their own and the methods they define shouldn’t have any implementation. Usually, you inherit from an interface and implement the methods defined in the abstract class in a concrete subclass. Interfaces provide the skeletons and concrete classes provide the implementation of the methods based on those skeletons. Depending on the ways you can architect interfaces, they can be segmented into two primary categories. Informal Interfaces Formal Interfaces Informal interfaces Informal interfaces are classes which define methods that can be overridden, but there’s no strict enforcement. Let’s write an informal interface for a simple calculator class: class ICalc: \"\"\"Informal Interface: Abstract calculator class.\"\"\" def add(self, a, b): raise NotImplementedError def sub(self, a, b): raise NotImplementedError def mul(self, a, b): raise NotImplementedError def div(self, a, b): raise NotImplementedError Notice that the ICalc class has four different methods that don’t give you any implementation. It’s an informal interface because you can still instantiate the class but the methods will raise NotImplementedError if you try to apply them. You’ve to subclass the interface to use it. Let’s do it: class Calc(ICalc): \"\"\"Concrete Class: Calculator\"\"\" def add(self, a, b): return a + b def sub(self, a, b): return a - b def mul(self, a, b): return a * b def div(self, a, b): return a / b # Using the class c = Calc() print(c.add(1, 2)) print(c.sub(2, 3)) print(c.mul(4, 5)) print(c.div(5, 6)) 3 -1 20 0.8333333333333334 Now, you might be wondering why you even need all of these boilerplate code and inheritance when you can directly define the concrete Calc class and call it a day. Consider the following scenario where you want to add additional functionalities to each of the method of the Calc class. Here, you’ve two options. Either you can mutate the original class and add those extra functionalities to the methods or you can create another class with similar footprint and implement all the methods with the added functionalities. The first option isn’t always viable and can cause regression in real life scenario. The second approach ensures modularity and is generally quicker to implement since you won’t have to worry about messing up the original concrete class. However, figuring out which methods you’ll need to implement in the extended class can be hard because the concrete class might have additional methods that you don’t want in the extended class. In this case, instead of figuring out the methods from the concrete Calc class, it’s easier to do so from an established structure defined in the ICalc interface. Interfaces make the process of extending class functionalities more tractable. Let’s make another class that will add logging to all of the methods of the Calc class: import logging logging.basicConfig(level=logging.INFO) class CalcLog(ICalc): \"\"\"Concrete Class: Calculator with logging\"\"\" def add(self, a, b): logging.info(f\"Operation: Addition, Arguments: {(a, b)}\") return a + b def sub(self, a, b): logging.info(f\"Operation: Subtraction, Arguments: {(a, b)}\") return a - b def mul(self, a, b): logging.info(f\"Operation: Multiplication, Arguments: {(a, b)}\") return a * b def div(self, a, b): logging.info(f\"Operation: Division, Arguments: {(a, b)}\") return a / b # Using the class clog = CalcLog() print(clog.add(1, 2)) print(clog.sub(2, 3)) print(clog.mul(4, 5)) print(clog.div(5, 6)) INFO:root:Operation: Addition, Arguments: (1, 2) INFO:root:Operation: Subtraction, Arguments: (2, 3) INFO:root:Operation: Multiplication, Arguments: (4, 5) INFO:root:Operation: Division, Arguments: (5, 6) 3 -1 20 0.8333333333333334 In the above class, I’ve defined another class called CalcLog that basically extends the functionalities of the previously defined Calc class. Here, I’ve inherited from the informal interface ICalc and implemented all the methods with additional info logging capability. Although writing informal interfaces is trivial, there are multiple issues that plagues them. The user of the interface class can still instantiate it like a normal class and won’t be able to tell the difference between a it and a concrete class until she tries to use any of the methods define inside the interface. Only then the methods will throw exceptions. This can have unintended side effects. Moreover, informal interfaces won’t compel you to implement all the methods in the subclasses. You can easily get away without implementing a particular method defined in the interface. It won’t complain about the unimplemented methods in the subclasses. However, if you try to use a method that hasn’t been implemented in the subclass, you’ll get an error. This means even if issubclass(ConcreteSubClass, Interface) shows True, you can’t rely on it since it doesn’t give you the guarantee that the ConcreteSubClass has implemented all the methods defined in the Interface. Let’s create another class FakeCalc an only implement one method defined in the ICalc abstract class: class FakeCalc(ICalc): \"\"\"Concrete Class: Fake calculator that doesn't implement all the methods defined in the interface.\"\"\" def add(self, a, b): return a + b # Using the class cfake = FakeCalc() print(cfake.add(1, 2)) print(cfake.sub(2, 3)) 3 --------------------------------------------------------------------------- NotImplementedError Traceback (most recent call last) in 10 cfake = FakeCalc() 11 print(cfake.add(1,2)) ---\u003e 12 print(cfake.sub(2,3)) in sub(self, a, b) 6 7 def sub(self, a, b): ----\u003e 8 raise NotImplementedError 9 10 def mul(self, a, b): NotImplementedError: Despite not implementing all the methods defined in the ICalc class, I was still able to instantiate the FakeCalc concrete class. However, when I tried to apply a method sub that wasn’t implemented in the concrete class, it gave me an error. Also, issubclass(FakeCalc, ICalc) returns True which can mislead you into thinking that all the methods of the subclass FakeCalc are usable. It can cause subtle bugs can be difficult to detect. Formal interfaces try to overcome these issues. Formal interfaces Formal interfaces do not suffer from the problems that plague informal interfaces. So if you want to implement an interface that the users can’t initiate independently and that forces them to implement all the methods in the concrete sub classes, formal interface is the way to go. In Python, the idiomatic way to define formal interfaces is via the abc module. Let’s transform the previously mentioned ICalc interface into a formal one: from abc import ABC, abstractmethod class ICalc(ABC): \"\"\"Formal interface: Abstract calculator class.\"\"\" @abstractmethod def add(self, a, b): pass @abstractmethod def sub(self, a, b): pass @abstractmethod def mul(self, a, b): pass @abstractmethod def div(self, a, b): pass Here, I’ve imported ABC class and abstractmethod decorator from the abc module of Python’s standard library. The name ABC stands for Abstract Base Class. The interface class needs to inherit from this ABCclass and all the abstract methods need to be decorated using the abstractmethod decorator. If your knowledge on decorators are fuzzy, checkout this in-depth article on python decorators1. Although, it seems like ICalc has merely inherited from the ABC class, under the hood, a metaclass2 ABCMeta gets attached to the interface which essentially makes sure that you can’t instantiate this class independently. Let’s try to do so and see what happens: i = ICalc() --------------------------------------------------------------------------- TypeError Traceback (most recent call last) in ----\u003e 1 i = ICalc() TypeError: Can't instantiate abstract class ICalc with abstract methods add, div, mul, sub The error message clearly states that you can’t instantiate the class ICalc directly at all. You’ve make a subclass of ICalc and implement all the abstract methods and only then you’ll be able to make an instance of the subclass. The subclassing and implementation part is same as before. class Calc(ICalc): \"\"\"Concrete calculator class\"\"\" def add(self, a, b): return a + b def sub(self, a, b): return a - b def mul(self, a, b): return a * b def div(self, a, b): return a / b # Using the class c = Calc() print(c.add(1, 2)) print(c.sub(2, 3)) print(c.mul(4, 5)) print(c.div(5, 6)) In the case of formal interface, failing to implement even one abstract method in the subclass will raise TypeError. So you can never write something the like the FakeCalc with a formal interface. This approach is more explicit and if there is an issue, it fails early. Interfaces vs abstract base classes You’ve probably seen the term Interface and Abstract Base Class being used interchangeably. However, conceptually they’re different. Interfaces can be thought of as a special case of Abstract Base Classes. It’s imperative that all the methods of an interface are abstract methods and the classes don’t store any state (instance variables). However, in case of abstract base classes, the methods are generally abstract but there can also be methods that provide implementation (concrete methods) and also, these classes can have instance variables. This generic abstract base classes can get very interesting and they can be used as mixins but more on that in the later sections. Both interfaces and abstract base classes are similar in the sense that they can’t stand on their own, that means these classes aren’t meant to be instantiated independently. Pay attention to the following snippet to understand how interfaces and abstract base classes differ. Interface from abc import ABC, abstractmethod class InterfaceExample(ABC): @abstractmethod def method_a(self): pass @abstractmethod def method_b(self): pass Here, all the methods must have to be abstract. Abstract Base Class from abc import ABC, abstractmethod class AbstractBaseClassExample(ABC): @abstractmethod def method_a(self): pass @abstractmethod def method_b(self): pass def method_c(self): # implement something pass Notice how method_c in the above class is a concrete method and can have implementation. The two examples above establish the fact that All interfaces are abstract base classes but not all abstract base classes are interfaces. A complete example Before moving on to the next section, let’s see another contrived example to get the idea about the cases where interfaces can come handy. I’ll define an interface called AutoMobile and create three concrete classes called Car, Truck and Bus from it. The interface defines three abstract methods start, accelerate and stop that the concrete classes will need to implement later. from abc import ABC, abstractmethod class Automobile(ABC): \"\"\"Formal interface: Abstract automobile class.\"\"\" @abstractmethod def start(self): pass @abstractmethod def accelerate(self): pass @abstractmethod def stop(self): pass class Car(Automobile): \"\"\"Concrete Class: Car\"\"\" def start(self): return \"The car is starting\" def accelerate(self): return \"The car is accelerating\" def stop(self): return \"The car is stopping\" class Truck(Automobile): \"\"\"Concrete Class: Truck\"\"\" def start(self): return \"The truck is starting\" def accelerate(self): return \"The truck is accelerating\" def stop(self): return \"The truck is stopping\" class Bus(Automobile): \"\"\"Concrete Class: Bus\"\"\" def start(self): return \"The bus is starting\" def accelerate(self): return \"The bus is accelerating\" def stop(self): return \"The bus is stopping\" car = Car() truck = Truck() bus = Bus() print(car.start()) print(car.accelerate()) print(car.stop()) print(truck.start()) print(truck.accelerate()) print(truck.stop()) print(bus.start()) print(bus.accelerate()) print(bus.stop()) The car is starting The car is accelerating The car is stopping The truck is starting The truck is accelerating The truck is stopping The bus is starting The bus is accelerating The bus is stopping The above example delineates the use cases for interfaces. When you need to create multiple similar classes, interfaces can provide a basic foundation for the subclasses to build upon. In the next section, I’ll be using formal interfaces to create Mixin classes. So, before understanding mixin classes and how they can be used to inject additional plugins to your classes, it’s important that you understand interfaces and abstract base classes properly. Mixins Imagine you’re baking chocolate brownies. Now, you can have them without any extra fluff which is fine or you can top them with cream cheese, caramel sauce, chocolate chips etc. Usually you don’t make the extra toppings yourself, rather you prepare the brownies and use off the shelf toppings. This also gives you the ability to mix and match different combinations of toppings to spruce up the flavors quickly. However, making the the toppings from scratch would be a lengthy process and doing it over an over again can ruin the fun of baking. While creating software, there’s sometimes a limit to the depth we should go. When pieces of what we’d like to achieve have already been executed well by others, it makes a lot of sense to reuse them. One way to achieve modularity and reusability in object-oriented programming is through a concept called a mixin. Different languages implement the concept of mixin in different ways. In Python, mixins are supported via multiple inheritance. Overview In the context of Python especially, a mixin is a parent class that provides functionality to subclasses but is not intended to be instantiated itself. This should already incite deja vu in you since classes that aren’t intended to be instantiated and can have both concrete and abstract methods are basically abstract base classes. Mixins can be regarded as a specific strain of abstract base classes where they can house both concrete and abstract methods but don’t keep any internal states. These can help you when - You want to provide a lot of optional features for a class. You want to provide a lot of not-optional features for a class, but you want the features in separate classes so that each of them is about one feature (behavior). You want to use one particular feature in many different classes. Let’s see a contrived example. Consider Werkzeug’s3 request and response system. Werkzeug is a small library that Flask4 depends on. I can make a plain old request object by saying: from werkzeug import BaseRequest class Request(BaseRequest): pass If I want to add accept header support, I would make that: from werkzeug import BaseRequest, AcceptMixin class Request(AcceptMixin, BaseRequest): pass If I wanted to make a request object that supports accept headers, etags, user agent and authentication support, I could do this: from werkzeug import ( BaseRequest, AcceptMixin, ETagRequestMixin, UserAgentMixin, AuthenticationMixin, ) class Request( AcceptMixin, ETagRequestMixin, UserAgentMixin, AuthenticationMixin, BaseRequest, ): pass The above example might cause you to say, “that’s just multiple inheritance, not really a mixin”, which is can be true in a special case. Indeed, the differences between plain old multiple inheritance and mixin based inheritance collapse when the parent class can be instantiated. Understanding the subtlety in the differences between a mixin class, an abstract base class, an interface and the scope of multiple inheritance is important, so I’ll explore them in a dedicated section. Differences between interfaces, abstract classes and mixins In order to better understand mixins, it’s be useful to compare mixins with abstract classes and interfaces from a code/implementation perspective: Interfaces Interfaces can contain abstract methods only, no concrete methods and no internal states (instance variables). Abstract Classes Abstract classes can contain abstract methods, concrete methods and internal state. Mixins Like interfaces, mixins do not contain any internal state. But like abstract classes, they can contain one or more concrete methods. So mixins are basically abstract classes without any internal states. In Python, these are just conventions because all of the above are defined as classes. However, one trait that is common among interfaces, abstract classes and mixins is that they shouldn’t exist on their own, i.e. shouldn’t be instantiated independently. A complete example Before diving into the real-life examples and how mixins can be used to construct custom data structures, let’s have a look at a self-contained example of a mixin class at work: import inspect from abc import ABC, abstractmethod from pprint import pprint class DisplayFactorMult(ABC): \"\"\"Mixin class that reveals factor calculation details.\"\"\" @abstractmethod def multiply(self, x): pass def multiply_show(self, x): result = self.multiply(x) print(f\"Factor: {self.factor}, Argument: {x}, Result: {result}\") return result class FactorMult(DisplayFactorMult): \"\"\"Concrete class that uses the DisplayFactorMult mixin.\"\"\" def __init__(self, factor): self.factor = factor def multiply(self, x): return x * self.factor # Putting the FactorMult class to use f = FactorMult(10) f.multiply_show(20) # Use the inspect.getmembers method to inspect the methods pprint(inspect.getmembers(f, predicate=inspect.ismethod)) Factor: 10, Argument: 20, Result: 200 [('__init__', ","permalink":"http://rednafi.com/python/mixins/","publishDate":"2020-07-03","summary":"Imagine a custom set-like data structure that doesn’t perform hashing and trades performance for tighter memory footprint. Or imagine a dict-like data structure that automatically stores data in a PostgreSQL or Redis database the moment you initialize it; also it lets you to get-set-delete key-value pairs using the usual retrieval-assignment-deletion syntax associated with built-in dictionaries. Custom data structures can give you the power of choice and writing them will make you understand how the built-in data structures in Python are constructed.\n","tags":["Python"],"title":"Interfaces, mixins and building powerful custom data structures in Python"},{"content":"Updated on 2023-09-11: Fix broken URLs. In Python, metaclass is one of the few tools that enables you to inject metaprogramming capabilities into your code. The term metaprogramming refers to the potential for a program to manipulate itself in a self referential manner. However, messing with metaclasses is often considered an arcane art that’s beyond the grasp of the plebeians. Heck, even Tim Peters1 advices you to tread carefully while dealing with these. Metaclasses are deeper magic than 99% of users should ever worry about. If you wonder whether you need them, you don’t (the people who actually need them know with certainty that they need them, and don’t need an explanation about why). Metaclasses are an esoteric OOP concept, lurking behind virtually all Python code. Every Python class that you create is attached to a default metaclass and Python cleverly abstracts away all the meta-magics. So, you’re indirectly using them all the time whether you are aware of it or not. For the most part, you don’t need to be aware of it. Most Python programmers rarely, if ever, have to think about metaclasses. This makes metaclasses exciting for me and I want to explore them in this post to formulate my own judgement. Metaclasses A metaclass is a class whose instances are classes. Like an “ordinary” class defines the behavior of the instances of the class, a metaclass defines the behavior of classes and their instances. Metaclasses aren’t supported by every object oriented programming language. Those programming language, which support metaclasses, considerably vary in way they implement them. Python provides you a way to get under the hood and define custom metaclasses. Understanding type and class In Python, everything is an object. Classes are objects as well. As a result, all classes must have corresponding types. You deal with built in types like int, float, list etc all the time. Consider this example: a = 5 print(type(a)) print(type(int)) ","permalink":"http://rednafi.com/python/metaclasses/","publishDate":"2020-06-26","summary":"Updated on 2023-09-11: Fix broken URLs.\nIn Python, metaclass is one of the few tools that enables you to inject metaprogramming capabilities into your code. The term metaprogramming refers to the potential for a program to manipulate itself in a self referential manner. However, messing with metaclasses is often considered an arcane art that’s beyond the grasp of the plebeians. Heck, even Tim Peters1 advices you to tread carefully while dealing with these.\n","tags":["Python"],"title":"Deciphering Python's metaclasses"},{"content":"In Python, there’s a saying that “design patterns are anti-patterns”. Also, in the realm of dynamic languages, design patterns have the notoriety of injecting additional abstraction layers to the core logic and making the flow gratuitously obscure. Python’s dynamic nature and the treatment of functions as first-class objects often make Java-ish design patterns redundant. Instead of littering your code with seemingly over-engineered patterns, you can almost always take the advantage of Python’s first-class objects, duck-typing, monkey-patching etc to accomplish the task at hand. However, recently there is one design pattern that I find myself using over and over again to write more maintainable code and that is the Proxy pattern. So I thought I’d document it here for future reference. The proxy pattern Before diving into the academic definition, let’s try to understand the Proxy pattern from an example. Have you ever used an access card to go through a door? There are multiple options to open that door i.e. it can be opened either using access card or by pressing a button that bypasses the security. The door’s main functionality is to open but there is a proxy added on top of it to add some functionality. Let me better explain it using the code example below: # src.py class Door: def open_method(self) -\u003e None: pass class SecuredDoor: def __init__(self) -\u003e None: self._klass = Door() def open_method(self) -\u003e None: print(f\"Adding security measure to the method of {self._klass}\") secured_door = SecuredDoor() secured_door.open_method() \u003e\u003e\u003e Adding security measure to the method of \u003c__main__.Door object at 0x7f9dab3b6670\u003e The above code snippet concretizes the example given before. Here, the Door class has a single method called open_method which denotes the action of opening on the Door object. This method gets extended in the SecuredDoor class and in this case, I’ve just added a print statement to the method of the latter class. Notice how the class Door was called from SecuredDoor via composition1. In the case of proxy pattern, you can substitute primary object with the proxy object without any additional changes in the code. This conforms to the Liskov Substitution Principle2. It states: Objects of a superclass shall be replaceable with objects of its subclasses without breaking the application. That requires the objects of your subclasses to behave in the same way as the objects of your superclass. The Door object can be replaced by the SecuredDoor and the SecuredDoor class does not introduce any new methods, it only extends the functionality of the open_method of the Door class. In plain words: Using the proxy pattern, a class represents the functionality of another class. Wikipedia says: A proxy, in its most general form, is a class functioning as an interface to something else. A proxy is a wrapper or agent object that is being called by the client to access the real serving object behind the scenes. Use of the proxy can simply be forwarding to the real object, or can provide additional logic. In the proxy extra functionality can be provided, for example caching when operations on the real object are resource intensive, or checking preconditions before operations on the real object are invoked. Pedagogically, the proxy pattern belongs to a family of patterns called the structural pattern3. Why use it? Loose coupling Proxy pattern let’s you easily decouple your core logic from the added functionalities that might be needed on top of that. The modular nature of the code makes maintaining and extending the functionalities of your primary logic a lot quicker and easier. Suppose, you’re defining a division function that takes takes two integer as arguments and returns the result of the division between them. It also handles edge cases like ZeroDivisionError or TypeError and logs them properly. # src.py from __future__ import annotations import logging from typing import Union logging.basicConfig(level=logging.INFO) Number = Union[int, float] def division(a: Number, b: Number) -\u003e float: try: result = a / b return result except ZeroDivisionError: logging.error(f\"Argument b cannot be {b}\") except TypeError: logging.error(\"Arguments must be integers/floats\") print(division(1.9, 2)) \u003e\u003e\u003e 0.95 You can see this function is already doing three things at once which violates the Single Responsibility Principle4. SRP says that a function or class should have only one reason to change. In this case, a change in any of the three responsibilities can force the function to change. Also this means, changing or extending the function can be difficult to keep track of. Instead, you can write two classes. The primary class Division will only implement the core logic while another class ProxyDivision will extend the functionality of Division by adding exception handlers and loggers. # src.py from __future__ import annotations import logging from typing import Union logging.basicConfig(level=logging.INFO) Number = Union[int, float] class Division: def div(self, a: Number, b: Number) -\u003e Number: return a / b class ProxyDivision: def __init__(self) -\u003e None: self._klass = Division() def div(self, a: Number, b: Number) -\u003e Number: try: result = self._klass.div(a, b) return result except ZeroDivisionError: logging.error(f\"Argument b cannot be {b}\") except TypeError: logging.error(\"Arguments must be integers/floats\") klass = ProxyDivision() print(klass.div(2, 0)) \u003e\u003e\u003e ERROR:root:Argument b cannot be 0 None In the example above, since both Division and ProxyDivision class implement the same interface, you can swap out the Division class with ProxyDivision and vice versa. The second class neither inherits directly from the first class nor it adds any new method to it. This means you can easily write another class to extend the functionalities of Division or DivisionProxy class without touching their internal logics directly. Enhanced testability Another great advantage of using the proxy pattern is enhanced testability. Since your core logic is loosely coupled with the extended functionalities, you can test them out separately. This makes the test more succinct and modular. It’s easy to demonstrate the benefits with our previously mentioned Division and ProxyDivision classes. Here, the logic of the primary class is easy to follow and since this class only holds the core logic, it’s crucial to write unit test for this before testing the added functionalities. Testing out the Division class is much cleaner than testing the previously defined division function that tries to do multiple things at once. Once you’re done testing the primary class, you can proceed with the additional functionalities. Usually, this decoupling of core logic from the cruft and the encapsulation of additional functionalities result in more reliable and rigorous unit tests. Proxy pattern with interface In the real world, your class won’t look like the simple Division class having only a single method. Usually your primary class will have multiple methods and they will carry out multiple sophisticated tasks. By now, you probably have grasped the fact that the proxy classes need to implement all of the methods of the primary class. While writing a proxy class for a complicated primary class, the author of that class might forget to implement all the methods of the primary class.This will lead to a violation of the proxy pattern. Also, it can be hard to follow all the methods of the primary class if the class is large and complicated. Here, the solution is an interface that can signal the author of the proxy class about all the methods that need to be implemented. An interface is nothing but an abstract class that dictates all the methods a concrete class needs to implement. However, interfaces can’t be initialized independently. You’ll have to make a subclass of the interface and implement all the methods defined there. Your subclass will raise error if it fails to implement any of the methods of the interface. Let’s look at a minimal example of how you can write an interface using Python’s abc.ABC and abc.abstractmethod and achieve proxy pattern with that. # src.py from abc import ABC, abstractmethod class Interface(ABC): \"\"\"Interfaces of Interface, Concrete \u0026 Proxy should be the same, because the client should be able to use Concrete or Proxy without any change in their internals. \"\"\" @abstractmethod def job_a(self, user: str) -\u003e None: pass @abstractmethod def job_b(self, user: str) -\u003e None: pass class Concrete(Interface): \"\"\"This is the main job doer. External services like payment gateways can be a good example. \"\"\" def job_a(self, user: str) -\u003e None: print(f\"I am doing the job_a for {user}\") def job_b(self, user: str) -\u003e None: print(f\"I am doing the job_b for {user}\") class Proxy(Interface): def __init__(self) -\u003e None: self._concrete = Concrete() def job_a(self, user: str) -\u003e None: print(f\"I'm extending job_a for user {user}\") def job_b(self, user: str) -\u003e None: print(f\"I'm extending job_b for user {user}\") if __name__ == \"__main__\": klass = Proxy() print(klass.job_a(\"red\")) print(klass.job_b(\"nafi\")) \u003e\u003e\u003e I'm extending job_a for user red None I'm extending job_b for user nafi None It’s evident from the above workflow that you’ll need to define an Interface class first. Python provides abstract base classes as ABC in the abc module. Abstract class Interface inherits from ABC and defines all the methods that the concrete class will have to implement later. Concrete class inherits from the interface and implements all the methods defined in it. Notice how each method in the Interface class is decorated with the @abstractmethod decorator. If your knowledge on decorator is fuzzy, then checkout this5 post on Python decorators. The @abstractmethod decorator turns a normal method into an abstract method which means that the method is nothing but a blueprint of the required methods that the concrete subclass will have to implement later. You can’t directly instantiate Interface or use any of the abstract methods without making subclasses of the interface and implementing the methods. The second class Concrete is the actual class that inherits from the abstract base class (interface) Interface and implements all the methods mentioned as abstract methods. This is a real class that you can instantiate and the methods can be used directly. However, if you forget to implement any of the abstract methods defined in the Interface then you’ll invoke TypeError. The third class Proxy extends the functionalities of the base concrete class Concrete. It calls the Concrete class using the composition pattern and implements all the methods. However, in this case, I used the results from the concrete methods and extended their functionalities without code duplication. Another practical example Let’s play around with one last real-world example to concretize the concept. Suppose, you want to collect data from an external API endpoint. To do so, you hit the endpoint with GET requests from your HTTP client and collect the responses in json format. Then say, you also want to inspect the response header and the arguments that were passed while making the request. Now, in the real world, public APIs will often impose rate limits and when you go over the limit with multiple get requests, your client will likely throw an http connection-timeout error. Say, you want to handle this exceptions outside of the core logic that will send the HTTP GET requests. Again, let’s say you also want to cache the responses if the client has seen the arguments in the requests before. This means, when you send requests with the same arguments multiple times, instead of hitting the APIs with redundant requests, the client will show you the responses from the cache. Caching improves API response time dramatically. For this demonstration, I’ll be using Postman’s publicly available GET API. https://postman-echo.com/get?foo1=bar_1\u0026foo2=bar_2 This API is perfect for the demonstration since it has a rate limiter that kicks in arbitrarily and make the client throw ConnectTimeOut and ReadTimeOutError. See how this workflow is going to look like: Define an interface called IFetchUrl that will implement three abstract methods. The first method get_data will fetch data from the URL and serialize them into json format. The second method get_headers will probe the data and return the header as a dictionary. The third method get_args will also probe the data like the second method but this time it will return the query arguments as a dictionary. However, in the interface, you won’t be implementing anything inside the methods. Make a concrete class named FetchUrl that will derive from interface IFetchUrl. This time you’ll implement all three methods defined in the abstract class. However, you shouldn’t handle any edge cases here. The method should contain pure logic flow without any extra fluff. Make a proxy class called ExcFetchUrl. It will also inherit from the interface but this time you’ll add your exception handling logics here. This class also adds logging functionality to all the methods. Here you call the concrete class FetchUrl in a composition format and avoid code repetition by using the methods that’s been already implemented in the concrete class. Like the FetchUrl class, here too, you’ve to implement all the methods found in the abstract class. The fourth and the final class will extend the ExcFetchUrl and add caching functionality to the get_data method. It will follow the same pattern as the ExcFetchUrl class. Since, by now, you’re already familiar with the workflow of the proxy pattern, let’s dump the entire 110 line solution all at once. from __future__ import annotations import functools import logging import sys from abc import ABC, abstractmethod from datetime import datetime from pprint import pprint import httpx from httpx import ConnectTimeout, ReadTimeout from typing import Any logging.basicConfig(level=logging.INFO) D = dict[str, Any] class IFetchUrl(ABC): \"\"\"Abstract base class. You can't instantiate this independently.\"\"\" @abstractmethod def get_data(self, url: str) -\u003e D: pass @abstractmethod def get_headers(self, data: D) -\u003e D: pass @abstractmethod def get_args(self, data: D) -\u003e D: pass class FetchUrl(IFetchUrl): \"\"\"Concrete class that doesn't handle exceptions and loggings.\"\"\" def get_data(self, url: str) -\u003e D: with httpx.Client() as client: response = client.get(url) data = response.json() return data def get_headers(self, data: D) -\u003e D: return data[\"headers\"] def get_args(self, data: D) -\u003e D: return data[\"args\"] class ExcFetchUrl(IFetchUrl): \"\"\"This class can be swapped out with the FetchUrl class. It provides additional exception handling and logging.\"\"\" def __init__(self) -\u003e None: self._fetch_url = FetchUrl() def get_data(self, url: str) -\u003e D: try: data = self._fetch_url.get_data(url) return data except ConnectTimeout: logging.error(\"Connection time out. Try again later.\") sys.exit(1) except ReadTimeout: logging.error(\"Read timed out. Try again later.\") sys.exit(1) def get_headers(self, data: D) -\u003e D: headers = self._fetch_url.get_headers(data) logging.info(f\"Getting the headers at {datetime.now()}\") return headers def get_args(self, data: D) -\u003e D: args = self._fetch_url.get_args(data) logging.info(f\"Getting the args at {datetime.now()}\") return args class CacheFetchUrl(IFetchUrl): def __init__(self) -\u003e None: self._fetch_url = ExcFetchUrl() self.get_data = functools.lru_cache()(self.get_data) # type: ignore def get_data(self, url: str) -\u003e D: data = self._fetch_url.get_data(url) return data def get_headers(self, data: D) -\u003e D: headers = self._fetch_url.get_headers(data) return headers def get_args(self, data: D) -\u003e D: args = self._fetch_url.get_args(data) return args if __name__ == \"__main__\": # url = \"https://postman-echo.com/get?foo1=bar_1\u0026foo2=bar_2\" fetch = CacheFetchUrl() for arg1, arg2 in zip([1, 2, 3, 1, 2, 3], [1, 2, 3, 1, 2, 3]): url = f\"https://postman-echo.com/get?foo1=bar_{arg1}\u0026foo2=bar_{arg2}\" print(f\"\\n {'-'*75}\\n\") data = fetch.get_data(url) print(f\"Cache Info: {fetch.get_data.cache_info()}\") # type: ignore pprint(fetch.get_headers(data)) pprint(fetch.get_args(data)) --------------------------------------------------------------------------- INFO:root:Getting the headers at 2022-01-31 16:54:36.214562 INFO:root:Getting the args at 2022-01-31 16:54:36.220221 Cache Info: CacheInfo(hits=0, misses=1, maxsize=32, currsize=1) {'accept': '*/*', 'accept-encoding': 'gzip, deflate', 'content-length': '0', 'host': 'postman-echo.com', 'user-agent': 'python-httpx/0.13.1', 'x-amzn-trace-id': 'Root=1-5ee8a4eb-4341ae58365e4090660dfaa4', 'x-b3-parentspanid': '044bd10726921994', 'x-b3-sampled': '0', 'x-b3-spanid': '503e6ceaa2a4f493', 'x-b3-traceid': '77d5b03fe98fcc1a044bd10726921994', 'x-envoy-external-address': '10.100.91.201', 'x-forwarded-client-cert': 'By=spiffe://cluster.local/ns/pm-echo-istio/sa/default; Hash=2ed845a68a0968c80e6e0d0f49dec5ce15ee3c1f87408e56c938306f2129528b;Subject=\"\"; URI=spiffe://cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account', 'x-forwarded-port': '443', 'x-forwarded-proto': 'http', 'x-request-id': '295d0b6c-7aa0-4481-aa4d-f47f5eac7d57'} {'foo1': 'bar_1', 'foo2': 'bar_1'} .... In the get_data method of the FetchUrl class, I’ve used the awesome HTTPx6 client to fetch the data from the URL. Pay attention to the fact that I’ve practically ignored all the additional logics of error handling and logging here. The exception handling and logging logic were added via ExcFetchUrl proxy class. Another class CacheFetchUrl further extends the proxy class ExcFetchUrl by adding cache functionality to the get_data method. In the main section, you can use any of the FetchUrl, ExcFetchUrl or CacheFetchUrl without any additional changes to the logic of these classes. The FetchUrl is the barebone class that will fail in case of the occurrence of any exceptions. The latter classes appends additional functionalities while maintaining the same interface. The output basically prints out the results returned by the get_headers and get_args methods. Also notice, how I picked the endpoint arguments to simulate caching. The Cache Info: on the third line of the output shows when data is served from the cache. Here, hits=0 means data is served directly from the external API. However, if you inspect the later outputs, you’ll see when the query arguments get repeated ([1, 2, 3, 1, 2, 3]), Cache Info: will show higher hit counts. This means that the data is being served from the cache. Should you use it? Well, yes obviously. But not always. You see, you need a little bit of planning before orchestrating declarative solution with the proxy pattern. It’s not viable to write code in this manner in a throwaway script that you don’t have to maintain in the long run. Also, this OOP-cursed additional layers of abstraction can make your code subjectively unreadable. So use the pattern wisely. On the flip side, proxy pattern can come in handy when you need to extend the functionality of some class arbitrarily as it can work a gateway to the El Dorado of loose coupling. Composition ↩︎ Liskov Substitution Principle ↩︎ Structural pattern ↩︎ Single Responsibility Principle ↩︎ Python decorators ↩︎ HTTPx ↩︎ Proxy pattern 7 ↩︎ Design patterns for humans - proxy pattern 8 ↩︎ ","permalink":"http://rednafi.com/python/proxy_pattern/","publishDate":"2020-06-16","summary":"In Python, there’s a saying that “design patterns are anti-patterns”. Also, in the realm of dynamic languages, design patterns have the notoriety of injecting additional abstraction layers to the core logic and making the flow gratuitously obscure. Python’s dynamic nature and the treatment of functions as first-class objects often make Java-ish design patterns redundant.\nInstead of littering your code with seemingly over-engineered patterns, you can almost always take the advantage of Python’s first-class objects, duck-typing, monkey-patching etc to accomplish the task at hand. However, recently there is one design pattern that I find myself using over and over again to write more maintainable code and that is the Proxy pattern. So I thought I’d document it here for future reference.\n","tags":["Python"],"title":"Implementing proxy pattern in Python"},{"content":"Updated on 2023-09-11: Fix broken URLs. Recently, I was working with MapBox’s1 Route optimization API2. Basically, it tries to solve the traveling salesman problem3 where you provide the API with coordinates of multiple places and it returns a duration-optimized route between those locations. This is a perfect usecase where Redis4 caching can come handy. Redis is a fast and lightweight in-memory database with additional persistence options; making it a perfect candidate for the task at hand. Here, caching can save you from making redundant API requests and also, it can dramatically improve the response time as well. I found that in my country, the optimized routes returned by the API do not change dramatically for at least for a couple of hours. So the workflow will look something like this: Caching the API response in Redis using the key-value data structure. Here the requested coordinate-string will be the key and the response will be the corresponding value. Setting a timeout on the records. Serving new requests from cache if the records exist. Only send a new request to MapBox API if the response is not cached and then add that response to cache. Setting up Redis \u0026 RedisInsight To proceed with the above workflow, you’ll need to install and setup Redis database on your system. For monitoring the database, I’ll be using RedisInsight5. The easiest way to setup Redis and RedisInsight is through Docker6. Here’s a docker-compose that you can use to setup everything with a single command. # docker-compose.yml version: \"3.9\" services: redis: container_name: redis-cont image: \"redis:alpine\" environment: - REDIS_PASSWORD=ubuntu - REDIS_REPLICATION_MODE=master ports: - \"6379:6379\" volumes: # save redisearch data to your current working directory - ./redis-data:/data command: # Save if 100 keys are added in every 10 seconds - \"--save 10 100\" # Set password - \"--requirepass ubuntu\" redisinsight: # redis db visualization dashboard container_name: redisinsight-cont image: redislabs/redisinsight ports: - 8001:8001 volumes: - redisinsight:/db volumes: redis-data: redisinsight: The above docker-compose file has two services, redis and redisinsight. I’ve set up the database with a dummy password ubuntu and made it persistent using a folder named redis-data in the current working directory. The database listens in localhost’s port 6379. You can monitor the database using redisinsight in port 8000. To spin up Redis and RedisInsight containers, run: docker compose up -d This command will start the database and monitor accordingly. You can go to this localhost:8000 link using your browser and connect redisinsight to your database. After connecting your database, you should see a dashboard like this in your redisinsight panel: Preparing Python environment For local development, you can set up your python environment and install the dependencies using pip. Here, I’m on a Linux machine and using virtual environment for isolation. The following commands will work if you’re on a *nix based system and have python 3.12 installed on your system. This will install the necessary dependencies in a virtual environment: python3.12 -m venv .venv source .venv/bin/activate pip install redis httpx Workflow Connecting Python client to Redis Assuming the database server is running and you’ve installed the dependencies, the following snippet connects redis-py client to the database. import redis import sys def redis_connect() -\u003e redis.client.Redis: try: client = redis.Redis( host=\"localhost\", port=6379, password=\"ubuntu\", db=0, socket_timeout=5, ) ping = client.ping() if ping is True: return client except redis.AuthenticationError: print(\"AuthenticationError\") sys.exit(1) client = redis_connect() The above excerpt tries to connect to the Redis database server using the port 6379. Notice, how I’m providing the password ubuntu via the password argument. Here, client.ping() helps you determine if a connection has been established successfully. It returns True if a successful connection can be established or raises specific errors in case of failures. The above function handles AuthenticationError and prints out an error message if the error occurs. If everything goes well, running the redis_connect() function will return an instance of the redis.client.Redis class. This instance will be used later to set and retrieve data to and from the redis database. Getting route data from MapBox API The following function strikes the MapBox Route Optimization API and collects route data. import httpx def get_routes_from_api(coordinates: str) -\u003e dict: \"\"\"Data from mapbox api.\"\"\" with httpx.Client() as client: base_url = \"https://api.mapbox.com/optimized-trips/v1/mapbox/driving\" geometries = \"geojson\" access_token = \"Your-MapBox-API-token\" url = ( f\"{base_url}/{coordinates}?geometries={geometries}\" f\"\u0026access_token={access_token}\" ) response = client.get(url) return response.json() The above code uses Python’s httpx7 library to make the get request. Httpx is almost a drop-in replacement for the ubiquitous requests8 library but way faster and has async support. Here, I’ve used context manager httpx.Client() for better resource management while making the get request. You can read more about context managers and how to use them for hassle free resource management here9. The base_url is the base url of the route optimization API and the you’ll need to provide your own access token in the access_token field. Notice, how the url variable builds up the final request url. The coordinates are provided using the lat0,lon0;lat1,lon1;lat2,lon2... format. Rest of the function sends the http requests and converts the response into a native dictionary object using the response.json() method. Setting \u0026 retrieving data to \u0026 from Redis database The following two functions retrieves data from and sets data to redis database respectively. from datetime import timedelta def get_routes_from_cache(key: str) -\u003e str: \"\"\"Get data from redis.\"\"\" val = client.get(key) return val def set_routes_to_cache(key: str, value: str) -\u003e bool: \"\"\"Set data to redis.\"\"\" state = client.setex( key, timedelta(seconds=3600), value=value, ) return state Here, both the keys and the values are strings. In the second function, set_routes_to_cache, the client.setex() method sets a timeout of 1 hour on the key. After that the key and its associated value get deleted automatically. The central orchestration The route_optima function is the primary agent that orchestrates and executes the caching and returning of responses against requests. It roughly follows the execution flow shown below. When a new request arrives, the function first checks if the return-value exists in the Redis cache. If the value exists, it shows the cached value, otherwise, it sends a new request to the MapBox API, cache that value and then shows the result. def route_optima(coordinates: str) -\u003e dict: # First it looks for the data in redis cache data = get_routes_from_cache(key=coordinates) # If cache is found then serves the data from cache if data is not None: data = json.loads(data) data[\"cache\"] = True return data else: # If cache is not found then sends request to the MapBox API data = get_routes_from_api(coordinates) # This block sets saves the respose to redis and serves it directly if data.get(\"code\") == \"Ok\": data[\"cache\"] = False data = json.dumps(data) state = set_routes_to_cache(key=coordinates, value=data) if state is True: return json.loads(data) return data Exposing as an API This part of the code wraps the original Route Optimization API and exposes that as a new endpoint. I’ve used FastAPI10 to build the wrapper API. Doing this also hides the underlying details of authentication and the actual endpoint of the MapBox API. from fastapi import FastAPI app = FastAPI() @app.get(\"/route-optima/{coordinates}\") def view(coordinates): \"\"\"This will wrap our original route optimization API and incorporate Redis Caching. You'll only expose this API to the end user.\"\"\" # coordinates = \"90.3866,23.7182;90.3742,23.7461\" return route_optima(coordinates) Putting it all together # app.py import json import sys from datetime import timedelta import httpx import redis from fastapi import FastAPI def redis_connect() -\u003e redis.client.Redis: try: client = redis.Redis( host=\"localhost\", port=6379, password=\"ubuntu\", db=0, socket_timeout=5, ) ping = client.ping() if ping is True: return client except redis.AuthenticationError: print(\"AuthenticationError\") sys.exit(1) client = redis_connect() def get_routes_from_api(coordinates: str) -\u003e dict: \"\"\"Data from mapbox api.\"\"\" with httpx.Client() as client: base_url = \"https://api.mapbox.com/optimized-trips/v1/mapbox/driving\" geometries = \"geojson\" access_token = \"Your-MapBox-API-token\" url = ( f\"{base_url}/{coordinates}?geometries={geometries}\" f\"\u0026access_token={access_token}\" ) response = client.get(url) return response.json() def get_routes_from_cache(key: str) -\u003e str: \"\"\"Data from redis.\"\"\" val = client.get(key) return val def set_routes_to_cache(key: str, value: str) -\u003e bool: \"\"\"Data to redis.\"\"\" state = client.setex( key, timedelta(seconds=3600), value=value, ) return state def route_optima(coordinates: str) -\u003e dict: # First it looks for the data in redis cache data = get_routes_from_cache(key=coordinates) # If cache is found then serves the data from cache if data is not None: data = json.loads(data) data[\"cache\"] = True return data else: # If cache is not found then sends request to the MapBox API data = get_routes_from_api(coordinates) # This block sets saves the respose to redis and serves it directly if data.get(\"code\") == \"Ok\": data[\"cache\"] = False data = json.dumps(data) state = set_routes_to_cache(key=coordinates, value=data) if state is True: return json.loads(data) return data app = FastAPI() @app.get(\"/route-optima/{coordinates}\") def view(coordinates: str) -\u003e dict: \"\"\"This will wrap our original route optimization API and incorporate Redis Caching. You'll only expose this API to the end user.\"\"\" # coordinates = \"90.3866,23.7182;90.3742,23.7461\" return route_optima(coordinates) You can copy the complete code to a file named app.py and run the app using the command below (assuming redis, redisinsight is running and you’ve installed the dependencies beforehand): uvicorn app.app:app --host 0.0.0.0 --port 5000 --reload This will run a local server where you can send new request with coordinates. Go to your browser and hit the endpoint with a set of new coordinates. For example: http://localhost:5000/route-optima/90.3866,23.7182;90.3742,23.7461 This should return a response with the coordinates of the optimized route. { \"code\":\"Ok\", \"waypoints\":[ { \"distance\":26.041809241776583, \"name\":\"\", \"location\":[ 90.386855, 23.718213 ], \"waypoint_index\":0, \"trips_index\":0 }, { \"distance\":6.286653078791968, \"name\":\"\", \"location\":[ 90.374253, 23.746129 ], \"waypoint_index\":1, \"trips_index\":0 } ], \"trips\":[ { \"geometry\":{ \"coordinates\":[ [ 90.386855, 23.718213 ], \"... ...\" ], \"type\":\"LineString\" }, \"legs\":[ { \"summary\":\"\", \"weight\":3303.1, \"duration\":2842.8, \"steps\":[ ], \"distance\":5250.2 }, { \"summary\":\"\", \"weight\":2536.5, \"duration\":2297, \"steps\":[ ], \"distance\":4554.8 } ], \"weight_name\":\"routability\", \"weight\":5839.6, \"duration\":5139.8, \"distance\":9805 } ], \"cache\":false } If you’ve hit the above URL for the first time, the cache attribute of the json response should show false. This means that the response is being served from the original MapBox API. However, hitting the same URL with the same coordinates again will show the cached response and this time the cache attribute should show true. Inspection Once you’ve got everything up and running you can inspect the cache via redis insight. To do so, go to the link below while your app server is running: http://localhost:8000/ Select the Browser panel from the left menu and click on a key of your cached data. It should show something like this: Also you can play around with the API in the swagger UI. To do so, go to the following link: http://localhost:5000/docs This will take you to the swagger dashboard. Here you can make requests using the interactive UI. Go ahead and inspect how the caching works for new coordinates. Remarks You can find the complete source code of the app [here]11. Disclaimer This app has been made for demonstration purpose only. So it might not reflect the best practices of production ready applications. Using APIs without authentication like this is not recommended. Mapbox ↩︎ Route optimization API ↩︎ Traveling salesman problem ↩︎ Redis ↩︎ RedisInsight ↩︎ Dockjer ↩︎ HTTPx ↩︎ requests ↩︎ contextmanager ↩︎ FastAPI ↩︎ HTTP request caching with Redis ↩︎ ","permalink":"http://rednafi.com/python/redis_cache/","publishDate":"2020-05-25","summary":"Updated on 2023-09-11: Fix broken URLs.\nRecently, I was working with MapBox’s1 Route optimization API2. Basically, it tries to solve the traveling salesman problem3 where you provide the API with coordinates of multiple places and it returns a duration-optimized route between those locations. This is a perfect usecase where Redis4 caching can come handy. Redis is a fast and lightweight in-memory database with additional persistence options; making it a perfect candidate for the task at hand. Here, caching can save you from making redundant API requests and also, it can dramatically improve the response time as well.\n","tags":["Python","API"],"title":"Effortless API response caching with Python \u0026 Redis"},{"content":"Updated on 2022-02-13: Change functools import style. When I first learned about Python decorators, using them felt like doing voodoo magic. Decorators can give you the ability to add new functionalities to any callable without actually touching or changing the code inside it. This can typically yield better encapsulation and help you write cleaner and more understandable code. However, decorator is considered as a fairly advanced topic in Python since understanding and writing it requires you to have command over multiple additional concepts like first class objects, higher order functions, closures etc. First, I’ll try to introduce these concepts as necessary and then unravel the core concept of decorator layer by layer. So let’s dive in. First class objects In Python, basically everything is an object and functions are regarded as first-class objects. It means that functions can be passed around and used as arguments, just like any other object (string, int, float, list, and so on). You can assign functions to variables and treat them like any other objects. Consider this example: def func_a(): return \"I was angry with my friend.\" def func_b(): return \"I told my wrath, my wrath did end\" def func_c(*funcs): for func in funcs: print(func()) main_func = func_c main_func(func_a, func_b) \u003e\u003e\u003e I was angry with my friend. \u003e\u003e\u003e I told my wrath, my wrath did end The above example demonstrates how Python treats functions as first class citizens. First, I defined two functions, func_a and func_b and then func_c takes them as parameters. func_c runs the functions taken as parameters and prints the results. Then we assign func_c to variable main_func. Finally, we run main_func and it behaves just like func_c. Higher order functions Python also allows you to use functions as return values. You can take in another function and return that function or you can define a function within another function and return the inner function. def higher(func): \"\"\"This is a higher order function. It returns another function. \"\"\" return func def lower(): return \"I'm hunting high and low\" higher(lower) \u003e\u003e\u003e Now you can assign the result of higher to another variable and execute the output function. h = higher(lower) h() \u003e\u003e\u003e \"I'm hunting high and low\" Let’s look into another example where you can define a nested function within a function and return the nested function instead of its result. def outer(): \"\"\"Define and return a nested function from another function.\"\"\" def inner(): return \"Hello from the inner func\" return inner inn = outer() inn() \u003e\u003e\u003e 'Hello from the inner func' Notice how the nested function inner was defined inside the outer function and then the return statement of the outer function returned the nested function. After definition, to get to the nested function, first we called the outer function and received the result as another function. Then executing the result of the outer function prints out the message from the inner function. Closures You saw examples of inner functions at work in the previous section. Nested functions can access variables of the enclosing scope. In Python, these non-local variables are read only by default and we must declare them explicitly as non-local (using nonlocal keyword) in order to modify them. Following is an example of a nested function accessing a non-local variable. def burger(name): def ingredients(): if name == \"deli\": return (\"steak\", \"pastrami\", \"emmental\") elif name == \"smashed\": return (\"chicken\", \"nacho cheese\", \"jalapeno\") else: return None return ingredients Now run the function, ingr = burger(\"deli\") ingr() \u003e\u003e\u003e ('steak', 'pastrami', 'emmental') Well, that’s unusual. The burger function was called with the string deli and the returned function was bound to the name ingr. On calling ingr(), the message was still remembered and used to derive the outcome although the outer function burger had already finished its execution. This technique by which some data (“deli”) gets attached to the code is called closure in Python. The value in the enclosing scope is remembered even when the variable goes out of scope or the function itself is removed from the current namespace. Decorators uses the idea of non-local variables multiple times and soon you’ll see how. Writing a basic decorator With these prerequisites out of the way, let’s go ahead and create your first simple decorator. def deco(func): def wrapper(): print(\"This will get printed before the function is called.\") func() print(\"This will get printed after the function is called.\") return wrapper Before using the decorator, let’s define a simple function without any parameters. def ans(): print(42) Treating the functions as first-class objects, you can use your decorator like this: ans = deco(ans) ans() \u003e\u003e\u003e This will get printed before the function is called. 42 This will get printed after the function is called. In the above two lines, you can see a very simple decorator in action. Our deco function takes in a target function, manipulates the target function inside a wrapper function and then returns the wrapper function. Running the function returned by the decorator, you’ll get your modified result. To put it simply, decorators wraps a function and modifies its behavior. The decorator function runs at the time the decorated function is imported/defined, not when it is called. Before moving onto the next section, let’s see how we can get the return value of target function instead of just printing it. def deco(func): \"\"\"This modified decorator also returns the result of func.\"\"\" def wrapper(): print(\"This will get printed before the function is called.\") ret = func() print(\"This will get printed after the function is called.\") return ret return wrapper def ans(): return 42 In the above example, the wrapper function returns the result of the target function and the wrapper itself. This makes it possible to get the result of the modified function. ans = deco(ans) print(ans()) \u003e\u003e\u003e This will get printed before the function is called. This will get printed after the function is called. 42 Can you guess why the return value of the decorated function appeared in the last line instead of in the middle like before? The @ syntactic sugar The way you’ve used decorator in the last section might feel a little clunky. First, you have to type the name ans three times to call and use the decorator. Also, it becomes harder to tell apart where the decorator is actually working. So Python allows you to use decorator with the special syntax @. You can apply your decorators while defining your functions, like this: @deco def func(): ... # Now call your decorated function just like a normal one func() Sometimes the above syntax is called the pie syntax and it’s just a syntactic sugar for func = deco(func). Decorating functions with arguments The naive decorator that we’ve implemented above will only work for functions that take no arguments. It’ll fail and raise TypeError if your try to decorate a function having arguments with deco. Now let’s create another decorator called yell which will take in a function that returns a string value and transform that string value to uppercase. def yell(func): def wrapper(*args, **kwargs): ret = func(*args, **kwargs) ret = ret.upper() + \"!\" return ret return wrapper Create the target function that returns string value. @yell def hello(name): return f\"Hello {name}\" hello(\"redowan\") \u003e\u003e\u003e 'HELLO REDOWAN!' Function hello takes a name:string as parameter and returns a message as string. Look how the yell decorator is modifying the original return string, transforming that to uppercase and adding an extra ! sign without directly changing any code in the hello function. Solving identity crisis In Python, you can introspect any object and its properties via the interactive shell. A function knows its identity, docstring etc. For instance, you can inspect the built in print function in the following ways: print \u003e\u003e\u003e print.__name__ \u003e\u003e\u003e 'print' print.__doc__ \u003e\u003e\u003e \"print(value, ..., sep=' ', end='\\\\n', file=sys.stdout, flush=False)\\n\\nPrints the values to a stream, or to sys.stdout by default.\\nOptional keyword arguments:\\nfile: a file-like object (stream); defaults to the current sys.stdout.\\nsep: string inserted between values, default a space.\\nend: string appended after the last value, default a newline.\\nflush: whether to forcibly flush the stream.\" help(print) \u003e\u003e\u003e Help on built-in function print in module builtins: print(...) print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream. This introspection works similarly for functions that you defined yourself. I’ll be using the previously defined hello function. hello.__name__ \u003e\u003e\u003e 'wrapper' help(hello) \u003e\u003e\u003e Help on function wrapper in module __main__: wrapper(*args, **kwargs) Now what’s going on there. The decorator yell has made the function hello confused about its own identity. Instead of reporting its own name, it takes the identity of the inner function wrapper. This can be confusing while doing debugging. You can fix this by using builtin wraps decorator from the functools module. This will make sure that the original identity of the decorated function stays preserved. import functools def yell(func): @wraps(func) def wrapper(*args, **kwargs): ret = func(*args, **kwargs) ret = ret.upper() + \"!\" return ret return wrapper @yell def hello(name): \"Hello from the other side.\" return f\"Hello {name}\" hello(\"Galaxy\") \u003e\u003e\u003e 'HELLO GALAXY!' Introspecting the hello function decorated with modified decorator will give you the desired result. hello.__name__ \u003e\u003e\u003e 'hello' help(hello) \u003e\u003e\u003e Help on function hello in module __main__: hello(name) Hello from the other side. Decorators in the wild Before moving on to the next section let’s see a few real world examples of decorators. To define all the decorators, we’ll be using the following template that we’ve perfected so far. from functools import wraps def decorator(func): @wraps(func) def wrapper(*args, **kwargs): # Do something before ret = func(*args, **kwargs) # Do something after return ret return wrapper Timer Timer decorator will help you time your callables in a non-intrusive way. It can help you while debugging and profiling your functions. from functools import wraps from time import perf_counter def timer(func): \"\"\"This decorator prints out the execution time of a callable.\"\"\" @wraps(func) def wrapper(*args, **kwargs): start_time = perf_counter() ret = func(*args, **kwargs) end_time = perf_counter() run_time = end_time - start_time print(f\"Finished running {func.__name__} in {run_time:.4f} seconds.\") return ret return wrapper @timer def dothings(n_times): for _ in range(n_times): return sum((i**3 for i in range(100_000))) In the above way, we can introspect the time it requires for function dothings to complete its execution. dothings(100_000) \u003e\u003e\u003e Finished running dothings in 0.0353 seconds. 24999500002500000000 Exception logger Just like the timer decorator, we can define a logger decorator that will log the state of a callable. For this demonstration, I’ll be defining a exception logger that will show additional information like timestamp, argument names when an exception occurs inside of the decorated callable. from functools import wraps from datetime import datetime def logexc(func): @wraps(func) def wrapper(*args, **kwargs): # Stringify the arguments args_rep = [repr(arg) for arg in args] kwargs_rep = [f\"{k}={v!r}\" for k, v in kwargs.items()] sig = \", \".join(args_rep + kwargs_rep) # Try running the function try: return func(*args, **kwargs) except Exception as e: print( \"Time: \", datetime.now().strftime(\"%Y-%m-%d [%H:%M:%S]\"), ) print(\"Arguments: \", sig) print(\"Error:\\n\") raise return wrapper @logexc def divint(a, b): return a / b Let’s invoke ZeroDivisionError to see the logger in action. divint(1, 0) \u003e\u003e\u003e Time: 2020-05-12 [12:03:31] Arguments: 1, 0 Error: ------------------------------------------------------------ ZeroDivisionError Traceback (most recent call last) .... The decorator first prints a few info regarding the function and then raises the original error. Validation \u0026 runtime checks Python’s type system is strongly typed, but very dynamic. For all its benefits, this means some bugs can try to creep in, which more statically typed languages (like Java) would catch at compile time. Looking beyond even that, you may want to enforce more sophisticated, custom checks on data going in or out. Decorators can let you easily handle all of this, and apply it to many functions at once. Imagine this: you have a set of functions, each returning a dictionary, which (among other fields) includes a field called “summary.” The value of this summary must not be more than 30 characters long; if violated, that’s an error. Here is a decorator that raises a ValueError if that happens: from functools import wraps def validate_summary(func): @wraps(func) def wrapper(*args, **kwargs): ret = func(*args, **kwargs) if len(ret[\"summary\"]) \u003e 30: raise ValueError(\"Summary exceeds 30 character limit.\") return ret return wrapper @validate_summary def short_summary(): return {\"summary\": \"This is a short summary\"} @validate_summary def long_summary(): return {\"summary\": \"This is a long summary that exceeds character limit.\"} print(short_summary()) print(long_summary()) \u003e\u003e\u003e {'summary': 'This is a short summary'} ------------------------------------------------------------------- ValueError Traceback (most recent call last) in 19 20 print(short_summary()) ---\u003e 21 print(long_summary()) ... Retry Imagine a situation where your defined callable fails due to some I/O related issues and you’d like to retry that again. Decorator can help you to achieve that in a reusable manner. Let’s define a retry decorator that will rerun the decorated function multiple times if an HTTP error occurs. import requests from functools import wraps def retry(func): \"\"\"This will rerun the decorated callable 3 times if the callable encounters http 500/404 error.\"\"\" @wraps(func) def wrapper(*args, **kwargs): n_tries = 3 tries = 0 while True: resp = func(*args, **kwargs) if ( resp.status_code == 500 or resp.status_code == 404 and tries \u003c n_tries ): print(f\"retrying... ({tries})\") tries += 1 continue break return resp return wrapper @retry def getdata(url): resp = requests.get(url) return resp resp = getdata(\"https://httpbin.org/get/1\") resp.text \u003e\u003e\u003e retrying... (0) retrying... (1) retrying... (2) '\u003c!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\"\u003e\\n404 Not Found\u003c/ title\u003e\\n\u003ch1\u003eNot Found\u003c/h1\u003e\\n\u003cp\u003eThe requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.\n\\n' Applying multiple decorators You can apply multiple decorators to a function by stacking them on top of each other. Let’s define two simple decorators and use them both on a function. from functools import wraps def greet(func): \"\"\"Greet in English.\"\"\" @wraps(func) def wrapper(*args, **kwargs): ret = func(*args, **kwargs) return \"Hello \" + ret + \"!\" return wrapper def flare(func): \"\"\"Add flares to the string.\"\"\" @wraps(func) def wrapper(*args, **kwargs): ret = func(*args, **kwargs) return \"🎉 \" + ret + \" 🎉\" return wrapper @flare @greet def getname(name): return name getname(\"Nafi\") \u003e\u003e\u003e '🎉 Hello Nafi! 🎉' The decorators are called in a bottom up order. First, the decorator greet gets applied on the result of getname function and then the result of greet gets passed to the flare decorator. The decorator stack above can be written as flare(greet(getname(name))). Change the order of the decorators and see what happens! Decorators with arguments While defining the retry decorator in the previous section, you may have noticed that I’ve hard coded the number of times I’d like the function to retry if an error occurs. It’d be handy if you could inject the number of tries as a parameter into the decorator and make it work accordingly. This is not a trivial task and you’ll need three levels of nested functions to achieve that. Before doing that let’s cook up a trivial example of how you can define decorators with parameters. from functools import wraps def joinby(delimiter=\" \"): \"\"\"This decorator splits the string output of the decorated function by a single space and then joins them using a user specified delimiter.\"\"\" def outer_wrapper(func): @wraps(func) def inner_wrapper(*args, **kwargs): ret = func(*args, **kwargs) ret = ret.split(\" \") ret = delimiter.join(ret) return ret return inner_wrapper return outer_wrapper @joinby(delimiter=\",\") def hello(name): return f\"Hello {name}!\" @joinby(delimiter=\"\u003e\") def greet(name): return f\"Greetings {name}!\" @joinby() def goodbye(name): return f\"Goodbye {name}!\" print(hello(\"Nafi\")) print(greet(\"Redowan\")) print(goodbye(\"Delowar\")) \u003e\u003e\u003e Hello,Nafi! Greetings\u003eRedowan! Goodbye Delowar! The decorator joinby takes a single parameter called delimiter. It splits the string output of the decorated function by a single space and then joins them using the user defined delimiter specified in the delimiter argument. The three layer nested definition looks scary but we’ll get to that in a moment. Notice how you can use the decorator with different parameters. In the above example, I’ve defined three different functions to demonstrate the usage of joinby. It’s important to note that in case of a decorator that takes parameters, you’ll always need to pass something to it and even if you don’t want to pass any parameter (run with the default), you’ll still need to decorate your function with deco() instead of deco. Try changing the decorator on the goodbye function from joinby() to joinby and see what happens. Typically, a decorator creates and returns an inner wrapper function but here in the repeat decorator, there is an inner function within another inner function. This almost looks like a dream within a dream from the movie Inception. There are a few subtle things happening in the joinby() function: Defining outer_wrapper() as an inner function means that repeat() will refer to a function object outer_wrapper. The delimiter argument is seemingly not used in joinby() itself. But by passing delimiter a closure is created where the value of delimiter is stored until it will be used later by inner_wrapper() Decorators with \u0026 without arguments You saw earlier that a decorator specifically designed to take parameters can’t be used without parameters; you need to at least apply parenthesis after the decorator deco() to use it without explicitly providing the arguments. But what if you want to design one that can used both with and without arguments. Let’s redefine the joinby decorator so that you can use it with parameters or just like an ordinary parameter-less decorator that we’ve seen before. from functools import wraps def joinby(_func=None, *, delimiter=\" \"): \"\"\"This decorator splits the string output of a function by a single space and then joins that using a user specified delimiter.\"\"\" def outer_wrapper(func): @wraps(func) def inner_wrapper(*args, **kwargs): ret = func(*args, **kwargs) ret = ret.split(\" \") ret = delimiter.join(ret) return ret return inner_wrapper # This part enables you to use the decorator with/without arguments if _func is None: return outer_wrapper else: return outer_wrapper(_func) @joinby(delimiter=\",\") def hello(name): return f\"Hello {name}!\" @joinby def greet(name): return f\"Greetings {name}!\" print(hello(\"Nafi\")) print(greet(\"Redowan\")) \u003e\u003e\u003e Hello,Nafi! Greetings Redowan! Here, the _func argument acts as a marker, noting whether the decorator has been called with arguments or not: If joinby has been called without arguments, the decorated function will be passed in as _func. If it has been called with arguments, then _func will be None. The * in the argument list means that the remaining arguments can’t be called as positional arguments. This time you can use joinby with or without arguments and function hello and greet above demonstrate that. A generic pattern Personally, I find it cumbersome how you need three layers of nested functions to define a generalized decorator that can be used with or without arguments. David Beazley in his Python Cookbook1 shows an excellent way to define generalized decorators without writing three levels of nested functions. It uses the built in functools.partial function to achieve that. The following is a pattern you can use to define generalized decorators in a more elegant way: from functools import partial, wraps def decorator(func=None, foo=\"spam\"): if func is None: return partial(decorator, foo=foo) @wraps(func) def wrapper(*args, **kwargs): # Do something with `func` and `foo`, if you're so inclined pass return wrapper # Applying decorator without any parameter @decorator def f(*args, **kwargs): pass # Applying decorator with extra parameter @decorator(foo=\"buzz\") def f(*args, **kwargs): pass Let’s redefine our retry decorator using this pattern. from functools import partial, wraps def retry(func=None, n_tries=4): if func is None: return partial(retry, n_tries=n_tries) @wraps(func) def wrapper(*args, **kwargs): tries = 0 while True: ret = func(*args, **kwargs) if ( ret.status_code == 500 or ret.status_code == 404 and tries \u003c n_tries ): print(f\"retrying... ({tries})\") tries += 1 continue break return ret return wrapper @retry def getdata(url): resp = requests.get(url) return resp @retry(n_tries=2) def getdata_(url): resp = requests.get(url) return resp resp1 = getdata(\"https://httpbin.org/get/1\") print(\"-----------------------\") resp2 = getdata_(\"https://httpbin.org/get/1\") \u003e\u003e\u003e retrying... (0) retrying... (1) retrying... (2) retrying... (3) ----------------------- retrying... (0) retrying... (1) In this case, you do not have to write three level nested functions and the functools. partial takes care of that. Partials can be used to make new derived functions that have some input parameters pre-assigned.Roughly partial does the following: def partial(func, *part_args): def wrapper(*extra_args): args = list(part_args) args.extend(extra_args) return func(*args) return wrapper This eliminates the need to write multiple layers of nested factory function get a generalized decorator. Defining decorators with classes This time, I’ll be using a class to compose a decorator. Classes can be handy to avoid nested architecture while writing decorators. Also, it can be helpful to use a class while writing stateful decorators. You can follow the pattern below to compose decorators with classes. import functools class ClassDeco: def __init__(self, func): # Does the work of the 'functools.wraps' in methods. functools.update_wrapper(self, func) self.func = func def __call__(self, *args, **kwargs): # You can add some code before the function call ret = self.func(*args, **kwargs) # You can also add some code after the function call return ret Let’s use the above template to write a decorator named Emphasis that will add bold tags \u003cb\u003e\u003c/b\u003eto the string output of a function. import functools class Emphasis: def __init__(self, func): functools.update_wrapper(self, func) self.func = func def __call__(self, *args, **kwargs): ret = self.func(*args, **kwargs) return \"\u003cb\u003e\" + ret + \"\u003c/b\u003e\" @Emphasis def hello(name): return f\"Hello {name}\" print(hello(\"Nafi\")) print(hello(\"Redowan\")) \u003e\u003e\u003e \u003cb\u003eHello Nafi\u003c/b\u003e \u003cb\u003eHello Redowan\u003c/b\u003e The __init__() method stores a reference to the function num_calls and can do other necessary initialization. The __call__() method will be called instead of the decorated function. It does essentially the same thing as the wrapper() function in our earlier examples. Note that you need to use the functools.update_wrapper() function instead of @functools.wraps. Before moving on, let’s write a stateful decorator using classes. Stateful decorators can remember the state of their previous run. Here’s a stateful decorator called Tally that’ll keep track of the number of times decorated functions are called in a dictionary. The keys of the dictionary will hold the names of the functions and the corresponding values will hold the call count. import functools class Tally: def __init__(self, func): functools.update_wrapper(self, func) self.func = func self.tally = {} self.n_calls = 0 def __call__(self, *args, **kwargs): self.n_calls += 1 self.tally[self.func.__name__] = self.n_calls print(\"Callable Tally:\", self.tally) return self.func(*args, **kwargs) @Tally def hello(name): return f\"Hello {name}!\" print(hello(\"Redowan\")) print(hello(\"Nafi\")) \u003e\u003e\u003e Callable Tally: {'hello': 1} Hello Redowan! Callable Tally: {'hello': 2} Hello Nafi! A few more examples Caching return values Decorators can provide an elegant way of memoizing function return values. Imagine you have an expensive API and you’d like call that as few times as possible. The idea is to save and cache values returned by the API for particular arguments, so that if those arguments appear again, you can serve the results from the cache instead of calling the API again. This can dramatically improve your applications’ performance. Here I’ve simulated an expensive API call and provided caching with a decorator. import time def api(a): \"\"\"API takes an integer and returns the square value of it. To simulate a time consuming process, I've added some time delay to it. \"\"\" print(\"The API has been called...\") # This will delay 3 seconds time.sleep(3) return a * a api(3) \u003e\u003e\u003e The API has been called... 9 You’ll see that running this function takes roughly 3 seconds. To cache the result, we can use Python’s built in functools.lru_cache to save the result against an argument in a dictionary and serve that when it encounters the same argument again. The only drawback here is, all the arguments need to be hashable. import functools @functools.lru_cache(maxsize=32) def api(a): \"\"\"API takes an integer and returns the square value of it. To simulate a time consuming process, I've added some time delay to it. \"\"\" print(\"The API has been called...\") # This will delay 3 seconds time.sleep(3) return a * a api(3) \u003e\u003e\u003e 9 Least Recently Used (LRU) Cache organizes items in order of use, allowing you to quickly identify which item hasn’t been used for the longest amount of time. In the above case, the parameter max_size refers to the maximum numbers of responses to be saved up before it starts deleting the earliest ones. While you run the decorated function, you’ll see first time it’ll take roughly 3 seconds to return the result. But if you rerun the function again with the same parameter it’ll spit the result from the cache almost instantly. Unit Conversion The following decorator converts length from SI units to multiple other units without polluting your target function with conversion logics. from functools import wraps def convert(func=None, convert_to=None): \"\"\"This converts value from meter to others.\"\"\" if func is None: return partial(convert, convert_to=convert_to) @wraps(func) def wrapper(*args, **kwargs): print(f\"Conversion unit: {convert_to}\") ret = func(*args, **kwargs) # Adding conversion rules if convert_to is None: return ret elif convert_to == \"km\": return ret / 1000 elif convert_to == \"mile\": return ret * 0.000621371 elif convert_to == \"cm\": return ret * 100 elif convert_to == \"mm\": return ret * 1000 else: raise ValueError(\"Conversion unit is not supported.\") return wrapper Let’s use that on a function that returns the area of a rectangle. @convert(convert_to=\"mile\") def area(a, b): return a * b area(1, 2) \u003e\u003e\u003e Conversion unit: mile 0.001242742 Using the convert decorator on the area function shows how it prints out the transformation unit before returning the desired result. Experiment with other conversion units and see what happens. Function registration The following is an example of registering logger function in Flask framework. The decorator register_logger doesn’t make any change to the decorated logger function. Rather it takes the function and registers it in a list called logger_list every time it’s invoked. from flask import Flask, request app = Flask(__name__) logger_list = [] def register_logger(func): logger_list.append(func) return func def run_loggers(request): for logger in logger_list: logger(request) @register_logger def logger(request): print(request.method, request.path) @app.route(\"/\") def index(): run_loggers(request) return \"Hello World!\" if __name__ == \"__main__\": app.run(host=\"localhost\", port=\"5000\") If you run the server and hit the http://localhost:5000/ url, it’ll greet you with a Hello World! message. Also you’ll able to see the printed method and path of your HTTP request on the terminal. Moreover, if you inspect the logger_list, you’ll find the registered logger there. You’ll find a lot more real life usage of decorators in the Flask framework. Python Cookbook - David Beazley ↩︎ Primer on Python decorator - Real Python 2 ↩︎ Decorators in Python - Datacamp 3 ↩︎ 5 reasons you need to write python decorators 4 ↩︎ ","permalink":"http://rednafi.com/python/decorators/","publishDate":"2020-05-13","summary":"Updated on 2022-02-13: Change functools import style.\nWhen I first learned about Python decorators, using them felt like doing voodoo magic. Decorators can give you the ability to add new functionalities to any callable without actually touching or changing the code inside it. This can typically yield better encapsulation and help you write cleaner and more understandable code. However, decorator is considered as a fairly advanced topic in Python since understanding and writing it requires you to have command over multiple additional concepts like first class objects, higher order functions, closures etc. First, I’ll try to introduce these concepts as necessary and then unravel the core concept of decorator layer by layer. So let’s dive in.\n","tags":["Python"],"title":"Untangling Python decorators"},{"content":"Writing concurrent code in Python can be tricky. Before you even start, you have to worry about all these icky stuff like whether the task at hand is I/O or CPU bound or whether putting the extra effort to achieve concurrency is even going to give you the boost you need. Also, the presence of Global Interpreter Lock, GIL1 foists further limitations on writing truly concurrent code. But for the sake of sanity, you can oversimplify it like this without being blatantly incorrect: In Python, if the task at hand is I/O bound, you can use use standard library’s threading module or if the task is CPU bound then multiprocessing module can be your friend. These APIs give you a lot of control and flexibility but they come at the cost of having to write relatively low-level verbose code that adds extra layers of complexity on top of your core logic. Sometimes when the target task is complicated, it’s often impossible to avoid complexity while adding concurrency. However, a lot of simpler tasks can be made concurrent without adding too much verbosity. Python standard library also houses a module called the concurrent.futures. This module was added in Python 3.2 for providing the developers a high-level interface to launch asynchronous tasks. It’s a generalized abstraction layer on top of threading and multiprocessing modules for providing an interface to run tasks concurrently using pools of threads or processes. It’s the perfect tool when you just want to run a piece of eligible code concurrently and don’t need the added modularity that the threading and multiprocessing APIs expose. Anatomy of concurrent.futures From the official docs, The concurrent.futures module provides a high-level interface for asynchronously executing callables. What it means is you can run your subroutines asynchronously using either threads or processes through a common high-level interface. Basically, the module provides an abstract class called Executor. You can’t instantiate it directly, rather you need to use one of two subclasses that it provides to run your tasks. Executor (Abstract Base Class) │ ├── ThreadPoolExecutor │ │ │A concrete subclass of the Executor class to │ │manage I/O bound tasks with threading underneath │ ├── ProcessPoolExecutor │ │ │A concrete subclass of the Executor class to │ │manage CPU bound tasks with multiprocessing underneath Internally, these two classes interact with the pools and manage the workers. Futures are used for managing results computed by the workers. To use a pool of workers, an application creates an instance of the appropriate executor class and then submits them for it to run. When each task is started, a Future instance is returned. When the result of the task is needed, an application can use the Future object to block until the result is available. Various APIs are provided to make it convenient to wait for tasks to complete, so that the Future objects do not need to be managed directly. Executor objects Since both ThreadPoolExecutor and ProcessPoolExecutor have the same API interface, in both cases I’ll primarily talk about two methods that they provide. Their descriptions have been collected from the official docs verbatim. submit(fn, *args, **kwargs) Schedules the callable, fn, to be executed as fn(*args **kwargs) and returns a Future object representing the execution of the callable. with ThreadPoolExecutor(max_workers=1) as executor: future = executor.submit(pow, 323, 1235) print(future.result()) map(func, *iterables, timeout=None, chunksize=1) Similar to map(func, *iterables) except: the iterables are collected immediately rather than lazily; func is executed asynchronously and several calls to func may be made concurrently. The returned iterator raises a concurrent.futures.TimeoutError if __next__() is called and the result isn’t available after timeout seconds from the original call to Executor.map(). Timeout can be an int or a float. If timeout is not specified or None, there is no limit to the wait time. If a func call raises an exception, then that exception will be raised when its value is retrieved from the iterator. When using ProcessPoolExecutor, this method chops iterables into a number of chunks which it submits to the pool as separate tasks. The (approximate) size of these chunks can be specified by setting chunksize to a positive integer. For very long iterables, using a large value for chunksize can significantly improve performance compared to the default size of 1. With ThreadPoolExecutor, chunksize has no effect. Generic workflows for running tasks concurrently A lot of my scripts contains some variants of the following: for task in get_tasks(): perform(task) Here, get_tasks returns an iterable that contains the target tasks or arguments on which a particular task function needs to applied. Tasks are usually blocking callables and they run one after another, with only one task running at a time. The logic is simple to reason with because of its sequential execution flow. This is fine when the number of tasks is small or the execution time requirement and complexity of the individual tasks is low. However, this can quickly get out of hands when the number of tasks is huge or the individual tasks are time consuming. A general rule of thumb is using ThreadPoolExecutor when the tasks are primarily I/O bound, like—sending multiple http requests to many urls, saving a large number of files to disk etc. ProcessPoolExecutor should be used in tasks that are primarily CPU bound, like—running callables that are computation heavy, applying pre-process methods over a large number of images, manipulating many text files at once etc. Running tasks with executor.submit When you have a number of tasks, you can schedule them in one go and wait for them all to complete and then you can collect the results. import concurrent.futures with concurrent.futures.Executor() as executor: futures = {executor.submit(perform, task) for task in get_tasks()} for fut in concurrent.futures.as_completed(futures): print(f\"The outcome is {fut.result()}\") Here you start by creating an Executor, which manages all the tasks that are running—either in separate processes or threads. Using the with statement creates a context manager, which ensures any stray threads or processes get cleaned up via calling the executor.shutdown() method implicitly when you’re done. In real code, you’d would need to replace the Executor with ThreadPoolExecutor or a ProcessPoolExecutor depending on the nature of the callables. Then a set comprehension has been used here to start all the tasks. The executor.submit() method schedules each task. This creates a Future object, which represents the task to be done. Once all the tasks have been scheduled, the method concurrent.futures_as_completed() is called, which yields the futures as they’re done – that is, as each task completes. The fut.result() method gives you the return value of perform(task), or throws an exception in case of failure. The executor.submit() method schedules the tasks asynchronously and doesn’t hold any contexts regarding the original tasks. So if you want to map the results with the original tasks, you need to track those yourself. import concurrent.futures with concurrent.futures.Executor() as executor: futures = {executor.submit(perform, task): task for task in get_tasks()} for fut in concurrent.futures.as_completed(futures): original_task = futures[fut] print(f\"The result of {original_task} is {fut.result()}\") Notice the variable futures where the original tasks are mapped with their corresponding futures using a dictionary. Running tasks with executor.map Another way the results can be collected in the same order they’re scheduled is via using executor.map() method. import concurrent.futures with concurrent.futures.Executor() as executor: for arg, res in zip(get_tasks(), executor.map(perform, get_tasks())): print(f\"The result of {arg} is {res}\") Notice how the map function takes the entire iterable at once. It spits out the results immediately rather than lazily and in the same order they’re scheduled. If any unhandled exception occurs during the operation, it’ll also be raised immediately and the execution won’t go any further. In Python 3.5+, executor.map() receives an optional argument: chunksize. While using ProcessPoolExecutor, for very long iterables, using a large value for chunksize can significantly improve performance compared to the default size of 1. With ThreadPoolExecutor, chunksize has no effect. A few real world examples Before proceeding with the examples, let’s write a small decorator that’ll be helpful to measure and compare the execution time between concurrent and sequential code. import time from functools import wraps def timeit(method): @wraps(method) def wrapper(*args, **kwargs): start_time = time.time() result = method(*args, **kwargs) end_time = time.time() print(f\"{method.__name__} =\u003e {(end_time-start_time)*1000} ms\") return result return wrapper The decorator can be used like this: @timeit def func(n): return list(range(n)) This will print out the name of the method and how long it took to execute it. Download \u0026 save files from URLs with multi-threading First, let’s download some pdf files from a bunch of URLs and save them to the disk. This is presumably an I/O bound task and we’ll be using the ThreadPoolExecutor class to carry out the operation. But before that, let’s do this sequentially first. from pathlib import Path import urllib.request def download_one(url): \"\"\" Downloads the specified URL and saves it to disk \"\"\" req = urllib.request.urlopen(url) fullpath = Path(url) fname = fullpath.name ext = fullpath.suffix if not ext: raise RuntimeError(\"URL does not contain an extension\") with open(fname, \"wb\") as handle: while True: chunk = req.read(1024) if not chunk: break handle.write(chunk) msg = f\"Finished downloading {fname}\" return msg @timeit def download_all(urls): return [download_one(url) for url in urls] if __name__ == \"__main__\": urls = ( \"http://www.irs.gov/pub/irs-pdf/f1040.pdf\", \"http://www.irs.gov/pub/irs-pdf/f1040a.pdf\", \"http://www.irs.gov/pub/irs-pdf/f1040ez.pdf\", \"http://www.irs.gov/pub/irs-pdf/f1040es.pdf\", \"http://www.irs.gov/pub/irs-pdf/f1040sb.pdf\", ) results = download_all(urls) for result in results: print(result) \u003e\u003e\u003e download_all =\u003e 22850.6863117218 ms ... Finished downloading f1040.pdf ... Finished downloading f1040a.pdf ... Finished downloading f1040ez.pdf ... Finished downloading f1040es.pdf ... Finished downloading f1040sb.pdf In the above code snippet, I have primary defined two functions. The download_one function downloads a pdf file from a given URL and saves it to the disk. It checks whether the file in URL has an extension and in the absence of an extension, it raises RunTimeError. If an extension is found in the file name, it downloads the file chunk by chunk and saves to the disk. The second function download_all just iterates through a sequence of URLs and applies the download_one function on each of them. The sequential code takes about 22.8 seconds to run. Now let’s see how our threaded version of the same code performs. from pathlib import Path import urllib.request from concurrent.futures import ThreadPoolExecutor, as_completed def download_one(url): \"\"\" Downloads the specified URL and saves it to disk \"\"\" req = urllib.request.urlopen(url) fullpath = Path(url) fname = fullpath.name ext = fullpath.suffix if not ext: raise RuntimeError(\"URL does not contain an extension\") with open(fname, \"wb\") as handle: while True: chunk = req.read(1024) if not chunk: break handle.write(chunk) msg = f\"Finished downloading {fname}\" return msg @timeit def download_all(urls): \"\"\" Create a thread pool and download specified urls \"\"\" with ThreadPoolExecutor(max_workers=13) as executor: return executor.map(download_one, urls, timeout=60) if __name__ == \"__main__\": urls = ( \"http://www.irs.gov/pub/irs-pdf/f1040.pdf\", \"http://www.irs.gov/pub/irs-pdf/f1040a.pdf\", \"http://www.irs.gov/pub/irs-pdf/f1040ez.pdf\", \"http://www.irs.gov/pub/irs-pdf/f1040es.pdf\", \"http://www.irs.gov/pub/irs-pdf/f1040sb.pdf\", ) results = download_all(urls) for result in results: print(result) \u003e\u003e\u003e download_all =\u003e 5042.651653289795 ms ... Finished downloading f1040.pdf ... Finished downloading f1040a.pdf ... Finished downloading f1040ez.pdf ... Finished downloading f1040es.pdf ... Finished downloading f1040sb.pdf The concurrent version of the code takes only about 1/4 th the time of it’s sequential counterpart. Notice in this concurrent version, the download_one function is the same as before but in the download_all function, a ThreadPoolExecutor context manager wraps the executor.map() method. The download_one function is passed into the map along with the iterable containing the URLs. The timeout parameter determines how long a thread will spend before giving up on a single task in the pipeline. The max_workers means how many worker you want to deploy to spawn and manage the threads. A general rule of thumb is using 2 * multiprocessing.cpu_count() + 1. My machine has 6 physical cores with 12 threads. So 13 is the value I chose. Note: You can also try running the above functions with ProcessPoolExecutor via the same interface and notice that the threaded version performs slightly better than due to the nature of the task. There is one small problem with the example above. The executor.map() method returns a generator which allows to iterate through the results once ready. That means if any error occurs inside map, it’s not possible to handle that and resume the generator after the exception occurs. From PEP-2552: If an unhandled exception– including, but not limited to, StopIteration –is raised by, or passes through, a generator function, then the exception is passed on to the caller in the usual way, and subsequent attempts to resume the generator function raise StopIteration. In other words, an unhandled exception terminates a generator’s useful life. To get around that, you can use the executor.submit() method to create futures, accumulated the futures in a list, iterate through the futures and handle the exceptions manually. See the following example: from pathlib import Path import urllib.request from concurrent.futures import ThreadPoolExecutor def download_one(url): \"\"\" Downloads the specified URL and saves it to disk \"\"\" req = urllib.request.urlopen(url) fullpath = Path(url) fname = fullpath.name ext = fullpath.suffix if not ext: raise RuntimeError(\"URL does not contain an extension\") with open(fname, \"wb\") as handle: while True: chunk = req.read(1024) if not chunk: break handle.write(chunk) msg = f\"Finished downloading {fname}\" return msg @timeit def download_all(urls): \"\"\" Create a thread pool and download specified urls \"\"\" futures_list = [] results = [] with ThreadPoolExecutor(max_workers=13) as executor: for url in urls: futures = executor.submit(download_one, url) futures_list.append(futures) for future in futures_list: try: result = future.result(timeout=60) results.append(result) except Exception: results.append(None) return results if __name__ == \"__main__\": urls = ( \"http://www.irs.gov/pub/irs-pdf/f1040.pdf\", \"http://www.irs.gov/pub/irs-pdf/f1040a.pdf\", \"http://www.irs.gov/pub/irs-pdf/f1040ez.pdf\", \"http://www.irs.gov/pub/irs-pdf/f1040es.pdf\", \"http://www.irs.gov/pub/irs-pdf/f1040sb.pdf\", ) results = download_all(urls) for result in results: print(result) The above snippet should print out similar messages as before. Running multiple CPU bound subroutines with multi-processing The following example shows a CPU bound hashing function. The primary function will sequentially run a compute intensive hash algorithm multiple times. Then another function will again run the primary function multiple times. Let’s run the function sequentially first. import hashlib def hash_one(n): \"\"\"A somewhat CPU-intensive task.\"\"\" for i in range(1, n): hashlib.pbkdf2_hmac(\"sha256\", b\"password\", b\"salt\", i * 10000) return \"done\" @timeit def hash_all(n): \"\"\"Function that does hashing in serial.\"\"\" for i in range(n): hsh = hash_one(n) return \"done\" if __name__ == \"__main__\": hash_all(20) \u003e\u003e\u003e hash_all =\u003e 18317.330598831177 ms If you analyze the hash_one and hash_all functions, you can see that together, they are actually running two compute intensive nested for loops. The above code takes roughly 18 seconds to run in sequential mode. Now let’s run it parallelly using ProcessPoolExecutor. import hashlib from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor def hash_one(n): \"\"\"A somewhat CPU-intensive task.\"\"\" for i in range(1, n): hashlib.pbkdf2_hmac(\"sha256\", b\"password\", b\"salt\", i * 10000) return \"done\" @timeit def hash_all(n): \"\"\"Function that does hashing in serial.\"\"\" with ProcessPoolExecutor(max_workers=10) as executor: for arg, res in zip( range(n), executor.map(hash_one, range(n), chunksize=2) ): pass return \"done\" if __name__ == \"__main__\": hash_all(20) \u003e\u003e\u003e hash_all =\u003e 1673.842430114746 ms If you look closely, even in the concurrent version, the for loop in hash_one function is running sequentially. However, the other for loop in the hash_all function is being executed through multiple processes. Here, I have used 10 workers and a chunksize of 2. The number of workers and chunksize were adjusted to achieve maximum performance. As you can see the concurrent version of the above CPU intensive operation is about 11 times faster than its sequential counterpart. Avoiding concurrency pitfalls Since the concurrent.futures provides such a simple API, you might be tempted to apply concurrency to every simple tasks at hand. However, that’s not a good idea. First, the simplicity has its fair share of constraints. In this way, you can apply concurrency only to the simplest of the tasks, usually mapping a function to an iterable or running a few subroutines simultaneously. If your task at hand requires queuing, spawning multiple threads from multiple processes then you will still need to resort to the lower level threading and multiprocessing modules. Another pitfall of using concurrency is deadlock situations that might occur while using ThreadPoolExecutor. When a callable associated with a Future waits on the results of another Future, they might never release their control of the threads and cause deadlock. Let’s see a slightly modified example from the official docs. import time from concurrent.futures import ThreadPoolExecutor def wait_on_b(): time.sleep(5) print(b.result()) # b will never complete because it is waiting on a. return 5 def wait_on_a(): time.sleep(5) print(a.result()) # a will never complete because it is waiting on b. return 6 with ThreadPoolExecutor(max_workers=2) as executor: # here, the future from a depends on the future from b # and vice versa # so this is never going to be completed a = executor.submit(wait_on_b) b = executor.submit(wait_on_a) print(\"Result from wait_on_b\", a.result()) print(\"Result from wait_on_a\", b.result()) In the above example, function wait_on_b depends on the result (result of the Future object) of function wait_on_a and at the same time the later function’s result depends on that of the former function. So the code block in the context manager will never execute due to having inter dependencies. This creates the deadlock situation. Let’s explain another deadlock situation from the official docs. from concurrent.futures import ThreadPoolExecutor def wait_on_future(): f = executor.submit(pow, 5, 2) # This will never complete because there is only one worker thread and # it is executing this function. print(f.result()) with ThreadPoolExecutor(max_workers=1) as executor: future = executor.submit(wait_on_future) print(future.result()) The above situation usually happens when a subroutine produces nested Future object and runs on a single thread. In the function wait_on_future, the executor.submit(pow, 5, 2) creates another Future object. Since I’m running the entire thing using a single thread, the internal future object is blocking the thread and the external executor.submit() method inside the context manager can not use any threads. This situation can be avoided using multiple threads but in general, this is a bad design itself. Then there’re situations when you might be getting lower performance with concurrent code than its sequential counterpart. This could happen for multiple reasons: Threads were used to perform CPU bound tasks. Multiprocessing were used to perform I/O bound tasks. The tasks were too trivial to justify using either threads or multiple processes. Spawning and squashing multiple threads or processes bring extra overheads. Usually threads are much faster than processes to spawn and squash. However, using the wrong type of concurrency can actually slow down your code rather than making it any performant. Below is a trivial example where both ThreadPoolExecutor and ProcessPoolExecutor perform worse than their sequential counterpart. import math PRIMES = [num for num in range(19000, 20000)] def is_prime(n): if n \u003c 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True @timeit def main(): for number in PRIMES: print(f\"{number} is prime: {is_prime(number)}\") if __name__ == \"__main__\": main() \u003e\u003e\u003e 19088 is prime: False ... 19089 is prime: False ... 19090 is prime: False ... ... ... main =\u003e 67.65174865722656 ms The above examples verifies whether a number in a list is prime or not. We ran the function on 1000 numbers to determine if they’re prime or not. The sequential version took roughly 67ms to do that. However, look below where the threaded version of the same code takes more than double the time (140ms) to so the same task. from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor import math num_list = [num for num in range(19000, 20000)] def is_prime(n): if n \u003c 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True @timeit def main(): with ThreadPoolExecutor(max_workers=13) as executor: for number, prime in zip(PRIMES, executor.map(is_prime, num_list)): print(f\"{number} is prime: {prime}\") if __name__ == \"__main__\": main() \u003e\u003e\u003e 19088 is prime: False ... 19089 is prime: False ... 19090 is prime: False ... ... ... main =\u003e 140.17250061035156 ms The multiprocessing version of the same code is even slower. The tasks doesn’t justify opening so many processes. from concurrent.futures import ProcessPoolExecutor import math num_list = [num for num in range(19000, 20000)] def is_prime(n): if n \u003c 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True @timeit def main(): with ProcessPoolExecutor(max_workers=13) as executor: for number, prime in zip(PRIMES, executor.map(is_prime, num_list)): print(f\"{number} is prime: {prime}\") if __name__ == \"__main__\": main() \u003e\u003e\u003e 19088 is prime: False ... 19089 is prime: False ... 19090 is prime: False ... ... ... main =\u003e 311.3126754760742 ms Although intuitively, it may seem like the task of checking prime numbers should be a CPU bound operation, it’s also important to determine if the task itself is computationally heavy enough to justify spawning multiple threads or processes. Otherwise, you might end up with complicated code that performs worse than the naive solution. References [concurrent.futures - the official documentation] [Easy concurrency in Python] [Adventures in Python with concurrent.futures] GIL ↩︎ PEP-255 ↩︎ concurrent.futures - the official documentation 3 ↩︎ easy concurrency in python 4 ↩︎ Adventures in python with concurrent.futures 5 ↩︎ ","permalink":"http://rednafi.com/python/concurrent_futures/","publishDate":"2020-04-21","summary":"Writing concurrent code in Python can be tricky. Before you even start, you have to worry about all these icky stuff like whether the task at hand is I/O or CPU bound or whether putting the extra effort to achieve concurrency is even going to give you the boost you need. Also, the presence of Global Interpreter Lock, GIL1 foists further limitations on writing truly concurrent code. But for the sake of sanity, you can oversimplify it like this without being blatantly incorrect:\n","tags":["Python"],"title":"Effortless concurrency with Python's concurrent.futures"},{"content":"When I first encountered Python’s pathlib module for path manipulation, I brushed it aside assuming it to be just an OOP way of doing what os.path already does quite well. The official doc also dubs it as the Object-oriented filesystem paths. However, back in 2019 when ticket1 confirmed that Django was replacing os.path with pathlib, I got curious. The os.path module has always been the de facto standard for working with paths in Python. But the API can feel massive as it performs a plethora of other loosely coupled system related jobs. I’ve to look things up constantly even to perform some of the most basic tasks like joining multiple paths, listing all the files in a folder having a particular extension, opening multiple files in a directory etc. The pathlib module can do nearly everything that os.path offers and comes with some additional cherries on top. Problem with Python’s path handling Traditionally, Python has represented file paths as regular text strings. So far, using paths as strings with os.path module has been adequate although a bit cumbersome. However, paths are not actually strings and this has necessitated the usage of multiple modules to provide disparate functionalities that are scattered all around the standard library, including libraries like os, glob, and shutil. The following code uses three modules just to copy multiple python files from current directory to another directory called src: from glob import glob import os import shutil for fname in glob(\"*.py\"): new_path = os.path.join(\"src\", fname) shutil.copy(fname, new_path) The above pattern can get complicated fairly quickly and you have to know or look for specific modules and methods in a large search space to perform your path manipulations. Let’s have a look at a few more examples of performing the same tasks using os.path and pathlib modules. Joining \u0026 creating new paths Say you want to achieve the following goals: There is a file named file.txt in your current directory and you want to create the path for another file named file_another.txt in the same directory. Then you want to save the absolute path of file_another.txt in a new variable. Let’s see how you’d usually do this via the os module. from os.path import abspath, dirname, join file_path = abspath(\"./file.txt\") base_dir = dirname(file_path) file_another_path = join(base_dir, \"file_another.txt\") The variables file_path, base_dir, file_another_path look like this on my machine: print(\"file_path:\", file_path) print(\"base_dir:\", base_dir) print(\"file_another_path:\", file_another_path) \u003e\u003e\u003e file_path: /home/rednafi/code/demo/file.txt \u003e\u003e\u003e base_dir: /home/rednafi/code/demo \u003e\u003e\u003e file_another_path: /home/rednafi/code/demo/file_another.txt You can use the usual string methods to transform the paths but generally, that’s not a good idea. So, instead of joining two paths with + like regular strings, you should use os.path.join() to join the components of a path. This is because different operating systems do not define paths in the same way. Windows uses \"\\\" while Mac and *nix based OSes use \"/\" as a separator. Joining with os.path.join() ensures correct path separator on the corresponding operating system. Pathlib module uses \"/\" operator overloading and make this a little less painful. from pathlib import Path file_path = Path(\"file.txt\").resolve() base_dir = file_path.parent file_another_path = base_dir / \"another_file.txt\" print(\"file_path:\", file_path) print(\"base_dir:\", base_dir) print(\"file_another_path:\", file_another_path) \u003e\u003e\u003e file_path: /home/rednafi/code/demo/file.txt \u003e\u003e\u003e base_dir: /home/rednafi/code/demo \u003e\u003e\u003e file_another_path: /home/rednafi/code/demo/file_another.txt The resolve method finds out the absolute path of the file. From there you can use the parent method to find out the base directory and add the another_file.txt accordingly. Making directories \u0026 renaming files Here’s a piece of code that: Tries to make a src/stuff/ directory when it already exists. Renames a file in the src directory called .config to .stuffconfig. import os import os.path os.makedirs(os.path.join(\"src\", \"stuff\"), exist_ok=True) os.rename(\"src/.config\", \"src/.stuffconfig\") Here is the same thing done using the pathlib module: from pathlib import Path Path(\"src/stuff\").mkdir(parents=True, exist_ok=True) Path(\"src/.config\").rename(\"src/.stuffconfig\") \u003e\u003e\u003e PosixPath('src/.stuffconfig') Notice the output where the renamed file path is printed. It’s not a simple string, rather a PosixPath object that indicates the type of host system (Linux in this case). You can almost always use stringified path values and the Path objects interchangeably. Listing specific types of files in a directory Let’s say you want to recursively visit nested directories and list .py files in a directory called source. The directory looks like this: src/ ├── stuff │ ├── __init__.py │ └── submodule.py ├── .stuffconfig ├── somefiles.tar.gz └── module.py Usually, glob module is used to resolve this kind of situation: from glob import glob top_level_py_files = glob(\"src/*.py\") all_py_files = glob(\"src/**/*.py\", recursive=True) print(top_level_py_files) print(all_py_files) \u003e\u003e\u003e ['src/module.py'] \u003e\u003e\u003e ['src/module.py', 'src/stuff/__init__.py', 'src/stuff/submodule.py'] The above approach works perfectly. However, if you don’t want to use another module just for a single job, pathlib has embedded glob and rglob methods. You can entirely ignore glob and achieve the same result in the following way: from pathlib import Path top_level_py_files = Path(\"src\").glob(\"*.py\") all_py_files = Path(\"src\").rglob(\"*.py\") print(list(top_level_py_files)) print(list(all_py_files)) This will also print the same as before: \u003e\u003e\u003e [PosixPath('src/module.py')] \u003e\u003e\u003e [PosixPath('src/module.py'), PosixPath('src/stuff/__init__.py'), PosixPath('src/stuff/submodule.py')] By default, both Path.glob and Path.rglob returns a generator object. Calling list on them gives you the desired result. Notice how rglob method can discover the desired files without you having to mention the directory structure with wildcards explicitly. Pretty neat, huh? Opening multiple files \u0026 reading their contents Now let’s open the .py files and read their contents that you recursively discovered in the previous example: from glob import glob contents = [] for fname in glob(\"src/**/*.py\", recursive=True): with open(fname, \"r\") as f: contents.append(f.read()) print(contents) \u003e\u003e\u003e ['from contextlib ...'] The pathlib implementation is almost identical as above: from pathlib import Path contents = [] for fname in Path(\"src\").rglob(\"*.py\"): with open(fname, \"r\") as f: contents.append(f.read()) print(contents) \u003e\u003e\u003e ['from contextlib import ...'] You can also cook up a more robust implementation with generator comprehension and context manager: from contextlib import ExitStack from pathlib import Path # ExitStack ensures all files are properly closed after o/p with ExitStack() as stack: streams = ( stack.enter_context(open(fname, \"r\")) for fname in Path(\"src\").rglob(\"*.py\") ) contents = [f.read() for f in streams] print(contents) \u003e\u003e\u003e ['from contextlib import ...'] Anatomy of the pathlib module Primarily, pathlib has two high-level components, pure path and concrete path. Pure paths are absolute Path objects that can be instantiated regardless of the host operating system. On the other hand, to instantiate a concrete path, you need to be on the specific type of host expected by the class. These two high level components are made out of six individual classes internally coupled by inheritance. They are: PurePath (Useful when you want to work with windows path on a Linux machine) PurePosixPath (Subclass of PurePath) PureWindowsPath (Subclass of PurePath) Path (Concrete path object, most of the time, you’ll be dealing with this one) PosixPath (Concrete posix path, subclass of Path) WindowsPath (Concrete windows path, subclass of Path) This UML diagram from the official docs does a better job at explaining the internal relationships between the component classes. Unless you are doing cross platform path manipulation, most of the time you’ll be working with the concrete Path object. So I’ll focus on the methods and properties of Path class only. Operators Instead of using os.path.join you can use / operator to create child paths. from pathlib import Path base_dir = Path(\"src\") child_dir = base_dir / \"stuff\" file_path = child_dir / \"__init__.py\" print(file_path) \u003e\u003e\u003e PosixPath('src/stuff/__init__.py') Attributes \u0026 methods The following tree shows an inexhaustive list of attributes and methods that are associated with Path object. I have cherry picked some of the attributes and methods that I use most of the time while doing path manipulation. Head over to the official docs for a more detailed list. We’ll linearly traverse through the tree and provide necessary examples to grasp their usage. Path │ ├── Attributes │ ├── parts │ ├── parent \u0026 parents │ ├── name │ ├── suffix \u0026 suffixes │ └── stem │ │ └── Methods ├── joinpath(*other) ├── cwd() ├── home() ├── exists() ├── expanduser() ├── glob() ├── rglob(pattern) ├── is_dir() ├── is_file() ├── is_absolute() ├── iterdir() ├── mkdir(mode=0o777, parents=False, exist_ok=False) ├── open(mode='r', buffering=-1, encoding=None, errors=None, newline=None) ├── rename(target) ├── replace(target) ├── resolve(strict=False) └── rmdir() Let’s dive into their usage one by one. For all the examples, We’ll be using the previously seen directory structure. src/ ├── stuff │ ├── __init__.py │ └── submodule.py ├── .stuffconfig ├── somefile.tar.gz └── module.py Path.parts Returns a tuple containing individual components of a path. from pathlib import Path file_path = Path(\"src/stuff/__init__.py\") file_path.parts \u003e\u003e\u003e ('src', 'stuff', '__init__.py') Path.parents \u0026 Path.parent Path.parents returns an immutable sequence containing the all logical ancestors of the path. While Path.parent returns the immediate predecessor of the path. file_path = Path(\"src/stuff/__init__.py\") for parent in file_path.parents: print(parent) \u003e\u003e\u003e src/stuff ... src ... . file_path.parent \u003e\u003e\u003e PosixPath('src/stuff') Path.name Returns the last component of a path as string. Usually used to extract file name from a path. from pathlib import Path file_path = Path(\"src/module.py\") file_path.name \u003e\u003e\u003e 'module.py' Path.suffixes \u0026 Path.suffix Path.suffixes returns a list of extensions of the final component. Path.suffix only returns the last extension. from pathlib import Path file_path = Path(\"src/stuff/somefile.tar.gz\") file_path.suffixes \u003e\u003e\u003e ['.tar', '.gz'] file_path.suffix \u003e\u003e\u003e'.gz' Path.stem Returns the final path component without the suffix. from pathlib import Path file_path = Path(\"src/stuff/somefile.tar.gz\") file_path.stem \u003e\u003e\u003e 'somefile.tar' Path.is_absolute Checks if a path is absolute or not. Return boolean value. from pathlib import Path file_path = Path(\"src/stuff/somefile.tar.gz\") file_path.is_absolute() \u003e\u003e\u003e False Path.joinpath(*other) This method is used to combine multiple components into a complete path. This can be used as an alternative to \"/\" operator for joining path components. from pathlib import Path file_path = Path(\"src\").joinpath(\"stuff\").joinpath(\"__init__.py\") file_path \u003e\u003e\u003e PosixPath('src/stuff/__init__.py') Path.cwd() Returns the current working directory. from pathlib import Path file_path = Path(\"src/stuff/somefile.tar.gz\") file_path.cwd() \u003e\u003e\u003e PosixPath('/home/rednafi/code/demo') Path.home() Returns home directory. from pathlib import Path Path.home() \u003e\u003e\u003e PosixPath('/home/rednafi') Path.exists() Checks if a path exists or not. Returns boolean value. from pathlib import Path file_path = Path(\"src/stuff/thisisabsent.py\") file_path.exists() \u003e\u003e\u003e False Path.expanduser() Returns a new path with expanded ~ symbol. from pathlib import Path file_path = Path(\"~/code/demo/src/stuff/somefile.tar.gz\") file_path.expanduser() \u003e\u003e\u003e PosixPath('/home/rednafi/code/demo/src/stuff/somefile.tar.gz') Path.glob() Globs and yields all file paths matching a specific pattern. Let’s discover all the files in src/stuff/ directory that have .py extension. from pathlib import Path dir_path = Path(\"src/stuff/\") file_paths = dir_path.glob(\"*.py\") print(list(file_paths)) \u003e\u003e\u003e [PosixPath('src/stuff/__init__.py'), PosixPath('src/stuff/submodule.py')] Path.rglob(pattern) This is like Path.glob method but matches the file pattern recursively. from pathlib import Path dir_path = Path(\"src\") file_paths = dir_path.rglob(\"*.py\") print(list(file_paths)) \u003e\u003e\u003e [PosixPath('src/module.py'), PosixPath('src/stuff/__init__.py'), PosixPath('src/stuff/submodule.py')] Path.is_dir() Checks if a path points to a directory or not. Returns boolean value. from pathlib import Path dir_path = Path(\"src/stuff/\") dir_path.is_dir() \u003e\u003e\u003e True Path.is_file() Checks if a path points to a file. Returns boolean value. from pathlib import Path dir_path = Path(\"src/stuff/\") dir_path.is_file() \u003e\u003e\u003e False Path.is_absolute() Checks if a path is absolute or relative. Returns boolean value. from pathlib import Path dir_path = Path(\"src/stuff/\") dir_path.is_absolute() \u003e\u003e\u003e False Path.iterdir() When the path points to a directory, this yields the content path objects. from pathlib import Path base_path = Path(\"src\") contents = [content for content in base_path.iterdir()] print(contents) \u003e\u003e\u003e [PosixPath('src/stuff'), PosixPath('src/module.py'), PosixPath('src/.stuffconfig')] Path.mkdir(mode=0o777, parents=False, exist_ok=False) Creates a new directory at this given path. Parameters: mode:(str) Posix permissions (mimicking the POSIX mkdir -p command) parents:(boolean) If parents is True, any missing parents of this path are created as needed. Otherwise, if the parent is absent, FileNotFoundError is raised. exist_ok: (boolean) If False, FileExistsError is raised if the target directory already exists. If True, FileExistsError is ignored. from pathlib import Path dir_path = Path(\"src/other/side\") dir_path.mkdir(parents=True) Path.open(mode=‘r’, buffering=-1, encoding=None, errors=None, newline=None) This is same as the built in open function. from pathlib import Path with Path(\"src/module.py\") as f: contents = open(f, \"r\") for line in contents: print(line) \u003e\u003e\u003e from contextlib import contextmanager ... from time import time ... ... Path.rename(target) Renames this file or directory to the given target and returns a new Path instance pointing to target. This will raise FileNotFoundError if the file is not found. from pathlib import Path file_path = Path(\"src/stuff/submodule.py\") file_path.rename(file_path.parent / \"anothermodule.py\") \u003e\u003e\u003e PosixPath('src/stuff/anothermodule.py') Path.replace(target) Replaces a file or directory to the given target. Returns the new path instance. from pathlib import Path file_path = Path(\"src/stuff/anothermodule.py\") file_path.replace(file_path.parent / \"Dockerfile\") \u003e\u003e\u003e PosixPath('src/stuff/Dockerfile') Path.resolve(strict=False) Make the path absolute, resolving any symlinks. A new path object is returned. If strict is True and the path doesn’t exist, FileNotFoundError will be raised. from pathlib import Path file_path = Path(\"src/./stuff/Dockerfile\") file_path.resolve() \u003e\u003e\u003e PosixPath('/home/rednafi/code/demo/src/stuff/Dockerfile') Path.rmdir() Removes a path pointing to a file or directory. The directory must be empty, otherwise, OSError is raised. from pathlib import Path file_path = Path(\"src/stuff\") file_path.rmdir() So, should you use it? Pathlib was introduced in python 3.4. However, if you are working with python 3.5 or earlier, in some special cases, you might have to convert pathlib.Path objects to regular strings. But since python 3.6, Path objects work almost everywhere you are using stringified paths. Also, the Path object nicely abstracts away the complexity that arises while working with paths in different operating systems. The ability to manipulate paths in an OO way and not having to rummage through the massive os or shutil module can make path manipulation a lot less painful. Replace os.path with pathlib ↩︎ pathlib - Object-oriented filesystem paths 2 ↩︎ Python 3’s pathlib Module: Taming the File System 3 ↩︎ Why you should be using pathlib 4 ↩︎ ","permalink":"http://rednafi.com/python/pathlib/","publishDate":"2020-04-13","summary":"When I first encountered Python’s pathlib module for path manipulation, I brushed it aside assuming it to be just an OOP way of doing what os.path already does quite well. The official doc also dubs it as the Object-oriented filesystem paths. However, back in 2019 when ticket1 confirmed that Django was replacing os.path with pathlib, I got curious.\nThe os.path module has always been the de facto standard for working with paths in Python. But the API can feel massive as it performs a plethora of other loosely coupled system related jobs. I’ve to look things up constantly even to perform some of the most basic tasks like joining multiple paths, listing all the files in a folder having a particular extension, opening multiple files in a directory etc. The pathlib module can do nearly everything that os.path offers and comes with some additional cherries on top.\n","tags":["Python"],"title":"No really, Python's pathlib is great"},{"content":"Pre-commit hooks1 can be a neat way to run automated ad-hoc tasks before submitting a new git commit. These tasks may include linting, trimming trailing whitespaces, running code formatter before code reviews etc. Let’s see how multiple Python linters and formatters can be applied automatically before each commit to impose strict conformity on your codebase.\nTo keep my sanity, I only use three linters in all of my python projects:\nIsort: Isort is a Python utility to sort imports alphabetically, and automatically separate them by sections and type. It parses specified files for global level import lines and puts them all at the top of the file grouped together by the type of import:\nFuture\nPython Standard Library\nThird Party\nCurrent Python Project\nExplicitly Local (. before import, as in: from . import x)\nCustom Separate Sections (Defined by forced_separate list in the configuration file)\nCustom Sections (Defined by sections list in configuration file)\nInside each section, the imports are sorted alphabetically. This also automatically removes duplicate python imports, and wraps long from imports to the specified line length (defaults to 79). Black: Black is the uncompromising Python code formatter. It uses consistent rules to format your python code and makes sure that they look the same regardless of the project you’re reading.\nFlake8: Flake8 is a wrapper around PyFlakes, pycodestyle, Ned Batchelder’s McCabe script2. The combination of these three linters makes sure that your code is compliant with PEP-83 and free of some obvious code smells.\nInstalling pre-commit Install using pip:\npip install pre-commit Install via curl:\ncurl https://pre-commit.com/install-local.py | python - Defining the pre-commit config file Pre-commit configuration is a .pre-commit-config.yaml file where you define your hooks (tasks) that you want to run before every commit. Once you have defined your hooks in the config file, they will run automatically every time you say git commit -m \"Commit message\". The following example shows how black and a few other linters can be added as hooks to the config:\n# .pre-commit-config.yaml repos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: check-yaml - id: end-of-file-fixer - id: trailing-whitespace - repo: https://github.com/psf/black rev: 19.3b0 hooks: - id: black Installing the git hook scripts Run:\npre-commit install This will set up the git hook scripts and should show the following output in your terminal:\npre-commit installed at .git/hooks/pre-commit Now you’ll be able to implicitly or explicitly run the hooks before each commit.\nRunning the hooks against all the files By default, the hooks will run every time you say:\ngit commit -m \"Commit message\" However, if you wish to run the hooks manually on every file, you can do so via:\npre-commit run --all-files Running the linters as pre-commit hooks To run the above mentioned linters as pre-commit hooks, you need to add their respective settings to the .pre-commit-config.yaml file. However, there’re a few minor issues that need to be taken care of.\nThe default line length of black formatter is 88 (you should embrace that) but flake8 caps the line at 79 characters. This raises conflict and can cause failures.\nFlake8 can be overly strict at times. You’ll want to ignore basic errors like unused imports, spacing issues etc. However, since your IDE / editor also points out these issues anyway, you should solve them manually. You will need to configure flake8 to ignore some of these minor errors.\nThe following one is an example of how you can define your .pre-commit-config.yaml and configure the individual hooks so that isort, black, flake8 linters can run without any conflicts.\n# .pre-commit-config.yaml # isort - repo: https://github.com/asottile/seed-isort-config rev: v1.9.3 hooks: - id: seed-isort-config - repo: https://github.com/pre-commit/mirrors-isort rev: v4.3.21 hooks: - id: isort # black - repo: https://github.com/ambv/black rev: stable hooks: - id: black args: # arguments to configure black - --line-length=88 - --include='\\.pyi?$' # these folders wont be formatted by black - --exclude=\"\"\"\\.git | \\.__pycache__| \\.hg| \\.mypy_cache| \\.tox| \\.venv| _build| buck-out| build| dist\"\"\" language_version: python3.6 # flake8 - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: flake8 args: # arguments to configure flake8 # making isort line length compatible with black - \"--max-line-length=88\" - \"--max-complexity=18\" - \"--select=B,C,E,F,W,T4,B9\" # these are errors that will be ignored by flake8 # check out their meaning here # https://flake8.pycqa.org/en/latest/user/error-codes.html - \"--ignore=E203,E266,E501,W503,F403,F401,E402\" You can add the above lines to your configuration and run:\npre-commit run --all-files This should apply the pre-commit hooks to your code base harmoniously. From now on, before each commit, the hooks will make sure that your code complies with the rules imposed by the linters.\npre-commit hooks ↩︎\nmccabe script ↩︎\nPEP-8 ↩︎\n","permalink":"http://rednafi.com/python/pre_commit/","publishDate":"2020-04-06","summary":"Pre-commit hooks1 can be a neat way to run automated ad-hoc tasks before submitting a new git commit. These tasks may include linting, trimming trailing whitespaces, running code formatter before code reviews etc. Let’s see how multiple Python linters and formatters can be applied automatically before each commit to impose strict conformity on your codebase.\nTo keep my sanity, I only use three linters in all of my python projects:\n","tags":["Python"],"title":"Running Python linters with pre-commit hooks"},{"content":"Updated on 2022-02-13: Change import style of functools.singledispatch. Recently, I was refactoring a portion of a Python function that somewhat looked like this: def process(data): if cond0 and cond1: # apply func01 on data that satisfies the cond0 \u0026 cond1 return func01(data) elif cond2 or cond3: # apply func23 on data that satisfies the cond2 \u0026 cond3 return func23(data) elif cond4 and cond5: # apply func45 on data that satisfies cond4 \u0026 cond5 return func45(data) def func01(data): ... def func23(data): ... def func45(data): ... This pattern gets tedious when the number of conditions and actionable functions start to grow. I was looking for a functional approach to avoid defining and calling three different functions that do very similar things. Situations like this is where parametric polymorphism1 comes into play. The idea is, you have to define a single function that’ll be dynamically overloaded with alternative implementations based on the type of the function arguments. Function overloading \u0026 generic functions Function overloading is a specific type of polymorphism where multiple functions can have the same name with different implementations. Calling an overloaded function will run a specific implementation of that function based on some prior conditions or appropriate context of the call. When function overloading happens based on its argument types, the resulting function is known as generic function. Let’s see how Python’s singledispatch decorator can help to design generic functions and refactor the icky code above. Singledispatch Python fairly recently added partial support for function overloading in Python 3.4. They did this by adding a neat little decorator to the functools module called singledispatch. In Python 3.8, there is another decorator for methods called singledispatchmethod. This decorator will transform your regular function into a single dispatch generic function. A generic function is composed of multiple functions implementing the same operation for different types. Which implementation should be used during a call is determined by the dispatch algorithm. When the implementation is chosen based on the type of a single argument, this is known as single dispatch. As PEP-4432 said, singledispatch only happens based on the first argument’s type. Let’s take a look at an example to see how this works! Example-1: Singledispatch with built-in argument type Let’s consider the following code: # procedural.py def process(num): if isinstance(num, int): return process_int(num) elif isinstance(num, float): return process_float(num) def process_int(num): # processing interger return f\"Integer {num} has been processed successfully!\" def process_float(num): # processing float return f\"Float {num} has been processed successfully!\" # use the function print(process(12.0)) print(process(1)) Running this code will return: \u003e\u003e\u003e Float 12.0 has been processed successfully! \u003e\u003e\u003e Integer 1 has been processed successfully! The above code snippet applies process_int or process_float functions on the incoming number based on its type. Now let’s see how the same thing can be achieved with singledispatch: # single_dispatch.py from functools import singledispatch @singledispatch def process(num=None): raise NotImplementedError(\"Implement process function.\") @process.register(int) def sub_process(num): # processing interger return f\"Integer {num} has been processed successfully!\" @process.register(float) def sub_process(num): # processing float return f\"Float {num} has been processed successfully!\" # use the function print(process(12.0)) print(process(1)) Running this will return the same result as before. \u003e\u003e\u003e Float 12.0 has been processed successfully! \u003e\u003e\u003e Integer 1 has been processed successfully! Example-2: Singledispatch with custom argument type Suppose, you want to dispatch your function based on custom argument type where the type will be deduced from data. Consider this example: def process(data: dict): if data[\"genus\"] == \"Felis\" and data[\"bucket\"] == \"cat\": return process_cat(data) elif data[\"genus\"] == \"Canis\" and data[\"bucket\"] == \"dog\": return process_dog(data) def process_cat(data: dict): # processing cat return \"Cat data has been processed successfully!\" def process_dog(data: dict): # processing dog return \"Dog data has been processed successfully!\" if __name__ == \"__main__\": cat_data = {\"genus\": \"Felis\", \"species\": \"catus\", \"bucket\": \"cat\"} dog_data = {\"genus\": \"Canis\", \"species\": \"familiaris\", \"bucket\": \"dog\"} # using process print(process(cat_data)) print(process(dog_data)) Running this snippet will print out: \u003e\u003e\u003e Cat data has been processed successfully! \u003e\u003e\u003e Dog data has been processed successfully! To refactor this with singledispatch, you can create two data types Cat and Dog. When you make Cat and Dog objects from the classes and pass them through the process function, singledispatch will take care of dispatching the appropriate implementation of that function. from dataclasses import dataclass from functools import singledispatch @dataclass class Cat: genus: str species: str @dataclass class Dog: genus: str species: str @singledispatch def process(obj=None): raise NotImplementedError(\"Implement process for bucket\") @process.register(Cat) def sub_process(obj): # processing cat return \"Cat data has been processed successfully!\" @process.register(Dog) def sub_process(obj): # processing dog return \"Dog data has been processed successfully!\" if __name__ == \"__main__\": cat_obj = Cat(genus=\"Felis\", species=\"catus\") dog_obj = Dog(genus=\"Canis\", species=\"familiaris\") print(process(cat_obj)) print(process(dog_obj)) Running this will print out the same output as before: \u003e\u003e\u003e Cat data has been processed successfully! \u003e\u003e\u003e Dog data has been processed successfully! References [Transform a function into a single dispatch generic function] [Function overloading] [Parametric polymorphism] Parametric polymorphism ↩︎ PEP-443 ↩︎ Transform a function into a single dispatch generic function 3 ↩︎ Function overloading 4 ↩︎ ","permalink":"http://rednafi.com/python/singledispatch/","publishDate":"2020-04-05","summary":"Updated on 2022-02-13: Change import style of functools.singledispatch.\nRecently, I was refactoring a portion of a Python function that somewhat looked like this:\ndef process(data): if cond0 and cond1: # apply func01 on data that satisfies the cond0 \u0026 cond1 return func01(data) elif cond2 or cond3: # apply func23 on data that satisfies the cond2 \u0026 cond3 return func23(data) elif cond4 and cond5: # apply func45 on data that satisfies cond4 \u0026 cond5 return func45(data) def func01(data): ... def func23(data): ... def func45(data): ... This pattern gets tedious when the number of conditions and actionable functions start to grow. I was looking for a functional approach to avoid defining and calling three different functions that do very similar things. Situations like this is where parametric polymorphism1 comes into play. The idea is, you have to define a single function that’ll be dynamically overloaded with alternative implementations based on the type of the function arguments.\n","tags":["Python"],"title":"Generic functions with Python's singledispatch"},{"content":"Python’s context managers are great for resource management and stopping the propagation of leaked abstractions. You’ve probably used it while opening a file or a database connection. Usually it starts with a with statement like this: with open(\"file.txt\", \"wt\") as f: f.write(\"contents go here\") In the above case, file.txt gets automatically closed when the execution flow goes out of the scope. This is equivalent to writing: try: f = open(\"file.txt\", \"wt\") text = f.write(\"contents go here\") finally: f.close() Writing custom context managers To write a custom context manager, you need to create a class that includes the __enter__ and __exit__ methods. Let’s recreate a custom context manager that will execute the same workflow as above. class CustomFileOpen: \"\"\"Custom context manager for opening files.\"\"\" def __init__(self, filename, mode): self.filename = filename self.mode = mode def __enter__(self): self.f = open(self.filename, self.mode) return self.f def __exit__(self, *args): self.f.close() You can use the above class just like a regular context manager. with CustomFileOpen(\"file.txt\", \"wt\") as f: f.write(\"contents go here\") From generators to context managers Creating context managers by writing a class with __enter__ and __exit__ methods, is not difficult. However, you can achieve better brevity by defining them using contextlib.contextmanager decorator. This decorator converts a generator function into a context manager. The blueprint for creating context manager decorators goes something like this: @contextmanager def some_generator(): try: yield finally: When you use the context manager with the with statement: with some_generator() as : # ...body It roughly translates to: try: = finally: The setup code goes before the try..finally block. Notice the point where the generator yields. This is where the code block nested in the with statement gets executed. After the completion of the code block, the generator is then resumed. If an unhandled exception occurs in the block, it’s re-raised inside the generator at the point where the yield occurred and then the finally block is executed. If no unhandled exception occurs, the code gracefully proceeds to the finally block where you run your cleanup code. Let’s implement the same CustomFileOpen context manager with contextmanager decorator. from contextlib import contextmanager @contextmanager def CustomFileOpen(filename, method): \"\"\"Custom context manager for opening a file.\"\"\" f = open(filename, method) try: yield f finally: f.close() Now use it just like before: with CustomFileOpen(\"file.txt\", \"wt\") as f: f.write(\"contents go here\") Writing context managers as decorators You can use context managers as decorators also. To do so, while defining the class, you have to inherit from contextlib.ContextDecorator class. Let’s make a RunTime decorator that’ll be applied on a file-opening function. The decorator will: Print a user provided description of the function. Print the time it takes to run the function. from contextlib import ContextDecorator from time import time class RunTime(ContextDecorator): \"\"\"Timing decorator.\"\"\" def __init__(self, description): self.description = description def __enter__(self): print(self.description) self.start_time = time() def __exit__(self, *args): self.end_time = time() run_time = self.end_time - self.start_time print(f\"The function took {run_time} seconds to run.\") You can use the decorator like this: @RunTime(\"This function opens a file\") def custom_file_write(filename, mode, content): with open(filename, mode) as f: f.write(content) Using the function like this should return: print(custom_file_write(\"file.txt\", \"wt\", \"jello\")) This function opens a file The function took 0.0005390644073486328 seconds to run. None You can also create the same decorator via contextlib.contextmanager decorator. from contextlib import contextmanager @contextmanager def runtime(description): print(description) start_time = time() try: yield finally: end_time = time() run_time = end_time - start_time print(f\"The function took {run_time} seconds to run.\") Nesting contexts You can nest multiple context managers to manage resources simultaneously. Consider the following dummy manager: from contextlib import contextmanager @contextmanager def get_state(name): print(\"entering:\", name) yield name print(\"exiting :\", name) # multiple get_state can be nested like this with get_state(\"A\") as A, get_state(\"B\") as B, get_state(\"C\") as C: print(\"inside with statement:\", A, B, C) entering: A entering: B entering: C inside with statement: A B C exiting : C exiting : B exiting : A Notice the order they’re closed. Context managers are treated as a stack, and should be exited in reverse order in which they’re entered. If an exception occurs, this order matters, as any context manager could suppress the exception, at which point the remaining managers will not even get notified of this. The __exit__ method is also permitted to raise a different exception, and other context managers then should be able to handle that new exception. Combining multiple context managers You can combine multiple context managers too. Let’s consider these two managers. from contextlib import contextmanager @contextmanager def a(name): print(\"entering a:\", name) yield name print(\"exiting a:\", name) @contextmanager def b(name): print(\"entering b:\", name) yield name print(\"exiting b:\", name) Now combine these two using the decorator syntax. The following function takes the above define managers a and b and returns a combined context manager ab. @contextmanager def ab(a, b): with a(\"A\") as A, b(\"B\") as B: yield (A, B) This can be used as: with ab(a, b) as AB: print(\"Inside the composite context manager:\", AB) entering a: A entering b: B Inside the composite context manager: ('A', 'B') exiting b: B exiting a: A If you have variable numbers of context managers and you want to combine them gracefully, contextlib.ExitStack is here to help. Let’s rewrite context manager ab using ExitStack. This function takes the individual context managers and their arguments as tuples and returns the combined manager. from contextlib import contextmanager, ExitStack @contextmanager def ab(cms, args): with ExitStack() as stack: yield [stack.enter_context(cm(arg)) for cm, arg in zip(cms, args)] with ab((a, b), (\"A\", \"B\")) as AB: print(\"Inside the composite context manager:\", AB) entering a: A entering b: B Inside the composite context manager: ['A', 'B'] exiting b: B exiting a: A ExitStack can be also used in cases where you want to manage multiple resources gracefully. For example, suppose, you need to create a list from the contents of multiple files in a directory. Let’s see, how you can do so while avoiding accidental memory leakage with robust resource management. from contextlib import ExitStack from pathlib import Path # ExitStack ensures all files are properly closed after o/p with ExitStack() as stack: streams = ( stack.enter_context(open(fname, \"r\")) for fname in Path(\"src\").rglob(\"*.py\") ) contents = [f.read() for f in streams] Using context managers to create SQLAlchemy session If you are familiar with SQLALchemy, Python’s SQL toolkit and Object Relational Mapper, then you probably know the usage of Session to run a query. A Session basically turns any query into a transaction and make it atomic. Context managers can help you write a transaction session in a very elegant way. A basic querying workflow in SQLAlchemy may look like this: from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from contextlib import contextmanager # an Engine, which the Session will use for connection resources some_engine = create_engine(\"sqlite://\") # create a configured \"Session\" class Session = sessionmaker(bind=some_engine) @contextmanager def session_scope(): \"\"\"Provide a transactional scope around a series of operations.\"\"\" session = Session() try: yield session session.commit() except: session.rollback() raise finally: session.close() The excerpt above creates an in memory SQLite connection and a session_scope function with context manager. The session_scope function takes care of committing and rolling back in case of exception automatically. The session_scope function can be used to run queries in the following way: with session_scope() as session: myobject = MyObject(\"foo\", \"bar\") session.add(myobject) Abstract away exception handling monstrosity with context managers This is my absolute favorite use case of context managers. Suppose you want to write a function but want the exception handling logic out of the way. Exception handling logics with sophisticated logging can often obfuscate the core logic of your function. You can write a decorator type context manager that will handle the exceptions for you and decouple these additional code from your main logic. Let’s write a decorator that will handle ZeroDivisionError and TypeError simultaneously. from contextlib import contextmanager @contextmanager def errhandler(): try: yield except ZeroDivisionError: print(\"This is a custom ZeroDivisionError message.\") raise except TypeError: print(\"This is a custom TypeError message.\") raise Now use this in a function where these exceptions occur. @errhandler() def div(a, b): return a // b div(\"b\", 0) This is a custom TypeError message. --------------------------------------------------------------------------- TypeError Traceback (most recent call last) in ----\u003e 1 div('b',0) /usr/lib/python3.8/contextlib.py in inner(*args, **kwds) 73 def inner(*args, **kwds): 74 with self._recreate_cm(): ---\u003e 75 return func(*args, **kwds) 76 return inner 77 in div(a, b) 1 @errhandler() 2 def div(a, b): ----\u003e 3 return a // b TypeError: unsupported operand type(s) for //: 'str' and 'int' You can see that the errhandler decorator is doing the heavylifting for you. Pretty neat, huh? The following one is a more sophisticated example of using context manager to decouple your error handling monstrosity from the main logic. It also hides the elaborate logging logic from the main method. import logging from contextlib import contextmanager import traceback import sys logging.getLogger(__name__) logging.basicConfig( level=logging.INFO, format=\"\\n(asctime)s [%(levelname)s] %(message)s\", handlers=[logging.FileHandler(\"./debug.log\"), logging.StreamHandler()], ) class Calculation: \"\"\"Dummy class for demonstrating exception decoupling with contextmanager.\"\"\" def __init__(self, a, b): self.a = a self.b = b @contextmanager def errorhandler(self): try: yield except ZeroDivisionError: print( f\"Custom handling of Zero Division Error! Printing \" \"only 2 levels of traceback..\" ) logging.exception(\"ZeroDivisionError\") def main_func(self): \"\"\"Function that we want to save from nasty error handling logic.\"\"\" with self.errorhandler(): return self.a / self.b obj = Calculation(2, 0) print(obj.main_func()) This will return (asctime)s [ERROR] ZeroDivisionError Traceback (most recent call last): File \"\", line 25, in errorhandler yield File \"\", line 37, in main_func return self.a / self.b ZeroDivisionError: division by zero Custom handling of Zero Division Error! Printing only 2 levels of traceback.. None Persistent parameters across HTTP requests with context managers Another great use case for context managers is making parameters persistent across multiple HTTP requests. Python’s requests library has a Session object that will let you easily achieve this. So, if you’re making several requests to the same host, the underlying TCP connection will be reused, which can result in a significant performance increase. The following example is taken directly from the official docs of the requests1 library. Let’s persist some cookies across requests. with requests.Session() as session: session.get(\"http://httpbin.org/cookies/set/sessioncookie/123456789\") response = session.get(\"http://httpbin.org/cookies\") print(response.text) This should show: { \"cookies\": { \"sessioncookie\": \"123456789\" } } Remarks To avoid redundencies, I have purposefully excluded examples of nested with statements and now deprecated contextlib.nested function to create nested context managers. Session objects in requests ↩︎ Python contextlib documentation 2 ↩︎ Python with context manager - Jeff Knupp 3 ↩︎ SQLAlchemy session creation 4 ↩︎ Scipy lectures: context managers 5 ↩︎ Merging context managers 6 ↩︎ ","permalink":"http://rednafi.com/python/contextmanager/","publishDate":"2020-03-26","summary":"Python’s context managers are great for resource management and stopping the propagation of leaked abstractions. You’ve probably used it while opening a file or a database connection. Usually it starts with a with statement like this:\nwith open(\"file.txt\", \"wt\") as f: f.write(\"contents go here\") In the above case, file.txt gets automatically closed when the execution flow goes out of the scope. This is equivalent to writing:\ntry: f = open(\"file.txt\", \"wt\") text = f.write(\"contents go here\") finally: f.close() Writing custom context managers To write a custom context manager, you need to create a class that includes the __enter__ and __exit__ methods. Let’s recreate a custom context manager that will execute the same workflow as above.\n","tags":["Python"],"title":"The curious case of Python's context manager"},{"content":"Recently, my work needed me to create lots of custom data types and draw comparison among them. So, my code was littered with many classes that somewhat looked like this: class CartesianPoint: def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): return f\"CartesianPoint(x = {self.x}, y = {self.y}, z = {self.z})\" print(CartesianPoint(1, 2, 3)) \u003e\u003e\u003e CartesianPoint(x = 1, y = 2, z = 3) This class only creates a CartesianPoint type and shows a pretty output of the instances created from it. However, it already has two methods inside, __init__ and __repr__ that do not do much. Dataclasses Let’s see how data classes can help to improve this situation. Data classes were introduced to python in version 3.7. Basically they can be regarded as code generators that reduce the amount of boilerplate you need to write while generating generic classes. Rewriting the above class using dataclass will look like this: from dataclasses import dataclass @dataclass class CartesianPoint: x: float y: float z: float # using the class point = CartesianPoint(1, 2, 3) print(point) \u003e\u003e\u003e CartesianPoint(x=1, y=2, z=3) In the above code, the magic is done by the dataclass decorator. Data classes require you to use explicit type annotations1 and it automatically implements methods like __init__, __repr__, __eq__ etc beforehand. You can inspect the methods that dataclass auto defines via Python’s help. help(CartesianPoint) Help on class CartesianPoint in module __main__: class CartesianPoint(builtins.object) | CartesianPoint(x:float, y:float, z:float) | | Methods defined here: | | __eq__(self, other) | | __init__(self, x:float, y:float, z:float) -\u003e None | | __repr__(self) | | ---------------------------------------------------------------------- | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | ---------------------------------------------------------------------- | Data and other attributes defined here: | | __annotations__ = {'x': ","permalink":"http://rednafi.com/python/dataclasses/","publishDate":"2020-03-12","summary":"Recently, my work needed me to create lots of custom data types and draw comparison among them. So, my code was littered with many classes that somewhat looked like this:\nclass CartesianPoint: def __init__(self, x, y, z): self.x = x self.y = y self.z = z def __repr__(self): return f\"CartesianPoint(x = {self.x}, y = {self.y}, z = {self.z})\" print(CartesianPoint(1, 2, 3)) \u003e\u003e\u003e CartesianPoint(x = 1, y = 2, z = 3) This class only creates a CartesianPoint type and shows a pretty output of the instances created from it. However, it already has two methods inside, __init__ and __repr__ that do not do much.\n","tags":["Python"],"title":"Reduce boilerplate code with Python's dataclasses"},{"content":"March 29 dotGo 2014 - John Graham-Cumming - I came for the easy concurrency I stayed for the easy composition I had to watch this twice to fully appreciate it. John Graham-Cumming crams a 40-minute talk into 14 minutes. First, he shows how Go’s basic types and stdlib make writing a DNS lookup program quite trivial. Then he walks through how he generalized it with a simple interface. The code can be found here. February 23 GopherCon 2019: Socket to me: Where do sockets live in Go? - Gabbi Fisher Gabbi Fisher shows how to write basic TCP and UDP socket servers in Go. It’s pretty easy since the net package in the standard library offers some handy abstractions. Plus, making the servers concurrent is quite simple because you can easily spin up a new goroutine to handle each connection. I’ve written socket servers in Python before, and making them concurrent wasn’t fun—even with the nice abstractions of the socketserver library. But holy smokes, why is it so hard in Go to customize socket behavior—like enabling SO_REUSEADDR to let multiple servers bind to the same port?. The last 10 minutes of the talk explores that. February 21 GopherCon 2021: Becoming the metaprogrammer: Real world code generation - Alan Shreve This is one of the best talks I’ve seen on Go code generation. Alan Shreve explains what code generators are and surveys several popular tools, such as stringer, go mock, ffjson, sqlc, protoc, and babel. He then shows how to write a simple code generator in Go. One thing I learned is how go test works underneath. Instead of running your test functions directly, go test reads your test source code, identifies the tests, and writes a new piece of source code—a test harness—that calls those test functions. This harness is then compiled, executed, and discarded. Similarly, when you run go test -cover, it generates a modified copy of your source code with extra statements inserted to track which parts of your code are executed. February 18 GopherCon 2022: Compatibility: How Go programs keep working - Russ Cox Russ Cox talks about Go’s compatibility promises and how to write code that makes it easier for the Go team to guarantee compatibility. February 15 The official ten-year retrospective of NewSQL databases - Andy Pavlo I still have a hard time explaining the difference between wide-column (column-family) databases like Cassandra/ScyllaDB and NewSQL databases like Spanner/CockroachDB. Column-family databases are generally designed as AP systems—they favor availability and partition tolerance over strong consistency. They let you define a schema upfront but store rows sparsely, so each row can have different columns. This design is especially useful in write-heavy environments where sacrificing strict consistency can improve performance. In contrast, NewSQL databases are typically CP systems that prioritize consistency and partition tolerance. They provide full SQL support and strong ACID guarantees, ensuring every transaction remains consistent across distributed nodes. However, under heavy write loads, if you’re willing to relax strong consistency, column-family databases can often outperform NewSQL systems. February 14 What goes around comes around… and around… - Andy Pavlo (Dijkstra Award 2024) Andy Pavlo’s enthusiasm for databases is infectious. I remember reading the paper mentioned here sometime last year, and this talk is a great complement to it. Andy explains why SQL databases work and why RDBMS should be your default choice when building applications. It’s also fun to see database vendors trying to move away from SQL, only to add support for it a few years later. While I default to SQLite/Postgres for my applications, I’ve also been fortunate enough to work in places where I’ve seen Postgres fail under massive write loads. As a result, I tend to steer clear of the overzealous Hacker News crowd that loves to bash NoSQL and NewSQL databases without considering why they were introduced in the first place. February 11 Curious channels - Dave Cheney Dave shows a neat way to notify multiple goroutines using close(ch). Plus, a nil channel can be used to wait for multiple channels to close. Februrary 10 Rob Pike - what we got right, what we got wrong | GopherConAU 2023 Go definitely got concurrency right. However, I still think that channels, wait groups, and mutexes are a bit too low-level, and Go missed out on providing some higher-level building blocks based on these. Also, it’s interesting to see Rob Pike admit that dependency management and generics are areas where Go could have done better. I still think generics came to the language a tad too late—but eh, better late than never. January 14 On Ousterhout’s dichotomy – Alex Kladov For performance, what matters is not so much the code that’s executed, but rather the layout of objects in memory. And the high-level dialect locks-in pointer-heavy GC object model! Even if you write your code in assembly, the performance ceiling will be determined by all those pointers GC needs. To actually get full “low-level” performance, you need to effectively “mirror” the data across the dialects across a quasi-FFI boundary. January 11 GopherCon UK 2019: Fun with pointers – Daniela Petruzalek Daniela explains how Go’s pointers are much simpler and safer than those in C/C++, while still providing users the power of indirection. One example surprised me: I was expecting the following snippet to raise a nil pointer dereference error. However, it doesn’t. In the StructGoesBoom method, m = nil only modifies the local copy of the pointer to MyLittleStruct. This means the original struct remains unaffected. Running the code confirms that the struct wasn’t modified at all: package main import ( \"fmt\" ) type MyLittleStruct struct { something int } func (m *MyLittleStruct) StructGoesBoom() { fmt.Println(\"Boom!\") // This only changes the local copy of pointer m m = nil } func main() { x := MyLittleStruct{1337} fmt.Printf(\"address: %p\\n\", \u0026x) // Desugar x.StructGoesBoom() (*MyLittleStruct).StructGoesBoom(\u0026x) fmt.Printf(\"%#v\\n\", x) } Output: address: 0xc0000140a0 Boom! main.MyLittleStruct{something:1337} The “active enum” pattern – Glyph Glyph shows a neat pattern for encoding behaviors in a Python enum. Instead of defining an enum and then handling behaviors in a function like this: from enum import Enum, auto class SomeNumber(Enum): one = auto() two = auto() three = auto() def behavior(number: SomeNumber) -\u003e int: match number: case SomeNumber.one: print(\"one!\") return 1 case SomeNumber.two: print(\"two!\") return 2 case SomeNumber.three: print(\"three!\") return 3 A better way to do it is: from dataclasses import dataclass from enum import Enum from typing import Callable @dataclass(frozen=True) class NumberValue: result: int effect: Callable[[], None] class SomeNumber(Enum): one = NumberValue(1, lambda: print(\"one!\")) two = NumberValue(2, lambda: print(\"two!\")) three = NumberValue(3, lambda: print(\"three!\")) def behavior(self) -\u003e int: self.value.effect() return self.value.result January 10 Be aware of the Makefile effect – William Woodruff An excellent name for the situation where a tool is so complex to use that people simply copy existing configurations and tweak them until they work for their specific cases. Tools and systems that enable this pattern often have less-than-ideal diagnostics or debugging support: the user has to run the tool repeatedly, often with long delays, to get back relatively small amounts of information. Think about CI/CD setups, where users diagnose their copy-pasted CI/CD by doing print-style debugging over the network with a layer of intermediating VM orchestration. Ridiculous! January 09 Ghostty by Mitchell Hashimoto How do you pronounce Ghostty? Ghos-tty Ghost-ty Something else Funny name, but an excellent terminal. It’s the first emulator to pull me away from the default macOS/Ubuntu terminal. Since I spend most of my time in VS Code’s integrated terminal, the rough edges of Ghostty don’t bother me much. In fact, I’m quite enjoying it. My config is minimal: theme = \"catppuccin-mocha\" font-family = \"JetBrains Mono\" font-size = 20 # Background configuration background-opacity = 0.95 background-blur-radius = 20 GopherCon 2015: Embrace the interface – Tomas Senart One great thing about Go is that you can grok a 10-year-old talk and still find it relevant. Here, Tomas demonstrates the decorator pattern in idiomatic Go, showing how to add logging, instrumentation, or retry functionality to an RPC function without polluting its core logic. The result is a Python-like decorator workflow without syntactic sugar that feels native to Go. January 08 Go developer survey 2024 h2 results Similar to previous years, the most common use cases for Go were API/RPC services (75%) and command line tools (62%). More experienced Go developers reported building a wider variety of applications in Go. This trend was consistent across every category of app or service. We did not find any notable differences in what respondents are building based on their organization size. Respondents from the random VS Code and GoLand samples did not display significant differences either. January 07 How I program with LLMs – David Crawshaw My LLM workflow is pretty similar to David’s. Instead of dumping my whole codebase into the model, I find it way more effective to use the chat UI and tackle a problem piece by piece. The responses are way better when the problem space is smaller and the model needs less out-of-band information to work—probably how a human would perform in a similar context. The reservoir sampler for the quartiles of floats example is a perfect demonstration of why LLMs are so useful in programming, even with their tendency to make things up or get stuck in a rut. January 06 GopherCon 2020: Go is boring… and that’s fantastic - Jonathan Bodner When you have a struct in Go that contains other structs, all the data is stored sequentially in memory. This is different from classes in all those other languages, where each field in a class is actually a pointer to some other memory, which means your memory access in those languages is effectively random (random memory access is slower than sequential access). January 05 Back to basics: why we chose long polling over WebSockets – Nadeesha Cabral Long polling has its fair share of issues, but in my experience, it’s been more reliable than WebSockets in most cases where I’ve needed to maintain long-running HTTP connections. Modern databases can handle a surprising number of connections these days, and adding proper indexes can mitigate the risk of overwhelming the database with too many open connections. Sure, Server-Sent Events (SSE) and WebSockets exist, but reliably detecting changes in the backend and delivering them to the right client still feels like an unsolved problem. Until that’s resolved, long polling remains a surprisingly simple and robust solution that just works. It’s already used as the fallback solution in most WebSocket setups. January 04 Kids can’t use computers… and this is why it should worry you – Marc Scott Damn, the anecdotes just keep getting better and better. A sixth-former brings me his laptop, explaining that it is running very slowly and keeps shutting down. The laptop is literally screaming, the processor fans running at full whack and the case is uncomfortably hot to touch. I run Task Manager to see that the CPU is running at 100% despite the only application open being uTorrent (which incidentally had about 200 torrent files actively seeding). I look at what processes are running and there are a lot of them, hogging the CPU and RAM. What’s more I can’t terminate a single one. ‘What anti-virus are you using?’ I ask, only to be told that he didn’t like using anti-virus because he’d heard it slowed his computer down. I hand back the laptop and tell him that it’s infected. He asks what he needs to do, and I suggest he reinstalls Windows. He looks at me blankly. He can’t use a computer. A kid puts her hand up in my lesson. ‘My computer won’t switch on,’ she says, with the air of desperation that implies she’s tried every conceivable way of making the thing work. I reach forward and switch on the monitor, and the screen flickers to life, displaying the Windows login screen. She can’t use a computer. A teacher brings me her school laptop. ‘Bloody thing won’t connect to the internet.’ she says angrily, as if it were my fault. ‘I had tonnes of work to do last night, but I couldn’t get on-line at all. My husband even tried and he couldn’t figure it out and he’s excellent with computers.’ I take the offending laptop from out of her hands, toggle the wireless switch that resides on the side, and hand it back to her. Neither her nor her husband can use computers. January 03 Be a property owner and not a renter on the internet – Den Delimarsky I’m loving this renaissance of personal blogs. Den’s take on POSSE (Publish Own Site, Syndicate Elsewhere) really resonates—it’s the only viable way to create content without getting trapped in the walled gardens of billion-dollar megacorps. About five years ago, Medium burned me, so I followed Simon Willison’s advice and started this blog. Easily one of the best decisions I’ve made in the past decade. I would recommend avoiding any places where there is content lock-in. You want to optimize for future portability. That is, if you can’t easily export your full content history (e.g., blog posts) and move them somewhere else, don’t use that service. If your content is locked into a service, and at some point that service decides that you are no longer a wanted customer, all that effort you put into making it available to your customers can vanish on a moment’s notice. Prefer sites that allow you to publish in open formats, such as Ghost. Minifeed Minifeed is a delightful blog aggregator I discovered today. It’s full of blogs I’d never heard of and uses a smart, vector-driven approach to group similar posts. Modern for Hacker News I spend an absurd amount of time browsing Hacker News and just stumbled upon this gem. On iOS, Hack is my go-to app for exploring the orange site, but on desktop, I’ve mostly stuck with the original UI. Enter Modern—a Chrome extension that transforms the brutalist interface into something sleek, like this: January 02 Stop designing languages. Write libraries instead – Patrick S. Li When I first started dabbling with Go, I couldn’t help but wonder why it didn’t have anything as slick as Python’s FastAPI. Generating API docs directly from code required a flexible type system and robust introspection capabilities, which Go just didn’t have back then. But now, with generics in the mix, libraries are finally emerging that can generate OpenAPI specs and docs from code without much fuss. The design of the programming language directly determines what sort of libraries you can write and how easy they are to use in the end. In the C language, the only major feature provided for enabling reuse is the ability to declare and call functions. So guess what? The majority of C libraries are basically large collections of functions. Ruby on Rails provides a concise way for expressing: do this when the button is clicked. The “do this” part is implemented in Ruby as a first-class function. How would it be implemented in languages like Java which don’t support them? Well, the behaviour of first-class functions can be mocked by defining a new event handler class with a single perform_action method and then passing an instance of this class to the button object. So guess what? Using a Java library typically entails declaring a humongous number of handler classes. The programming language directly shapes the design of its libraries. January 01 Databases in 2024: A year in review – Andy Pavlo From Redis and Elasticsearch’s licensing drama to Databricks vs. Snowflake’s billion-dollar sparring, to DuckDB’s integration into Postgres—this post offers a great overview of the major database events of 2024. I’ve been reading Andy’s database event reviews for the past three years and love his work. However, I’m not sure how to feel about the unwarranted shade thrown at Redis. While I don’t agree with Redis Ltd.’s licensing decisions, I still think it’s a marvelous engineering artifact that has shaped the query languages of many similar key-value storage systems. ","permalink":"http://rednafi.com/feed/2025/","publishDate":"1970-01-01","summary":"March 29 dotGo 2014 - John Graham-Cumming - I came for the easy concurrency I stayed for the easy composition I had to watch this twice to fully appreciate it. John Graham-Cumming crams a 40-minute talk into 14 minutes. First, he shows how Go’s basic types and stdlib make writing a DNS lookup program quite trivial. Then he walks through how he generalized it with a simple interface. The code can be found here.\n","tags":[],"title":"2025"},{"content":"December 31 Exploring network programming by building a Toxiproxy clone – Jordan Neufeld Great talk by Jordan Neufeld on building a toy proxy server in Go that adds latency between upstream and downstream connections. It sits between a client and server, introducing delays, dropping connections, and simulating errors for chaos testing. December 26 Reflecting on life – Armin Ronacher The best way to completely destroy your long term satisfaction is if the effort you are putting into something, is not reciprocated or the nature of the work feels meaningless. It’s an obvious privilege to recommend that one shall not work for exploitative employers but you owe yourself to get this right. With time you build trust in yourself, and the best way to put this trust to use, is to break out of exploitative relationships. December 24 How I write HTTP services in Go after 13 years – Mat Ryer What I love about the Go ecosystem is its stability. I read an earlier version of this post a few years ago, and the fundamentals haven’t changed much since then. I like writing HTTP servers with Go’s net/http library, and here Matt shows how he organizes his HTTP server codebase for better readability and testability. I’ve picked up a few neat patterns from this. His take on testing is spot on—rather than testing each handler individually, it’s better to mimic production conditions and test the entire workflow. December 21 Program your next server in Go – Sameer Ajmani While I don’t get to write much Go at work these days, I love using it to build my tools and have been doing so for the past few years. Go doesn’t need much evangelizing anymore, but there’s one talk I often point people to because it perfectly captures why writing servers in Go makes so much sense. Sameer starts with a toy search server and gradually makes it concurrent as the talk progresses. It showcases goroutines, buffered and unbuffered channels, and waiting on multiple channels with select, all in one coherent example. December 10 Taming flaky systems w/o DDoSing yourself in Python – Safe Retries with stamina I’ve been dabbling with Hynek’s stamina for a while. It’s a Python tool for retrying flaky service calls, built on top of the battle-tested tenacity library. It comes with a more ergonomic API, saner defaults, and some cool hooks for testing. One neat pattern I learned from reading the source code is using a context manager in a for-loop to retry a block of code. If you’re writing a library that handles retries, you can’t add exception handling with try...except inside a user-written for-loop. Using a context manager for this is a clever trick. It allows you to write code like this: for attempt in stamina.retry_context(on=httpx.HTTPError): with attempt: resp = httpx.get(f\"https://httpbin.org/status/404\") resp.raise_for_status() December 08 Transactions: myths, surprises and opportunities — Martin Kleppmann This is hands down one of the best talks I’ve seen on the topic. Martin points out that in ACID, consistency doesn’t carry the same rigid meaning as the other three constituents. It was kinda shoved in there to make the mnemonic work. He also highlighted that, while terms like read uncommitted, read committed, snapshot isolation, and serializable are widely used to describe different isolation levels, few can recall their exact meanings off the top of their heads. This is because the names reflect implementation details from 1970s databases rather than the actual concepts. Beyond clarifying isolation levels, the talk also explores how incredibly hard it is to achieve transactions across multiple services without centralized coordination. November 13 November ramble — Oz Nova I generally prefer not to comment on software development practices, because of something I’ve observed often enough that it feels like a law: for every excellent engineer who swears by a particular practice, there’s an even better one who swears by the opposite. Some people couldn’t imagine coding without unit tests, or code review, or continuous integration, or step-through debugging, or [your preferred “best practice”]. Yet, there are people out there who do the exact opposite and outperform us all. November 11 Brian Kernighan reflects on Unix: A History and a Memoir — Book Overflow So is it possible for, you know, two or three people like us to have a really good idea and do something that does transform our world? I suspect that that still can happen. It’s different, and certainly at the time I was in, you know, early days of Unix, the world was smaller and simpler in the computing world, and so it was probably easier, and there was more low-hanging fruit. But I suspect that there’s still opportunities like that. I think the reason Unix and all of the things that went with it worked so well was there was a big contribution from Doug’s ability to improve people’s lives so that what we did was described well as well as working well. So I think Doug in that sense would be the unsung person who didn’t get as much recognition as perhaps deserved. — Brian Kernighan November 10 Python’s finally gotchas Python core developer Irit Katriel recently shared a short piece discussing a few gotchas with Python’s finally statement. I don’t think I’ve ever seen continue, break, or return statements in a finally block, but if you do use them there, avoid it, as they can lead to some unusual behavior. The return statement in the finally block can suppress exceptions implicitly. For example: def foo() -\u003e int: try: 1 / 0 except Exception: raise finally: return 0 Running this function will suppress the exception and return 0. While this might seem surprising, it works this way because Python guarantees that the finally block will always run. This issue can be avoided by removing the finally block and dedenting the return. Similarly, continue and break behaves differently in that block. This behavior is documented in the official docs. However, maintainers are considering making this a warning and, eventually, illegal. November 9 Software engineering at Google I’ve been skimming through Software Engineering at Google over the past few days. It’s available online for free, which is a nice bonus. Rather than focusing on specific technologies or operational mechanisms, the book highlights the organization-wide engineering policies that have helped Google scale. The text is sparse and, at times, quite boring, but there are definitely some gems that kept me going. Here are three interesting terms I’ve picked up so far: Beyoncé Rule – Inspired by Beyoncé’s line, “If you liked it, then you should have put a ring on it.” If you think something’s important, write a test for it and make sure it’s part of the CI. Chesterton’s Fence – Don’t dismantle an established practice without understanding why it exists. Consider why certain legacy systems or rules are in place before changing or removing them. Haunted Graveyard – Parts of the codebase no one wants to touch—difficult to maintain or just feel “cursed.” They’re usually left alone because the cost to update them is high, and no one fully understands them. I’ve always wanted to put names on these things, and now I can! November 08 Books on engineering policies vs mechanisms The further I got in my career, the less value I gained from books on mechanisms and more from books on policies. But policy books are boring. My 17th book on writing better Python or Go was way more fun to read than Software Engineering at Google but yielded far less value—the age-old strategy vs. operations dichotomy. October 27 Understanding round robin DNS Round Robin DNS works by adding multiple IP addresses for the same domain in your DNS provider’s settings. For example, if you’re using a VPS from DigitalOcean or Hetzner, you’d add a bunch of A records for the same subdomain (like foo.yourdomain.com) and point each to a different server IP, like: 203.0.113.45 198.51.100.176 5.62.153.87 89.160.23.104 When a request comes in, the DNS resolver picks one of the IPs and sends the request to that server—basically a poor man’s load balancer. But there are some client-side quirks in how browsers pick the IPs, and this blog digs into that. Writes and write-nots — Paul Graham These two powerful opposing forces, the pervasive expectation of writing and the irreducible difficulty of doing it, create enormous pressure. This is why eminent professors often turn out to have resorted to plagiarism. The most striking thing to me about these cases is the pettiness of the thefts. The stuff they steal is usually the most mundane boilerplate — the sort of thing that anyone who was even halfway decent at writing could turn out with no effort at all. Which means they’re not even halfway decent at writing. October 14 OpenTelemetry client architecture At the highest architectural level, OpenTelemetry clients are organized into signals. Each signal provides a specialized form of observability. For example, tracing, metrics, and baggage are three separate signals. Signals share a common subsystem – context propagation – but they function independently from each other. Each signal provides a mechanism for software to describe itself. A codebase, such as web framework or a database client, takes a dependency on various signals in order to describe itself. OpenTelemetry instrumentation code can then be mixed into the other code within that codebase. This makes OpenTelemetry a cross-cutting concern - a piece of software which is mixed into many other pieces of software in order to provide value. Cross-cutting concerns, by their very nature, violate a core design principle – separation of concerns. As a result, OpenTelemetry client design requires extra care and attention to avoid creating issues for the codebases which depend upon these cross-cutting APIs. OpenTelemetry clients are designed to separate the portion of each signal which must be imported as cross-cutting concerns from the portions which can be managed independently. OpenTelemetry clients are also designed to be an extensible framework. To accomplish these goals, each signal consists of four types of packages: API, SDK, Semantic Conventions, and Contrib. October 05 Private DNS with MagicDNS — Tailscale blog Tailscale runs a DNS server built-in on every node, running at 100.100.100.100. Yes, Tailscale on your phone includes a DNS server. (We admit that “even on your phone!” is a little silly when phones are basically supercomputers these days.) The IP 100.100.100.100, usually pronounced “quad one hundred,” is part of the private Carrier-Grade NAT range. That means, just like IPs in the common private ranges, 192.168.1/24, 172.16/12, and 10/8, it is not routable on the public internet. So when software on your computer sends a traditional, unencrypted UDP packet to 100.100.100.100, no standard router will send it anyway. We then tell your OS that its DNS server is 100.100.100.100. Because operating system DNS clients are largely stuck in 1987, they then forward all their DNS queries over old-school insecure UDP DNS to 100.100.100.100. Tailscale also installs a route to 100.100.100.100/32 back into Tailscale and it then hands those packets over to Tailscale’s built-in DNS server, so unencrypted queries don’t leave your device. October 04 Git reset vs revert I misunderstood git revert and made a mess out of my main branch today. Thought it worked like git reset—but they’re not quite the same. Here’s the breakdown: git reset --soft moves the branch back to the specific commit but keeps your changes. It rewrites history, so you’ll need a force push to update the remote. git revert creates a new commit that undoes the changes from that commit without meddling with history. No force push needed. Seems like revert is what you need if you accidentally merge something into main. Keeps things clean without rewriting history. September 28 Rails World 2024 opening keynote — David Heinemeier Hansson I was really hyped about this year’s Rails World, even though I don’t code much in Ruby or Rails. I’ve been following 37signals’ work on simplifying deployment complexity and dogfooding their own tools to show how well they work. It’s also refreshing to see someone with more influence acknowledging that the JS ecosystem is unsustainably complex. Not everyone digs that, no matter how hip it might be. Personally, I usually have a higher tolerance for backend and infra complexity than for frontend. Kamal 2.0 now makes it easy to deploy multiple containers behind SSL on a single VM without dealing with the usual infrastructure idiosyncrasies. Then we have Kamal 2. This is how you’re going to get your application into the cloud, your own hardware, or any container, because we’re not tying ourselves to a PaaS. Kamal 2 levels this up substantially. It does Auto SSL through Let’s Encrypt, so you don’t even have to know anything about provisioning an SSL certificate. It allows multiple applications to run on a single server, scaling down as well as up. It comes with a simple declaration setup for detailing what your deployment looks like, encapsulated in the fewest possible pieces of information to get as close as possible to no config. The initial trigger for me to get interested in no build for Rails 7 was an infuriating annoyance: being unable to compile a JavaScript project I had carelessly left alone for about five minutes. None of the tools worked; everything was outdated. And when I tried to update it so I could compile it again, I literally couldn’t figure it out. I spent half a day wrestling with Webpacker at the time, and I did turn over the table, saying, ‘No, I made the integration for Webpacker to Rails, and I cannot figure out how this works. There’s something deeply, fundamentally broken in that model.’ And that’s when I realized the truth: only the browser is forever. September 25 The man who killed Google search — Edward Zitron Every single article I’ve read about Gomes’ tenure at Google spoke of a man deeply ingrained in the foundation of one of the most important technologies ever made, who had dedicated decades to maintaining a product with a — to quote Gomes himself — “guiding light of serving the user and using technology to do that.” And when finally given the keys to the kingdom — the ability to elevate Google Search even further — he was ratfucked by a series of rotten careerists trying to please Wall Street, led by Prabhakar Raghavan. September 23 Microservices are technical debt — Matt Ranney, Principal Engineer, Doordash Microservices are technical debt because while they initially allow teams to move faster by working independently, they eventually create a distributed monolith, where services become so intertwined that they require excessive maintenance and coordination, slowing down future development. The real driver for adopting microservices is not necessarily scaling traffic, but scaling teams—when too many developers are working on the same monolith, they step on each other’s toes during deployments, forcing the need for smaller, independently deployable services. Surely at this point the comment threads are going to explode with people saying that microservices should never share databases—like, can you believe that sacrilege of having two services share the same database? How do you live with yourself? September 22 How streaming LLM APIs work — Simon Willison While it’s pretty easy to build a simple HTTP streaming endpoint with any basic framework and some generator-like language construct, I’ve always been curious about how production-grade streaming LLM endpoints from OpenAI, Anthropic, or Google work. It seems like they’re using a similar pattern: All three of the APIs I investigated worked roughly the same: they return data with a content-type: text/event-stream header, which matches the server-sent events mechanism, then stream blocks separated by \\r\\n\\r\\n. Each block has a data: JSON line. Anthropic also include a event: line with an event type. Annoyingly these can’t be directly consumed using the browser EventSource API because that only works for GET requests, and these APIs all use POST. It seems like all of them use a somewhat compliant version of Server-Sent Events (SSE) to stream the responses. September 17 DHH talks Apple, Linux, and running servers — How About Tomorrow During yesterday evening’s walk, I had a lot of fun listening to DHH rant about the Apple ecosystem and the big cloud providers. I can totally get behind how so many people find deployment harder than it actually is, and how the big cloud providers are making bank off that. We were incredibly proud that we were going to take on Gmail with a fresh new system based on thinking from 2020, not 2004, and we thought that was going to be the big boss, right? We’re going to take on Google with an actually quite good email system. But we didn’t even get to begin that fight because before a bigger boss showed up and just like Apple sat down on our chest and said, ‘Give me your—you’re going to give me your lunch money and 30% of everything you own in perpetuity going forward.’ We used to be in the cloud. We used to be on AWS. We used to be on all this stuff for a bunch of our things with Basecamp and Hey, and we yanked all of it out because cost was just getting ridiculous, and we built a bit of tooling, and now I’m on a goddamn mission to make open source as capable, as easy to use as all these AWS resellers against any box running basic Linux with an IP address you can connect to. September 16 The many meanings of event-driven architecture — Martin Fowler In this 2017 talk, Martin Fowler untangled a few concepts for me that often get lumped together under the event-driven umbrella. He breaks event-driven systems into four main types: Event notification: A system sends out a signal (event) when something happens but with minimal details. Other systems receive the notification and must request more information if needed. This keeps things simple and decoupled but makes tracking harder since the event doesn’t include full data. Event-carried state transfer: Events carry all the necessary data upfront, so no extra requests are needed. This simplifies interactions but can make events bulky and harder to manage as the system scales. Event sourcing: Instead of storing just the current state, the system logs every event that occurs. This allows you to reconstruct the state at any time. It’s great for auditing and troubleshooting but adds complexity as log data grows. CQRS: Commands (write operations) and queries (read operations) are handled separately, letting each be optimized on its own. It works well for complex domains but introduces more architectural overhead and needs careful planning. Interestingly, I’ve been using the second one without knowing what it was called. September 15 Founder Mode, hackers, and being bored by tech — Ian Betteridge On a micro scale, I think, there’s still a lot to be excited about. But on the macro level, this VC-Founder monoculture has been stealing the thunder from what really matters—the great technology that should have been a testament to the hive mind’s ingenuity. Instead, all the attention is on the process itself. Tech has become all Jobs and no Woz. As Dave Karpf rightly identifies, the hacker has vanished from the scene, to be replaced by an endless array of know-nothing hero founders whose main superpower is the ability to bully subordinates (and half of Twitter) into believing they are always right. September 14 Simon Willison on the Software Misadventures podcast I spent a delightful 2 hours this morning listening to Simon Willison talk about his creative process and how LLMs have evolved his approach. He shared insights into how he’s become more efficient with time, writing consistently on his blog, inspired by things like Duolingo’s streak and Tom Scott’s weekly video run for a decade. Another thing I found fascinating is how he uses GitHub Issues to record every little detail of a project he’s working on. This helps him manage so many projects at once without burning out. Simon even pulled together a summary from the podcast transcript that captured some of the best bits of the discussion. About 5 years ago, one of Simon’s tweets inspired me to start publishing my thoughts and learnings, no matter how trivial they may seem. My career has benefited immensely from that. The process of reifying your ideas and learning on paper seems daunting at first, but it gets easier over time. September 09 Canonical log lines — Stripe Engineering Blog I’ve been practicing this for a while but didn’t know what to call it. Canonical log lines are arbitrarily wide structured log messages that get fired off at the end of a unit of work. In a web app, you could emit a special log line tagged with different IDs and attributes at the end of every request. The benefit is that when debugging, these are the logs you’ll check first. Sifting through fewer messages and correlating them with other logs makes investigations much more effective, and the structured nature of these logs allows for easier filtering and automated analysis. Out of all the tools and techniques we deploy to help get insight into production, canonical log lines in particular have proven to be so useful for added operational visibility and incident response that we’ve put them in almost every service we run—not only are they used in our main API, but there’s one emitted every time a webhook is sent, a credit card is tokenized by our PCI vault, or a page is loaded in the Stripe Dashboard. September 07 Recognizing the Gell-Mann Amnesia effect in my use of LLM tools It took time for me to recognize the Gell-Mann Amnesia effect shaping how I use LLM tools in my work. When dealing with unfamiliar tech, I’m quick to accept suggestions verbatim, but in a domain I know, the patches rarely impress and often get torn to shreds. September 04 On the importance of ablation studies in deep learning research — François Chollet This is true for almost any engineering effort. It’s always a good idea to ask if the design can be simplified without losing usability. Now I know there’s a name for this practice: ablation study. The goal of research shouldn’t be merely to publish, but to generate reliable knowledge. Crucially, understanding causality in your system is the most straightforward way to generate reliable knowledge. And there’s a very low-effort way to look into causality: ablation studies. Ablation studies consist of systematically trying to remove parts of a system—making it simpler—to identify where its performance actually comes from. If you find that X + Y + Z gives you good results, also try X, Y, Z, X + Y, X + Z, and Y + Z, and see what happens. If you become a deep learning researcher, cut through the noise in the research process: do ablation studies for your models. Always ask, “Could there be a simpler explanation? Is this added complexity really necessary? Why? September 01 Why A.I. Isn’t Going to Make Art — Ted Chiang, The New Yorker I indiscriminately devour almost everything Ted Chiang puts out, and this piece is no exception. It’s one of the most articulate arguments I’ve read on the sentimental value of human-generated artifacts, even when AI can make perfect knockoffs. I’m pro-LLMs and use them to aid my work all the time. While they’re incredibly useful for a certain genre of tasks, buying into the Silicon Valley idea that these are soon going to replace every type of human-generated content is incredibly naive and redolent of the hubris within the tech bubble. Art is notoriously hard to define, and so are the differences between good art and bad art. But let me offer a generalization: art is something that results from making a lot of choices. This might be easiest to explain if we use fiction writing as an example. When you are writing fiction, you are—consciously or unconsciously—making a choice about almost every word you type; to oversimplify, we can imagine that a ten-thousand-word short story requires something on the order of ten thousand choices. When you give a generative-A.I. program a prompt, you are making very few choices; if you supply a hundred-word prompt, you have made on the order of a hundred choices. Generative A.I. appeals to people who think they can express themselves in a medium without actually working in that medium. But the creators of traditional novels, paintings, and films are drawn to those art forms because they see the unique expressive potential that each medium affords. It is their eagerness to take full advantage of those potentialities that makes their work satisfying, whether as entertainment or as art. Any writing that deserves your attention as a reader is the result of effort expended by the person who wrote it. Effort during the writing process doesn’t guarantee the end product is worth reading, but worthwhile work cannot be made without it. Some individuals have defended large language models by saying that most of what human beings say or write isn’t particularly original. That is true, but it’s also irrelevant. When someone says “I’m sorry” to you, it doesn’t matter that other people have said sorry in the past; it doesn’t matter that “I’m sorry” is a string of text that is statistically unremarkable. If someone is being sincere, their apology is valuable and meaningful, even though apologies have previously been uttered. Likewise, when you tell someone that you’re happy to see them, you are saying something meaningful, even if it lacks novelty. August 31 How to Be a Better Reader — Tina Jordan, The NY Times To read more deeply, to do the kind of reading that stimulates your imagination, the single most important thing to do is take your time. You can’t read deeply if you’re skimming. As the writer Zadie Smith has said, “When you practice reading, and you work at a text, it can only give you what you put into it.” At a time when most of us read in superficial, bite-size chunks that prize quickness — texts, tweets, emails — it can be difficult to retrain your brain to read at an unhurried pace, but it is essential. In “Slow Reading in a Hurried Age,” David Mikics writes that “slow reading changes your mind the way exercise changes your body: A whole new world will open up, you will feel and act differently, because books will be more open and alive to you.” August 26 Dark Matter — Blake Crouch I just finished the book. It’s an emotional rollercoaster of a story, stemming from a MacGuffin that enables quantum superposition in the macro world, bringing the Copenhagen interpretation of quantum mechanics to life. While the book starts off with a bang, it becomes a bit more predictable as the story progresses. I still enjoyed how well the author reified the probable dilemma that having access to the multiverse might pose. Highly recommened. I’m already beyond excited to read his next book, Recursion. ","permalink":"http://rednafi.com/feed/2024/","publishDate":"1970-01-01","summary":"December 31 Exploring network programming by building a Toxiproxy clone – Jordan Neufeld Great talk by Jordan Neufeld on building a toy proxy server in Go that adds latency between upstream and downstream connections. It sits between a client and server, introducing delays, dropping connections, and simulating errors for chaos testing.\nDecember 26 Reflecting on life – Armin Ronacher The best way to completely destroy your long term satisfaction is if the effort you are putting into something, is not reciprocated or the nature of the work feels meaningless. It’s an obvious privilege to recommend that one shall not work for exploitative employers but you owe yourself to get this right. With time you build trust in yourself, and the best way to put this trust to use, is to break out of exploitative relationships.\n","tags":[],"title":"2024"},{"content":" Gdynia, Poland Ahoy, fellow daywalkers! I’m Redowan Delowar, also go by ‘rednafi’ on most platforms.\nCirca 2018, a glitch in the matrix slingshotted me from electrical engineering to data science, eventually landing me in brick-and-mortar software work.\nI enjoy exploring system architecture, databases, data analysis, and API design. In my spare time, I moonlight as a reader, writer, hiker, and a pathological minimalist.\nWork I’m currently based in Berlin and working at Wolt as a software engineer. Before this, I was exploring healthtech in the US and fintech in Southeast Asia.\nContact Email GitHub LinkedIn Bluesky Colophon Hugo powers this blog. It’s rocking the Papermod theme with some handrolled CSS flair. I write in plain markdown and push the content to GitHub, triggering the GitHub Actions CI, which then deploys the site to GitHub Pages.\nLearn more about the stack, deployment, and writing process here.\nGreatest hits Oh my poor business logic The diminishing half-life of knowledge Annotating args and kwargs in Python An ode to the neo-grotesque web HTTP requests via /dev/tcp You probably don’t need a DI framework Dotfiles stewardship for the indolent Configuring options in Go Recipes from Python SQLite docs Limit goroutines with buffered channels Pesky little scripts I kind of like rebasing ","permalink":"http://rednafi.com/about/","publishDate":"0001-01-01","summary":" Gdynia, Poland Ahoy, fellow daywalkers! I’m Redowan Delowar, also go by ‘rednafi’ on most platforms.\nCirca 2018, a glitch in the matrix slingshotted me from electrical engineering to data science, eventually landing me in brick-and-mortar software work.\nI enjoy exploring system architecture, databases, data analysis, and API design. In my spare time, I moonlight as a reader, writer, hiker, and a pathological minimalist.\n","tags":[],"title":"About"},{"content":"Here’s a non-exhaustive list of blogs I enjoy reading.\nAnton Zhiyanov Brandon Rhodes Brandur Charity Majors Dan Luu Drew DeVault’s Blog Fabien Sanglard’s Website Harmful Stuff Hynek Schlawack Joel on Software John Gruber Julia Evans Preslav Rachev Simon Willison’s Weblog ","permalink":"http://rednafi.com/blogroll/","publishDate":"0001-01-01","summary":"Here’s a non-exhaustive list of blogs I enjoy reading.\nAnton Zhiyanov Brandon Rhodes Brandur Charity Majors Dan Luu Drew DeVault’s Blog Fabien Sanglard’s Website Harmful Stuff Hynek Schlawack Joel on Software John Gruber Julia Evans Preslav Rachev Simon Willison’s Weblog ","tags":[],"title":"Blogroll"},{"content":"","permalink":"http://rednafi.com/misc/sidecar_communication/","publishDate":"0001-01-01","summary":"","tags":["Docker","Networking"],"title":"Docker sidecar communication with Unix Domain Socket (UDS)"}]